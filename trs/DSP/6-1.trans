那我想這部分我們說到這裡
那這個是我們五點零就說到這裡
那等於是我們把這個h m m 的部分
講到這裡為止我們現在要進入六點零
六點零要講的是language model
那language model 最基本的原理就是這個bigram trigram 這些個n gram
這些我們都已經說過了
那這裡面其實還有很多進一步的學問要講的
那我們現在來說這些language model 
那麼那這部分的話大概這幾本書都一樣的提到大概類似的東西
所以呢我這邊寫一個or
就是說你其實大概只要選其中一個來看就夠了
你如果看這本的話你如果看這本的話
大概是這些
看這本的話大概是這些
看這本大概是這些
那這裡面應該說是講得最淺最容易看的
可能是第二個
那個講得比較完整都說到的可能是第一個
那比起來三呢就是稍微難看一點就是它講的這個中間跳的東西比較多一點就是了
大概是這樣
那我想你只要所謂or 就是你只要選裡面的一個來看就好了
那在開始講之前我們再回到我們前兩天補課的時候所說的information theory 裡面的這個entropy 的東西
因為我們底下還是要用這個來解讀這裡面講的事情
我們舉一個非常簡單的例子就是
這個這個entropy 呢是我們說它的上限是log n 下限是零
那它所描述的是這個一個變化的這個uncertainty
或者說是一個distribution 的分散的程度
或者是它的純度等等的
意思我們上次都已經說過
那麼我們是在講一個這樣的東西
如果我有一個一個information source 
出來一堆m one m two m 三一直到m j 等等
那麼m j 是指第j 個
那每一個是什麼每一個是一個random variable
它都可以是譬如說x one x two 到x 大m 對不對
這就是那每一個都有個機率
這個就是p 的x one p 的x two 等等
那麼因此呢我們可以畫一個distribution 說
它們怎樣
這個是x one 的機率x two 的機率
一直到x m 的機率
這是我們上次補課的時候所說的我們基本上在講這麼一件事情
如果說它的每一個時間t 的時候
j 的時候送出一個m j 的一個symbol
那它們都可以是這大m 的裡面的一個
那每一個都有一個機率它都有一個distribution 
那這些東西就是我們講的p of xi 的這些東西
那然後呢那我們就說呢我們現在可以根據這個
來算它的entropy
這個entropy 就是h of s
就是這個東西
那麼它的上限是log m 下限是零
什麼時候上限是log m 呢
就是如果這些機率都一樣
變成一個完全相同的uniform 
的從x one 到x m 
是完全平的時候
那這個時候它的它的這個entropy 最大
就是所謂的它的值就是log m m 就是總數
那什麼時候最小呢
最小的時候是只有一個對不對
那個x j 的機率是一
其它全部都是零
這個時候呢就變成零
所以它的這個它的這個這個entropy 呢是介於這個之間
這是我們上次所說的事情
好那我們現在以這個以這個狀況
我們以這個狀況來現在看假設是我現在的這個是一個language source
這個跑出language 出來
舉例來講假設你在用你的手機或什麼在接收某一個massage 
假設這個massage 是這樣進來
那麼如果說假設這個massage 一個字母一個字母進來
一次跳出一個字母一次跳出一個字母來
所以進來一個t 一個h 然後一個i 一個s 哦我知道這個字是this
然後等等那因為跳進來的話
你可以想像成這裡每一個字母就是一個m j 
就是一個字母
那這樣來看我就把每一個字母看成是那這裡講的random variable
這裡的每一個是一個random variable 嘛
那我就把這裡面的每一個字母看成是random variable 的話
那你想我這樣的話總共有多少個
我的entropy 有多大
那你可以想這是那就是那這就每一個字母就這些x m 
那我這個m 呢應該是二十六乘以二還要再多
為什麼二十六個字母
所以我只有二十六種
乘以二是為有大寫有小寫嘛所以乘以二
然後還有一些標點符號啊還有空白也算是一個symbol 啦等等
所以大概是比這個還要多一些
不過應該小於六十四
六十四是二的六次方
所以你如果取log m 的話呢
應該是大約是六個bits
比六個bits 還要少
所以呢你可以說如果它是一個字母一個字母跳出來的
每一個字母看成是一個random variable 的話
那那個字母帶給我的information 是多少
大概是比六個bits 少一點
這個是這句話的意思
所以你一個字母大概給我這個一個字母給我大概六個bits 少一點的information
那這個其實是可以仔細算的我們這個只是很粗的這樣子講
我用這個上限等於log m 的這個來看
那上限這個是在講上限嘛
就是log m 就是六個bits
那事實上其實你是可以算的
那你知道我們的英文字母其實每一個字母它的機率都可以算的出來
那麼有的字母是機率比較高的有的字母是機率比較低的
我把這個二十六個字母如果算成a b c d 這樣算到z 的話
你知道什麼字母是機率很高的譬如說t 
是機率很高的
那麼e 也是機率很高的
什麼是機率很低的
z 機率很低的
那麼q 機率很低的等等
你其實每個字母的機率都算得出來的
所以你可以真的可以照我們那個公式就是p log p 
這個東西不是p log p 嗎我們上次講過
就是p xi log p xi 
然後summation over i 嘛
就是這個東西嘛你可以算嘛
你其實可以算精確的算出來每一這樣子的話一個字母倒底給我多少information 
這可以算得出來的
那我們這邊沒有去算它我只是舉個例子算它的上限
這個上限是log m 
那麼這個log m 大約是六個bits of information per character 
所以呢你如果說假設說我的手機可以一個一個字母跳出來看到收到我的message 的話
每一個字母給我多少information 
大概是六個bit 
那這個應該很接近我們的直覺
因為你知道我們本來英文字母一個字母就是用這個ascii representation 就是六個bit 嘛
所以六個bits 大概給我一個字母是沒有錯的
那如果它不是一個字母一個字母跳出來是一個字一個字跳出來的話
就第一次跳出來就是一個this
後面跳出來是course 
一個一個字跳出來的話
我就把每一個字看做一個random variable 可不可以
也可以
我如果把一整個字看做一個random variable 的話
那就是那我這就變成一個一個word 了
一個一個word 的話我總共有多少個word 呢
那英文的word 數目很多了
我們假設是三萬個的話
三萬個大概是二的十五次方這個寫錯了應該是二
是二的十五次方
那你如果算成二的十五次方就表示說呢
我那也是一樣其實英文的每一個字
每一個word 都有它的機率
譬如說這個字的出現機率都可以算的嘛
就是unigram 
所以你可以算它的機率
你機率算出來你可以算那個p log p
你可以算這個entropy 算得出來的
那我現在也是只是簡單地用log m 這個上限來說的話
假設它是三萬個它的上限就是二的十五次方
那就是十五個bit 
所以你可以說喂一個如果這樣一個字一個字跳出來的話呢
這個字給我大概十五個bit information
你大概可以這樣子看
那這個是我們用information theory 裡面的entropy 的觀點
來解讀這個這個language source 譬如說英文
假設我現在這個出現的是中文的話
那還有不同的情形啊
假設你用聽的
我是一個一個音聽到
我聽到這個音聽到這個音我一個一個音聽到的話
我聽到一個音我得到多少information 呢
那我們剛才說我們音的總數大概一千三百個
這大概是二的十一次方
因此呢你聽到一個音大概是聽到十一個bit 
或者更少一點
我還是一樣我用log m 來算
假設它的上限是log m
那麼我就是用這個log m 來算
假設這樣的distribution 的話
那麼那麼譬如說你在聽你的你聽你的手機的電話
我每聽到一個音
那那個音等於是這裡面的一個random variable
那它有一千三百種嘛
所以呢我所得到的information 應該是log m 
為上限的一個information 的量
所以大約是十一個bit 或者更少
那也是一樣其實每個音有它的distribution 
有些音機率比較高有些音機率比較低的
我們隨便舉例你也知道
機率比較高的是什麼譬如說ㄕ啊
因為施時使是都有很多常用詞
常用字所以ㄕ是機率很高的
那有的機率很低的等等
那那你也可以真的去算把它們機率高低通通都算出來
你也可以得到真正精確的每一個音給我多少information
但是我們不算我們只算上限的話
你聽到一個音大概是這樣
如果如果我聽不出它的它的聲調了
我剛才是including 的tone 是一千三百個
我如果聽不出聲調的話會怎樣
我們假設一個外國人他聽不出聲調來
對他而言不管是第幾聲聽起來都一樣
那這個時候他只聽到四百多個聲音
那那個時候大概是九個bits 
那這個合理嘛因為你少了四聲
四個聲調大概是兩個bit 嘛對不對
所以你如果聽不出聲調來的人
他聽到一個字大概聽到九個bit 的information
那當然你也可以是一個字一個字跳出來當成字來看
如果一個一個字來看的話呢
那depend on 我們算是多少字
我也可以算這樣的事情那假設我們常用字的數目是八千的話
那八千是這樣子二的十三次方
所以呢你每看到一個字
每跳出一個字來看到一個字
大概是十三個bit information
那這樣只是一個general idea 你比較有數我們怎麼在算這些東西
那這底下這個只是隨便意思意思我們舉個例子了解一下這個玩一玩而已
譬如說這女孩相對於girl 這可能是一個最match 的例子
為什麼呢
這裡有兩個字
如果一個字是十三個bit 的話
那兩個字大概是二十六個bit information
那這四個英文字母呢一個英文字母如果是六個bit 的話呢
四個大概是二十四個bit
這個二十四個bit 這個二十六個bit 很接近嘛
所以也就是說你如果從英文來看從中文來看
in general 它們給我們的information 的量是接近的
就因為我們這些language 所靠的東西是大概類似的concept 所以是差不多的
那當然這裡只是一個特別的例子那剛好看起來很接近
有些是不接近的啦
這裡也是兩個字這邊有這麼多字母嘛
三個字有這麼多字母嘛所以這個就不見得接近這只是一個例子而已
