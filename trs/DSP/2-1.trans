我們今天是二點零喀
那麼二點零要講的事情其實是
呃我們一點零裡面講過一張圖阿
我已經他們有個 mouse 所以我剛才已經
如果記得的話我們上週有一張圖在講這個大字彙的語音辨識
那張圖呃裡面有好多塊東西
那我們其實今天是把那張圖裡面的幾塊重要東西簡單的說一下他的基本的原理
那呃第一個要講的是 hidden  markov  model
那麼之後我們底下要講的是
這個怎麼樣前面求 feature
然後我們會講後面的 language  model 等等
那麼我們主要講這三塊
那麼這三塊是我們在一點零上週有過那麼一張圖裡面的三個重要的部份
那這三塊我們今天講的也只是個非常簡單的簡介
這個讓各位有一點初步的 feeling 那個是在幹什麼的
那個那麼等到後面我們還有陸續好幾周都要繼續在講這三塊
比如說我們後面的四點零五點零還是在講這個 hidden  markov  model
然後六點零會講 language  model
七點零會講那個 feature
那都是在這裡
然後八點零在講怎麼作 search
所以我們一直到八點零講完都還在講這堆東西
那今天這裡只是一些最基本的觀念
我們把一些基本東西先說一次這樣你比較容易有概念他們是什麼
那第一個我們先說得是 hidden  markov  model
那這個呢應該可以算是今天所有的作語音的研究裡最主要的最常用的 model
也是最成功的 model
雖然他不是唯一的
那也有人用其他的
不過這個到目前為止仍然是最成功的用的最普遍的
所以呢我們來說一下
那麼 hidden  markov  model 是什麼呢
你基本上可以想像成是一個我們把每一個想要辨識的聲音都建一個 model
舉例來講呢假設我們用最簡單的例子來講
我現在要辨識零到九的十個聲音
那我就為每一個聲音建一個 model
這個 model 就長得像這個樣子
那麼於是我就會零有一個 model 一有一個 model 二有一個 model 一直到到九
總共呢就有十個 model
那麼每一個 model 是怎麼樣的呢
有這麼多個 state  n 個 state
那麼舉例來說假設零
這是一個為零所建的 model 的話
那他的第一個 state 第二個 state 分別代表我發那個零的時候的聲音訊號變化的狀態
舉例來講呢
那這第一個 state
很可能是我在發這個零的時候那個了
那個了我這個還沒有發出來的那個時候那個了那個時候那個就是可能是第一個 model
第一個 state 所描述的
第二個 state 可能是了發出來了
不過那個了可能後面帶著了一
那個了一的時候可能是第二個 state 等等
你到後面第三第四個 state 可能是一
零裡面那個了一一的音
然後等等等等到最後一個音可能是
最後一個 state 可能是零的那個登登的音等等
那這樣的話那我這個聲音一路慢慢跳過去呢
我就描述了這個零這個聲音的情形
基本上是有時間順序的
從一二三這樣一路下去的話
我描述那一個聲音的情形
那除了這樣子之外呢
那麼我們也可以通常把語音的訊號看成一系列的 vector
那這一點呢我們在上週可能有畫過
我有一點忘了
不過 anyway 我們重來一次
假設說我的聲音是假設說我的零是這樣子的
那麼如果是這樣的話呢
我的取第一個 frame 譬如說是裡面的兩百五十六點
或者是五百一十二點
那你知道我裡面每一個都是一堆 sample嘛齁
都是一堆 sample嘛
那等等假設說我的第一個 frame 是兩百五十六點
我把他呢經過某一個演算法之後
把他變成一系列的參數
這堆參數就是我們所謂的這個 feature  parameters
那麼他就構成一個 vector 我們叫做 o  one
那就是這邊的這個 o  one
就是我的第一個 feature  vector
那麼這種東西呢我們稱之為 feature  vector
因為這裡面的每一個呢就是我們所稱的 feature  parameter
也就是說他是描述這堆聲音裡面的一些特徵
那麼待會我把這個 window 向右 shift 過來之後
我可以產生我可以算出第二個來
他也是這堆那麼這個呢我們叫做 o  two
待會兒我再 shift 一個過來我就算出一個來
那這個呢就是 o 三等等等等
那麼換句話說我們上週提過
這個訊號這裡面千變萬化
你每一個聲音永遠不一樣
但是呢這個通常你在訊號裡看不出什麼東西出來
但是你如果把他求想辦法從裡面求出一些特別的特徵參數來描述這個聲音就會比較清楚
這就是我們所謂 feature  parameter 跟 feature  vector
那我們把這些 parameter 兜成一個 vector 就是所謂的 feature  vector
那麼於是呢現在我可以把一系列的訊號變更成一系列的 vector
那這些 factor 我稱為 o  one  o  two 等等等等
我叫做 observation  sequence
因為這是真正我可以 observe 的我可以看得到的我收的到的我量的到的這是我的 observation  sequence
那麼每一個呢我叫的 o  one  o  two 呢我就通稱為 o  t
這個小 t 呢就等於我的 time  index
不過這個 t 是整數
這個 t 是整數就是一二三四這樣就是第 t 個 factor 的意思
那麼那麼每一個 t 的 ot 裡面呢
就是這些東西呢我總共有幾個參數呢
X 一 X 二到 X 大 D
這個大 D 是我們這個 feature 的 dimension
或者說就是我的參數的總數
那我們後面就會說我們這個大 D 其實滿大的
我們常用的大 D 大約是三十九的 order喔
不是一定要三十九啦大概是這樣的數字
三十多個到五十多個差不多這樣的數字才夠
你大概會有那麼多個
那麼所以這個大 D 是一個滿大的數字
這樣這些 x 就是我們所謂的 feature  parameter
那這樣的 o  t 這些 o 呢就是我的 feature  vector
就構成我們所謂的 observation  sequence
那如果你是這樣看的話呢
那麼現在我假設這個是零好了
這是零的 model
這是零的聲音
那如果是這樣的話呢
那我事實上是會有
舉例來講也許前面這前面這三個是屬於第一個 state
我們剛剛說的第一個 state 可能是在描述那個了那個還沒有出來的聲音
如果是那樣的話也許是前面三個 state 前面三個的 vector
到第四五六七的時候很可能是代表第二個 state
這個時候是了一出來了
那麼如果是這樣來看的話你可以把這個 observation  sequence拆成 一段一段一段
那這一段我們可以想像他好比是描述這個 state 這一段好比是這個 state 這一段好比是這個 state 等等
那為了要區別這件事我們就 define 第二個 sequence 叫做 state  sequence
這些 q  one  q  two 就是分別對應到 o  one  o  two
那他是什麼呢他就是一到 n 的 state  number
那換句話說如果我們說一二三這三個是屬於 state  one 的話
表示說我就讓 q  one  q  two  q 三都等於一
q  one  q  two  q 三都等於一的話表示說他是第一個 state
如果 q 四這個四到七是在第二個 state 的話呢
我就是 q 四 q 五 q 六 q 七等於二就表示他們是在第二個 state 等等
因此呢我這個 q 的這個 sequence 其實只是一一一二二二二三三等等等等
因為他們都是一到 n 的整數
分別 indicate 我的第幾個 observation 是掉在第幾個 state 裡面的意思
所以呢這是我的 q 的這個 state  sequence
那這個時候有一個重要的東西在這裡
就是我們有這個 state 會跳的這個 a 一二 a 二二這些東西
那這些東西呢我們稱之為 state  transition  PROBABILITY
那這裡的 a  i  j 的意思是說我在 t 減一的時候是在 state  i 那
t 的時候是在 j 那裡的跳的機率
ok 換句話說呢就是我在前一個瞬間
這個 t 就是這個 t 嘛就是這個 t 嘛就是說我的前一瞬間
前一個 t 還在 state  i 但是下一個 t 會到 state  j 的機率叫做 a  i  j
所以呢 a 一一就是跳回原來的 a 一二就是從一跳到二的 a 一三就是從一跳到三的等等
這是 a 二二 a 二三 a 二四等等
喀喀那麼這個究竟是什麼意思我們舉個簡單的例子來看你就可以想像他是在說什麼
譬如說呢現在如果這個是這個 state 這是下一個 state
我們假設說這裡只有兩個
如果這個是零點一而這個是零點九的話
那表示什麼意思呢絕大多數的狀況他仍然會下一個仍然是原來的
只有零點百分之十的機會他下一個是跳到下一個去了
對不對所以呢你可以想像假設這是一這是二
q  one 呢可能是等於一 q  two 可能還是等於一 q 三可能還是一
因為我一開始是這個的話呢有百分之九十的機會下一個仍然會是他
對不對然後呢還是有百分之九十的機會下來還是他
還是有百分之九十的機會下來還是他所以呢他可能一直會這樣在這邊好多個
你可以想像可能到了八九個以後他可能會跳到下一個來
因為我十分之一的機會會到下一個去嘛
所以大概八九個才會到這邊來對不對
因此呢這個時候你可以想像我的 state  one 可能有好多個
要夠多個之後他才會跳過來
那反過來呢我現在如果這個是零點一這個是零點九的話
表示什麼意思呢
表示 q  one 是一的時候 q  q  two 很可能就跳到二了對不對因為我有百分之九十的機會會過來嗎
所以 q  two 很可能就匯二了是不是
那麼我會繼續留在這裡的機會很小呀只有十分之一的機會呀
所以他就不斷的往這邊一進來就會跳出去嘛對不對
那麼因此呢這些個機率就告訴我說
這個訊號會停留在這個 state 裡面長短
以我們剛才的例子如果是這個是零點一這個是零點九的話
在這邊就會比較長要夠長之後他才會有機率跳過來嘛對不對
那麼如果現在現在是零點九這個是零點一的話就表示這裡面很短一下子就可以跳出來嘛 ok
那麼因此呢我們事實上是在描述一個現象
因為在在我們的聲音裡面我們永遠不知道我們隨便講一個講一個字的時候我們永遠不知道我們到底會在哪一個 state 到底會多長
這是一個 random 的情形
我們舉個簡單的例子譬如說零有的人可以說是零有的人說是這個零
那麼你的每一個這裡面每一個 state 到底有多長是不一定的是可長可短的
我們舉更複雜的例子譬如說一個英文字 San  FRANCISCO
San  FRANCISCO 這裡有好多好多的音
你是 San  FRANCISCO 還是 San  FRANCISCO 還是怎樣你的每一個音是可長可短的
換句話說在我們講話的時候的任何一個你講的任何一個
如果用一個 model 來描述他的話到底哪一段到底哪一個要多長齁
我們剛才講譬如說這是屬於第一個 state 這屬於第二個 state 這屬於第三個第四個第五個第六個 state 的時候
到底每一個 state 的時間是多長多短實在是很難說的他是非常 random 的
同一個人講兩次的話他的長短都不一樣的
所以我們只能用一個這樣的 model 來描述他的一個時間上的長短伸縮的 random 的特性
那麼我們剛才講過這個這個意思就是
這個大小的話就可以看得出來他在這裡會長還是會短
那同理呢我有的時候可以跳像這個地方我可以從 a  one 會跳到 s
從 state  one 會直接跳到三的意思是有些音我其實可能會根本就沒有念就滑過去了
那麼一個簡單例子譬如說零
你念零的時後裡面到底有沒有那個一的音不一定有的時候那個一根本沒有發他就是跳掉了啊
那麼那麼因此呢你有的時候是可以跳的
所以呢那這些呢就是我們這一堆 a  i  j 這些個 state  transition  PROBABILITY 的角色的功能
那 in  general 這應該是 define  for 所有的 i 跟 j
因為任何一個 state 可以跳到另一個 state 去
所以呢他是一個 a  i  j 的 matrix
那這個 matrix  in  general 應該是 n  by  n 的
我的總共有 n  by  n 個 element
但是在大多數狀況我們都把他簡化到只有這樣子就是他跳回自己跟下一個跟下兩個
再多我們就不弄了因為太複雜了
那你可以想像如果是只有這三個的話我的這個 matrix 會簡化到只有每一行只有三個嘛
a 一一 a 一二 a 一三只有這三個別的都是零
然後呢這是 a 二一 a 二二
呃沒有二一應該是呃 a 二二 a 二三 a 二四然後呢是零等等
所以呢我基本上只有這一排讓他不是零其他的呢都是零
那如果是這樣的話我們的 model 稍微簡化一點
我們通常會作這樣的假設以免太過複雜
那事實上你也可以更 general 的情形應該是讓他每一個 a  i  j 都可以存在
如果每一個 a  i  j 都可以存在的話他甚至可以從後面跳到前面來
對不對你四可以跳到二來那這樣的話太複雜了我們通常簡化到只讓他只讓他那樣就好了
好那這是講這些 state 的情形
那再來呢你如果是知道這些聲音 o 一 o 二 o 三是在這個 state 一裡面呢
他會長怎樣還是不一定的
即使這個是發了的那個音的話
我都知道是那個音了並不表示他的 o 會長的怎樣他會長任何一個樣子
這個是語音特別難的地方就是即使是同樣的聲音同樣的人發兩次訊號都是絕對不一樣的
我們舉例來講同樣的一個人一個人發啊你把你發啊發三次
你用你用這個這個電腦收進去把 wave  form 畫出來看看那三個啊鐵定不一樣
絕對不會三個一樣的阿
你就是同樣的人發同樣的音三次鐵定是不一樣的阿
那麼同音詞你算出來的這個東西鐵定是不一樣的
所以即使你知道他們都是這個 state 並不表示他們會長怎樣他會長任何一個樣子
所以怎麼辦我們只能用一個機率的 distribution 來描述他這就是 b  one 的 o 的意思
就是說你如果是在 state  one 的話這個 o 會長怎樣呢我們不知道
我們只知道他會有一個某一種 distribution 這是 b  one
你如果說這些東西是在 state  two 裡面的話呢他會長怎樣我們不知道
我們只知道他會有一種 distribution 那這個音跟這個音有何不同呢
這兩個 distribution 不一樣就是了
但是我並不能夠知道說這個 o 一定是怎樣跟這個 o 一定怎樣這個我們很難講
只是我只能說他們是這個 distribution 他們是這個 distribution 而這兩個不一樣
ok 那這些 b  one  b  two  b 三就是當我這些 o 在 state  one  two  three 的時候他們的 distribution
那我們叫做 b  j 的 o 那我們叫做 observation 的 probability
那也就是這個 b  j 的 o 你看這就知道了
這個 j 就是這個 state 一到 n 嘛這個 j 就是這個 state 的 index
然後他有一個 distribution
那麼這個會是怎樣呢
你可以想像他的複雜性就是他
配這任何一個 distribution 去找任何一個樣子
那麼我們簡直無法去描述了
那我們的辦法就是說我們無法描述我只知道他是這個 distribution 我就說他是 gaussian
我就說他是 gaussian
那你說怎麼可能是 gaussian 呢因為我可以是任何長相呀
沒有錯你可以是任何長相因為他可以是任何長相
我們不見得能夠說他是一個 gaussian
這是一個 gaussian 對不對
如果是一個 gaussian 他的長相就是這個樣子啦那不見得是這樣啦
那怎麼辦呢我們就說他是一把 gaussian 齁
所以呢我們這邊是是一堆 gaussian 的組合
換句話說呢我如果是亂七八糟一堆東西的話呢我永遠可以說他是一堆 gaussian
譬如說這是一個 gaussian 這是一個 gaussian 這是一個 gaussian 這是一個 gaussian
我永遠可以說他是一堆 gaussian 的組合
當我的 gaussian 夠多的時候他大概可以描述他的 distribution
ok 因此呢我們從這個觀念來講不論他長得多麼複雜
如果給我一把夠多數目的 Gaussian 的話那麼他大概可以描述他這個樣子
那就是我們這邊的意思
所以呢是譬如說我不是一個 Gaussian
而是大 M 個大 M 這個大 M 是這個小 k 從一到 M 小 k 就是 Gaussian 的 index
所以呢總共有 k 個 Gaussian 總共有 k 個 Gaussian
然後呢把他們分別 waited  by 這個 c 的喔然後呢加起來才是我這個 distribution
這樣的話呢我就能描述一個任何一個 distribution 的長相我都可以用這個方式來描述了
那麼因此呢我的 b  j  k 的第一個 index  j 是這個 j 就是這個 j 就是我的 state  index 是指我第 j 個 state 的 distribution
那第二個 index  k 呢就代表他是第 k 個 Gaussian 我總共有大 M 個
那麼通常呢這裡的每一個 Gaussian 我們通常叫另一個名字叫做 mixture
所以第 k 個的 mixture 其實就是第 k 個 gaussian 的意思
我就是把 k 個 Gaussian 混在一起啦所以變成 k 個所以呢總共有這是第 k 個 Gaussian  mixture 第 k 個 mixture
然後第 j 個 state
然後呢如果是這樣的話我必須這個這個 weight 這個 c 的 j  k 是 b 的 j  k 的 weight
這些 weight 加起來要等於一
換句話說這個 distribution 我積分還是要等於一嘛
我本來一個 Gaussian 積分是一
我這邊如果是十個 Gaussian 積分不是等於是十了嗎
所以顯然不行我一定要我一定要全部積分仍然是一呀
所以顯然譬如說這個 Gaussian 的積分是零點一這個 Gaussian 我乘上 weight 是零點一
這個乘上 weight 是零點零五那個是零點二等等這樣加起來我積分才會是一嘛
所以呢我就必須要有所有的 weight 要 summation 等於一的這個條件
那這樣的話呢我的這個積分起來才會仍然是一嘛
好那麼我們這樣畫好像很簡單其實不對因為我這個只是一個 one  D 的 DISTRIBUTION
我現在不是我現在這個 o 是什麼 o 是一個很多 D 我們說三十九維的 vector
那這個的 Gaussian 是怎樣呢
這個有點複雜那麼我們用一個呃簡單的式子來寫也不簡單
那我現在有一把我這邊有一把 gaussian 大 M 個那這一把 gaussian 呢就東一個西一個那麼散成一團
那麼他們加在一起的話那這個呢那這就是我的譬如說 b  one 的 o
就是一個長成這樣的東西 ok
所以呢假設我的 b  one 的 o
我的在 state  one 的話是一個這樣子的東西
那在 state  two 的時候他就是不一樣就對了
state  two 的時候我可能是另外一堆譬如說這個 gaussian 是長這樣的這個 gaussian 長這樣的
那你可以想像這是一堆 gaussian 這是一個 gaussian 這是一個 gaussian
那麼可能還有這也是一個這也是一個啊等等
那這個呢就是 b  two  o
那換句話說呢
我們剛才講當你是在 state  one 跟 state  two 的時候我 really 並不知道這堆 o 跟這堆 o 會長怎樣
在這個 state 裡面這個 o 是任何一個可以長任何樣子
我只知道他有一個 distribution
那那個 distribution 是這樣的
同樣呢我如果知道這個
這堆東西在第二個 state 裡面的話呢那他是怎樣我仍然不知道他是可以任何一個長相
不過呢我知道他的 distribution 是這樣的
那現在第一個 state 跟第二個 state 有何不同呢
只是說這兩個 distribution 不一樣而已
ok 因此呢是有一個問題譬如說某一個 vector  o 四
到底是在一裡面還是二裡面我們其實不知道
因為我只有 observe 到這裡而已這是我的 observation
我只有 observe 到這個他並沒有告訴我誰在哪一個 state 裡面
那譬如說 o 四真的是在第二個裡面不在第一個裡面嗎我們不知道
我們只知道他的機率不一樣
o 四是某一個 vector
譬如說這裡的時候是掉在這個位置
在這裡的話是掉在這個位置
嗯你會發現如果是掉在這個位置的話呢他接近這個 gaussian 的 mean
所以他機率是滿大的
這個的話呢是在他的邊緣他機率是很小的
我只知道這個區別而已 ok
所以呢對這裡面任何一個 observation  o 是一個 vector 而言
他掉到任何一個地方去
他在哪一個 state 裡面其實是我們是不知道的
那麼我只知道我可以算機率那麼他在這裡的位置在這裡
他比較接近這個橢圓的 mean 所以他是機率是比較大的
我如果掉在這裡的話他只在某一個橢圓的邊緣它機率是比較小的我只知道這樣子而已
ok 那我就是用這堆機率來描述每個 state 不一樣
所以呢那你就可以想像我的真實狀況變成一群這樣子的 state
譬如說我是這是第一個 state 這是第二個 state 這是第三個 state 等等
這是什麼音這是什麼音這樣一路滑過來就是某一個聲音
那所不同的就是說他們的第一個 state 呢有他的一個 distribution
那麼第二個 state 呢有他的 distribution
是有不同的
第三個 state 也有他的 distribution
他們就是不一樣就是了
那這個就是所謂 b  one 的 o 這是所謂 b  two 的 o 這是 b 三的 o
ok 那這堆不同的 o 呢 b  one  b  b  j 的 o 呢就描述了這個 state 長他們會怎樣
ok 那就是我們底下這段所說的事情
那麼於是呢我現在就把所有這些 b  j 的 o 我用一個大 B 來代表
也就是說呢這些東西呢
我叫做大 B 叫做大 B
那我用大 B 也就是這一堆 distribution 來描述每一個 state 的聲音會長的怎樣
那然後呢大 A 是什麼呢大 A 是我們的剛才的 state  transition  PROBABILITY
這些 a  i  j 我們說他是一個 matrix 嘛
那這些 A  I  J 構成一個大 A 呢就代表我所有的 state  transition  PROBABILITY 所構成的集合就大 A
那除了這個大 A 大 B 之外我還有第三個參數叫 pi
pi 是什麼呢 pi 是 initial  PROBABILITY 就是 q  one 等於 i 的機率
q  one 是什麼 q  one 是第一個 observation 我的第一個 vector 會掉在哪一個 state 裡面
ok 這個 q  one 等於一就表示我的第一個 state
第一個 observation  o  one 掉在 state  one 裡面
q  one 等於二呢表示我一開始是從二開始的
q  one 等於三呢表示是從三開始的
所以呢那麼這個 q  one 等於 i 呢叫做 pi  i
換句話說就是我的整個的 observation 會從哪一個 state 開始跳
理論上我不見得需要從第一個 state 開始跳我可以從第三個開始跳我可以從第四個開始跳我可以從任何地方開始跳
看你從哪一個開始跳的機率是多少
所以呢現在 pi  one 就是我從第一個開始跳的機率 pi  two 就是我從第二個開始跳的機率等等
那這些個 pi  i 構成的集合呢叫做一個 pi
ok 於是呢我這三組參數加起來就構成我所說的一個 hidden  markov  model
那麼這個 A 跟這個 B 跟這個 pi
就構成我的一個我的 hidden  markov  model 我們叫做 lambda
這都是我們的簡寫因為這東西太多了
一個 hidden  markov  model 這樣的一個 model 有這麼一大把的參數
喔有一大把參數那麼因此呢我們就簡寫一個是大 A 一個是大 B 一個是 pi
那這個 pi 裡面呢
我們可以簡單說一下這個 pi 基本上應該
任何一個 pi  one  pi  two 都可以是不同的機率
對不對他們加起來是一嘛
你可以想像我可以是譬如說
我有零點五的機率在第一個 state 開始從第一個 state 開始跳
零點一的機率在第二個 state 開始跳
零點一零點一譬如說這樣子後面是零
對不對我有一半的機率從第一個開始跳另外有零點一的機率從第二個或第三個等等等等
這可以這樣子的這是我的 pi
所以這是 pi  one 這是 pi  two 等等
不過這樣有點太複雜了
就如我們剛才講我的 transition  PROBABILITY 這個 a  i  j 我後來簡化了
這裡我們通常也簡化
我們絕大多數可以就讓他從第一個開始跳不要那麼複雜
因此很多時候我們都簡化成為就是 pi  one 等於一後面全部都是零
我就一律讓他從第一個開始跳
這樣比較簡單
所以我們後面做的時候常常是把他作這個簡化
雖然 in  general 他們可以是任何一個樣子
