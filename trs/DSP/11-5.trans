好那有了這個pca 之後我們現在要來看的
是怎麼樣用pca 來來做這個eigen voice 
那麼eigen voice 的想法是延續剛才講的
就是說我們這個聲音
我現在一個新的speaker 來
我怎麼樣子在很多unseen 的data 裡面
我要能夠一起調嘛
我不能只聽到那幾個聲音之後調那幾個聲音
我要一起調
我怎麼樣可以一起調呢
我們m l l r 等等有它的辦法它做一個tree structure 來來做這些事情等等
那eigen voice 是另外一個想法
也是一樣的目的
我希望能夠在最少的聲音聽到最少的那些音
我要整個model 全部一起調
那怎麼做這件事呢
那我們現在來看
我先是假設我有這個一群這個train training speaker 
每一個training speaker 我可以為他train 出他的speaker independent phone model 
那麼我們舉例來講假設每一個train 每一個training 的speaker 
那麼我就請那個speaker 發夠多的聲音
把他的所有的音都唸到之後
train 出他的speaker dependent phone model 來
那如果是這樣的話呢我現在就可以把它所有的phone model 兜起來兜成一個很大的vector 
就是這邊所謂的super vector super vector 
什麼意思呢
譬如說我現在有一個speaker one 
那個speaker one 他train 他唸了夠多聲音之後為他的每一個phone 
都train 出它的model 
假設說這個這個是某一個tri phone 這個是ㄧ這個是ㄚ這個是ㄨ這個是ㄊ等等等等
那然後呢每一個model 裡面的每一個state 
每一個model 裡面的每一個state 都是一堆gaussian 
每一個gaussian 的mean 就是這些東西
那我就可以把所有的這些個gaussian 的mean 
把它全部串接起來
然後變成一個很大的vector 叫做super vector 
舉例來講它的
它的每一個mean 是一個一個的mean 
我就把它一路這樣串接起來
那這是一個非常大的vector 可以多大呢
我們舉個例子像這樣子
假設它是五千個tri phone 
五千個tri phone 
每一個tri phone 有三個state 
每一個state 裡面有八個gaussian 
每一個gaussian 有一個mean 
那個mean 是三十九維我們算是四十維
那這樣一乘是多少是四百八十萬個參數
構成一個四百八十萬維的一個非常大的matrix 
ok 所以呢
那一個speaker 我就得到一個很大的一個vector 
大到什麼程度呢有四百八十萬個component 在這裡
是一個四百八十萬維這個n 是很大很大的的一個vector ok 
那這是一個一個training speaker 可以這樣
那我現在呢有一群
譬如說我有一千個training speaker 
那每一個每一個training speaker 都做這件事
那第二個training speaker 呢他的聲音不一樣啦
所以他有另外一堆點
他有另外一堆點
那邊就可以得到另外一個
也是四百八十萬維的另外一個ok 等等
那這樣子的話呢我現在如果有有一千個training speaker 的話
我就得到一千個這樣子這個四百四百八十萬維的這個大的vector 
既然有一千個了
我可以想像成是一個random vector 
它有一千個sample
就好像這邊的一千個一樣
這邊有一千個點嘛
或者說你可以想像成是這邊的一千個點
那所不同的是我現在這個空間非常大
不是這邊的三度
我這邊是四百八十萬維的
ok 那麼因此呢如果是這樣的話你可以想我我這個空間是什麼
這個空間好比就是這個空間好比就是一個四百八十萬維的一個空間
那每一個speaker 其實是裡面的一個點對不對
我第一個speaker 得到一個四百八十萬第一個speaker 得到一個四百八十萬維的vector 
相當於這裡面的一個點嘛
這裡面的一個點
它是一個第一個speaker 得到一個四百八十萬維的vector 
相當於一個這個四百八十萬維空間的裡面的一個點
第二個speaker 呢也得到一個四百八十萬維的vector 
是這裡面的另外一個點等等
那我現在有一千個speaker 就是這裡一千個點嘛
對不對我就一千個點在這裡
所以呢我等於是有一個四百八十萬維的空間
那這個空間上的任何一點其實都相當於一套model 
因為你空間上的任何一點你都可以想像是這空間上的任何一點
譬如說這裡的任何一點
你都可以想像是一個就是一個四百八十萬維的一一個一個這個vector 
如果這樣想的話
那麼喔不是這邊上的任何一點是是那邊那個空間上的任何一個點
都是一個四百八十萬維東西
那這裡面的譬如說前面若干維相當於某一個mean 
這邊若若若干維相當於某一個mean 對不對是不是這樣子
就好像原來的這邊的若干維是相當於某一個mean 
這邊的若干維相當於某一個mean 
它是這串起來的嘛對不對
我本來這個就是這樣做的嘛把一個一個mean 把一個一個mean 串起來變成一個大vector 
等等一個一個mean 串起來變成一個大vector 
那這樣構成那那個空間裡面的構成那個空間裡面的那一點
因此呢現在那個空間裡面的任何一點你也可以想像成相當於某一個mean 某一個mean 某一個mean 
所以呢你任何一點呢相當於某一組這個model 
然後呢也就是相當於某一個speaker 可能是這樣子的
因此呢你可以想像這裡的這個上面的每一點
都可以相當於那一大群的
這裡的每一點相當於四百八十萬維
相當於那一大群的phone model 的的這些個mean 
那麼如果是這樣的話
那上面的每一點其實相當於一個speaker 
也就是說每一個training speaker 是它那裡的一個點
好如果是這樣的話我現在可以對那個點對那堆我現在有一千個點在這裡啦
我就可以對這一千個點來做pca 
怎麼做
第一個要把它變成zero mean 
所以減掉mean 嘛
因為我們剛才講了我pca 都是都是當它是在zero mean 之下才有這堆solution 
所以呢我要先讓它是zero mean 
所以我第一個呢減掉mean 
減掉mean 之後我再求它的covariance matrix 
求出來之後呢
這個covariance matrix 我就可以求它的eigen value 跟eigen vector 
那底下這個式子就是我這邊的這個式子
就是你這邊的是k 個
這k 個就是我這邊紅色的這k 個
就是這第一個
然後這個lambda i 就是我中間這個這k 乘k 個lambda i 的matrix 
然後右邊的這個呢
這個的transpose 就是這k 個
那這樣乘起來呢幾乎就是原來的covariance matrix 
所以這個式子就是我這邊所紅色的這個式子
而這個lambda i 就是我的那i 那k 個最大值的eigen value 
那這k 個呢就是我的eigen vector
有最大的它的lambda one 大於lambda two 大於lambda k 
就是第一大第二大第三大這樣我總共k 個
這樣的k 個呢就是我的k 個最大eigen value 的那k 個eigen vector 
然後呢那當然你要怎麼選擇k 
你要使得大於k 的已經小到夠小了
也就是說你怎麼選擇這個k 呢
一定要讓這後面已經很小很小零點零零零多少
很小所以呢它們的效果在這裡不明顯了
那在我們做過的經驗這個k 大概從五十到兩百五十之間的差不多啦喔
你雖然原來這邊有四百八十萬個或者多少個
你這邊的非常大
這邊的dimension 可能是這個n 可能是
當然不一定要四百八十萬
可能是夠大的至少上萬哪喔
成千上萬的
但是你最後可能只要五十到兩百五十個
就變成一個相當小的就夠了
因為其他東西都已經效果非常小
因為這些值都是非常趨近於零的eigen value 都可以不用了
那這個意思等於是說
我這邊本來是一個四百八十萬維的空間
每一點是一個speaker 
他有他的全套的model 的參數
是一個點
那我現在呢等於是說
我把它reduce 到一個五十維或者是兩百五十維的一個小的sub space 
那這個小的sub space 裡面的每一點都是對應到那一點的
舉例來講呢譬如說這一點就是投影下來對應到這一點
這一點就投影下來對應到這一點
這一點就投影到這一點
這一點呢是投影到這一點
那每一點在這邊都有一個它對應的
那這些它所對應的就是我們剛剛講的y 
那它跟它的關係就是y 等於a 的transpose x 
就是就是我們剛剛講的這個嘛
喔就是這個
我投影過來就是y 原來是x 
就是這個關係
好那有了這個之後呢
那我現在可以怎樣呢我下一頁的上半段
