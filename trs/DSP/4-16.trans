開始
所以呢這個這個 problem 三的 forward  backward  algorithm
我們說它是一個微調的工作
我們還要一個 initialization 的過程
我還需要叫做 model  initialization
我要有一個好的開始的 model 我才可以算
就好像我剛才說 OK 　假設這是已經有一群人的聲音我已經做好了一個 model
我現在把這個聲音把它 train 進去
這樣講當然可以但是那個 model 怎麼來
你那個 model 總該有一個要有一個 model 才能夠做嘛對不對
所以一開始怎麼做這件事情
model  initialization 是一個很重要的問題
那麼嗯這個你可以想像這個工作呢是相當於我們講的粗調
而我們現在講的這個 iteration 的過程是等於是微調精調或者是微調
所以呢我們第三個 problem 講完我們只是講微調怎麼微怎麼調
但是 initialization 也就是粗調呢我們還沒有講
所以我們底下要講
怎麼做這個粗調怎麼做這個 initialization 
OK 那麼我們這邊講的是微調就是已經有了一個 initial  model 之後
怎麼樣去把這個東西去調慢慢調得更細更好一點調得更精緻一點
但是我還有一個粗調的工作就是 initialize 還是要做的
好那有了這個那我們又講另外一個問題就是這是所有的 EM
用 EM 的基礎來做都有共同的問題就是說它嗯它可它可能 converge 到 local  optimum
也就是說 EM 說了半天它其實只是證明這件事情
你每一個 iteration 都會提高
那最後它一定會收斂到某一個可以可以這個最大的值是沒有錯
但是它也許如果是一個這樣的東西你在這邊調的話
你如果從這裡開始調你調調調調調就調到這裡就停在這裡
你還會不會走到這邊來呢不會走過來了所以你就會停在這裡對不對
那你如如果從從這裡開始調的話你也是就回到這來
你必須要從這裡開始你才會到這上面來
所以這個時候呢是這個嗯
也就是說這個嗯 results  depend  on heavily 
depend  on initialization
depend  on 你從哪裡開始
因為它幫你往上走
但是你如果開始點不對的話就走到一個比較可能走到一個更小的地方去
那麼因此呢這個 initialization 非常重要
那麼同樣的一堆 data 讓兩個人去 train 它
那 depend  on 它怎麼做這個 initialization
最後 train 出來的 model 會不一樣
所以這是一個很難做的問題就是這樣子
因為你的 initialization 不一樣的話就會不一樣
那麼雖然同樣的 data 你會得到不同的結果
那麼事實上同樣那一堆 data 可以得到很多不同的 model 那會有不同的結果
那這就是你很容易 converge 到 local  optimum 的問題
所以呢我們 initialization 非常重要
好以上這段講的是 problem 三或者說是 forward  backward  algorithm
那它的目的是解這部分的微調的工作
那麼再下去呢我們要來講粗調
ㄦ粗粗調就是 initialization
這個應該是我的最後是不是這個的最後一章了
對這是最後一章了
OK 那麼要講到這個粗調 initialization 我們就要回到 power  point 的這個地方的
那一般的這個粗調的這個 model  initialization 都是使用 vector  quantization 的方法
那麼因此呢底下我們會先講一下 vector  quantization
這個東西是相當有用的在很多地方都很有用
因此呢你也許在別的課裡面有學過我們會講的快一點
不過基本上呢在我們這裡也一樣用它來做我們要做的這個嗯 initialization 的工作
OK 我們先在這裡休息十分
 OK 我們來講下一段
我們把 HMM 的那些複雜數學暫時丟掉了
我們先講到這裡為止
我們換一些東西不然一直搞那個是很頭大
我們底下講的是嗯其它的東西了
那第一個我們先說的是這個
這個我們要用 v q 來做這個粗調
我們簡單講一下 v q
那麼 vector  quantization 是在很多地方都很有用
那麼你可能別的地方都學過
不過如果你學過的話我們就算是一個複習就是了
那麼 v q 的用處在很多地方
一個常用的是說做為 data  compression
那麼當然還有另外的用途是 clustering
那這個底下我們就會解釋
那麼我們先從 data  compression 的觀點來解釋的話呢
那 vector  quantization 原始的來源是來自
這個數位通訊裡面的那個pulse  code  modulation 所謂的 pcm
那麼它的觀念就是所謂的 scalar  quantization
是從它衍生出來
那麼這個的觀念講起來很簡單就是說
假設我有一個 signal 我有這堆 sample
我怎麼把它送到遠方去呢
那一個可能的辦法是我把它整個的 range 切成若干格
然後每一格代表用一個 bit  pattern 來代表
舉例來講我這邊切成八格
如果切成八格的話總共只有三個 bit 就夠了
也就是這邊只有零零零零零一零一零零一一等等
於是我的第一個 sample 如果掉到這一格我就是一一零
表示說它是掉到這格
第二個呢還是這一格還是一一零
第三個呢還是這一格還是一一零
第四個呢還是這一格就一一零
第五個呢變成一零一
第六個呢一零零等等
那我就用這個 bit  pattern
來描述說我的這些點的位置在哪裡
那當然如果這樣做的話我這些點的位置其實只說明了它在哪一格裡
至於在那一格裡的位置它已經丟掉了
我們不管它在哪一格裡的什麼位置
只管它在哪一格就好了
當我把這一串零跟一送到遠方去的時候
接收端呢它其實無法判斷
它只知道它在哪一格不知道它那格在哪裡
所以呢它就會怎麼辦譬如說它就會把這些一一零呢
它就是一律以一一零這格裡面的中央那一點來代表
於是呢它就會看成這幾點都是一樣的
都是中央那一點
然後一零一呢也一律用中央那一點
一零零也一律用中央那一點
因此呢到你遠端它會把這個連起來
就會變成一個這樣子的曲線
跟原來會有點不一樣
那這樣子的過程呢我們稱為 quantization
那麼在中文當時通常翻做量化
那麼其實它就是把這個你該有的 range
切成若干段
每一段用一個值來代表它
然後這個段數總共是二的 r 次方
於是我就只要多少個 bit 就可以了
那這件事情就是我們這邊講的 quantization
那麼後來因為要把它變成 n  dimension 變成 vector
所以這個就叫做 scalar  quantization
一開始的時候這個就叫做 quantization
後來因為變成 n  dimension 以後 vector  quantization 這個就叫做 scalar
那它的意思就是把一個 single  real  number 用一個 r  bit  pattern 來代表
就像這本來是一個 ream  number
一個 real  number 但是我變成一個 r  bit  pattern 來代表它
那我做的方法就像這邊講的一樣我這個 range 就是正 a 到負 a
就是我這邊的這個正 a 到這個負 a
那我就把這個 range 呢分成大 L 個段
每一段叫做 j  k
所以呢我這邊所以我這個橫這個畫的橫軸就是我這邊的縱軸
所以這裡面的某一段
我就叫做 j  k
就是第 k 段叫做 j  k
那那裡面有一個代表值就是中間那個代表值
叫做小 v  k
那就是我這邊所畫的
所以呢我總共有多少個呢有 L 個
L 是我總共的總數
因此呢我就會有 L 個 j  k  k 等於一到 L
它們的聯集就是整個的 s
所以 s 就是整段的
整段的 real  number 叫做 s
也就是說這整段就是我的 s
它就是所有的這些 j  k 的聯集
那另外呢這些代表值 v  one  v  two 到 v  l
就這些中間代表值呢的集合呢叫做大 V
如果是這樣的話呢我的 quantization 不過就是一個從
大 s 到大 v 的 mapping  relation
這個 mapping 條件是什麼其實很簡單
就看你的 sample 掉在哪一個裡面嘛
譬如說這個 sample 是掉在這一格裡面
我就把它把那個值 map 到它的代表值
對不對就是我們這邊的意思
看它掉在哪一格我就用它的代表值來代表它
所以我最後就變成這種的就變成黃色的這個
也就是我把它的在那一格裡面的精確的位置都丟掉了
只留下它的代表值的值
那麼如果是這樣子的話呢這個
那我這個就等於是一個 mapping  relation 嘛喔
完全看每一個 sample 的值掉在哪一塊裡面
那麼我就用那一塊的代表值來代表它
那麼這個時候呢這個 r 的代表值呢我全部可以存在遠端都可以存好
所以我每一個每一每一個代表值我就只要用 r 個 bit  pattern
就可以代表了
所以我在傳送的時候我只要送這些少數個 bit 這個 bit 數目很少
就可以代表這些東西了
但是當然我也同時丟掉了重要的 information
就是這裡面的每一個點到底在這裡面的什麼位置是丟掉了
我只知道它是那個代表值的位置而已
那這個過程就是所謂的 quantization
那如果是這樣想的話呢
那麼嗯這裡面很重要的一件事這到底要怎麼做
我們這邊的說我把它等分成八塊
那當然沒有理由要等分
我可以做不等分的
那麼你最容易想的情形就是
把它分成譬如說中間比較小外面比較大
我這個隨便畫畫
中間比較小外面比較大這裡面你可以想像有幾個原因
第一個原因是說它也許有一個 distribution 是這樣子的
這我們剛才講的這個它有一個 distribution
這它的 probability  density  function
如果是這樣子的話
我就有理由
中間比較小外面比較大
為什麼因為這邊機率那麼大嘛
經常出現我就做得比較精細一點
因為我的我每次做的時候我就會把我的真正的值在那個裡面的精確的位置丟掉了
所以呢我如果這個值越細的話呢那我丟掉的東西越少嘛
越能夠精密
當變成這麼大的時候當然就搞不準一定會有很大的誤差嘛對不對
所以呢你這個越大的話我的 error 越大
越小的話我的 error 越小嘛
那麼中間這個機率那麼大我儘量用細一點
它這個這個 error 比較小
那外面的話反正難得發生所以呢給它粗一點大一點就算了
阿這個是一個很容易想的想法就變成這樣
那這個就是我們講的這個 probability  distribution  of  X  M
你可以根據這個東西來 design 怎麼做這件事
那同樣 error  sensitivity 是說呢
有的時候其實這個 error 大小是跟我們的感覺的敏感度有關
舉例來講呢你如果是在這些地方的話呢可能我的 signal 本來就很大
signal 本來那麼大所以中間如果多一點 error 可能影響不大
可是呢你如果是在這邊的話呢
它可能是很小的 signal 
這個時候你的一點點的 error 可能都很影響都很大
所以呢如果這樣來看的話我就會希望說我的 error 呢
在中間會很小在外面可以比較大
所以我是會讓外面這格比較大裡面這格比較小等等
那這就是所謂的 error  sensitivity
因此呢我們至少可以考慮我們至少 at  least 考慮這兩個因素
就是 x 本身會有 distribution
然後包括它的 error 我們可以容許的程度我們敏感的程度來 design 這個東西
那這個東西是什麼東西就是你如何來分
我不一定要是等分我可以分得大大小小
同樣我那個代表值 j  k 這個 v  k 的位置
也不是一定要在正中央我可以放在邊邊上
我可以偏離中央也沒關係
那麼 depends  on 怎麼樣比較好
那這些都是可以考慮的
那也就是說你每一個 J L 這個區這個劃分怎麼劃分
還有裡面的代表值 v  k 倒底怎麼取
這些東西的加起來就是我們講的一個 quantization 的 characteristics
也就是一個 code  book
那我們要想辦法做這件事情
那這個是所謂的 scalar  quantization