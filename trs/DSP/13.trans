那麼因此我想想看我今今天的目標應該是
不是十十四十四跟十五吧
我們今天的目標先是
耶不是十五那是十三
十三跟十四阿
十三是retrieval 
十四是今天的目標是把十三跟十四十三是retrieval 
十四是這個understanding organization 
我希望今天是把這兩個講完
然後下週我們應該還需要完成的包括十就是utterance verification 跟key word spotting 
key phrase spotting 然後呢十六dialogue 
dialogue 十七是dsr 然後十八是conclusion 
所以這個這個現在希望做到這樣
那我們儘可能在最後的時間把所有的還沒有說說的都說到
那我們今天天先從十三點零的retrieval 開始
那麼retrieval 要講的事情是喔你如果記得我們開學第一週給你看的demo 裡面就有這樣的retrieval 
也就是說我們只要輸入入說我要找以色列與阿拉法特
他自動就會把所有跟以色列與阿拉法特相關的新聞通通抓出來
那那個case 就是一個以語音為基礎的information retrieval 
那麼所謂的information retrieval 
那麼你知道這這個詞是一個重要的研究領域
那他的的最明顯的代表就是你今天的google 
所以你上google 的時候你只要輸入你的你要的keyword 你
他就會給你所有跟這個keyword 相關關的一大堆的網頁
然後裡面說的所有東西那你都有了
那那個就是information retrieval 的最具體的代表
那這個領域事實上是一個重重要的領域
裡面的學問非常多
不過以google 他們的工的動作主要的是這個文文字跟文字的比對
也就是他把網頁上所有的全球網網路上所有的網頁上的文字
跟你現在輸入的keyword 去做比對
那當然你要同時跟全球網頁去做比對這有很多技術
那不過那就是所謂的information retrieval 
那我們現在講的是把語音跟它整合
因此我現在要做的事情是我我輸入的keyword 可能是語音的
那我那些網頁可能是指指語音的information 
那麼因此呢這裡面至少有一個是語音的
那就是我們這邊講的事情那這裡面的第一篇可能是最早而有最完整的描述述的
這篇文章相當長
但是他的內容把一些重要的觀念都提到了
那麼說的是相當完整而清楚的
那也是最早的一篇最完整的paper 
所以我把它它列在這裡
那我想雖然早
了一點不過你今天來看仍然是非常好的的reference 
好所以對這個領域而言我想這個仍然算是非常好的
所以呢這個是recommend 你可以看的
那另外呢這個文字的retrieval 本身是是一個重要的學術領域
那麼有很多的教科書
這邊舉其中的一本為例
那不見得只有這一本
那不過這應該把裡面很多重要的基礎基本的知識都在都有提到
那此外呢information retrieval 在恩學會而言最重要的activity 應應該是在acm 的special interest group 
所以呢他他們有他們的網站
他們每年有每年他們的會議
每年都有非常多的paper 
那那都在這裡面等等
那那麼其他的reference 待會說到的時候再來說
ok 那這邊講的大概我們都說過所以大概都知道我們講的事情就是還是一樣去上上網去找要找的東西
唯一不同的是今天我們講的是今天你上google 這個網頁上基本上上是文字的比對
網頁雖然也有一些聲音也有一些畫面
不過基本上不是在比那些而是在比比對文字
但是呢未來很可能網頁上越來越多的東西可能能是multimedia 
那他們可能並沒有multimedia 他都帶了聲音帶了語音
但但那些語音可能並沒有人去幫你把它轉寫成為文字
就直接就是語音在那裡
於是你就變成要用語音直接用聲音去找到你要找的的東西
那反過來呢user 這一端也很可能是用手機pda 或者在車上什麼的
的去找要找的東西
那這時候user 的query 進進去也可能不是打的keyword 
而是用語音講的
那因此呢我就變成兩面都可能是語音的
那就是是我們這邊所講的這個這個用語音來做的這個information retrieval 
那麼這個時候呢那麼基本上我們要用他們原來的這些這些文字比對的所有有的技術
但是我們得把它轉到語音來
那麼語音裡面面有一堆語音的問題需要解決
除此之外呢那當然網頁上還有很很多文字
如果我我只是用這種東西在在上網的話怎麼辦呢
那就是這些文字我要要語音合成
變成聲音
那再來呢你今天上網是很多互動的
就說你你輸入keyword 然後他給一堆東西去選
你在選選你在動是很多互動
那這個互動呢就用dialogue 用對話等等
那就我們這邊所說的事情
那這個圖我們很早就講過了
那也就是說你今天的的google 是輸入一堆文字
就網路上去找網路所有的文字
文字跟文字再做比比對
那我們未來輸入的可能是聲音
音而網路上的東西也很可能是multimedia 的你
你就是靠它的聲音
他可能沒有人把幫你你寫成文字
那你靠它的聲音
於是你可能用聲音去找聲音
你也可能還是用聲音音去找一堆還有很多網頁是文字
聲音還要去找文字
然後你回到家裡的時候你的你的輸輸入可能還是文字
那這個時候你要找找的也是聲音
所以這些都是我們所說的範圍圍之內
ok 那我想我們用這張張圖簡單的解釋所謂的information retrieval 
那也就是我們說這個information retrieval 
它本身如果以文字去找文文字就像google 的動作
那樣的一件事
那麼最基本的就是這張圖所說的事情那
那網頁上有千千萬萬的幾十億的網頁
那所有的網頁頁我們就稱之為document 
那每一個document 我們都想辦法用某一些個feature 
把它呈現變成通常是一個feature vector 
那就是這個這個動作我們稱之為indexing 
那我們想辦法那像像今天的google 他我們基本上是用文字
以他文字上面面的一些東西西來做feature 
但是我也是一樣把每一篇文件每一個document 
想辦法用一堆feature 
最後是用一個feature vector 
用一個vector 來呈
呈現那就是所謂的那那這個動作就是所謂的document representation 
也就是indexing 
於是呢網路上上的千千萬萬的網頁或者文件就都變成千千萬萬個vector d 
那user 輸入一個query 
輸入一個keyword keyword 或者一組keyword 或者什麼
那user 輸入東西呢
也經過類似的動作把它變成另外一個vector q 
q 也是用一堆feature 把它呈現變成一個vector q 
那這就是所謂的query formation 
或者是user request representation 
因此呢所所謂的retrieval 的動作呢就是把你這個q 拿來
去跟把user 這個q 拿來跟網
網路上千千萬萬的d 去做一個matching 
看看誰跟誰比較像
算他們之間所謂的relevant 這個relevant score 
就他們相關程度有多多近
那然後把根據這個relevant score 排出來
來把分數最高的一堆寄送回來給你
那這個就是retrieval 的動作
那通常有所謂的relevant feedback 
也就是說呢user 或者是是自動的這兩種
那意思思都一樣就是假設你照我的google 回來可能找回來一萬筆
不過裡面呢排名名最前面的一百筆可能是真正跟你比較有關的
那排名最前面一百筆裡面可能是最最前面的十筆出現在妳的螢幕上
那最前面的十筆裡面
如果你你自己去看一看可能有五六六筆真的是你要的
那你如果把那五六筆圈出來
我把那五六筆再輸入一次做為query 
那這個時候就會找到跟你要的很接近的
那這這種動作是所謂relevance feedback 
那剛才講的這個情形是user manual 去做但
但是你當然也可以自動的
你可以說凡是找回來的前五筆筆或者前十筆或者前二十筆
我就當他整這個整筆筆東西當成是我要的
那麼再回去一次把那一堆東西通通加回去
到那個query 裡面去再找一次
通常都會更好那
那我可以自動來做這個事情那
那或者這個是自動或叫做blind 就是user 沒有去選自動來做也可以
的詳細我們們後面還會提到
那這個的基本的意思是說
絕大多數的user 在輸入query 的時候都是是很懶惰的
所以通通常user 輸入的query 都非常的簡單
而這個並不真的描述他真的要的所有的東西
那麼我我們舉例來講如果他要找這個個九一一恐怖攻擊
那他他也許就輸入一個九一一或者只輸入一個賓賓拉登
那其實你它你要找的事情可能包括這個keyword 可能包括紐紐約
包括這個痾阿富汗包括這個恐怖攻擊包括很多詞
但是呢你只輸入入個九一一
那所所有的文件裡面有九一一的才會被找到
沒有九一一的就沒有
但是是你如果用這個方法的話呢
那個時候你找到前二十名或者前一百筆
那它裡面可能就把跟九一一相關的所有的詞都在裡面了
這個時候如果果你再回去一次的話就可以把很多東西找回來等等阿
所以這個是這個feedback 基本的精神
神那然後呢我們的看找回來的結果就可以來做performance evaluation 
那這個performance evaluation 我們從前也提提到過
你可以想像成網路上所有的東西相關的是這個圈
所有相關的文件
可是系統找給你的是這堆是這個圈
那麼因此呢真正找對的是是a 的部份
那麼b 的部份是找來其實不相關的
你找到的前一百前一百裡面面可能有五十筆都不是你要的
那這個是找錯的
另外你會呢你會丟掉一些c 是你丟掉的
你沒有找到的
因此有兩個正確確率
就是一個所謂的recall 一個叫做precision 
recall 是a 除以a 加c 
也就是到底真正有關關的裡面你找到了多少
那precision 呢是你找到的裡面真正有關的是多少
這是這兩個正確率
那麼痾那那通常是這個寫在這裡就是說呢這個recall 是我們們通常user 通通常感覺的是precision precision rate 
也就是這一個a 除以a 加b 
我看我找到到了一百篇裡面到底跟我要的是三十篇還是五十篇
裡面有一大堆我我不要的
這是user 所以所可以感覺到的你找的東西好不好
所以那這這個是precision rate 這是通常user 感覺到的
但是recall 到底網路上還有多少你沒找到
我們通常很難判斷因為網路上你也不能去真的用眼眼睛去看
所以通常不太容易判斷
那麼怎麼去這個這個痾recall 到底有多少是不太容易做的
那麼那即使是這個個這個precision 是多少其實也很難
因為那你看你怎麼看法譬如說你是從一百百篇裡面
前一百筆裡面我我對的是幾筆呢還是前五十筆
還是前二十筆還是第一頁頁
第一頁裡面還是第二頁堶±這個都很難算阿
所以呢那麼那另外一個重要原因因應該講這兩個正確率應該是互相trade off 
的那你可猜的出來是我retrieval 的時候我選擇的那個threshold 的高低的問問題
我如果選擇的threshold 很高的話
我會使得很多不相關的不會進來
來我threshold 選的高的話我進來的都是相關的
我的precision 可以很高
可是那個結果我也會丟掉很多所以recall 會低
那反過來我如果threshold 低的話
話那麼很可能就是就是一大堆都進來了
所以我的recall 會高
可是我的precision 也就低了因為很多不相關的也都進來了
所以這兩個通常是是你你調那個threshold 的時候這兩個一起動
一個高一個會低一個個低一個會高
這兩個是是trade off 
因此有所謂的這個recall precision pron 
就說你不不能算一個數值而是應該是是這兩個一算
畫成一條curve 的
那這個curve 通常會變成這樣這
這depends on 你怎麼畫
如果是一一面是pre precision 的話
一面是recall 的話
那麼這個痾痾你可能會變成像這類的圖
也就是說你你這邊要高高的話這邊就低了
這邊要的話這邊就低了那
那當然這時候候的最idea 的case 是
在這裡這是idea case 那麼
他他們是痾也也許我畫錯了我們
也許是這樣可
可能是要這樣子阿
我我如果recall 要高的話precision 會低
recall 要這個precision 要高的話呢recall 會低
那麼因因此呢我的idea 是是這一條
那麼你你調你的threshold 的時候
你調調你的threshold 他是在這上面動
那麼threshold 如果高的話
你的一邊到這邊來一邊到這邊去
那你的系統有多好就看這條曲線如何的接近上面那個case 
那在這邊是idea 
就是我不管怎樣我的我的這個這個個precision 永遠是最高的
recall 永遠是最高的
你如果這樣畫的話大概是這這樣
那我剛才那個畫法是用一減跟一減來畫就會到這邊來
那因為這樣的關係你如如果動不動就要用一條curve 來描述的話實在很麻煩
那麼因此呢他他們真正在算的時候候常常用一個數字來算
那個數字就是所謂的non interpolate average precision 
就是以precision 為準
然後呢我只用一個數數字
那個數字怎麼算呢
就是averaged 在所有的你凡是找到對的那裡去算那個recall 那去算那個precision 
我們舉個個例子來講
假設你你找到的前十筆裡面
前十筆裡面
真正你要的是第一筆第五筆跟第十筆
第五筆第一筆第五筆跟第十筆是你要的其
其他不是
中間這些都不都不對
如果這樣的話我怎麼算
我算到第一第一筆的時候我的的precision 是一分之一
然後這些都不對
算到到第五筆的時時候我的precision 是五分之二
因為五筆裡面有兩筆是我要的
這些都不算
算到第十筆的時候呢
我是十十筆裡面有三筆這是十分之三
因此譬如說我我算到我的cut off 如果在這裡的話
我就拿這三個數字來平均
那這三個數字來平均這樣得到一一個數字
那然後我現在在如果用一千個一千個實驗做一千個query 進去
或者怎樣的話呢
呢我把全部平均起來
那這個數字是所謂的non non interpolate average precision 
這樣我就用一個數字來說
那還是要說我我的cut off 是多少我是算到第十的還是是算到哪裡的
的那在這個情形下
我的分數是多少那就用這個數字來算
好那以上上是簡單的介紹
那底下我們就要說的我們要做語音怎麼辦
用語音的話呢我現在變成是說這邊的document 裡面可能是語音那
那這個query 也可可能是語音
那麼於是呢原來原來的這套套學問是用文字做的
裡面如果是語音怎麼辦呢你馬上想到最直接的就是我都把它它做recognition 之後都變成文字
就可以帶進進來這一套
那如果用這個方法的話話呢
那就是所謂用word 來做indexing indexing element 
我就是都做一次recognition 
那基本上就是我把所有的document 如果是語音的
所有的query 如果是語音的我我都去做speech recognition 
一但做完speech recognition 之後呢
我就都變成文字了嘛
那就用文字的方法來做它
那當然這個時候你顯然要用large vocabulary 的recognition 
去做它
所以這是一個個large vocabulary based 的方法
那這個顯然是可以的但是不會很好
最明顯的原因你馬馬上知道就是error propagation 
我等等於是把兩個系統串連起來
那第一個系統的error 就到了第二個系統去
那換句話說
你這個語音去做recognition 的的時候
你已經得到一堆堆錯字
那些錯字也就做了indexing 
所以你得到到的這些d 裡面就有錯字嘛
那同理你的query 裡面做recognition 也會有錯然
然後你再進來也有一堆錯
所以錯的去找錯的所以就找到一堆錯的
所以以呢這個error 就等於是在兩個系統統你語音辨識的系統跟這個系統兩個串聯在一起的結果
那就是error 一路propagate propagate 下去
那第二個明顯的問題就是oov 
因為很多的的keyword 真正要要找的keyword 
並不在在辭典裡面都是oov 
你譬如說我我今天要找的是這個個奧斯卡的得獎影片斷臂山
這個keyword keyword 這個斷臂山顯然是oov 
你說我要找趙建銘的事件
趙建銘顯然是oov 
你那個鐵定都是找不到的
因此你你這些真正的keyword 你都找不到到的話你找到都是一些不重要的字所以其實是不太容易
那再者呢special term 很多
如果是在在特別的領域
譬如說你要找的是這個醫學有關的新的的文件
或這你要找computer 有關的文件
或者是要找這個痾這這個股票有關的文件
凡是一個special area 的話還還有一堆special term 
可能不都不在你的recognition 的那個痾vocabulary 裡面
可能都是oov 你都辨識不出來
那麼但但是這裡這個問題常常是針對某某些領域的
那你又又不可能為每一個領域都去做你的辭典
所以這是用word 來做indexing element 
這種東西我們是所所謂的indexing element 也就是你將來的你後面的這些個這些個indexing 的動作
是要要以他們為element 為單位來做的
那麼你用word 來做是有這些缺點
但是呢word 顯然是一個重要的方法
法因為有這些問題所以就就有人想說那我應該用subword unit 
那這就是所謂的subword based 
所謂subword unit 的意思就是說我現再用的是比一個word 還要要小的單位
什麼是比一個word 還要小的單位
位像phone syllable 或者類似的
那麼這有什麼好處呢
這個時候我就不再被vocabulary 所限制
而是我只要音對就可以了
那麼舉例來講如果這個user 說我要找bill gates 
他說我要找bill gates 的話這可能是一個oov 
你辭典裡面沒有這個字你就抓不到這個字了當然就沒有了
但是如果果你是以音為單位的話以phone 為單位的話呢
是這幾個字
所所以呢我只要抓到這個phone 跟這個譬如說這個phone 跟這個個phone 連在一起跟這個phone 連在一起
那在那一邊文件裡面的那一句話它也有這幾個phone 連在一起
那那我至少呢這個會抓的到
我只要這些音這些phone 對的話
話我這些phone 連在一起我抓到這三個phone 的一個phone sequence 那
那一邊我如果user 說的話裡面有這三個phone 構成的這個phone sequence 
sequence 在這個文文件裡面你做recognition 我也抓到這三個phone 的phone sequence 你就可以對應出來
那雖然我的辭典裡面沒有這個字
所以我真的辨識不出這個字來
但是我這個還還是可以比對的到的
那在我們中文而言同樣的情形譬如說我要找呂呂秀蓮我
我只要抓到譬如說syllable 為單位的話我有這個秀這個個蓮
雖然說我的辭典裡面沒有這個字所以我不會辨識出這個來可
可是我如果query 裡面有秀秀有蓮我已經抓到秀蓮了
那麼在這個文件裡面他也有這個個講到秀蓮怎樣怎樣的話
我其實就可以這個就對對應到他了
我即使沒有辭典沒有這個字我沒有抓到他沒有關係
那這個就是用subword unit 來做的基本的想法
那麼因此呢我可能是用phone 
或者phone 的sequence 
sequence 或者一系列的兩三個四五個phone 為單位
同樣我也可以用syllable 
那syllable 其實就是是一個phone sequence 就是一個兩個phone 或三個phone 的phone sequence 啊
這些些東西來做
那最明顯的一個好處就是他不不再limited by vocabulary 嘛
就是我們剛
剛才講的
我辭典典沒有抓到的時候我音抓到就可以啦
那當然你如果這邊如果是文字的話
文字我可以查字典
我知道它的音哪對不
不對所以我只要知道這個音我知道這這個音
我知道音是可以抓的到啦
但是當然因為你音抓到你可能因此會跑出更多的ambiguity 出來
這是會發生生的
你你這邊說是呂秀蓮
蓮但是這篇文章可能有張秀蓮還是王秀蓮怎樣的話
或者什麼什麼什麼修什麼臉臉什麼的耶
耶結果你可能也就抓出來來了
所以呢會跑出一些ambiguity 
是自然會發生的
但是呢它也有好處
就是說我通常這個個phone 的數目比較少
你用用word 的話可能你的你要幾萬萬個word 才夠
這裡可能phone 只有幾幾十個
你syllable 只只有一一兩千個對不對
那這時候呢其實我數目還比較少
我可以handle 一些個oov 痾這這是他最大的好處
那基本上就是是我用一個segment of 一個或幾個subword units 
那麼幾個phone 或幾個syllable 來做
這是所謂的subword based 
那當然你也也可以想像到這兩個其實是互補的
所以何不可以把它兜兜起來
我也用這個也用這個嘛
那用這個的時候候可以當我的word 能夠辨識出來的時候
候那他們他有它的好處
如果他不是oov 的話
它的意義明確比較不會ambiguity 
但是這裡的話呢就是你如果oov 的話呢呢我就可以用這個所以這兩個是是可以整合的嘛
那再來呢keyword based 
keyword based 其實是最古老的方法
你知道在還沒有google 的年代在還沒有網網路搜尋的年代
所有的文件就是給我一堆keyword 那
那麼每一個人寫一篇文章寫完的時候上面都會設幾個keyword 
然後就用這個keyword 去找那
那當然問題就來了就是誰來設這個keyword 
在古古代沒有網路搜尋的的年代
那就是user 去設嘛
所有寫文章的人寫完的時候就規定你要說好
你的keyword 是什麼
那就用那一個那
那當那當然今天時代不同了已經沒有人要求你做做一個網頁還要說你的keyword 是什麼
所以呢你的key 而網路上的東西隨時在千變萬化
所以你的你的keyword 最好是自動產生嘛
那你這時候就要要有一堆好的演算法
那麼為每一張網頁自動產生它的keyword 
才能夠做這件事
otherwise 你的你的你的這個keyword base 就不容易做
那那也因為這樣因為為網路東西是dynamic 的
所以你的keyword 就不太是fixed 
而是是變成是要會動的
是dynamic 的
那麼因此此呢它有它的難度
那這這是keyword based 
那真正的好的辦法呢常常其實這三個一起用嘛
我也用用words 我也用subword subword units 我也用keyword 
凡是有的都可以把它合起來
就是fusion 就就是把它的整合
就是hybrid 方法
那這些東西是我們講的我們所用的indexing 的element 
那我我真的做的時候呢
我可以用一個element 來做也
也可以用combination of more than one 
那combination of more than one 就是是我們剛才講的
譬如說我用三個phone 
每三個個phone 構成一個phone sequence 可以呀
譬如說這是bill 這是一個三個個phone 
然後elk 這是一個三這是一個三個phone 的sequence 
那麼然後呢這是一個三個phone 的sequence 我可以用這樣三個phone 的sequence 來做等等
那這就是這個combination of more than than one element 
這也可以做
然後呢那這一些就是我們所就是一般稱之為indexing feature 你真正用的feature 
也就是我們到時候把它表示成成這邊的feature vector 的時候
是用這樣的東西就是所謂的indexing feature 
來做這些事
那麼呢這個時候呢
這個痾當然你也可很多時候是pre define 
就譬如說每一個word 
在這這裡我每一個word 就是一個feature 
那你也可以是三個phone phone sequence 
連續的三個phone sequence 是一個feature 
這個都可以預先define 好
那當然還有一種呢是自動產生生data driven 
也就是說當你有夠夠多data 的時候
這麼多的文件
這麼多的
那我可以用data driven 的方式去抽
什麼是最有效的feature 
可以用抽的等等
那所所有的這裡的每一個feature 
他們通常在他們的領域裡面有一一個專有名詞詞叫做term 
所謂indexing term 
那就是一個這樣的東西西每一個這樣的feature 呢叫做一個indexing term 
然後後有了這一堆indexing term term 之後我基本上就是譬如說比對這一堆東西比
比對你如果你這邊有提到gates 
這三個phone 連起來的gate 
那如果你有這三個phone 的話你這邊如果也有三個phone 
我就找找找到你
那那這些你就再看他們之間間到底相似程度有多少
那麼給他一個多少的分數等等
那主要就就在做這樣的事情
那因此呢所有事情都是以這些indexing 的term 或者是是feature 為基礎來做的
那真正做的時候呢有很多多種不同的model 
那我們這邊列的是大概是最常用的一些model 
痾還有沒有列的
不過基本上呢最基礎最常見的最簡單而有效的就就是vector space model 
這待會底下我們就會講
那除此之外
這個latent sementic 就是我們在十二點零所說的那一個l s a 
那麼這是一個非常有有效的方法在這裡
那麼其實l s a 當初是為了這個發展出來的
那同樣的的也可以有很多其他的統計的模型我們後面也會說到
譬如說用h m m 來做也可以等等
你也可以再整合用不只一個model 來做hybrid combination combination 這都可以好
那底下呢我們就先來說第一個這個vector space model 
這也是retrieval model 裡面最基礎
最常見
見那麼最簡單也
其實它也非常有效的
那喔這個的基本精神就是我
我為每一個每一個document d d 或者每一個query q 
的representation 就是一一個vector 
那也就是說我們這邊的剛才這邊的每每一個document 的representation 那個d 
就是一個vector 
每一個q 就是一個vector 那
那一個vector 是什麼呢
就是我們這邊的用這一些個每一個indexing feature 
當成他的喔其實每一個type of feature 就有一個vector 什
什麼叫每一個type 
譬如說我們們剛才講如果以word word 為單位的話呢
這個word 是一個type 
所以我就為word 建一個vector 
如果我是以三個phone 為三個phone 的phone sequence 為單位
那我也為它建一個個vector 
那這個每每一個呢這叫做一個type of indexing feature 
那麼我們舉例來講假設說我以word 為單位
我以我如果以word word 為單位的話呢
假設gates 在我的辭典裡面
所以gates 是一個word 
那麼我我每一個word 我就有一個element 
就有一個dimension 
這是第一一個word 第二個word 
一直到譬如說我有五萬個word 
我就要五萬個dimension 
每每一個word 裡面有一個東西有一個component 
那這個時候呢呢假設這個gates 是這裡面的一個字的話
是一個word 的話我
我針對這個gate 會有一個數數字在這裡
等等那每一個word 都有一個
個那這樣構成一個vector 
是我們講的這個以這個type j 如果這個j 是word 的話
我就有一個word 
那我如果是以phone 為為為為這個element 
然後我如果是以每三個phone 的
的一個phone sequence 當成一個element 的話呢
那就變成說
我譬如說我的phone 有六十個
phone 有六十個
那三個phone 連連起來的sequence 
sequence 至少是有六十的三次方個
那這裡的每一個都有都有一個在這裡
譬如如說在這個case 
這個ga 後面接ts 
ts 那這是這三個phone 連起來
這是一個
然後呢譬如說這裡有一個
這三個phone 連起來這是一個等等等
那總共有這麼多個
個那每一個是一個element 
那這樣我構成一個這樣的vector 等等好
是這樣的意思
所以呢所謂的的每一個每一個type of indexing feature 
像這就是一一個type of indexing feature 
這就是一個type of indexing feature 
我都有一個vector vector 
那這個vector 裡面面的每一個
我們說每一個這個element 我會有一個component 有一個數值啦那
那這個數值是什麼呢
就是這個z 的j t 
那所以以呢這個j 就是我的type j 
t 呢就是針對某一個對對某一個indexing term t 
譬譬如說在這個case 這個個indexing term 
我們就叫做t 
所以這裡在這個case 而言我這個t 有譬如說五萬個
那這裡的每一個都是是一個t 
所以每一個這邊的那個這個這個東西就這邊講的z j t 
就是是這個z j t 
那同理呢那麼在這個case 的話呢
這一個這三個phone sequence 
這三個phone sequence 連起來的這個phone sequence 是
是我們這邊所謂的一個term t 
那這裡的一個element 叫做z 的的j t 等等
好那如果是這樣子的話呢
那麼這個z j t 等於什麼呢
最重要的就是這一個count 
也就是這個c t 
c t 是什麼呢就是就是這個term 的count count in the document 
如果這篇文章是在在講微軟什麼什麼裡面bill gates 出現了十次
那這個e 那個那個數字就是十
那你可可以想得到如果這篇文章是在講微軟是在講這個什麼麼的話
那很可能bill gates 會出現十次
windows 會出現現二十次
然後什麼dot net 會出現個十五次
microsoft 會出現一這個一百次等等
但是會有一大堆東西是沒有的
這是空的都沒沒有
好那麼因此呢這個feature vector vector 
基本上就已經描述了我的這個這篇文章
它裡面會說些什麼什麼東西喔
等等所以呢你就就是把它的裡面的每每一個word 把它count 好
那就他的這個那就是我的這個痾這個ct 就是我的的frequency count 我
我或者是用這個word 或者是用這個phone sequence sequence 都一樣我這樣得到一堆count 
那不同的人用的公式不太一樣
最多的人寫成這樣樣子
不過有的人不用log 有的人取log 
取log 沒有什麼特別的道理
只是有人做實驗取log 比較好就是了
那你知道取log 比較好的原因是說因為它是一一個數目太大的時候他會把他壓下來
它不是linear 上去的
那那麼因此呢當你數目多的時候不要增加那麼快
它有這個好處
那麼麼這個有的實驗顯示這樣比較好
那還有的人喜歡加一個e 有的人不加
那加個e 那也也是類似情形
那你可以想像的是
多的還是照樣會多嘛
但是呢原來如如果只只出現一次log 之後log 一變成零
那這裡都至少加了一就不會變變成零了之類的
那也是有人做實驗他比較好
那有的人不要加所以這個不不一定
那不管怎樣呢這個東西有個專有名詞就叫做做term frequency 
那你可可以顧名思義就知道就是指這一個term 
這是所謂的一個term 
這個term 的frequency 
這個term 的frequency 
那這是一一個非常重要的訊息說明這篇文章
這篇文章他哪一些個term 出現
哪一些個term 出現現表示這篇文章在說的是什麼東西
啊因為到時候我就是要要比對這些字有沒有比對這些word 嘛那
那麼因此呢這所謂的term frequency t f 
那還有另外一個東西也很重要呢就是後面這這一項
那這個東西是什麼呢
這個n t 呢是指total number of term of documents in the database 它包括這個term term 的
我們以這個gates 而而而而為例的話呢
我整個網頁一百萬篇篇裡面
講微軟的講gates 的會有gates 這個字的
有一千篇的話
那這個n t 呢就是一千
那麼這個n 是什麼n 是total number number of documents 
那麼因此呢n 是什麼假設我我總共是一百萬篇就是一百萬萬
那麼一百萬除以一千之後呢
那還有還有好多萬阿
那這個數字很大
痾對沒有錯
這這個數字很大就表示說這個很重要
那麼也就是說假設我我全部的是一百萬篇裡面
只有一千篇或者五百篇或者五十篇
有這個字的話
那顯然這個字出現顯示抓到這這些文章的能裡很強
所以呢這個是很重要的
那麼因此呢我用這個一除之後
那你可以反過來
來像我們之前提到過的例子
在這裡的話我們譬如說妳可以想到的
的的也是一個字阿
你如果的是這這裡的話
那在每一篇文章裡面都有
那這個就是沒有意義的字
你譬如說this 
這些function word 
或者at 
這些些個word 在這裡都是沒有意意義的
每一篇文章裡面都有的
那這些東西呢你或者說是它的音
譬如說this 這
這個phone sequence 在這裡的話呢
就是沒有意義的
因為它每一個裡面都都會有
那麼那麼因此呢
如果是這一些個東西的話呢
呢那麼我在一百萬篇裡面每一百萬篇都都有
一百萬除以一百萬之後是一log 一就是零
零那這個就沒有了
啊這個值就變成零了
那那麼因此呢這個等於是在在說到底我的所有的我的整個database 裡面我用一個database 去算
我就會算出來說那麼這堛漕c 一個term 它的重要性
那麼麼如果說是每每一篇都會有的的話呢那那這個就是一了
那這個這個log 一就是零就是沒有重要性
那反過來如果在一百萬篇裡面只有十篇有的
那這個顯然很重要
你有了這個幾乎就是要找到那十篇去了
那因此呢那這就是所謂的這個那這個值就會很大
那這就是所謂的inverse document frequency 
因為它把document 的數目做在分母母上面
所以是跟document 數目是相反的
所謂inverse document frequency 就是i d f 
那麼i d f 非常清楚的告訴我們哪一個term 比較重要
那這個東西跟我們在十二點零所說的另外一個東西西意思是非常像的
你如如果記得的話其實在那埵酗個很類似的東西是entropy 
我們在十二點零的的時候
我們數算這個的時候
這個term document 的的這一個term document 的matrix 裡面的那一個element 裡面
我也在數count 
這是非常像的喔
非常像的我也在數count count 
在數count 之後呢我再做一個entropy 的動作
這個normalize entropy 其實是相很像的意思
那麼麼我做normalize entropy 的結果也會使把這些個word 的效果降成很小很小
然後會把特別的word 的效果果拉的很高
好那意思是一樣的那
那只是有不同的作法
這個normalize entropy 是一個很好的作法
同樣的這裡的i d f 也是一個個這裡的i d f 也是一個重重要的很好的作法
所以這個是痾目的是非非常接近的
好那這兩個連起來就是所謂的tfidf 
在information retrieval 的領域裡面這是一一個非常常用的詞
所謂的tfidf 就是指這兩個東西相乘
那一個個代表的是它的count 
一個代表的是它的重要性
那這兩個一乘起來的結果就告訴我這個element 這個值應該要多少
好那如果這個有了的話
我現現在就是為每一個每一個document 
我為每一個document 我都建了一個這個這個個vector 
然後呢同樣的我也為query 建相同的一個vector 
我今天user 輸入的說他他要找的是什麼
他要要找的是microsoft 
那這個時候呢我也一樣為他建這個vector 
不過現在user 很懶他只輸入了了一個字
就是microsoft 
所以呢就是這個字出現了一次而已
別的什麼都沒有別的全全部都是零
但是這樣的user query 我也建一個相同的vector 
那這個做法跟剛才一樣的作法
我也是是不是只算一個count 我也要算i d f 
然後就是把這個t f i 都算出來之後呢得到一個數值
這是z j t 在這這裡
那這個個是我為user 所建的
user query 所建的
那user 也建了這個vector 
我這這邊也建了這個vector 
之後我這兩個vector 就可以做內積
因此呢我底下就可以做這個內內積
這裡的這個符號不對阿這個是電腦的問題
所以應該就是我就是為這這兩個vector 做內積
那做內積積之後其實再normalize 它的長度
就是這個這這個cosine 嘛你知道
就是這個這兩個vector 做內積
但是這個內積是是包含了它的vector 自己的大小
所以我要把大小normalize 掉
因為它也許這這篇文章長達十萬十萬個word 
那裡面這個數目就很多啦
那這篇文章只有二十個word 
這數目就很少啦啦所以這個大小顯然不對嘛
所以你要把它大小的normalize 掉
把vector 的大小normalize 掉之後
後剩下其實就是它的內積
也就是cosine theta 這個東東西
那這個值基本上是在一到負一之間嘛
那你可以知道他在在一附近的時候就是是他們最像的時候嘛等等
那零左右就是他們最不像的時候
那所以你只要用用這個方法來做的話這個
其實就是所謂d j q j j 就是q j 就是user 的query 
user 輸入的說我要找microsoft 
那這個時候呢user 的那個query 就是q j 
我對這個對user 做的這個que 這個vector 
那麼我每一篇文章所做的呢就是d j 
我就分別都去做這個內內積
然後看誰的內積大就就是跟誰接近嘛
那你可以想像其實的結果是說
因為user 只輸入這個而已
大部分都是零的
這邊全部都是零
所以這個內積結果其實就是把這邊的microsoft 
這邊如果這個字是microsoft 的話
這個不是零的那些文章就會抓出來
就會抓到microsoft microsoft 嘛阿
所以這個只要這樣內積一乘的話呢就會把你要的抓出來
那然後這個j 是我們們剛才講的type 
所以呢我譬如說這個個word 是一個j 
這個三個phone 連起來這也是一個j 
我有好好多個j 
我就把它們通通都每對每一種type 我都做一次次這個內積
都有一個分數
然後我更成上一個weight 把它全部加起來
那這樣子的話我就得到一個個total 的分數
那於是就知道user 要輸入的跟他跟誰比較像
那這個就是最基本的這個痾痾vector space model 的原理
那你在做文字的時候也是這麼做的
我們如果是是這個這個如果是文字的retrieval 的話呢
他也也也這樣做
那這就是retrieval 裡面最基本vector space model 
那我們不同的是說文字的時候呢
他大概不需要用這個phone sequence 這種東西
他就用這個word 就好了嘛好
但它它可以有more than two words 
譬如說你你應該是兩三個words 連起來是更好的term 
譬如說bill gates 
要bill 跟跟gate 連在一起那
那個bill 跟gate 連在一起的時候才是有意義的
否則一個bill 是很多bill 
bill 但是呢有gates 的bill 那就是特別的
所以你可以把兩三個個words 連在一起
所以他還是有可以有很多個不同的term 
但是它基本上文字只要要用word 來做就好了因
因為word 一定對
只有在語音的時候候才我要用像phone 這種東西來做
那在語音的時候除了用phone 這種東西來做做之外呢我其實還可以做什什麼呢
我這時候我的這個除了用count 之外
我還還可以用這個分recognition score 你辨識的分數啊
confidence measure 這個我們本來在十點零裡面講的
不過我們現在在十點零還沒講
所以呢不過你可以了解所謂confidence measure 意思就是我每一個辨識結果
我都可以以給他一個分數
當我辨識出來得到一個word sequence 的時候
我可以為每一個word 我每一個辨識結果給他一個個分數
譬如說在零到一之間
譬如說這個是零點九
這是零點七七這是零點三這個零點五
那給他一個分數的意思是是說我對於這個recognition 的結果的confidence 有多多少
那越接近於一的表示越可靠
大概是對的
那如果很低的表示說我雖然辨識出這個來我對他覺得他很可能是錯的
的那我可以用很多在辨識中間的各種的的information 
來算出這樣子的所謂的confidence measure 
那麼那麼等於說是我對每一個word 都可以給他一個我的confidence 
那你可以把這一類的東西
不論是這一種種的confidence measure 
或者是recognition score 
放到這裡面來也就是你在數count 的時候
不光是數count 
你每一個個count 可以把這些東西加起來
那他就不見得是出現一次不是一次他只有零點點三分
因為我我不太相信他
啊可以把這些算算進去
那這樣的話就變成語音的
所以語音可以用這個方式就可以這樣做了
那這這個所謂的vector space model 是是基本上的model 我們可以看成是這樣子的
大的vector 來這樣來算
並不表示你寫程式的時時候真的要這樣子寫
那為什麼呢因為你可以想像其實在真正程式在操作的時候
可能遠比這個要來的簡單
為什麼呢
因為user user 輸入他只輸入microsoft 一個字
所以以其實呢我所謂的這個user 這個q 的這個microsoft 
就是只有這個element 
別的沒有嘛
那因此呢當這個跟這個來做內積的時候
其實只是在算這個個的microsoft 裡面面的這個幾分然後這個跟這個個相乘就好了
別的根本就不要嘛
那麼因此呢你真的在寫程式的時候候你很可能是變成
我這邊雖然我每一篇文章的這個q 的
這個d 我要這樣建起來
來可是到時候我並不見得真的要做內積
我只要看user 輸入的那個是什麼之後我直接這個從那個element 去找這個element 就對了嘛
嘛所以呢真正做的時候不見得是是真的要有這麼多vector 在做內積就是了
好那那這個是基本的vector space model 
那要再進一步改進的話呢怎麼樣做的更好
最常用的第一個就就是我們之前提到的這個relevance feedback 
以也就是說你可以把第一次找到的結果feedback 回去
通常你第一次找到的最前面可能就是最重要的information 
你把它feedback 回去那
那一個最直接的標準的作法就是像這樣子那
那這是什麼意思呢就是假設說
user 輸入一個microsoft 那
那麼你找到的你你你得到的文章呢你會從會從第一名排到第一百萬名
那你可以把最前面的譬如說十筆還是二十筆還是五十筆還是五筆
你把最前面的這個呢叫做叫做這個痾這個relevant doc doc 最接近的relevant document d r 
然後你把這邊最不最不像的這堆呢叫做irrelevant 
就是d 的irrelevant 
那你如果這樣子的話呢你就可以把他們的那一些個的的d vector 加起來
然後呢加回原來的那個q 去
那什麼意思呢
這就是我們剛才講的user 通常很懶惰
當user 輸入一個microsoft 的時候
其實他要找的也許是bill gate 
也許是windows windows 
也許是dot net 
也許是office 或者是什麼什麼
那麼但是是你現在只找了一個這一個他就只會找microsoft 的啦
如果裡面文章裡面面沒有microsoft 這個字的話你那些什麼什麼都都沒有用
那麼因此呢比較好的辦法是
你如果把把用microsoft 所找到的最前面的譬譬如說五筆十筆二十筆一百筆
這些東東西的vector 加回去
你把這些個vector 加起來
乘上一個weight 加回去
那麼於是就會把你雖然一開始user user 只輸入一個microsoft 
你就會把bill gate 也加回去了
你會把windows 也加回去了
你會把這些跟他相關的都加進這個q 裡面
那麼因此呢你原來輸入的user 很懶惰他只輸輸入一個q 
那個q 只有一個microsoft 
但是呢你現在把那個最最前面的五筆十筆一百筆加回去之後
你就會把跟microsoft 相關的的很多word 
都加回去你的q 就會比較多東西了
好那麼那你這邊用一個weight 來來來加回去
同樣呢你也可可以最不相關的去扣掉
那麼這個效果比較小前面那個比較大
你把這個最不相關的裡面這個裡面的這個這裡面的vector 大概跟你你要的是最不像
你可以把他扣掉啊
所以這個也可以用減的那麼乘上上某一個weight 
那如果這樣的話呢我就可以把原來的q 變成一個新的q pron 
那這個q pron 顯然會比原來q 好很很多
那這個就是這個所謂的這個relevance feedback 
那這個做法是完全全自動的
所以這這是所謂blind 
那你就直接每一次的時候他自動可以做甚至於幾個iteration 
那麼這麼一來的話呢通常常就會把很多相關而user 沒有用的term 都加進來
那麼因此你會找到比較好的
那這一類的東的的作法呢有另另外一個名詞其實就是所謂的query expansion 
就是你怎麼樣把原來的query 把它expend 
因為user 懶惰所以user 用的query 是最簡單的
那你如何把它expend 
那底下這裡講的是你還有另這另外外一個也常用的辦法就是你有一個term association 
你如果為這裡的的所有的term 之間建立一個他們之間的association 的關係
誰跟誰是有關係的
因此呢你你的bill gates 跟microsoft 就是是有關係的
他們的association 是很高的
納麼因此呢當我user 輸入bill gates 的時候
我就把自動把microsoft 加進來
當我輸入microsoft 我就自動把bill gate 加進來
那這個個就是我在做這個term association 
所以呢這個講個就是用term association 來term association association 
來做這個query expansion 
那這個的的的基本精神是說ok 
那你就要有一個term association 的matrix 
一個association matrix 
說明明第i 個term 跟第j 個term 就是是ti 跟t j 
他們的關係到底怎樣
那你其實有很多方法來算
那詳細的我想我們這裡不講
但是你基本上你可以猜的出出來
哪個term 跟哪個term 會有什麼樣樣的關係你是可以算出他們的關係來的
有很多種算法那
那這裡講一種最簡單的算法
就是假設對我這邊講的假
假設他們如果出現
在同一篇文章裡面越多次
就表示他們關係越高那
那bill gate 跟這個microsoft 老老是出現在同一篇文章裡面
他們的關係就高
microsoft 裡面老是會有software software 那
那他們的關係就高
microsoft 裡面老是會出現windows 他們們的關係就高
那反過來呢microsoft 跟譬如說跟george george bush 有沒有關係
可能很少
microsoft 跟賓拉登有沒有關係
可能根本就不會出現在同一篇文章裡面
那他們的關係就很低
所以以呢你最簡單的辦法就是我直接算他們有沒有出現在同一一篇文章裡面的算他們的這個frequency 
那這個例子就是這這樣做
那裡面的我現在是如果算term i 跟term j 的話
就是t i 跟t j 
那麼第i 個跟第j j 個他們的關係如何呢
就數他們出現在一起的次數
那麼fi 就是這個term i term t i 出現的文章的總數
f j 呢就是term t j 文章出現的總數
那麼f i j 呢就就是i 跟j 同時時出現在文章裡面的文章的總數
那如果這樣一算的話這個就是這樣的意意思
他就是個介於一到零之間的值
你看什麼時候是一
如果i 跟j 永遠出現在一起
譬如說bill gate 跟microsoft 
假設他們永遠出現在一起
任任何一篇有了microsoft 就有bill gate 有了bill gate 就有microsoft 的話
那麼他們在總共有幾篇呢有一百篇
那fi 就是bill gate 一百篇
f j 是microsoft 也也是一百篇
f i j 是這兩個同時出現也也是一百篇
如果這樣的話呢最後這個是一百這個也是是一百
除起來就是一
所以如果這兩個永遠出現在在同一篇文章裡面的話
就得到一
反過來如果一個是microsoft 另外一個是bill gate 另外一個是賓拉登
好這這回完全沒有出現在一起
那這這個是零這個值就是零
所以他在介介於一跟零之間
那你有有了這個值之後你就可以做像上上面這類似的事情
不過我不用這個方法做我就直接把query 直接做expansion 
就是我這個query 就改一一改
我凡是以現在輸入的是這個term 這個query 裡面就是在講這個
user 只有輸入這個字
那這這跟term 跟誰有關的
他跟這個的關係比較多
他跟這個的關係比較多我就把這這邊也加一點分數進來
乘某一個weight 的比例
這個weight 跟他們之間的這個相相關關相關度有關嘛
嘛如果他跟他關係比較大
我乘一個比較大的weight 放進來
那關係係比較小我乘一個個比較小的weight 放進來等等我就把一個相關相關的東西放進來我我就得到一個比較好的一個query 
那就是query expansion 
by term association 
那把這些東西都做進來之後
那基本本上vector space model 是一個最簡單的model 
那效果也也不錯
啊那麼你基本上來講你如果自己做個寫個程式來做這件事情的話你
你只要做這個效果就不錯了
那當然要真的要做的非常好像google 那樣
那有很多學問
那像google 那樣他得把全球球所有的網網頁
他要在全球各地設sever 
把全球的網頁都都把他index 進來通通弄好之後
我可以瞬間找全球的東西
那當然那就很很有學問
但基本上你如果果光是講這個基本原理
其實是很簡單他只是這樣的而已
好那底下我們要講的是中文阿
阿那麼其實我底下要講的這一段就是我們在開學第一天給你看那那個demo 的時候
我們那個demo 系統怎麼做的
那基本上來講呢這個喔你記得我們那個時候就是喔我只要說我要找以
以色列阿拉法特
所有的跟以色列阿拉法特相關的的全部都可以出來
喔我只要找這個sars 疫情
喔所有跟跟sars 那些就可以出來
來那事實上效果是相當不錯的那
那個怎麼做的其實是喔我們就講在在底下這一段
段基本上主要的就是用syllable 
阿那在中文而言我們最好的單位顯然是像這種subword unit 
那是什麼呢是syllable 
那最最好是幾個syllable 連在一起
就像呂秀秀蓮阿等等
那這樣用幾個syllable 幾個syllable 連在一起的就像這裡幾個phone 連在一起一樣
用這種東西來做的話呢你你就可以得到類似的效果就是我們這裡所講的東西ok 
我們先停在這裡休息十分鐘
我們接下去講中文的怎麼做的那中文的怎麼做的我們要先說就是
中文其實本本身的喔文字的的information retrieval 也不一樣的
為什麼麼呢因為中文沒有word 
中文是中文的文字是一堆字
那麼你並不知道哪裡是一個詞
你需要去斷詞才知道說OK 
這是一個三字詞這是一個兩字字詞這是一個四字詞
這是一個單字詞
這是一個三等等你
你你得斷出來才知道哪裡是word 
那麼因此呢你在做比對的時候
即使是即使是文字的文字的的IR 
也文字的information retrieval 
像我我們這邊所說的這一些
他也有它的需要克服的問題
因為在英文裡面你可以用這些word 
用這些word 或者兩個word 三個word Bill Gates 要連在一起的時候才是一個是一個term 
啊你把兩三個個word 連在一起他就代表比較怎麼樣的意思
september eleven 
你要把這這兩個連在一起才代表某個意義等等
這個在中文的困難是是說
你現在根本本不知道詞在哪裡
然後你的斷詞不見得會對
那麼然後你可能重要的keyword 也一樣是OOV 
所以根本就斷不出來阿
那我們這邊舉幾個例子
就是說其實那你怎麼辦那你直接比對字好不好
好直接比對字也有困難為
為什麼直接比對字也有困難因為我們辭的結構是非非常flexible 的
我們舉例來講你如果要找李登輝
但是在文章裡面他可能是李前總統豋輝
那它的這個李跟登輝中間拆得很遠
那你得知道拆的這麼遠其實還是同一個
你得知道這這個李跟這個登輝連起來就是你要找的李登輝
那另外呢我們在中文裡面是是很長的詞自動縮短成為短詞
選哪些字呢
選最有意義的代表表性的字
而這是人自己做的
不需要解釋
所以說譬如說北部第二高速公路那
那這裡面呢就用了一個北跟一個二一個高
這三個字最清楚代表這裡面的意思
所以就得到北二高
那麼因此你如果user 輸入我要找北二高的路況
那網站上是說北部第二高速公路目前車多
那你要知道那個北部第二高速公路就是這邊的北二高
那事實上呢那那那這三個字憑什麼是從這裡面挑這兩這三個字出來呢
那另外呢在中文裡面很多詞中間換掉一兩個字其實意思還並沒有改改變
那麼因此你輸入的是這個我要找的是這個
其實可能文章裡面是講那個
那是要你要得知道得找得到
那當然這個這個其他語言翻譯過來的詞就更難了
我的字可能都不一樣
我所以我要做字的比對也會出問題等等
那麼因此呢在在這個中文裡面
其實要做這些事情你你要斷詞你可能斷不對
然後可能會這這個搞不好
就算你那你就就直接比對字的話呢也也是有問題所以他有它的難度
那當然斷詞本身是一個重大的問題
那這是一個我們們常舉的例子
就說你如果有一個人要研究腦科他上網去找腦科
結果找到一大堆腦科
但是其實這些腦科都不是腦科
是電腦科學
那麼你問題就是其實你這個斷詞的時候你你沒有做好斷詞的關係
那有一個人他要寫一篇文章關於台灣的民間信仰土地公
他就上網去找土地公
結果找到一大堆
怎麼有這麼多土地公呢
而且這些土地公都有政策
策那然後你就知道喔不是的
其實它是因為是他斷詞應該是在這裡斷開來的
那這是中文的斷詞的問題
那這邊講的都還沒有包括語音
這光是文字的時候
所以中文的google 
那它裡面有一堆中文的技術不是光是用英文的方法就可以做的
因為有這堆問題喔
所以呢那這個個有一堆問題有有有一堆方法去做的
那麼如果再加上語音的話
第一個問題就是語音辨識會錯嘛
所以語音辨識有一堆error
deletion or SUBSTITUTION insertion 的各種error 然後有OOV 嘛
那中文的OOV 是出奇的多
那你知道因為我們可以隨便湊成詞嘛隨便湊隨便湊成詞嘛
你譬如說這個終統
或者是個廢統阿
還是一個什麼你這個中文是因為英文每個字都是有意義的所以我們很容易把任任意幾個字兜起來變成我們所要的一個詞所以他的OOV 是 中文的OOV rate 是特別的高的
那麼而通常我們retrieval 你要找的那些個key phrase 
常常就是OOV 嘛
那我要找一個這個賓拉登相關的
我要找一個這個這個喔這個這個新光三越
或者是我要找一個什麼鼎泰豐喔
這個都是OOV 嘛
所以呢這個是中中文裡裡面的很多困難的問題
那怎麼解決這個問題呢那我們後來發現最直接而單純的辦法就是用syllable 
那麼為什麼是syllable 呢
那就跟我們之前講這個syllable 跟他們講的的phone 是很像
其實一個syllable 就是兩三個phone sequence 好
但是因為中文是一個每一個syllable 是一個字嘛
所以用syllable 是有很多好處的
但是呢不是光靠syllable 而是用很多層的syllable 
這一堆我們叫做做這個overlapping syllable segment with length N 
N 等於一的時候單一的syllable 
譬如說這假設我有我辨辨識出來它的他的文章
或者是你的query 裡面
有這十個十個syllable 的話
那麼N 等於一就是每一個syllable 各自都是一個term 
那麼你譬如說我今天如果如果我要找李登輝
那至少你在那裡有李有登有輝嘛
你如如果把每一個單一的syllable 都拿出來做indexing term 的話
至少有李有登有輝跟你這個是像的嘛
阿你就不一定要把它連起來看的話
單一的syllable 是有道理的
同樣呢你這裡北北部第二至少有北有二有高嗎阿
所以呢你這個這個我寫在這裡就是說
這個所有的word 都是compose by syllables 
所以他基本上你如果用單一的syllable 
至少你的OOV 都在裡面
那麼就像譬如說你你也許巴塞隆納翻成成巴瑟隆那翻成這樣子
但你如果用syllable 來看的話至少對了三個嘛
這三個syllable 是一樣的嘛
那那這裡的話至少syllable 會像嘛
那那所以呢你單一的syllable 是有這個好處的
我們說這個這個你你所有word 都是是syllable 所構成的
通常呢如果相關的word 
常常會有一些syllable 是一樣的
那那所謂相關的word 會有一些syllable 是一樣的就像我們這邊的例子
就是像譬如說這個跟這個
他們是兩個不同的word 
但是是耶
他們就是有一些syllable 是一樣的
就像這個跟這個
耶他就是有一些SYLLABLE 是一樣的
所以單一syllable 是有這個好處的
可是單一SYLLABLE 有個最大的問題就是造成ambiguity 
因為很多同音字嘛
每一個syllable 有一大堆同音字
所以你如果光靠單一syllable 的話是沒用的
的因為你說我這邊有李有登有輝所以我可以找到李登輝嗎
但是登有一大堆堆登阿
輝有一大堆輝阿所以你會找出一大堆其他的什麼登什麼登
然後輝什麼輝什麼
那麼因此呢你不能光靠單一syllable 
那怎麼辦呢
那就兩個syllable 啊
啊所以兩個syllable 就進來啦
就是譬如說這個一二跟二三跟三四
那就變成就好像
呂秀蓮
秀蓮副啊
蓮副總
副總統阿
這是兩個兩個或者三個三個
你如果是是兩個兩個就是呂秀秀秀蓮蓮副副總
總統
你如果兩兩這樣去抓的話兩兩做為一個term 的時候
那可以抓到一堆東西
那我你如果看這裡的話
譬如說你這邊可以抓到登輝
有登輝這兩個連在一起的話
那八成就是這個李登輝啦
啊你有二高抓到的話這個就是二高啦
所以呢你你如果兩兩兩連在一起的話實際上是是會抓到一堆的
那當然你也會抓到一堆error
抓到一堆noise 
譬如說蓮副這個不曉得是什麼
副總這個不曉得副總你可能變成另外一堆副總總去了
等等
你會抓到一堆error 的那
那麼因此呢那你還有別的嘛
那麼因此呢就是我還有N 等於三
N 等於三就三個
一二三二三四三四五嘛
就變成呂秀蓮秀秀蓮副蓮副總副總統
那你這些東西通通都拿來做嘛好等等
你可以N 等於一等於二等於三等於四等於五那
那我們實驗結果是
N 等於二效果最好
那原因其實很簡單就是中文裡面最多重要的詞是雙字詞嘛
雙字詞的比例最高而且最重要要的詞常常都是雙字詞
所以N 等於二效果最好
但是光靠N 等於二是不夠的
N 等於於一是有幫助的
N 等於三也是有幫助的阿
他們都有助於你要把它們通通用進來
那用進來的方法就是我們這邊邊講的剛才的這個weight 嘛
你N 等於二有一個內積N 等於一也有一個內積N 等於三也有一個等等
那分別用不同的weight 加起來
這個就是我們這邊講的你overlapping syllable segment with length N 
那這個時候呢就有一些poly selected word 
就是就是不不只不只是單音的而是雙音三三音四音的詞
你都可以抓的到
然後呢中文詞裡面最多的是雙字詞所以雙音呢是最有效的
那麼你如果抓的的到的話譬如說副副總統
有了副總統這三個音的時候大概不是別的東西就是副總統
因為就是說你如果是這個多音節的word 的話
基本上幾乎就是同幾乎就是抓得到了
雙音還不一定如果超過雙音的話好
呂秀蓮你有這三個音的話
大概就是這個呂秀蓮會找到別人的機會不大等等
雙音還是會有很多
譬如說你知道譬如說香蕉跟這個相交
所以如果說雙音可能還是很多同音的
但是三音以上幾乎就是你要的
那這些呢就構成我們這邊講的這一系列就
就是overlapping syllable segment with length length N 
N 等於一等於二等於三等於四五
那我們基本上是效果最好的是N 等於二
然後你可以加上一加上上三都可以加分
你只要weight 好
加上四跟五的時候太多noise 了
其實沒有什麼用了
加上四跟五大概不會再好多少
所以最主要是一二三
那還有另外一組呢就是syllable pair SEPARATE by M 
就是你跳一個syllable 
兩兩兩算
跳中間的一個
個譬如說中間如果M 等於一的話你就一三二四
一三二四三五的跳一個
跳兩個的話呢你就是一四二五這樣子
跳兩個跳一個跳兩個是幹麻的呢
那你可以想像的到
我這邊有一堆這種事情他就是跳了
譬如說北二高這是北部第二高
這個就是跳的
那同樣你這個這個中間這個syllable 不對
那你這邊跳出來是對的嘛
好你這邊可能這個賽這個是塞這兩個個不一樣
可是這邊可能跳過來就是對的嘛
所以你跳一跳之後事實上克服一些困難那
那同時呢那還有一個好好處就是說
我們的辨識裡面可能有錯
那譬如說你你如果有deletion 有insertion 
這時候你錯錯掉一個的話你跳的也算的話反而會對
呂秀蓮你萬一這個秀字沒有辨識對
呂跟蓮你如果那邊可以抓到呂跟連搞不好就是同一個嘛所
所以這個是有幫助
那這個我們的實驗結果是這這個M 等於一是效果再加上來來可以增是加分的
M 等於二也可以再加一點分
M 等於三以上就大概加不上去了
那用了這一堆feature 之後
那這一些feature 的理由就我剛才講就是說在這一些裡面
那就有幫助了
可是這樣呢還是有很多的問題
一個最大的問題就是中文的syllable 本身是很難辨識的
因為中文的的syllable 的錯誤率非常高
他們的con 非常confused 
為什麼呢你馬上想想到嘛
譬如說八跟搭是像的
八跟搭我耳朵聽都聽不清楚
那他跟他很像的是趴跟這個塌
那還有咖還有這個什麼喔這個很多很多那
那這一大堆其實都很像
那麼我我如果光是辨識這些syllable 的話呢其實錯誤是錯誤率是非常高的
那有阿同樣別的音也是一樣
譬如說晡跟督鋪跟禿跟哭的
都是非常像的
逼跟低
批跟踢跟key 
阿都是非常像的
阿那你這一堆東西都很像
所以其實我們syllable 的正確率是不高的
syllable 正確率不高可是我們為什麼可以辨識對
是因為詞
那也就是說呢你雖然這個八很難辨識
可是你如果有一個巴比倫
那就是巴比倫嘛對不對
你你就自動雖然說這個八跟這個搭跟這個趴很像
但是這個詞呢你有了這些東西連起來那就是巴比倫不會是別的
所以所以你你這個時候其實我們們中中文的這些個syllable 單獨去辨識syllable 正確確率是不會高的
因為他們有很多問題
可是呢那我怎麼會對呢我是要是靠詞的
那也就是說我們在八點零那裡面所說的那個key lexicon 
我的那個lexicon 
我把lexicon 變成這個tree 之後
tree lexicon 
那這樣的話呢我其實就自動自動的限制哪一些syllable 不會不會出現喔
這樣的話呢我才我可以得到很好的結果
可是這回不對啦我這回變成說是我要要抓syllable 
對不對
我現在要用syllable 來做了
那syllable 這裡面錯很多怎麼辦呢
這如果錯很多的話就不對啦
那所以呢這個辦法就是這邊講的我用syllable LATTICE 
用很多這個multiple syllable hypothesis 
譬如說你現在辨識的這一串syllable 
我每一個syllable 都把前五名放進來
因為它很可能不對
前五名裡面比較可能是有一個對的
就譬如說這五個這五個很像那
那我可能弄錯我選的第一名可能不對第三名才對
所以但是我如果選個前五名大概是正確就在裡面阿
所以我就可以把每一個syllable 我不是只取第一名
而是取譬如說前五名或者前十名
等等
那這樣我就變成一個這個syllable LATTICE 
syllable align 的LATTICE 
也就是說我其實就是align 好的
那可是如果這麼一來的話這個太多了嘛
那於是你譬如說兩個syllable 你這兩個也可以這兩個也可以這兩個也可以這兩個也可可以
這樣馬上就有這五個的話就有就有二十五種雙音的這個的太多了
那怎麼辦呢
那麼我們做syllable level 的utterance verification 
這是我們十點零裡面講的東西
不過基本上意思就是說你可以verify 一次
就像我們之前講的confidence measure 一樣
你這邊辨識前十前五名或者前十名
你每一個都有一個分數
就是我的confidence 的分數
那如果分數really 低的話我就可把它拿掉
所以呢我可以有一個用一個這個喔我就可以把這個confidence score 低的東西先拿掉
所以雖然說我每一個都取前五名
譬如說我這裡畫成我每一個取前五名
但是呢我把它的這個分數低的先拿掉
所以藍色的都拿掉
於是剩下要考慮的就少了
那在這個剩下的裡面呢
我才來算他雙音哪什麼什麼的
所以呢那這個就是
我建lattice 把可能的前五名都放進來
但同時我做了verification 
根據我的confidence score 
把不可能的先拿掉
那之後呢我還可以多拿掉一點
拿掉哪些呢
譬如說我可以用文字的語料去train 
你就會發現不是所有的雙音都會會出現
我們的syllable 有我們的syllable 有一千三百個
那雙音的話一千三百的平方
不是所有這麼多都會出現
那有些雙音是不會出現的
好因此那種就是所謂的低頻的term 
那那些低頻的term 我們可以先拿掉
就知道那應該是錯的嘛
那那譬如說這個是啊這個是這個低頻的
就是低頻的term 那麼就是低低頻的雙音
那這些如果果這個我用我用一個大的語料庫train 出來都都知道他們這這個雙音不太會出現的話
那這個顯然應該裡面有有錯
我就可以把它拿掉
好那這個是用根據這個文字的語料料去train 的
把低頻的拿掉
那還有呢就是把I D F 低的
那是特別高頻的
譬如說這個這個I D F 就是我們之前講的
這個I D F 
那這個這個這個如果分數低的話表示他這個term 沒有什麼意義
譬如說非常
這個雙音非常沒有什麼意義
因為它只是一個function word 
那麼在每一篇文章裡面都可以有這個東西
他不代表什麼好
那你譬如說這個今天
這可能也不代表什麼
那這個也是你在每一篇文章裡面都可以有的
那這種就是I D F 非常低的word 
那非常低的雙音
那我可以把這一些個這個I D F 非常低的那些個雙音我也拿掉
阿像這些
當我這些都拿掉之後我剩下這些就是reliable 的
而且正確的應該在裡面的
那用這個來做就會好很多等等
好那這個大概講了我們中文怎麼做的
痾你如果有興趣的話詳細的寫再蔗一篇裡面就是這個第二篇喔
這篇其實是非常完整的paper 
裡面講的非常清楚阿裡面每一步怎麼做我們都講的很很清楚
你如果自己寫程式你even 可以做出這個來
這個是作得到這
這個都寫的非常清楚這篇是保證很好看的阿
這是講中文怎麼做的
那麼再下來的話呢我們來講一下其他的
剛才基本上是用這個喔講的是vector space model 
但是當然我可以不用VECTOR space 
那可以用別的
那第一個呢我用h m m 
那麼用h m m 的話呢喔其實我們中文也一樣
也可以用syllable 來做這個h m m 
那這個是怎樣呢
基本上就是我現在把user 的query 
看成一個SEQUENCE of input observation 
然後呢我每一篇文章看成是一個h m m 
那你記得我們原來h m m 是幹麻的
譬如說我每一個我要辨識零到到九
我零有一個h m m 
一有一個h m m 
到九有一個h m m 
然後呢這個聲音進來的是八
那這個聲音我把它放到每一個h m m 裡面去看他的分數
喔結果如果這個聲音是八的話呢
那個八的那個字的那個h m m 分數就會高
這個是我們原來h m m 的作法
這個h m m 是非常有用的東西我們可以拿來做很多別的事
譬如說在這裡
那我現在變成不是這樣子了
我現在是每一篇文章是一個h m m 
每一個文件譬如說這個文件是一個h m m 
這個文件是一個h m m 
這個文件每一篇文件都是一個h m m 
那user 的query 是一個observation 
好所以呢user query Q 
那裡面是一堆term T one T two 到T N 
那這個是observation 
那我現在要把這個observation 放進每一個h m m 裡面去看他的分數
跟那個完全一樣
那你這個user 的query 看成一堆term 
那就是這邊的意思嘛
你可以用譬如說word 
你可以用word 
你可以用用phone 
用phone sequence 
這些東西就可以當成你可以所以在query 裡面找他的words 
或找他的phone sequence 什麼什麼的這些當成他的term 
然後呢那你用這個query 就變成一串term 
那麼這串term 這一串的term 你把它放到這些h m m 裡面去
看他的分數
那這裡面有一點不同的地方
就是那在這裡做的時候
到目前為止他們用這個方法來做用h m m 來做這個retrieval 的話呢
大概每個h m m 都只有一個state 啊
應該說還沒有人想出來兩個state 怎麼做
所以都是一個state 
所以呢在這上面的時候呢每一個有好多個state 
那這邊呢每一個都
只有一個state 
那這一個state 的時候呢這個放進去算他的機率怎麼算
我們在這裡的時候是怎麼算的呢
這個這個state 裡面機率呢我們把它變成一堆Gaussian 對不對
變成OK 這有一個Gaussian 這有一個Gaussian 
我把它想像成是一堆Gaussian 的組合
那麼於是呢我用一堆Gaussian 來model 
於是呢譬如說呢這個state 我就把它看成是這樣的一個一堆Gaussian 
於是呢我這個observation 可以放到這個裡面去看
算他的分數
那我現在這個怎麼辦呢
這裡面是是這個喔phone 或者是word 
或者是什麼那種syllable 這種東西的話呢
我怎麼辦呢
我這邊沒有辦法當成什麼Gaussian 
但是我這可以什麼呢用N gram 
所以呢我這個變成是用N gram 
用N gram 來做成這個機率
所以這個方法這個這個妳基本上來看
他的整個的觀念跟上面是完全一樣的
只是呢其實我們剛好全部都反過來
你原來的這些個h m m 裡面的這每一個feature 都是acoustic signal 
我現在的每一個feature 都是linguistic 的unit 
都word 還是term 還是phone 
都是DISCRETE 
不像這邊是continue 的signal 的
原來這些feature 是acoustic signal 
這邊都變成linguistic term 
那原來這裡面用的我的N gram 
在這裡跑跑到這個機率裡面來了
所以我的N gram 原來在這裡面的時候是來算那些個term 之間的關係的
我現在的N gram 變成h m m 裡面的東西了好
所以這個是倒過來
但是事實上所有的觀念都一樣
因此呢我現在可以用這篇文章我可以算這篇文章的N gram 
於是這個於是這些term 進入這個N gram 就可以算他的N gram 機率
uni gram 就是這一個一個的機率
bi gram 就是兩兩相連的機率等等
我就可以算這些這個N gram 就當成這些東西阿等等
那基本觀念就是這樣
那就是這邊所說的就是說我把query Q 看成是一個一個sequence of of observation 
也就是一堆term
然後呢每一個document D 呢看成是一個h m m 
只是我現在H h m m 都只有一個state 
那麼沒有人還沒有人想出來兩個以上state 會怎樣就是了
然後呢那這個h m m 裡面那一個state 裡面我沒辦法做那什麼Gaussian 什麼沒有這這種Gaussian 那怎麼辦呢
就是N gram 
所以呢就
就是由N gram 來兜兜成的
在這裡面所舉的例子就是用uni gram 跟bi gram 喔
這個是用用這篇文章的用這篇文章的文的文字可以train 出他的uni gram 跟他他的bi gram 
這個就是他的uni gram 跟他的bi gram 
我就是用這篇文章train 的
那就是這個uni gram 跟這個bi gram 
但是通常因為這一篇文章
文字量實在是很少
你train 出來的uni gram 跟bi gram 可能不夠好所以呢
你再加一個這個是用一個比較大用一個大的corpus 
train 的比較標準的uni gram 
跟bi gram 加在一起喔
所以呢我再加一個用這個用一個大個corpus 所train 的一個uni gram 跟一個bi gram 
那就是後面這是given C 的這個就是這個大的corpus 所train 的uni gram 跟bi gram 
等等你也可以有有有這個tri gram 什麼也可以
那其實你看uni gram 是什麼uni gram 也就相當於會不會出現某個term 的意思嘛
所以uni uni gram 
gram 其實也就是在算這邊的每一個term 他出現的count 嘛
是一樣的
那bi gram 就等於是算他的sequence 
對不對兩個term 連在一起出現的這個count 
其實是一樣的東西
那如果是這樣的話呢我們以這個為例的話
我們這邊就是用了用了用了兩個uni gram 
這個是這個文件自己的
這個是用一個corpus train 的
兩個bi gram 
這個是文件自己的
然後呢這是corpus train 的
那我就變成這四個機率相加
各有一個weight 
M one M two 這是他的weight 
這四個機率相加這四個四個N gram 
就好比這邊有四個Gaussian 是一樣的意思
我現在是變成是用N gram 來做就是了
那如果這樣的話我的retrieval 我也可以用M A P 
那這個意思這個就是M A P 的意思
就是說我given 一個OBSERVATION 
我的Q 
Q 就是user 的輸入嘛
就跟user 的輸入聲音是一樣的
我user given given一個user Q 
那我就可以去算說某一篇文章是relevant 
這個意思R 就是relevant 的意思就是相關的
那麼那麼他的機率是多少
然後呢我對所有的每一篇文章都去算
那如果這樣的話呢我看哪一個最大
最大就是我的答案
這個就是MAP 嘛
所以呢我就可以用這個方式given 這個Q 咳
given 這個Q 
那我就可以算每一篇文章的相的的可能的機率
那當然我不一定要只選只選最最大那一個
就這裡而言 我們可以選最大的一百篇
或者最大的五十篇等等
那這個就是這個就是我們平常講的這個retrieval 的方法
可以這樣子來作
那當你變成這個的時候呢一樣的就是MAP 的這個這個機機率我不會算
我把它倒過來
所以呢我把它倒過來變成
如果是講這篇文章的話那麼會看到這個Q 的機率
如果是這篇文章的話
我有這個N gram 在這裡
given 這個N gram 
我會看到這個Q 的機率是多少
所以我就是算這個Q 用這個N gram 來算機率
就跟把它放到這裡面來算機率是一樣的
所以呢我只要倒過來之後我就可以算
given 這個N gram 
那麼這個Q 的機率
那應該還要乘上這個機率
喔這個只是我們這個MAP 裡面你知道MAP 我們每次把它倒過來
然後其實應該還要除上一個機率不過那個機率不算
反正是一樣的所以可以不算
但是現在是這個機率也無法算
這個機率因為不知道是什麼
所以我們也不算就算這一個
如果算這個的話呢
那這個就變成MAXIMUM likelihood 
因為這是一個likelihood function 
把這個倒過來了
這個是MAP 
這個變成MAXIMUM likelihood 
當你變成這樣之後呢
那其實就是那這個其實就就是把那個Q 放到這個個D 裡面去
把這個Q 放到這個D 裡面去放到這個N gram 你算這個N gram 的意思
當你算他的N gram 那就是底下這個式子
底下這個式子沒什麼特別
就是在算uni gram 跟bi gram 
也就是說我現在如果我現在如果是這個痾T one T two T 三T 四
的時候到T N 
這是我的user query 
變成一串term 的時候
那我第一步呢第一個要算他們這這個T one 的uni gram 
那就是這一個這T one 的uni gram 
他在這篇文章裡面自己的uni gram 
以及在一個比較大的corpus 所train 的那一個uni gram 
然後從二以後呢我前面這兩個是uni gram 這兩個是bi gram 
二開始始呢我也算他的uni gram 
我也算他的喔他的bi gram 
也就是前面這個來的
所以呢二的時候我有他的uni gram 
有他的bi gram 
那那就是當N 等於二的時候
這兩個就是uni gram 然後這兩個就是bi gram 喔
那這是我的document 這是這篇文章自己的
這個是我整個的corpus 的bi gram 
然後三的時候呢那當然三也有他的uni gram 
也有他的bi gram 喔等等
那你可以再加tri gram 等等再一直加到N 就是了
那你這些個這四個N gram 
你有他的weight 
那這個weight 可是可以train 出來的
你用EM 也可以train 你用M C E 也可以train 喔
那當然要train 你要有一個標準答案
就是說這些是query 這些是找到哪些文章
那有個標準答案你就可以train 這些東西
那這些都都可以train 出來
那這就是用h m m 來做
那我們之前講的用syllable 一樣可以做這件事
我們都做過用syllable 來一樣建這個這個model 
那這時候就是我每一個這裡的每一個term 變成一個單一的syllable 
雙音兩個syllable 三個syllable 就
就在這裡
或者跳音都在這裡弄
那這樣的話呢我也可以做用syllable 來做這個這個h m m 的retrieval 
那再來呢這個呢其實就是我們第十二點零講的latent SEMANTIC 
那這一塊其實跟我們十二點零講的沒什麼不同是一樣的
那本來十二點零的那一招就是當初人家發明他的時候就是為了做這件事情用的
那那他的point 在哪裡呢
它主要的point 應該是這是所謂的concept matching 以別於所謂的term matching 
這個這個意思是說就是我們之前講的
因為user 通常是懶惰的
user 只會講一兩個term 
他要找的東西可能不是那一兩個term 所能描述的
就好像user 他就他就輸入一個Bill Gate 
輸入一個Bill Gate 
但是事實上他要找的東西裡面可能包含他要找的可能包包括microsoft 
包括windows 
包括這個什麼dot net 
這個等等等等他有很多
甚至於有intel 啊什麼什麼
user 要找其實是很多這些東西
但是呢他可能只輸入一個Bill Gate 
如果他只輸入個Bill Gate 的話你那文章裡面必須要有Bill Gate 
否則就找不到
那這個就是所謂的term matching 
因為我們之前講的都是這樣做的
你如果看這個的話
其實是在match 這個term 
所以你如果沒有講到他的話
你沒有輸入那個term 他就是沒有嘛
你如果用前面的這些個都一樣
這些都是在term 做matching 
包括vector space 這些都是用term 在做match 
都是這個所謂的term matching 
但是你現在如果改用改用LSA 來來來做的話
就是我們這邊講的所謂的latent SEMANTIC indexing 
那這個其實就是我們十二點零所說的那一招
那也就是說我們十二點零的時候呢
我們把它把它變成這個這個這個term 跟document matrix 之後
你記得我們做了這個SINGULAR value decomposition 之後
轉成一個譬如說八百維的空間
在這裡面的每一維變成concept 了
所以這個時候當你每一每一維變成concept 之後
我現在其實在做的事情是把這些文章都轉到這這個空間來變成concept 
那麼因此你如果記得的話
我們當時就有譬如說這個我的我的這個fold in 
我可以把user 的query 當成一篇文章
把它放進來
於是呢我我把它放到這裡面來之後
我是在這八百維裡面
去看它是那一個concept 
那這個時候它裡面會有出現哪些term 就不重要了
他的那個concept 
很可能就是這一堆所構成的那個concept 
那你裡面講的究竟是講的Bill Gate 還是講的microsoft 其實不重要了
他都會幫你map 到那個concept 去
所以呢他就有concept matching 
他等於是針對concept 來做喔
這個是LSA 最大的好處
那這是我我們十二點零講的這堆東西
其實最早是用來做retrieval 之用的
那只是說這個後來人家拿來做language model 而已
那麼因此呢我們是可以回過頭來看
用這個的話
那就是一樣這個是跟我們十二點零講的完全一樣你就建一個term document matrix 
然後做SINGULAR value decomposition 
然後那你現在怎麼找
很簡單就你把user 那個query 
把它當成是一篇新的文章把它放進來
把這個user query 當成一篇新的文章
把它放進這個空間來
然後他也有一個vector 
於是呢你拿那個query 的vector 去跟所有的文件的vector 去做內積
那這樣你就可以找到它跟哪一篇最像
而這個像是在那個concept 的空間裡面像
不一定是term 像不像而是concept 像
所以這樣這個就答案就出來喔
那這個我想細節我們就不用講
你大概就知道這個就跟我們那個是完全一樣的
那麼再下來其實跟這個相關的有另外一堆
我們待會在說我們先把這個講完
就是這個keyword 
keyword base 是另外一堆
那麼基本上呢你可以想像就是
我們說你如果一堆文字的文字的文件的話
我要有一套演算法去自動抽keyword 
我如果有辦法自動抽keyword 的話
我就可以得到一堆keyword set 
有了keyword set 之後我們就可以做keyword spotting 
這個是在十點零裡面我現在還沒有講
我們盡快講到這個keyword spotting 
那你知道所謂keyword spotting 就是user 講了講了一段話
我不要辨識整段話
我只要看它裡面有沒有我的keyword set 裡面的某一個keyword 
我我如果有keyword set 裡面某一個keyword 我把它抓出來
那就是所謂的keyword spotting 
所以呢我今天如果我有辦法把每一篇文件自動抽他的keyword 
放在這個keyword set 裡面的話呢
我只要user 輸入什麼話我就去抓裡面的keyword 
我抓得到的話
我就說他有這個keyword 
那我就可以回去說耶剛才這些裡面是那些文章有這個keyword 
那就找出來了喔
那這就是keyword base 的方法來做這件事
那這個自動抽keyword 是基本上是用文件文章的文字的比較容易抽keyword 
你如果文字不容易的話如果是語音的話是比較難
語音的話呢比較常用的辦法是我去做個recognition 
做了recognition 之後呢變成文字之後再來抽
但是這個時候OOV 啊什麼已經已經掉了
所以呢如何如果是語音的文件
要如何讓裡面的keyword 出來這是蠻有學問的事情
這可以做我們今天都有技術阿
但是呢我們不見得有時間來說它
但是你基本上可以想成就是我得要有好的辦法
我不是是直接做recognition 
否則的話很可能OOV 全部都丟掉了
所以我要想辦法讓這裡面的keyword 能夠抓出來等等
底下這一章是在舉一個例子講
中文的文字怎麼抽KEYWORD 
那麼中文的文字面跟西方語言最大的不同
就是你even 不知道詞在哪裡
那你怎麼知道keyword 在哪裡那
英文而言的話呢他的keyword 不會太難抽的原因是
第一個就是你有boundary 
你所有的空白就是boundary 
你已經知道這是一個空格這是一個空格所以這是一個word 
這是一個word 每一word 都define 的很清楚
就是由這個boundary 所define 的
然後呢所有的專有名詞都有大寫
那麼所以呢通常的專有名詞都是大寫
有大寫的轉有名詞常常是keyword 
所以你先把這個有專有有專有名詞的大寫抓出來啊
有大寫專有名詞抓出來那你八成那個keyword 已經抓到一堆
然後你再用什麼TFIDF 那些東西一算的話
你那keyword 就可以出來
那中文的問題是說你它是一串字
你不曉得詞在哪裡
然後一大堆OOV 
所以你怎麼知道哪裡是一個詞
然後他該不該是一個keyword 
這就比較難
那這有很多特別的方法我們這邊並不細說
這裡舉一個舉一個例子
那這個例子是說你怎麼知道這幾個word 應該是一個keyword 呢
你先要看它是不是一個完整個pattern 
要看它是不是一個完整的pattern 都不容易
那在這個例子而言它是說OK 
譬如說這個是呂秀蓮副總統
那麼你拿掉最後一個字
或者拿掉兩個字
呂秀蓮副總
或者者呂秀蓮副
你會發現他不會單獨出現
他出現的時候一定是在呂秀蓮副總統一起出現的等等
那這樣就表示應該是這整個才是一個term 
或者是一個word 
而不應該是拿掉一兩個字的等等
那就是這邊講的就是你這個W S 如果always appear as part of W 的話
或者你前面拿掉一個字
呂秀蓮副總統跟秀蓮副總統
那你沒有這個秀蓮副總統你每次就是一定有呂的
那這樣的話呢那這個時候呢就應該是一個完整個term 
應該是這整個的等等
那另外呢你就是你除了這個之外你還要看後面會接什麼前
前面會接什麼
那麼也就是說如果它是一個完整的term 的話
後面應該會接不同的東西
那前面也會接不同的東西
那如果說他們後面老是接相同的喔
喔就是說如果你W 後面永遠接一個固定的東西的話
那他們可能是連在一起的
譬如說行政院長游錫坤
那你你如果發現他老是連在一起的話
他可能應該是連在一起才是一個term 
那反過來當然會不一樣
譬如說自動化
或者說OK 合法化
合法化的那個化你會發現合法自己常常出現的話
那麼合法跟化就應該拆開來合法才是一個term 
所以這裡面有一堆這類的問題
然後你看哪個是個term 
然後你再來看他重不重要好等等
那這個這是中文的key term 的的keyword 裡面有一些相關這類的問題
好那那我們這邊先把這個大致先說到這哩
那這個是我現在這個十四點零的部份
那跟這個相關的有另外一件事情我們在這裡順便提一下
就是我們剛才講這個L S A 
用這個十二點零的L S A 的時候我可以做這一類的concept matching 
那跟這個很像的就是加機率
那這個在我們十二點零的時候我們就提過這件事
在十二點零的最後一頁的時候
我們說過就是就是像像這個十二點零都是在講把這些term 
跟這些個文件之間
想辦法去找中間的那八百個concept 
那其實呢這個方法不算是最好的
因為他用用一大堆matrix 
但是它沒有機率的沒有太多機率的觀念
那到後來的時候就有了這個它就有一套全新的formulation 就是所謂的probabilistic L S A P L S A 
那他就重新建一套完全是意思還是很像的我這邊就是所有的文件
那邊是所有的term term 或者所有的word 或者什麼
term 啦就是我們這邊講的所所有的term 
或者所有的indexing element 
那麼你中間是什麼
是你真正的topic 
那你可以建這中間的的機率關係
因此呢對每一個文件而言呢
假設我這邊有八百個topic 的話
我每一個文件呢你可以分析說它是它是在講哪一個topic 的機率
都有一個機率
那對每一個topic 而言呢
每一個term 會出現有他的機率
那麼於是呢我這個時候某一個文件裡面會出現某一個term 
不是在這裡數他出現幾次的count 
而是算成一個這樣子的機率
就是這個文件他會講哪一個topic 的機率
以及在那個topic 裡面這個term 會出現的機率
然後呢我把所有的topic 加起來把八百個全部加起來
那這樣的話呢我可以拿來跟我真正數count 的時候
這個裡面的有多少我來比比看
那我可以maximize 這個likelihood function 
做為來train E M 的model 的方法
所以呢我用這個我要maximize 這個東西來用E M 就可以train 出這個model 出來
那這個東西的的的精神跟我們之前講的第十二點零講的這個這些東西是非常像的
可是他這個時候呢我就完全用機率方法來model 之後呢
我就可以用機率的各種方法來做它
那麼這個效果呢terns out 是比L S A 還要好
那同樣我這個可以做這裡的我可以做這裡的的的
這個retrieval 完全可以拿來做嘛
因為你你現在這個user 輸入一個query 
你也一樣可以把那個query 看成是一堆term 
然後那你也可以因此可以算他是他會在哪一個topic 裡面的機率等等等等因此
此你的都可以這樣算所以呢這些東西可以完全拿來來做做我們這邊講的retrieval 
那terns out 這個效果比L S A 通常是會好一些
這個到這裡呢相當於我現在把這個痾十三點零講完
十三點零我們這邊的這個reference 裡面
我們剛才講
因為information retrieval 是一個很大的領域
那最多的資訊來源應該是A C M 的這個special interest group on information retrieval 的這裡面
它每年有很多paper 
那麼然後呢底下這一篇是就是用H M M 來做的原始paper 
就是這個A C M sig I R 就是這個的就是這個
那底下這兩個都是這裡面的嘛
那咳那這個是用H M M 來做的
這個就是probability 
用就是我剛才講的用probabilistic 來做的L S A 的
那這兩個都是在他的原始paper 都出現在A C M sig I R 的一九九九年
那當然你現在看的時候這兩個原始paper 不見得是最好看的
那在這個後面都還會有好多篇的paper 
會寫的比較完整比較好看的
那你如果要看的話可以再從後面去找阿
這是講十三點零好
