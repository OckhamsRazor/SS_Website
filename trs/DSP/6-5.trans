那它是語言結構最單純的
那這個例子大概有一點感覺我們再回過頭來剛才上一頁
還有一樣東西沒有講
就是這個cross entropy
那麼
這個倒沒什麼特別這個只是在你去讀這些書的時候
你如果讀前面我們講的這些這些書裡面不管哪一本它都說
這個language model 的perplexity 是一種cross entropy 
它們都說它是一個cross entropy 
這個perplexity 為什麼是一個cross entropy 呢這點不太容易了解
所以我們這邊解這一頁其實是在解釋說perplexity 為什麼是一種cross entropy 
那麼我們在禮拜天補課的時候曾經說過這個
所謂的cross entropy 的定義是一個這樣的東西
也就是我有兩個distribution 一個是p 一個是q 
然後我p log p 除以減掉p log q 的這個東西
通常是大於零的
這個東西叫做cross entropy 
它是p 跟q 兩個distribution的一個
它是p 跟q 兩個distribution的一個distance measure 
或者叫做k l distance
這是我們上次所說的
那麼在那個情況之下呢我們也說過這個p log p 比p log q 大
我們可以把它寫出來一個是p log p 一個是p log q
那這兩個東西寫起來就變成這樣的一個式子
這個式子就是所謂的jason inequality 
那這個也是我們上次所說過的
那這個也是我們上次所說過的
那我現在要講的是說
它們課本上說這個perplexity 是一種它們說perplexity 是一種cross entropy 
這句話所講的cross entropy 不是這個cross entropy 
是哪一個是這一個
喔
也就是說
這個什麼叫做cross entropy 其實不同的人它的取的名字講的東西不太一樣
所以你不要被confuse 了
那麼很多人講的cross entropy 是講這一個
這是我們週末補課的時候講的是這個
但是呢也有很多人把這個叫做cross entropy
就是p 跟q 之間的p 跟q 之間的差異
p 跟q 之間的差異
那麼
這個叫做那我我這邊為了區別起見我把它寫成x 的
那麼這個叫做那我我這邊為了區別起見我把它寫成x 的p 跟q
這個x 也是cross 的意思嘛
那那麼有的人是把這個叫做cross entropy
那它們課本上所謂的perplexity 是一種cross entropy
是指是一個這種東西
那我們底下要解釋的是為什麼這個東西叫做
那我們底下要解釋的是為什麼這個東西叫做為什麼這個東西就是我們講的perplexity
那麼這個東西是什麼就是這我們禮拜六補課講的
就是說
就是說它是一種entropy
當p of x 是被當成q 了弄錯了的話
它整個的entropy 會變大對不對
我本來是算p log p 的
但是假設這個p 不是正確的p 是變成q 的話
它會變大
那那個變大的那個差異叫做cross entropy
那這是它們的這個定義
那底下我要講的事情是說
我們剛才講的這個perplexity
我們剛才講的這個perplexity剛才講的這個perplexity 其實就是一個這個東西
所以它們說它是一個cross entropy
為什麼是這個東西呢
因為它其實就是一堆
因為它其實就是一堆log 的平均值
喔
那這個式子其實就是我們上一堂課講的
你看我們上一堂課講的這個perplexity
這樣算的式子是這樣寫的
這是什麼就是一堆p
這是什麼就是一堆p一堆probability 然後這個取了log 之後全部加起來平均嘛
對不對
一堆probability 取了log 之後全部加起來然後全部平均嘛就是這個式子嘛
那跟我們現在寫的這個式子是完全一樣的
一堆probability 取了log 之後全部加起來然後平均嘛
對不對
喔
所以這個式子跟剛才是完全一樣所以我們那個perplexity 可以說就是這個
那其中的這個q 就是我們所說的這個language model 的這些n gram 的機率
就是我們這邊所講的q
那我其實這些perplexity 的這個東西就是這些q 的全部加起來平均
那如果是這樣的話
那麼我們要說呢這個其實可以看成是p log q
也就是一個cross entropy
為什麼呢
根據所謂的law of large numbers 
這個名詞你一定是熟悉的
你在小的時候就偷這個
機率還是統計什麼就學過所謂的law of large numbers
什麼是law of large numbers 呢
我們說假設有一個random variable 
你做了很多的experiment 去測它是多少
你得到一堆值
譬如說我們說它的值
跟它的次數
假設說是投一個擲一個骰子或者什麼東西
我的值是a one 的時候
我總共得到幾次呢總共得到n one 次
值是a two 的時候我得到
n two 次
等等值是ai 的時候我得到n i 次等等等等
我總共做了
n 次實驗
我做這麼多次實驗加起來有這麼多個n 次
假設是擲一個骰子這就是一二三四五六
各有多少次當然也可以是其它的各種random variable 的實驗
那麼這個時候到底平均值是多少呢
那麼我們的算法可以是這樣算就是summation 的ai n i
然後再除以n 
對不對
這就是平均
我就是把ok 得這麼多的這麼多次
那麼我就其實就是把全部乘起來
然後全部加起來
那總共有大n 次我就除以n 這就得到平均值
那這個平均值你可以有另外一個寫法
就是把它寫成
這個summation 的
嗯ai 乘上n i 除以n 
那這個其實就是機率pi 
也就是說
發生a one 有幾次是n one 除以n 的這樣的機率對不對
發生a two 是n two 除以n 等等
所以這個就是這個pi 就是發生
值是ai 的機率
我就每一個值乘上它發生的機率乘起來不就是平均嘛
我就可以算成這樣
那所謂的law of large numbers 是說呢你做了夠多的實驗之後
那麼這個limit
n 趨近於無限大的時候
這個東西會等於它
當你實驗做的不夠多的時候這個這個機率不準嘛
實驗做的不夠多這個這個不準你不見得是
等於但是當你實驗做到夠多次的實候n 趨近於無限大的時候你就可以用這個來算
這個叫law of large numbers
那這個應該很容易沒有問題你如果這個了解的話那我們這邊講的事情是一樣的事情
那我們說左邊的這個式子其實就是在
就是我們剛才算的perplexity
就是在把所有的機率加起來
那相當於就是
在我這個情形就變成是
a one 加a one 加a one 
總共加幾次加n one 次
再加上a two 加上a two
總共加了嗯不是小
n one 次總共加了n two 次
等等等等
這個a one 加了這個
a one 加了n one 次其實就是
就是a one n one
這邊都寫錯了這都是小寫
小寫這樣比較不會弄錯這是
這樣子
那麼你如果說這個
對不對你現在如果a one 總共
發生了n one 次你就全部給它加起來就是a one 乘上n one
然後a two 總共發生n two 次等等
就是這裡面的一項就是這裡面的一項
等等等等全部加起來的話你其實可以分別
你如果知道
你如果知道a one 發生的機率是多少乘上它的機率就好了嘛
就是這個意思
那你如果從這來看的話呢這就是我們那些language model 的機率
就是相當於這邊的
這些個東西
這些個a one 
就是因為我就是要算這些機率的平均嘛我的perplexity 就算這些機率的平均嘛
那我怎麼算我就是把它們全部加起來平均嘛就是左邊那個式子
就是把它們全部加起來平均嘛
但是你可以看成是用這個東西乘上它出現的a one 乘上它出現的機率
這樣也可以啊
如果你知道它的機率的話
所以呢你看我這個等號左邊寫的就是說
averaging by all sample 你把全部通通都加起來
你把它們全部通通加起來來做平均
就是averaging by all samples 
那右邊呢是averaging 如果它的機率是知道的話
如果你求的出來這個機率你知道的話
對不對我現在的對應到那上面的式子
這個ai 就是我的q of x 
我現在在求那個q of x
就是這些機率所以q of x k 其實就是
就是我們前面在算的那些perplexity 的機率
那我要算這些東西的平均
那我們的算法就把它全部加起來除以n 嘛
但是你如果知道它每一個的機率是多少的話
譬如說你知道它的ai 的機率是pi 的話
你其實這樣算就可以啦
當你寫成這個式子的時候
那麼於是呢就是當它的值是某一個值的時候它的機率是多少你把它寫成這樣子的話
那這個東西其實就是上面這個式子
它就是一個cross entropy
那你如果這樣看的話呢這個q of x 
就是我們的這個
而這個p of x 是什麼
就是真正的機率
所謂所謂的真正的機率是
這裡面算出來的機率
那麼
我現在的這個這個
n gram
是用這個算的
問題就在這裡因為這兩個不一樣
我必須用一個training data 去train 出這些n gram 來
這個n gram 所得到的這些機率跟你現在測試這篇文章裡面的本來就不見得一樣
那這兩個不一樣其實就是
我們這邊講的p 跟q 是不大一樣的就是這兩個p 跟q 不大一樣
所得到的cross entropy 就是這個p 跟q 的
它不大一樣
那麼換句話說
我要求的我這邊在算的這個perplexity
是在算這裡面的n gram 的機率
從這裡面來算的
那麼我得到的就是這個東西
但是我平均怎麼平均法
是在這上面做平均
對不對
我是在這上面做平均是在這上面做平均所以呢
我是在用這個做平均如果我知道它的每一個case 是機率是多少的話
如果每一個pi 是知道機率是多少我可以把它寫成這個式子
那麼在那個情形的話呢我的這個仍然是我的q of x 
而這個東西呢就是我的這邊所寫的p of x
那我如果知道說
有有百分之多少是這個值有百分之多少是這個值
我就根本用這個加就可以加出來嘛
那那這個東西p of x 機率是什麼
其實是在這個testing 的這個d 裡面的機率
而不是那個
那這兩個會不同
所以呢true probability 這個bar 加一個bar 這個true probability 是指這個
是在這裡面的true probability
跟我真正的那個是有差的
那麼因為這個的關係所以我就得到這個perplexity
所以這個perplexity 可以寫成這個式子
這個式子可以看成這個式子
那它就是一個cross entropy 
那什麼是cross entropy 呢也就是我的真正在這個測試環境裡面
在這個測試環境裡面的這個機率
其實跟你的training 裡面的不大一樣
那中間的差異所構成的
恩
所以我們說它是一個cross entropy 
當你true statistic 在測試裡面是interpolate estimate as 這個東西
by the language model 
ok
就是說你真正的
你應該是測試環境裡面的那個統計特性你現在用那個來做
恩
因此你可以說它是一個cross entropy
ok
那麼當然越大越不好
這個越大越不好也就是我們剛才所說的越小越好是一樣的
在這邊來講就是越大就是它們差異越大
這個對不對我們說過這個這個東西本來就是兩個distribution 的的difference 
所以越大表示它們差的越多就表示你估計的越不對
恩
這個是這個我們解釋一下是為什麼它叫做cross entropy 的意思
