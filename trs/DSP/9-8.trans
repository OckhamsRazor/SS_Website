那底下我們來講這一招是怎麼做的 
那它的基本精神就是說我改變我的原來的criteria  
原來是我原來是maximize 這個東西我現在要minimize 這個error  
那麼理由就是說 
我的我的這個test data 可能跟training data 不一樣 
就算你的training data 可以讓這兩個分的開 
不過也差了那一點點 
test data 再不一樣的結果就會把它們弄錯 
那它的這套方法的basic 的formulation 是靠這個所謂的classification principle  
那這個沒什麼特別這個跟我們講的其實是一樣的東西它只是另外一個說法而已 
譬如說我們這邊講的零到九 
十個model  
或者這個b c d e v t z  
有譬如說有十個不同的音 
那那那麼有m 個不同的的音 
就表示有m 個不同的class  
那我為每一個class train 一個model  
我為每一個class train 一個model 所以lambda i 呢就是class i 的model  
那每一個class i 就是一個譬如說零或者一或者二或者三 
喔 
那麼然後我所有這些model 的集合大lambda  
就是所有的model 的集合 
那我現在的observation 一個聲音進來譬如說六 
那這個聲音進來的話呢 
我就就可以去算那個聲音對每一個i 的model 的機率 
那我看它誰最大 
它就是誰嘛 
所以呢我我如果把我我我現在進來一個聲音如果是六的話 
我就去算那個六的聲音對每一個lambda i 就是每一個從零到九的每一個model 的這一個分數 
那我把這個分數寫成g sub i 的這個observation  
跟這整套model 因為我要在整套model 裡面都去算 
那麼然後呢我看誰最大 
看哪一個這個叫做g sub j 裡面看哪一個j 最大 
最大的那個j 就是我的答案
所以呢那這就是我的classification principle  
所以這邊講的其實是跟我們之前所講的其實是完全一樣 
那麼 
嗯 
唯一不同的是它把它稱做classification principle  
那這個是是在pattern recognition 裡面它們用的語言 
那意思是完全一樣的 
那當然它這樣寫的一個好處是說這個g sub i 的function 不一定是這個 
你還可以算別的譬如說你可以把這個prior 機率算進去變成m a p 的probable m 也可以哦等等 
所以不一定是這一個 
所以它就減就用一個符號來代表就是g sub i 的 
我就given 一個observation  
我就把它放到這一整堆的model 裡面去 
那麼然後呢假設如果它是第i 個model 的話 
就是g sub i  
然後我對每一個j 都去算這個分數 
看誰最大 
如果那個最大的是i 的話 
我的答案就是i  
所以呢什麼時候發生錯誤呢 
就是如果你你放到第i 個model 裡面去最大 
但是其實它不是i  
這個時候就發生error  
因此呢這個所謂minimal error 的這個criteria 我就是要要數這個error  
然後想辦法要把這個error minimize  
那麼當我在minimize 這個error 的時候呢 
就自然的就因為很多的error 會發生在這裡 
凡是譬如說我們說這個一跟七很像 
六跟九也很像 
那麼那你這個時候呢 
你很多個error 發生在這個時候所以你如果有夠多的training data 丟進來的話呢 
那如果我要minimize 的是這個error 的話 
那它就要調這些model 參數 
想辦法把它們拉開把它們拉開 
那這個就是這個m c e 基本的精神所在 
那詳細的做法呢在下一頁這裡 
嗯我們也許在這裡休息十分鐘好了
那麼m c e 的基本精神我們剛才已經提到了
就是說嗯我們原來的做法都朝向於讓這個maxima likelihood 最大
假設我我是六的話就讓六的model 最大就好了 
可是這個並不不表示六跟九很像的話六跟九不會confuse  
那麼他們這這些聲音如果很像的話 
雖然各自機率都最大但是他們可能很像 
因此我換一個別的testing data  
換了不同的人不同的的環境搞不好它就贏了它 
所以呢這些competing class 可能會贏過真真正的答案結果就會發生error  
因此呢怎麼辦呢 
我們就用底下的辦法就是反過來我來數error  
然後我要minimize 這個error  
那麼這個做法呢就是我重新定義這個東西叫做d sub i 的 
這個x 是我一堆training data 的observation  
這個lambda 是我原來已經train 好的model  
我們剛才說我的假設我零到九好了 
我零到九我每一個model  
每一個數字零或者一或者二都是一個class  
都有一個model 就是lambda i  
然後我的所有的lambda i 構成一大把的model 就是我的大lambda  
所以這個大lambda 就是我已經原來已經train 好的那一堆model  
那一堆大lambda 已經train 好的model 之後我現在一個新的一個observation  
譬如說是七進來的時候呢 
我就可以define 一個d sub i 的這個東西 
那它是什麼呢它是長這樣的一個東西 
其中第一項呢就是g sub i  
其實就是把它放到它該有的model 裡面去 
假設我這個聲音是七的話 
假設我這個training data 這個training 的聲音observation training 的observation x 是七的話 
我就把它放到七的model 裡面去 
所得到的分數叫做g sub i  
就是我們剛才前一頁的這個東西 
這個g sub i 就是把它放到第i 個model 裡面去的那個分數嘛 
那麼因此呢但就就是那個分數但是我把它變成負的 
那後面這些呢是g sub j j 不等於i  
其實放到其他不同的model 裡面去的分數 
舉例來講 
如果那個這個音是是一的話 
假設說這個x 是一的話 
那麼我把它放到一的model 裡面去得到這個分數 
那放到七呢很接近 
放到到六啊九啊八啊五啊那就很遠了 
譬如說五在在這裡 
ok 所以呢那這些個分數就是這些個g sub j 的 
換句話說假設我這個聲音是一 
這個聲音是屬於一是class 一的那個model  
所以我放到一裡面去呢 
suppose 這個個分數是最高的 
就是這個 
然後呢那其他的g sub j  
j 不等於i 就是其他的這些音 
就是比較低的 
其中呢七跟它是competing 所以呢七很接近 
其他呢跟它比較遠 
那我把其他所有這些東西呢做一個這樣子的計算這一項 
那這一項呢裡面有一個alpha 的值 
這邊是alpha 次方 
那我把其他所有這些東西呢把其他的全部加起來因為這邊m 是總共的class 的數目嘛 
我們這邊m 是總共的model 的數目是m  
所以呢m 減一就是其他所有的 
所以呢那這個是什麼東西呢我們來看 
如果alpha 等於一的話 
我們先看alpha 等於一 
就是這裡邊等於沒有alpha 這裡也沒有 
那就把它全部的平均 
那意思就是說我把這些個所有的從七開始到這這邊所有的其他的東西 
平均起來得到某一個平均的值對不對 
我如果是alpha 等於一的話這個沒有這個沒有嘛 
這個就是把其他的其他的class 都算進來得到一個平均 
有equal weight  
那就是這個值 
那這個時候會怎樣呢 
那顯然這些如果這個辨識是正確的的話 
一應該是最高分 
其他都比較低 
然後這些東西比較低的平均起來當然比它低啦 
所以嗯這個比它這個正確的答案一定比這些錯誤的平均來的低來來的高 
那然後呢我現在因為這個是負的 
這個負的比較多 
這邊加的比較少加起來是負的 
所以呢這個值是負的 
就表示說這個是一個正確的 
這個辨識是正確的 
因為你你你這些東西這些東西分數比較低 
這個個分數比較高嘛 
所以這個負的的比較多這個負的比較少對不對 
所以這個負的比較多這個這個正的比較少 
所以結果還是負的 
所以負的話呢大概可以表示說這是一個正確的classification  
如果是正的話呢就表示錯了 
如果是正的話呢就表示錯了是因為 
你正的話呢就表示這個比較小這個比較大 
表示說這裡面有些東西超過它了 
所以結果呢我辨識出來它的它的分數低 
就發生錯誤 
所以這是我用我用這個function 來來來算我的正確跟錯誤 
那當然你說這樣子這個比不太合理 
因為你把所有的這些東西平均起來拿這個平均值去跟它比 
那本來就應該比它小嘛 
是沒錯 
所以呢你可以那另外一種做法呢另外一個極端就是當你alpha 趨近於無限大的時候 
如果alpha 趨近於無限大會怎樣呢 
你看如果我這些東西都都做個無限大次方的結果就只有最大的那個才能夠算 
其他都其它都沒有了 
所以只有most competing one consider  
如果alpha 趨近於無限大你其實這邊是無限大次方這邊再無限大分之一 
其實還是原來那一個啦
但是呢就是除了最大的那一個以外其他都等於不要算了 
所以呢如果alpha 是無限大的話呢這些都這些都沒有了
那麼這個是這個是alpha 趨近於無限大的時候 
那紅的這個是alpha 等於一的時候 
alpha 趨近於無限大的話只有最大那個值才算 
那麼於是呢那你就變成是正確的跟最competing 的那個在比較 
那麼因此呢這個時候如果d 小於零的話呢表示說這個還是負的比較多 
這個是負的比較少 
所以這個還是比它大 
所以還是正確的 
所以呢這個d sub i 小於零表示是正確的 
而只要它大於零就表示是錯誤的了 
ok  
所以呢這個alpha 其實只是一個可以調的值 
看你要弄進來幾個 
因為搞不好我這邊有好幾個competing class  
我們講一跟七好像只有兩個competing  
但是呢其實不一定啊你如果b c d e v t z 的話 
一堆都在這裡啊 
你就應該把這一堆通通都都拿來平均或者怎樣喔 
所以呢你也可以加不同的weight 喔 
所以呢你其實譬如說你的的alpha 不是無限大但是alpha 是十 
或者二十 
或者你選一個比一大很多的值的話 
你就自動去weight 最靠近的對不對 
你這個alpha 越大就是weight 越靠近的 
alpha 越小就把越不靠近的的也都放進來的意思 
所以呢你可以選擇那個alpha 來調看你要把competing class 裡面放進來幾個 
那麼但是不管怎樣呢這個分數其實是告訴我 
如果這個是一的話 
一跟它的competing class 有多接近 
那它它會被competing class 勝過結果辨識錯誤的的危險度有多少 
就是這個d sub i  
那有了這個d sub i 之後呢 
我們當然希望minimize 這個d sub i  
因為現在如果它是越負的話就表示它的這個正確是一個正確的嘛 
所以這個個d sub i 凡是有一個正的就表示會錯嘛 
我把很多training data 放進來 
我把每一個一啊二啊三啊一七啊都放進來 
那麼每一次如果越是容易被發生越是這個competing 的越厲害 
差距越小 
越有可能錯的時候呢 
那這個分數就會越糟糕 
所以我現在只要算d sub i 這個這個越小就越好越大就越有問題 
所以我就是要minimize 這個東西 
所以這個數值其實是代表我的一種error  
那你注意到我現在不是真的算error rate  
不是minimize 那個error rate  
而是在算分數的差距 
也就是說我是在算這個這個正確的那個那個class 的分數 
跟competing 的分數差多少 
我要這個差這個差的越大越好的意思 
那差的越大的話就是這個東西負的越多 
所以我要minimize 這個東西 
那但是這個東西其實非常不homogeneous  
因為你如果是不同的不同的這個七放進來d 放進一放進來 
你譬如說一放進來t 放進來這個情形都不一樣 
這中間可能差很多 
那麼不太容易讓它們這個有一個比較統一的的minimize 的方法 
所以呢就想一個辦法就是把它做一個normalization  
那這個normalization 就是所謂的這個sigmoid function  
那它的長相是這樣子 
那這個圖畫的不太好我們畫的清楚一點的話呢應該是這樣子 
它是一個底下是零上面是一 
它在這個地方有一個很平滑的像像s 的形狀的一個function  
那麼它這個switch 過來的這一點呢叫做theta  
那這個斜率是影響斜率的是gama  
那麼因此呢這個就是所謂的l of d 的這個function 
就是這個function  
長的應該是像這樣 
也就是說呢這個在趨趨近於無限大的時候它一定是零 
所以當你這個這邊我這個這個橫軸是d 
縱軸是l 的d  
那麼在d 趨近於負無限大的時候它一定是零 
在趨近於正無限大的時候它一定是一 
那中間從零switch 到一的這個地方呢 
所以depends on 看你的theta 選什麼 
那麼那麼theta 就是中間switch 的這一點就是所謂的theta  
然後呢這個gama 決定我的斜率 
換句話說你你你這個中間是是這樣的呢還是這樣的 
你可以更陡嘛 
你可以更陡的這樣子 
或者你可以更斜的這樣子 
都可以 
那這個你要多陡多斜呢就是這個gama 來決定的 
那這樣一個function 這樣function 的好處就是說你把所有的值都normalize 到零跟一之間了 
你本來這個值我們剛才上面那個值的話呢 
我是在算每一個model 譬如說b c d e v t z  
這些每一個model 的分數都擠在一起有有有些d 進來的時候 
它全部擠在一起 
有的t 進來它分的比較開什麼的 
那麼這個分數結果有大有小這個這個很亂啊 
那你怎麼辦我把它全部normalize 在零跟一之間 
所以每一個每一個testing data 進來training data 進來 
每一個data 進來我的分數都是在零到一之間 
那在零到一之間之後呢 
我就比較好算 
然後呢越小的話表示越越越是會正確嘛 
因為越小的話就表示正確的答案比competing 的會是勝過competing 的 
那小的越多就是勝的越多嘛對不對 
所以呢越小就是越好的 
但是我經過這個這個東西之後呢我把它全部normalize 在零跟一之間 
每一個data 都給我一個零跟一之間的數字 
那另外一個好處是經過這個function 之後 
它是smooth 的function 所以微分比較好微 
我要minimize 我現在要minimize 這個東西 
這個東西越負越好 
越負表示都是正確的對不對 
你凡是有一個錯的它就會變就會變正嘛 
所以我是越負所以我就要minimize 這個東西
但minimize 的過程之中這個東西很難minimize  
這個不好minimize  
但是我變成這種東西之後這個好minimize  
所以呢讓它smooth 之後 
我來來來來minimize 它 
於是呢我真正要minimize 的這個overall 的這個performance measure 就是這個東西 
那那這個式子這樣寫有點怪怪的 
其實一個簡單的寫法應該寫成嗯應該是寫成這個 
就是嗯我看看對我就是把這個d sub i 放到這個l 的function 裡面去 
就是這個嗯這個式子 
應該是說我把這個d sub i 放到d sub i 是這個x 在放在這個lambda 裡面 
的這個d sub i 就是這個東西 
我放到這個l 的function 裡面去 
得到一個smooth 的零跟一之間normalize 的值 
然後呢我應該是summation over 所有的x 屬於c i 的 
如果它是它是這個它是b 就放在b 的那個裡面 
它是它是e 就放在e 的裡面 
它是它是z 就放在z 的裡面等等 
都分都分別放在它自己的那個class 裡面 
然後我再把所有的class 加起來的這個東西 
其實底下這個就就是這個東西 
那現在這樣這個這個是paper 上面的寫法有點有點這個不清楚 
其實是一樣的意思 
你看它的它其實就是講你如果是在哪一個class 裡面才算 
不在class 裡面就不算嘛喔 
你在哪個class 裡面的那一個呢 
你就把它放到這個裡面去算這個l i  
那其實就是把那個d 算那個d i 之後 
再放在這個l 的這個sigmoid function  
這個這個你知道這個所謂的sigmoid 就是一個像s 形狀的 
把它放進來
然後全部加起來 
那其實是就是這個意思 
也就是說假設我們是要分辨這個b c d e v t z 的話 
那麼你就把所有的training 的b 放在b 的那個model 裡面 
得到的這個 
但是呢你同時要把其他的competing 的都拿進來一起算 
得到這個東西的那個b  
然後去做一個sigmoid function  
然後全部的通通都算了之後 
b 通通放成b 的 
d 通通放成d 的 
z 都放在z 的等等 
你全部通通都通通加起來 
你所有的training data 全部加起來這個東西 
我要它是minimum  
當我要minimize 它的時候呢 
我現在呢就是微分 
讓它等於零呃 
那因此呢就是我最我我最後就做這件事情要讓它等於minimum  
那這個東西到時候是要微分不太容易喔 
所以呢我就是要我我現在就是在剛才這個嘛 
