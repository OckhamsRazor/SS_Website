我們從這裡開始四點零開始我們進入這個比較詳細的然後比較深入的部分
那四點零我們先講 Hidden  Markov  Model 
這個 H  M  M 畢竟是我們最核心的東西
所以我們現在開始再深入的來講 H  M  M 
我們雖然在上週已經說過一下不過我們現在再仔細的來說它
那這一部分最好的 reference 應該是這一本課本的第六章
這一本課本是比較古老的是在一九九三年的課本已經十二年了十三年了
那麼這本課本大概別的章節大概不見得還需要看它
但只有這一章的 H  M  M 我認為是它講的最好的因為當初他們都是發展這裡面東西的人
然後他們所寫的這段是寫的最清楚的
那麼之後的所有課本都會講它不過看起來他們都是照著它寫
然後希望寫的有點不一樣之後都沒有原來的好
所以呢我比較喜歡用原來的
所以我底下講的這一段基本上都是 based  on 他們這本書的這一章第六章講的
那麼我們講的 Hidden  Markov  Model 這個 hidden 是個多加的字比較多的是所謂的 Markov  Model 
那 Markov  Model 並不是什麼專門為語音想的事情 Markov  Model 是一個非常普通的 stochastic  process 
你如果修隨機程序應該就會學到所謂的 Markov  Model 
所以我們先從它來講
那麼 Markov  Model 其實是簡單的多 in  general 是一個這樣的東西
它並沒有像我們所規定的知道我們我們已經說 ok 我們是一個 one  d 的一個系列的 state 
我們把它想成這樣這是針對語音而言因為聲音是這樣有順序過來的
你說了什麼聲音一路是這樣有先後的所以我們就讓它是這樣做
但是 in  general 你如果去看隨機程序的課本的話一個 Markov  Model 也就是所謂一個 Markov  chain 它不需要是這樣子 one  d 的而是可以這樣子的任何一個 state 可以跳到任何一個 state 去
也就是說我們沒有讓它可以跳回去但其實 in  general 是可以跳的
在那樣的情形下我們看到的 Markov  Model 它是說是一個 triplet 
包括哪三樣東西呢
第一個就是所有 n 個 state 每一個 state 構成所謂的大 S 
然後呢 state 之間可以有 transition 我們叫做大 A 
那 in  general 的話呢這個 transition 應該是 given  on 它的 history 
我在 t 減一的時候是哪一個 state  t 減二的時候是哪一個 state 我 given 前面的這一堆 state 那麼我下一個會到哪一個 state 機率是多少
所以 in  general 的話這個 Markov  Model 是說 given 前面這些 state 的 history 下一個機率下一個是哪一個 state 的機率是多少是這一個
但是呢這個太複雜了所以我們通常把它簡化變成只 depend  on 一個
這就是我們在上週所說的 A  I  J 
我只看 t 減一個時候是在 i 那麼 t 的時候是 j 的機率就是 A  I  J 我變成只只 depend  on 一個了
那這種情形我們叫做 first  order  Markov  Chain 
所謂 first  order 就是我只 depend  on 一個雖然 in  general 我也可以 second  order  third  order 等等都可以
然後再來我開始是哪一個 state 就是在哪裡這是 initial  state  probabilities 
那麼在傳統的 Markov  Chain 就是這樣跟我們上週所說的 H  M  M 還有一個最大的差別是什麼呢
在這裡在這裡的話呢就是說每一個 state 的 output 都是確定的每一個 state 的 output 是確定的
也就是說如果它在 state  one 它就是什麼 output state  two 就是什麼 output 所以呢你其實很容易解
那我們上週所說的不一樣我們上週所說的是說在它在某一個 state 的時候它出來的東西不確定只有一個 distribution 對不對
所以即使在這個 state 我不知道它長怎樣我只知道它會是有這樣的 distribution 
那跟這個 state 是不同的如果是這個 state 的話呢它有另外一個 distribution 
那麼這個情形之下呢我看到的 output 在這裡
但是我並不知道它是那個 output 是在哪個 state 那那個才是 H  M  M 真正不同的地方
所以我們這邊講的還是很簡單的是說呢對某一個 state 而言那它其實是是有一個固定的 deterministic  output 
所以呢其實是不難做的
我們舉個例子來講這個簡單的 case 那就是假設這很容易就是假設只有三個 state 
那這三個 state 我讓它可以這個有很簡單 output 
所有 state  A 那個 S  one 的 state 的 output 都是 A 
 S  two 的 output 都是 B 
 S  three 的 output 都是 C 
這個跟剛才不一這個跟我們這邊講的不一樣
我們這邊講的是這個 output 的話呢有很多很多千千萬萬只有一個 distribution 
對不對它呢有很多種只是有一個 distribution 而已
但是這邊的話是說我都是確定的所以這是比較傳統的 Markov  Model 
它的 output 就是 A 它的 output 就是 B 它的 output 就是 C 
然後呢它們有它們的 state  transition  probabilities 在這裡就是這個 matrix 
 initial  probability 在這裡
那這個這樣其實很容易啦
我們舉例來講你如果有一個 ob  sequence 是你 observe 到一個 sequence 是 C  A  B  B  C  A  B  C 
那你馬上就知道因為如果第一個是 C 的話 C 是來自哪裡呢就是 S 三嘛
第二個是 A  A 來自哪裡呢就是 S  one 嘛
 B 呢就是 S  two 嘛等等
你馬上得它的 state  sequence 就就是這個嘛
既然是這個 state  sequence 是這個的話譬如說你要解我們剛剛我們上週說過你要解這個問題就很容易解啊
你要你要看到這個東西的機率是多少
很容易解啊那就是什麼你看
這個第一個第一個是 C 嘛是 S 三嘛
所以表示說我的 initial 要從三開始
所以我先從這個這個從這個 initial 是從三開始的這個機率就是這個零點一嘛就在這裡
然後再來呢就是從三跳到一嘛一跳到二嘛
所以我就從三跳到一嘛一跳到二嘛這就是一個一個的 state  transition  prob 把它走過去
那每一個都在這裡都有嘛就把它乘起來就好
所以這是一個非常容易做的情形這個就是傳統的 Markov  Model 那我們現在講的不是這樣現在講的是怎樣呢就是我現在講的是
每一個 state 我不是有確定的 output 而是它的 output 只是一個 distribution 
它是千千萬萬的
那麼因此我當我看到這一堆 distribution 的時候我不知道它到底我看到一個 output 它可以是在這裡也可以是在這裡只是機率不一樣而已
因此我不知道它到底是在哪一個 state 裡面
等於說我這個 state 這個 state 的 sequence 是躲在後面的看不到的是 hidden 
因此呢那就是我們所謂的 Hidden  Markov  Model 就是底下講的這一個
當我前面加了這個字加了 hidden 這個字之後有何不同
那就是說我的 observation 呢變成是一個機率
那這個機率呢那麼因此呢是 depend  on 一個 state 的
那麼在哪一個 state 會有不同的機率在哪一個 state 會有怎不同的機率
因此呢我就不再是一個剛才那麼簡單的 case 
那麼因此呢什麼東西是 hidden 
就是 state 是 state  sequence 是 hidden 
當我 observe 到一堆 output 的時候我只能猜這一堆 output 
我們上週說譬如說是這個這個 x  one  O  one  O  two  O  one  O  two  O 三的時候
我只能猜說這個在這裡的話有一個機率在這裡也有一個機率
不過呢在這個機率比較大所以呢我就認為它是它
那這個呢到底是在哪裡都有可能只是機率有大有小而已
但是我永遠不能確定到底它是在哪一個 state 裡面
所以這個 state  sequence 是 hidden 
那麼因此呢也就是這邊講的喔我根據這個 observation  sequence 
就是這些 O  one  O  two 的話根據這 observation  sequence 這些 O  one  O  two 的話呢我
 never  know 到底我的 state 是哪一個
我其實是永遠不知道我都是用猜的
因此呢這就是我們上週說過是個 double 是個雙重的 stochastic  process 
我有兩個兩層的 random 
第一層是這個 state 會跳來跳去這個也是 random 的有一個機率
然後我並不能確定它在哪裡
第二層是 given 每一個 state 它是哪一種也是不確定的
所以是雙層的 stochastic  process 
這個我們大概簡單的複習一下因為這個我們上週都已經講過了
因此呢它跟剛才有何不同就是多了一個 B 嘛
我現在會有一個 B 也就是這一堆 prob 這一堆 distribution 
 given 每一個 state 它的 distribution 會長怎樣呢那就是我們講的 B 嘛
就是上週說的那個 B 
那那個 B 長怎樣呢
那麼一般你去看那個課本的話通常它們會說這個 B 有長兩個樣子
就是我們這邊講的就是我現在因為有了這個 random 的 given 那個 state 我還我只有一個 distribution 嘛
那麼每一個 state  distribution 我們叫做 B  j 的這個 O  t 嘛
譬如說這個是 B  one 的話呢 j 等於一就是這個 state 它會有這樣 distribution 
 B  two j 等於二的話呢在這樣 state 它會有這樣的 distribution 等等
那這是所謂的這個 B  j 
那麼這些東西就構成我們講的 B 
這一堆的就是我們講的 B 所有的 j 
那就多了這個 B 
它是所有的這個 probability  function 
那麼每一個 describe 它的對某一個 state 而言它是怎樣的機率
就是多了這個 B
那麼這個 B 長怎樣呢
我們上週說我們 B 就是把它看成什麼呢看成一堆 Gaussian 
所以呢這邊可以你可以看成這是一個 Gaussian 這是一個把它看成 model 成為一堆 Gaussian 兜起來的等等看了一堆 Gaussian 
那就是我們這邊所說的底下這個
那這個其實沒什麼特別跟我們只是跟我們上週符號有一點不一樣而已
那其實是一樣的那這個就是 gaussian 嘛
我們上週所講的那個 multi  variate 就是有 n 個 vec  n 個 variable 兜在一起的那個大 gaussian 
那這個就是那個 vector 減掉 mean 然後呢 transpose 相乘
跟它的 inverse 這就是 co  variance  matrix  inverse 等等
所以這個就是我們上週所說的那個那這是一個大 gaussian 就是 B  j  k 的
這個就是我在 state  j 
 j 是那個 state  j 就是這個 state 的的 index 
 k 呢就是第 k 個 gaussian 
所以呢 B  j  k 就是第 k 個 gaussian 
然後呢我的這個有一個 weight 加起來等等
這應該是小 b 啊寫大 B 寫的應該是這個小 b 
那所以這就是我們上週所說的這個的東西
那這邊是寫的比較嚴謹一點
它說呢你可以想像成我的每一個 observation 
這個每一個 observation 相當於是我們這邊的 O  O  t 啦這種東西啦
這個每一個 observation 你可以看成是 d 的 dimension 的 vector 
每一個 component 是一個 real  number 
對不對寫這樣的意思就是說我這裡的每一個呢是一個是一個 vector 
總共有幾個呢總共有 d 個
 d 就是它的 DIMENSION 的數目
總共有 d 個
然後每一個呢可以是一個任何的 real  number 是一個 arbitrary  real  number 
所以呢這個寫法就這樣的意思
表示它是一個 d  DIMENSION 的的這個 real  number 所構成的 vector 
那麼因此它的機率呢就長成這樣等等
那這個就是指我在第 j 個 state 的時候
那它的 OBSERVATION 的長相就是一個這樣的 gaussian 
這是這所以這一塊就是我們這邊所說的喔喔上週所說的一堆 gaussian 這樣的 distribution 
那麼其實我們今天用的都是這一個
那你如果去看課本的話它還會講上面那一個
那這個是在九零年代的初期或者到八零年代的末期的時候人家用的是這一個
在當時的 computer 還很還很破
在那個年代這麼多的 gaussian 很難算
因此當時呢都用這個方法
所以這個是大概在九零年代的初期以及八零年代用的方法那是我們所謂的 discrete 那所謂 discrete 是什麼呢
我們底下這是叫 continuous 
我們今天其實都是用這個啦
我們簡單說一下 discrete 其實只是一個 discrete 的 approximation 
那麼換句話說呢我如果是有一個 distribution 是這樣子
我們叫做 f  of  x 的時候是說呢它可以它是在會有這樣的機率對不對
如果這個太難算因為如果動不動都要積分什麼很難算的話呢
我換一個辦法就是把它變成一個 discrete 
這也可以我用這個來 approximate 它
那麼我說呢譬如說這個機率只有零點零三這個有零點零一這個有零點零零五
我給它每一個我總共就是本來這邊是有無限多個值嘛
我這邊只剩下譬如說一百個值
那每一個值是多少我都有
那如果用這個的話呢也代表這個東西
只不過呢我現在呢我可能只有 x  k 
這個 k 呢就是從這邊到這邊對不對
這邊只有 x  one 到 x  m 
我就這 m 個值各有一個值其他就都沒有了
我這個呢是做為它的 approximation 這個比較容易算
同樣的情形那我們現在在這邊講的跟相對於這個關係就跟這個一樣
那在早年沒有辦法算這麼複雜的 gaussian 的時候呢它就是這樣算的它就是在這個在這個 n 維的這個 d 維的大空間裡面我們 define 一堆點
舉例來講五百一十二的點或是兩百一十二個點或者是多少個點
每一個點給它一個數字就是它
每一個點給它一個數字就是它
所以呢如果在 state  one 的話呢
你說 ok 這個是零點零零三這個是零點零零五這個是零點零二什麼什麼什麼你都給它一個值
那如果是在這個 state 的話呢它等於這堆值的話呢那就是這個 state 
那如果是在這個 state 的話呢那就是數字換了而已這就換另外一堆數字
那就是這個 state 等等
那跟這個來 approximate 來 approximate 它是完全一樣的
那那這個情形呢用這個來 approximate 它
那這個呢就是所謂的 discrete  time 就是 discrete  finite  observation 的做法
那所以你看這樣你就可以了解這邊講的就是這麼回事兒
這時候我就 define 大 M 個 r 的 d 次方的 real  number 
這大 M 個是什麼呢就是這邊的大 M 個點
我總共有這這這個大 M 我總共有大 M 個點在這裡
那然後呢我就是為每一個 state 看它是在哪一個 state 
那這裡的每一個點上面都有一個機率
就像這邊給他零點零零三這邊給他零點零零五等等等等
那這樣我得到這個東西用這個來取代它
那這就是所謂的 discrete 
所以呢你如果看比較早年的文章或者書裡面它會講有 discrete 跟 continuous 兩種 hidden  Markov  Model 
那就是這樣的來的
雖然我們今天都用底下的這一種不會再用上面那個
因為這個顯然比那個好嘛好這是一個簡單的解釋
有了以上的呢我們現在來看一個最簡單的例子
那就是喔這個跟剛才那個很像跟剛才的那張圖很像
唯一不同的是剛才的那張圖是普通的傳統的 Markov  Model 
你如果是 state  s  one 的話呢 output 就是 A  state  s  two 的話呢 output 就是 B 
那我現在不是了那我現在呢變成了 hidden 的 Markov  Model 
多了 hidden 這個字之後呢你會發現呢不管是 S  one 也好 S  two 也好 S 三也好它的 output 都是 A  B  C 三種都有
假設只有三的話呢它都是 A  B 三種都有
只是呢機率不同而已
所以呢在 s  one 的話機率是零點三零點二零點五
到 s  two 的話機率是這樣不同了就是了
但是呢它都會出現的
那麼於是呢
我今天如果 observe 到一個 sequence 是 A  B  C 的話
你不能確定這個 A 是從哪兒來的
雖然在這裡機率最大是零點七
你不能說它不會從這裡出來雖然它是零點三
對不對它也會從這兒出來因為它也是零點三
你這個 B 的話呢這邊是零點六好像是機率最大
不過呢零點一也會出來啊這個零點二也有可能啊
所以呢你這個都有可能
所以呢 A 會有三種可能的 state 來源
 B 也有三種 C 也有三種
所以總共有二十七種可能的機率
那這就是我們講的 hidden  Markov  Model 這就是有這個 hidden 的現象
那這個情形等於是一個簡化的這個 model 
那也有另外一個畫法就是這個我們上週也說過就是你可以把它想像成是三個桶子放在幕後
就是桶一桶二桶三
那每一個桶子裡面都有一堆 A  B  C 球
只是說它們的數目不一樣多
所以呢你在每一個桶子裡面你撈出球來它是 A 的機率在這裡是零點七
在這裡面撈出一個球它是 A 的機率只有零點三等等
因此呢就有一個人躲在幕後他不斷的 randomly 從一個 state 一個桶裡抽一個球出來
然後他唸給你聽說它是 A 它是 B 等等
你得到這個 A  B  C 是一樣的意思
但是那個人躲在幕後
你看不到所以呢你永遠不知道他是從這個 A 是從哪個 state 哪個桶出來的
所以你就有二十七種可能
好那麼在這個情形之下怎麼辦
那麼我們舉例來講其實也不難我們舉例來講
譬如說我現在要在這個 model 裡面這個 model 就是所謂的 Lambda 
 given 這個 model  Lambda 我要 observe 到這個 o 這個 o 是 A  B  C 的機率是怎麼算呢
你要把這個 o 看成是所有的這二十七種
這個 Q  i 呢就是一個 state  sequence 
譬如說是 s 二 s 三 s 一什麼之類的我有這麼多 sequence 我有二十七種
 Q  i 呢就從這個 i 等於一加到二十七
對每一個 case 的話呢如果 given 這個 model 而它是這個 state  sequence 的時候我看到這個的機率是多少
然後把這二十七種全部加起來
把這個全部加完不就是它
那你如果是這樣子看的話呢那這個怎麼算呢
這個再進一步拆開成兩個
這個其實很容易看就是只是一個條件機率
那麼你可以簡單的看法就是你可以先不看這個 Lambda 
因為它們都是 given  Lambda 都在 Lambda 都在這個槓槓的右邊都是屬於 given 的條件
所以你可以先不要看
如果不看那個就很清楚 o 跟 Q  i 的機率是 o  given  Q  i 再乘上 Q  i 
那這兩個機率一乘就是它嘛
那我現在通通都加上 Lambda 機率的條件還是一樣就是了
所以呢我的每一個呢都拆成兩個機率
一個是如果在這個 model 之下而且是這個 state  sequence 的話那麼我看到這個機率
乘上第二個是說在這個 model 之下會有這個 state  sequence 的機率
對不對所以呢我在這個 model 之下會有這個 state 這會有這個 state  sequence 的機率乘上
如果 given 是這個 state  sequence 的話我的看到這個的機率
那這樣一乘就是這個
然後我把二十七個全部加起來就可以了
我們舉一個例子來看這二十七個裡面的一個假設說它是 s 二 s 二 s 三
也就是假設這個 A 來自 s 二這個 B 也是來自 s 二 C 來自 s 三的話
那怎麼算我們舉例來講那麼這個機率這個機率其實很容易算
就是我已經知道第一個是來自 A 是來自 s  two  B 是來自 s  two  C 是來自 s 三
那我就把這三個裡面這個這個機率分別乘進去就可以了
所以呢這個只是這個這是第一個機率
我就分別把它的每一個 state 得到這個 observation 的機率是拿出來算就可以了
那第二個呢 given 那個 model 會有這個 state  sequence 的機率是什麼呢那也很容易
我的第一個 state 是 s  two 
所以我就從 initial 是 s  two 開始那就是這個
然後呢開始從 s  two 跳到 s  two  s  two 再跳到 s 三就一路跳
對不對就是它跳到它它跳到它
那就是這個 A  I  J 
二跳到二二跳到三把它們乘進來那就是了
那等等你就把它加起來那就是我們要的
所以這個是我們簡單的解釋
如果我是一個這麼簡單的 hidden  Markov  Model 的話是可以這樣算的
好那我想這個都不難所以我們只是簡單的這個 overview 底下我們要進入比較難的就是這三個 basic  problem 
那這個我們其實上週已經說過一次
我們真正要解要用 H  M  M 來做我們要做的事情的時候
最核心的三大問題就是這三大問題
那麼我們底下要做的事情就是講這三個問題的 solution 
那我們今天會講 problem  one 跟 two 
大概今天的時間是講這兩個
然後呢 problem 三留到下週
我們從這個 problem  one 開始講起
喔不在這兒了我要跳到另外一個去
 ok 在這裡
那麼這是我們來看 problem  one 來解這個問題
那麼 problem  one 其實是這裡面最容易解的一個
這個因為當初這個不在一不在同一個時間做的所以這個不是 powerpoint 
那麼喔 problem  one 是這裡面最容易解的一個問題
那麼它的問題就是算我們剛才講的那個機率
這就是我們剛才講的那個機率啊
 given 一個 model 我會看到一個 observation 的機率
我就是要算這個
那這所以 given 一個 model 呢就是 given 某一個 n 個 state 的 model 
它有 n 個 state 有 a 有 b 有 Pi 都已經在那裡了
所謂的 Lambda 就是 a  b  Pi 各是一堆參數它們的集合叫做 Lambda 
 given 這個 model 然後呢我 given 某一個 o 
 o 是什麼就是 o  one  o  two 到 o 的大 T 
這個這段聲音總共的長度是大 T 
有大 T 個 observation  vector 
那麼構成一個大 O 我 given 了這個
那麼我想知道它們這個會看到它的機率到底是多少
那麼痾我們講這樣這個 problem 本身的這個問題其實就是在做 recognition 
我們舉例來講如果我要辨識零到九的十個聲音
我就是有十個這樣子的 model 
今天進來一個聲音是八
我就把這個八放進來在每一個 model 去算一個機率
那照說會在八的 model 的時候機率會最大
因為我放在我把八放在一的 model 放在二的 model 裡面它進去會很小
等等我所以這就是我們基基本上在做 recognition 的時候要算的機率
那我們要來看怎麼算
那麼要看這個之前呢當然我們要為每一個 o 呢 define 一個它是在哪個 state 裡面
就是 q  one  q  two 就是指它的
如果 q  one 是一就是它的第一個 state 等等
就是有它的 state  sequence 
好那就是我們這邊要講的問題就是這個問題
我這邊的所有的符號所有的 notation 都是 follow 那本課本裡面的第六章
那原因是說這樣子的話你去看那本課本的時候比較容易對
所以呢我這邊的 notation 都是照那邊的課本的
也因此跟我的其他地方也許稍微有點不一樣不過大致是很接近的就是
那麼這個 problem 怎麼求呢
基本上就可以用剛才的那個方法
這邊講的這個式子就是我們剛才的那個方法
那這個的意思其實講起來很簡單
就是說我現在要算這個 probability 就是
我要算這個 probability 的 o  given 這個 Lambda 
根據我們剛才的講法我就是要算什麼東西呢
每一個 state 每一個 state  sequence 
不過它這邊的 state  sequence 叫做 q  bar 
我每一個 state  sequence 叫做 q  bar 
就是我上面這裡嘛就是這個這個 q  bar 嘛就是我的 state  sequence 
 given 每一個 q  bar 然後呢 Lambda 
然後我 summation  over 所有的 q  bar 
你可以先把它寫成這樣子
這跟剛才那個是完全相同的
我就是我現在不知道哪個 state  sequence 我就假設某一個 state  sequence 那這有一個機率
然後把所有可能的 state  sequence 全部加起來這就是我的答案
那然後這東西怎麼辦呢這東西把它拆成兩個
那就是第一個呢是
如果是 given 某一個 state  sequence 的話
然後呢第二個是
那一個 state  sequence 的機率還是一樣
所有的 q  bar 
我就是把這個機率呢把它拆成兩個
跟我們剛才那個簡單的例子是完全相同只是我現在比較複雜而已
因此呢我如果是在某一個 state  sequence 看到這個的機率的話
我可以拆成我先 given 這個 model 會有那個 state  sequence 的機率乘上 given 那個 state  sequence 看到它的機率這兩個相乘
當我看拆成這兩個之後呢那就是我們這邊所寫的再下一步
那其中的這裡的第一式再拆出來就是上面的這一個
所以你看到我上面的這一個呢就是上面這個嘛
上面這一大堆東西這一大堆東西就是上面這個
上面這個就是這裡的第一式
然後乘上底下這一這一大堆東西呢
底下這一大堆東西就是就是它的第二個就下面這個
這個就是這邊的第二式
所以呢我這個就會這個這個再出來就會變成那一大堆
然後那個再出來就會變成這一大堆
那就是我這邊的式子
那這一這兩大堆其實也都很容易看
那麼譬如說如果它是這個 state  sequence 如果它是這個 state  sequence  given 是這個 state  sequence 的話
我看到這個機率是什麼呢那就是
把 o  one 放在第一個 state 去把 o  two 放在第二個 state 等等全部乘起來
對不對那就跟剛才我們那個簡單的例子是一樣的對不對
如果說我已經知道了是這個 state  sequence 的話
它的第一個 state 是 q  one 第二個 state 是 q  two 就是這個嘛
第一個 state 是 q  one 第二個 state 是 q  two 嘛
那我就把 o  one 放在第一個 state 的那個 B 裡面去
把 o  two 放在第二個 state 的那個 B 裡面去等等
這樣乘起來不就是這個嗎
就是這個
那至於說第二個呢
你如果要看到這個 state  sequence 機率是什麼呢
那你就從 initial  probability 開始啊
我的第一個 state 會是 q  one 的機率就是 Pi 的 q  one 
然後就開始用跳的嘛
從 q  one 跳到 q  two 的是 A 這是 a  i  j 
 q  two 跳到 q 三
一路 q 一路 q 這樣跳過來跳到最後一個 q  t 
那因此我就得到以下那個機率
所以這兩個就這樣可以算的出來
那這個式子其實跟我們剛才的那個
這個跟我們剛才那個簡單的那個 case 是完全相同的
你如果再看一次的話
我們來看剛才的那個就是在
這裡面的這個
 yeah 就是這張
我們剛才講這一張其實是完全一樣的
那麼在這個簡單的例子裡面很容易看嘛
就是這樣子
然後這個拆成這個這個拆成這兩個
就是我們剛才講的就是這個拆成這個這個拆成這兩個
那這兩個就是這兩個嘛
就是兩個就是就是這邊這兩個
然後這裡面的第一個怎麼算就是用這些東西來算
那麼第一個怎麼算用這些東西來算就是我們剛才看到的這一堆東西
然後呢第二個怎麼算第二個用這些東西來算
那就是我們剛才看到的這一堆東西
所以呢我們看到的這一堆只是這個簡單的問題的一個比較複雜的寫法而已
好如果這個沒問題的話那我們回過頭來看
它的問題在哪裡問題在這個計算量太大了
因為我我這邊要對每一個 state  sequence 都去算這些東西
然後我的 state  sequence 呢有多少個
有 n 的 t 次方個 in  general 
其實會比這個少一點啦不過也夠大了
這個 in  general 的意思是說我現在等於是假設所有 state 都可以跳來跳去
我假設是我有 t 個 observation 
我有 t 個 observation 
第一個呢假設是有 n 個 state 都有可能
第二個也是 n 個 state 都有可能
第三個也是 n 個 state 都有可能
所以有我總共有多少呢有 n 的 t 次方個
但是我這個 t 可能很大喔
譬如說我一段聲音你記得我們這些是 o  o 是怎麼來的
它是它是它是這個我這個訊號我這樣子取取第一個 o 嘛
然後這 shift 過來我再取第二個 o 嘛對不對
因此我這樣一弄的話呢我可以這個
平常你一個 utterance 你講一句話
我這出來是幾百幾千個幾百幾千個的這個 o 
那如果你這個 n 在幾百幾千次方就很大很大了
這個是這是 order 非常高的一個計算量
雖然我可以用 computer 來算
這個還是會非常非常大
那麼即使說 ok 我們簡化到讓它一定從這裡開始
所以譬如說我開始不會從中間開始不會從後面開始一定會從這邊開始
然後呢我規定它不能跳回去
使得我的這個 state 狀況少一點
但是也這個數目還是非常大
所以呢這個計算量是非常大的
所以事實上呢這個我們這個問題我們不是用這個方式來解的那我們有一個更有效的方法
那就是我們底下要講的這一個
所謂的這個在那本課本裡面在十多年前的課本裡面它叫做 forward  procedure 
它是靠 define 一個叫做 forward  variable  Alpha  t 的 i 來解
那這個就完全用 iteration 的方式
就很方便就可以把它答案找出來
那在今天的話通常呢我們有一個名字它就叫做 forward  algorithm 
所以一般我們講的所謂的這個東西我們就說它是所謂的 forward  algorithm 
那麼我就用它來解就可以了
那借助一堆 iteration 就馬上就可以算出來
這個計算量大為簡單
 ok 我們這個留到下一堂課
我們在這裡休息十分鐘
 ok
 ok 我們接下去
接下來講這個 forward  problem
這個 forward  algorithm 那麼
講這個東西的時候從這裡開始我都用這麼一張圖所以我們來先來畫一張圖
我這裡的三個 basic  problem 都用這個圖來解釋
這個圖的橫軸是時間 t
所以呢
有 t 時間 t 等於一等於二等於三
到時間等於 t 到最後到大 t
這個是我的最後的
這個大 t 是我的最後的時間
因此 t 等於一的時候呢相當於我有一個
 o  one 在這裡
 t 等於小 t 的時候我相當於有一個 o  t 在這裡
等等這是我的橫軸
我的縱軸呢是 state  number
橫軸是 time  index
縱軸呢是 state  number
譬如說呢這是 state 一 state 二 state 三
一直到 state  n 喔這樣子
那麼舉例來講呢這個其實等於是說我把那個 hidden  markov 排到這邊來
這是第一個 state 這是第二個 state 第三個 state
一直到最後第 n 個 state 在這裡
那麼它會這樣跳過去他也可以跳回他自己他可以跳到下一個
等等喔那這是我的這個
縱軸是 state 是就那個 model 橫軸是我的時間
因此我會怎樣呢舉例來講
我的第一個如果是相當於第一個 state 的話呢等於是這一點
那麼下一瞬間在二的時候呢
我會有一個 o  two 了 o  two 呢可能還是回到原來的那我就表示這樣子
他二的時候仍然在一的地方
但是呢我也可能跳到下一個於是呢我就到這裡
這我只有兩種可能它可以到這裡也可以到這裡
他其實也可以變成三如果那樣的話呢我這邊還有一個他也可以跳到這裡來
等等那然後呢如果二的時候還是在
時間二的時候還在 state 一的話那三的話呢仍然可能還在一
因為我還是有個機率回到原來的嘛這還是可能是一
但是呢我也可能由這裡呢到三的時候會跳到二那就是跳到這裡
我也可能三的時候呢跳到三就到這裡
同理呢我在二的時候如果是二的話
剛才二就跳到這裡來的話呢二還是可以回到二
可以在這裡也可以跳到三也可以跳到四
等等那三呢如果一是二跳到三的話我也可以到這裡來也可以過來等等
那麼以此類推我就可以在這上面畫出這張圖出來就是我的所有的可能
那麼因此呢我的每一瞬間他是在
可以在哪一個 state 上然後我可以怎麼跳怎麼都在這邊畫出來
那然後我每一個我我要找的某一個 state  sequence 其實就是某一條 path
上面的某一條 path 走走走走到哪裡
那那一條 path 就是我的 state  sequence
那我們在之前說我有那麼多個 state  sequence 那其實所有的 state  sequence 都在這裡
我們剛才說我的 state  sequence 那麼多那麼多那麼多我每個都要算
那其實呢我可以不用算那麼複雜因為他們我都在這張圖上
 ok 好有了這個之後我們現在來看我們怎麼 define 這個
喔
做這個 forward  algorithm
那麼我們先 define 叫做這個
 forward  variable 就 alpha  t 的 i
這個定義是什麼呢
就是我從頭看看過來看到 t 為止
看到 t 為止其中呢在 t 的時候是在 state  i
至於前面是在哪裡呢我無所謂的
我沒有規定但是我就規定在 t 的時候要在在 i 上舉例來講譬如說這個是 state  i
那麼在 t 的時候要在 i 上
我只有規定在 t 的時候要在 i 上
我從頭看到 t 為止
那麼從一到 t 減一呢我沒有規定他要在哪裡
所以呢我可以隨便來設一個都行我們舉例來講
我們舉例來說我的
我可以看成是這樣
這個意思是說呢我從頭看到
從 t  o  one 一直到 o  t 我看到 o  t 為止
其中在 t 的時候呢規定一定要在 i 上面
除了 t 之外前面的到 t 減一為止沒有規定所以我在這整塊裡面
不管怎麼走都沒有關係
怎麼走都可以我只是都可以但是只有只有這一點一定要在這裡
那這個機率呢叫做 alpha  t 的 i
 ok 所以你看到呢 given 這個 model 之下呢
我要看到的就是這一堆一到 t 其中 t 的時候是 i 其它的沒有規定
這個叫做 alpha  t 的 i 那然後呢你看到他是兩個參數就是一個 t 一個 i 嘛
所以 alpha  t 的 i 他有兩個參數其實就是這個圖的橫軸跟縱軸 t 就是這個橫軸
 i 就是 state  number 就是這個縱軸所以呢這個意思是說這個 alpha  t 的 i 其實是 define  for
這上面的每一點這上面的每一點都可以有一個 alpha  t 的 i
因此你也可以想像成這個 alpha  t 的 i 呢
這個所謂的這個 forward  variable 這個東西呢
是一個數字可以放到這裡的每一點上去
每一點我都可以填上一個這個東西
那這整個就是一張表因此這整個 algorithm 在幹麻他就是在填這一張表
他的填法就是一行一行的填
我的前面 initialization 這個 initialization 就是
是如何填第一行第一行填完之後呢我就有了第一行就可以算下一行
就有了每一行就可以算下一行就是這邊講的這件事情其實就是一個 iteration
當我有了這一行我就填下一行
那麼這樣於是可以一路填過來等到這一行填完的時候
我的答案就出來了
那就是最後這就是所謂的 forward  algorithm
那為什麼叫 forward  algorithm 你也可以想的出來因為它就一行一行向前走
每一次就是多看一行每一次多看一行這樣子走
走到最後答案就出來了所以他叫做 forward  algorithm
那這些 alpha 叫做 forward  variable
好那我們現在看我如何填第一行
第一行其實很容易
因為第一行的話根本沒有前面的東西嘛
你看我的定義是如果是 alpha  t 的話呢是指
 t 一定要在 i 上面
至於 t 減一之前呢沒有規定嘛
可是如果是 t 等於一的話我現在如何填第一行
填第一行是 t 等於一當 t 等於一的時候
我其實根本沒有前面的東西
這個前面前面這些根本沒有嘛所以呢根本就是把它的這個會在這些點上面的機率算出來而已
那很簡單就是把 o  one 放在第一個 state 的機率是多少
把 o  one 放在第二個 state 的機率是多少乘上 initially 一開始他有個機率
就是 pi  one  pi  two 那麼因此就是這個對不對
就是 t 時間等於一的時候根本沒有比 t 減一之前這些問題都沒有這一這一點就在這裡嘛
這一點就在這裡嘛所以根本沒有這個前面的問題所以我只要看這個會在那個的機率是多少
所以呢就是要從這個 initial  probability 就是就是第一個要會在 state  i 的機率
就是 pi  i 然後呢我現在把第一個機率放進去
第一我再把第一個 vector 放在第 i 個 state
就是 b  i 的 o  one 那這樣子的話呢那我現在把 i 從一算到 n
我全部算出來那第一行就排出來了嘛
所以呢這是第一行其實很容易算這就 initialization
那這個 iteration 的核心就是第二這是第二個就是你如何有了前面一行如何算下一行
那這個其實也很容易你可以想像
根據這邊的式子是說如果我有了如果我有了第 i 行哦
如果我有了第 t 行要如何算 t 加一行
如果有了第 t 行我現在要算第 t 加一行怎麼算
我現在要算 t 加一行如果是在 j
我們舉例來講如果這個是 j 的話
我要算這一個
那這個是什麼呢
這個是跟剛才一樣
是我要是這個這樣的
對不對
也就是說呢我要把整個向前推一步
我現在是 t 加一要在 j 上面到 t 為止呢沒有規定通通都可以
那麼於是呢於是呢到 t 為止當然囉我可以從這個 i 過來
但是我也可以從另外的其他的東西過來
都可以那於是呢我們舉例來講
如果是在如果在 t 的時候是 i 的話那就是我們剛才的 alpha  t 的 i
就是這個東西然後我現在要從 i 跳到 j
所以要乘上一個 a  i  j 對不對這個 alpha  t 的 i 就是我們前面前一行已經算好的
如果前一行是 t 的時候是在 i
前面都有了前面都都算了在 t 的時候是 i 的這個
然後乘上這個 a  i  j 就是這個跳到這邊的機率
那就是這個東西但是現在這個 i 呢是每一個都可以阿
因為我也可能是從這個過來的
從這個過來的話應該有另外一個 alpha  t 的 i 也可以用的等等
舉例來說
我也可以是這一個那他有一個是這個跟這個也可以阿對不對
那那那表示說我在 t 的時間也許是在這個上面
那之前的所有的從這邊跳過來跳到這裡的那這個也可以有個 a  i  j 也可以到那兒去阿
等等那我可能還有另外一個是在這裡阿
 t 的時候可能是在這裡阿我也有另外一個在這裡呀那他也是前面的都算了
只有這個在這裡了那他也可以跳過去阿
那我如果把所有這個全部都加起來的話不表示把這一行也都所有都都算進去了嘛
那於是就得到下一個嘛
對不對那麼因此呢我就是把這邊的 alpha  t 的 i 的這個東西乘上 a  i  j
就在這裡乘上這個 a  i  j
那也可以是這個 i 乘上這個 a  i  j 也可以這個 alpha  t 的 i 乘上這個 a  i  j 那如果全部通通加起來的話
其實就是我把 t 的這一行也讓他每一點都可以了
那個加起來不就是我要的這個嗎對不對
所以呢我就加起來就是這個但是呢你加完之後還要記得這件事
我現在要把 t 加一的 vector
這是 t 加一的這個 vector 也放進
把這個 vector 要放進那個 j 裡面去得到這個機率
所以我最後乘上一個這個那這樣我就變成 t 加一的 j
那這就是說明我怎麼算下一行我只要這一行的通通都有了
我下一行的一個都可以那樣算那每一個都可以這樣算
那麼我這邊只要 j 等於一到 n 的話我的所有的都可以算出來
雖然我只要這一行算完之後下一行我就都可以照算
然後以此類推的話呢我就每一行每一行都可以算
所以 t 呢一直算到 t 減一於是我就把大 t 都算出來
因此呢我變成是
那麼從 initialization 到這個 iteration 我等於是在填這個表
一路填過來等到這邊都填完的時候這邊都有了
都填完了那麼這個時候呢這些就什麼呢就是
 alpha  t 的 i
這個小 t 已經變成大 t 了這是最後這一行
當我最後這一行算出來的時候呢我的答案很簡單我的我要的那個機率就是
所有的最後那一行加起來一加 i
這邊全部加起來就對了是不是這樣
因為就這個 case 而言他是講我的最後一個 state 在這裡
前面的所有的機率我都算進去了
等等我都已經算進去了
那是最後停在這裡的那我把這邊這個的話就這邊停在這裡全部算進去了
這個最後停在這裡全部算進去了那這樣一完全部加起來的話我是把全部所有的 path 都算完了
所以呢這麼一來的話呢我所有 path 全部算完他機率都算進去了
那麼因此就是我要的這個值 ok 那麼因此呢
我現在只要把這個 iteration 這樣一走的話
我的計算量大為減少只是填這張表
這張表不過是 t 乘以 n 而已 t 乘以 n
那麼那麼我算完就完了
那麼它的這個這就所謂的 forward  algorithm 講起來是蠻容易的
這是我們三個 problem 裡面最容易的一個 problem 那麼那麼他為什麼可以把那麼複雜的
本來是 n 的 t 次方變成 t 乘以 n 那主要就是說他所有的 state  sequence 呢
就底下這句話講的意思你雖然他的好像有那麼多 state  sequence 其實他們永遠在這張表上面而已
這張表已經把所有的那麼多都都算進去了
不管他前面怎麼走他每一個時間永遠只有 n 個點
他在每一瞬間永遠只有 n 種 state 因此我就是
把前面的 n 個算出來之後就得到下一個 n
那就是就像填表一樣我這一行填完填下一行我每一次算出這 n 個出來
這樣一路下去答案就都出來了
那就底下這句話講的不管你前面用多少個 state
怎怎麼走喔你但是你最後的話最後的話都是 merge 到這 n 個 state
所以呢你只要填每一個每一個時間只要填這 n 個數字就可以了
那這就是所謂的 forward  algorithm 這也就是我們第一個 problem 的 solution
這是最容易的一個 problem我們來看 problem  two  problem  two 就比較麻煩了
他我現在想希望得到的是 state  sequence
我我希望得到這個 state  sequence
我剛才並沒有真正的去求 state  sequence 我剛才是把全部的全部加起來了
這裡面有無限非常多個 state  sequence 就 n 的 t 次方種 state  sequence 都在這裡面
我可以這樣我可以這樣我我全部都算在裡面了
但是到底哪一個才是最可能的 state  sequence 我們沒有講
那我們 problem  two 是要來解說
到底哪一個是最可能的哪一個是最可能的 state  sequence
求這個 q
那也就是說呢我的 sequence 總共是有 n 的 t 次方個
但是其實裡面應該有個機率最高的那一個
機率最最高的那一個可能是譬如說這樣走這樣走這樣走這樣走
最後走到這來
那那一條才是機率最最高的那一個我要求那一個
這是 state  two 這個 problem  two 的問題
那 problem  one 的話我沒有算這個問題 problem  one 我是把全部通通加在一起算他的機率
那我 problem  two 現在是要找機率最高的那一條 path
在這個圖而言就是要找機率最高的那一條 path
那在這邊來講的話就是要找那個 state  sequence
那這裡我們要解釋一下就是
這邊講的這三個 problem 我們都做 general 的假設並沒有簡化的假設
我們講過簡化的假設是
我只是 one  d 的每一個 state 跳到下一個
他不可以跳回來的
那麼然後呢他一定從第一個 state 開始走的
把這些假設放進去之後在這個圖上有一點不一樣哪一些地方不一樣呢就是
譬如說這邊一開始是零這一堆有一堆是零
因為你要從這邊開始走
然後開始慢慢慢慢 spread 開來
一開始這上面是是沒有的一開始一定從這開始
如果有了那個假設的話而且我的 path 只能向上走不能向下走對不對
他那邊就是只能向右走不能回去的意思就是說只能向上走不能向下走
所以在在那樣的假設之下這個問題會稍微簡化一點
但是我們現在講的這三個 problem 我們現在講法是沒有做那個假設
沒有做那個假設所以我並沒有假設說這邊會是零
然後也沒有假設說他一定只能向上走他也可以向下走
好
那我們現在再來看這個 problem  two 怎麼解法呢
那麼一個簡單的辦法是
再定義第二個 beta 叫做 backward  variable
 beta 是一個 backward  variable
然後呢我們也有一套演算法來算這個 beta
那這個呢叫做 backward  algorithm 就是往回走的那麼就有這個所謂的
那麼這個是什麼呢我們底下來解釋不過他的意思是跟剛才剛好反過來
就是說呢我是從先算最後這一行
我也是一樣 define 另外一套東西叫做 beta  t 的 i
我 define 這堆東西
也是一樣他是對每一個 t 每一個 i 就跟這個完全相同
每一個 t 每一個 i 上面都有一點都有一個數字所以呢等於這每一點都有一個數字
不過他是這個東西然後呢我我反過來
我是先從先算最後一行
然後你只要有前面一行就可有後面一行就可以算前面一行有後面一行就可以算前面一行這樣一路倒回去
把它全部填滿這個叫做 backward  algorithm
然後呢我們求出來的這個東西叫做 backward  variable
 backward  variable  beta  t 的 i
那麼底下我們講的就是這件事
那我們後面後面會看到怎麼用這個來來用就是了
好那麼我現在先來看什麼是這個 backward  variable  beta  t 的 i
他的定義是 given 這個 model
然後呢我要 given 說我 state  i 在 state 這個在在在 time  t 的時候是在 state  i 上面
 ok  time  t 在 state  i 上面然後呢我是從 t 加一開始看到大 t
好那我們現在把剛才這個都擦掉
我們看這個 case 是什麼
如果我的時間 t 要在 i 上面的話
我還是一樣是時間 t 要在 i 上面
但是呢
有點不太一樣的是說我看到的是從 t 加一到大 t
所以我看到的是從 t 加一開始的
到大 t 但是我完全都沒有規定這樣子我這邊都看到了
從 t 加一開始都看到了我都完全沒有規定但是我要 t 的時候要在這點上
這個叫做 beta  t 的 i
 ok
那麼因此呢你看到就是這樣子
我要在時間 t 的時候是在 i 上面
然後我看到的是從 t 加一開始看到大 t
這個定義跟剛才有一點有一點對稱但是不完全像喔什麼地方
不完全像呢我們可以仔細看一下
剛才的這個時間 t 在 i 的時候是放在這條槓的左邊是要算機率的
我現在是放在這條槓的右邊是一個條件
你看一下
我剛才的 alpha  t 的 i 這個這個在在 time  t 是在 state  i 這件事情是在槓的左邊
是算機率要算進去的那我現在呢
如果在 beta 這裡的話呢是在槓的右邊是一個 condition 而不是算機率的是一個條件
那為什麼會這樣我們待會會解釋
還有一點不同的是
我現在如果是算 t 的 beta  t 的話這個如果是 t 的話
我這邊裡面沒有 t 我是從 t 加一開始往後看 t 是沒有的
對不對這裡沒有 o  t
但是如果剛才的 alpha 的話呢
這個如果 t 的話這邊 t 是在這裡的
所以呢我是如果算 alpha 的話是我是把 t 算進去的
而 beta 的話呢 t 是不算進去的
 ok 那就是為什麼我剛才算你記得我剛才算畫 alpha 這張圖的時候我是這樣畫的
我說如果時間 i
時間 t 在 state  i 的話呢
我是這樣子看的
也就是這個機率要算進去
另外前面的呢都可以算但是我這點是要算進去的
但是我現在的話這點是沒有算進去喔時間 t 在 i 的這點沒有算進去
所以我是這樣子這邊並沒有這樣包進來
那也就是說在這個 t 的時候
在這個 t 的時候在這邊是沒有 t 的
這為什麼也是有原因的我後面會解釋
哦那這是兩點不同的地方其他的看起來是很像
就一個是這個從一個是 given 前面一個是 given 後面
一個是看到前面一個是看到後面
好那我們現在先來看他的這個怎麼算這個這個所謂的 backward  algorithm
那這個講起來其實也一樣不難
那反過來意思就是說我先從最後這一行開始填起
我如果在 t 的部分都可以填起來的話呢我就可以填前面一行我的這個這個
 iteration 就是有了後面一行就算前面一行有了後面一行就算前面一行有了後面一行就算前面一行就這樣一路算下去
那麼一開始我後面一行最後一行怎麼算他說全部都給他一
這個條件其實有點怪怪的
但是呢哦為什麼怪怪的呢其實講起來很簡單因為其實在最後這一行是 undefined
為什麼呢你看我的 beta  t 呢是指看到 t 加一以後的
當我如果 beta  t 是這一點的話呢是指我看到 t 加一以後的
所以如果這個 t 變成大 t 的話在這邊的 beta 什麼是看到大 t 以後的阿
應該是講大 t 加一以後的機率
可是大 t 加一以後根本就沒有
所以那個算什麼其實是根本在不在上面的定義裡面 ok
也就是說你這裡的這個大 t 如果是大 t 的話其實不在上面這個定義裡面
因為上面定義是要講大 t 加一以後的東西根本就沒有嘛
所以這裡根本沒有上面的定義所以我們就隨便隨便 define
目的是說我 define 之後後面要通
也就是說像他這邊他就說我全部讓他是一
這當然不是隨便亂設的他讓他隨便都是一之後
你根據他的這個演算法
給我這一行去算前面一行給我這一行去算前面一行對這個演算法我也從這個通通都是一來算前面一個看看對不對
那它 turns  out 就是當我都設成一的時候把它再算前面一個的話呢用那個演算法來算它發現是對的
所以它可以這樣子做 ok 所以呢我們要先來講這個這個呃 algorithm 也就是說它的 iteration 的過程
如果我有了 t 加一的話要算 t 怎麼算
就是向前算嗯如果我 t 加一這一行如果我 t 加一這一行都有了的話
我如何算 t 的那一行
如果這個 iteration 沒問題了
那我們現在拿這個來做在我最後一行假設它都是一的情形之下看看通不通
欸也通於是我就假設它後面都是一是這樣來的
好那我們現在來看這個怎麼算這個其實跟剛才很像只不過反過來
如果我有了 t 加一怎麼算 t 呢
你看我要的 t 是這樣是指 t 的時候在 i 上面但是要前面全部的機率
那我如果有 t 加一的 j 的話 beta  t 加一加上 j 的時候是這個
那這個時候會是怎樣呢譬如說我們說這個是 j
 t 加一的 j 是這一個 beta 的 t 加一的 j 呢是說
我 t 加一的時候是在 j 上面但是我看到的是 t 加二以後的
這是 t 加二我看到的是 t 加二以後對不對根據我們剛才的定義對不對
也就是說你現在 t 加二以後我都有了然後 t 加一在 j 上面
不過我沒有算他的機率對不對就是那這這就是這個東西
那如果是這樣的話呢那我顯然應該還要再把 t 加一的那個機率算進來
因為我們剛才講過 beta 的定義都沒有把它算進去 beta 定義所以 t 加一的 beta 我沒有把 t 加一算進去
所以我現在要把 t 加一放到那裡去把它的機率算進去那這樣的話得到就是我黃色這個
這邊後面都有了但是這邊是在這裡
可是當然我也可以這邊在這裡阿
也可以這邊在這裡阿對不對譬如說在這邊在這裡這個 case 是
是 t 加一在這裡但是這邊是後面都有的這顯然是一個
然後呢那這裡顯然也是一個 t 加一在這裡那這後面都有的
也是一個對不對那你把這些都全部加起來那就是這個
然後你還要一個什麼呢還要一個 a  i  j
那這時候呢它可以這個可以這樣過來可以這樣過來可以這樣過來
我每一個機率都要算進去
那全部算進去之後呢我現在就得到現在在這裡而這以前都可以的
那就是這個就是 beta  t 的 i
 ok 所以你仔細看這個式子其實不難啦它的意思其實就是這樣子嘛
我的 beta 的 t 加一的 j 是指說我 t 加一的時候在 j 上面
然後前之後的通通都算但是這點呢沒有算
但是 t 加一可以是所有的 j 阿
那我就每一個 j 都要算都要嘛但是呢都是後面通通都算了
但是呢這個是在所有的 j 上面都要
那所以呢我就所有的 j 都要然後呢我都要 a  i  j 他們都要從 i 跳過來所以都要 a  i  j
然後呢我要把這個 t 加一的這個東西放在每一個 j 裡面去都有機率嘛那就是中間這一項
我這個通通都乘起來然後通通加起來於是我就把剛才的這一個
跳到向前剛才的這一個向前跳到這來於是我就把多把中間這一行的機率都算進去了
對不對我等於把中間這一行機率都算進去之後向前跳跳到這一行
那麼由此的話呢於是我這一行有了前面一行我就可以算後面一行
因此呢這就是我的這個 backward  algorithm
我就可以向前算
所以我從 t 減一 t 減二一路往前算一樣的填這張表我就可以把這個表全部填完
那每一點都是一個 beta  t 的 i
當這個 iteration 的這個式子看起來沒問題的時候我們再回來看剛才這個 initialization
我們說一開始把它設成一好像沒什麼道理我就我就說它在每一點都是一
有點沒道理但是我現在來看如果這個都是一的話我再算前面一個 t 減一怎麼算也是一樣阿
我這裡的某一個這裡 t 減一的某一點是什麼呢就是前面的所有的機率
那麼因此呢我現在如果把
如果那樣的話呢我現在是大 T 加一叫做大這個小 t 加一叫做大 T 嘛
這個叫做大 T 嘛那這個變成大 T 減一嘛
那如果這是這是這這個是大 T 的話我剛才已經設它都是一了
都不要看了就是這兩個乘起來加起來是不是等於這個呢對不對
這是我最後最後兩行的意思
假設這個都是一然後來算這一行那怎麼算呢這邊都是一嘛所以我就是把這個小 t 加一叫做大 T
於是於是這邊就都是一不用看了就是這兩個乘起來
然後呢這個 t 呢就變成是這個大 T 減一那你看這樣對不對呢
這是對的因為我就是把這是大 T 嘛我就把最後一個最後這一個 vector 放到每一個 state 去得到一個機率
然後分別乘上它的 a  i  j 就得到我的 i
所以呢譬如說把把這裡也有一個 o  t 阿
我這裡也有一個 o  t 阿我把這個 o  t 放到這個 state 去放到這個 state 去我都有一個機率
然後呢它呢可以跳過來可以跳過來可以跳過來阿
我就通通算進去阿把這些通通算起去所以我就把他們的機率
把這個 o  t 放到這些 state 裡面機率然後分別乘上這個 a  i  j 就是 given 這個 i 之後所有的可能
就是這樣子阿對阿嗯所以呢把這當成一就對了所以我們講說這個可以這樣設
嗯那麼這麼一來呢這兩個都合理於是我 beta 可以算了好當 beta 可以算之後
我們還沒說這個跟我們的 problem 有什麼關係
所以我們現在回來看當我的 forward 和 backward 都有了之後
我的第一件事情是可以把 forward 跟 backward  combine 起來
也就是 alpha  t 的 i 乘上 beta  t 的 i 兩個相乘會怎樣
我們剛才是用 forward 的話是在這張表上填出一個 alpha  t 的 i 來
我有一張表每一點都有一個數值就是 alpha  t 的 i
我現在做 backward 的話我又可以填一張表
上面每一點是 beta  t 的 i 我現在可以把這兩個相乘是第三張表 ok
這是第一張表這是第一張表這是第二張表第三張表是兩個的相乘 alpha  t 的 i  beta  t 的 i
這也是一張表那麼我們來看看這一張表是什麼意思
那這張表的意思其實很簡單
就是你看到全部的一到 t 全部的東西而時間 t 的時候是在 i 上面
 given 這個 Lambda 這個是 given  Lambda 那為什麼其實其實是很容易想像了
我仍然用剛才那一個圖那邊那個圖我如果這是這是 state  i 這是時間 t
我剛剛才 alpha  t 的 i 是這樣把這個機率算進來之後前面全部都有
對不對前面隨便怎麼樣都有這邊呢是在這個算進去這個是 alpha  t 的 i
那我 beta  t 的 i 是怎樣呢是我也是這個要在這裡但是我是算這後面的從 t 加一以後的
這是 beta  t 的 i
所以我這個再乘這個的話不就是把這兩個機率通通都算進來了嗎
於是我就是從從這個 o  one 一直算到 o  T
所有機率都都算進去了
除了在 t 的那點只有放在 i 上不可以 t 的那一點只可以在 i 上這邊沒有對不對
也就是說這一堆是不可以的這一堆是不可以的
除了這堆不可以這堆不可以之外其他全部都可以
然後我全部看到對不對那就是我這邊講的我看到全部的 o  one 到 o  T
除了在 T 的那點只有一個 i 以外其他我全部都看到了
那就是這兩個相乘的那就是我如果把這個 alpha 的表跟 beta 的表相乘得到第三張表的話
那個表上也是對每一個 t 每一個 i 都有一個值那個值就是這個意思
那個值就是這邊的意思就是我看到全部的
然後呢在 T 的時候等於 i 的
那這個說法你如果
我我想是很直覺的你可以想像就是這樣子因為我們已經講清楚它的意思就是這樣子
你如果覺得說我不太感覺它是這樣的話我們可以推一推
我們可以推一個簡單的假設我有一個我從 o  one  o  two 一直看到 o  t 的時候
 given 這個 Lambda 這是我的一個 event 叫做 a 我我我現在要推一下這個東西
就是推一下這個這個東西假設這個 event  a 就是在這個在這個 model 裡面我看到零到 T 的
然後還有一個呢我是看到 t 加一 t 加二一直看到大 T 的
這個叫做 b 也就是說我一個是從零看到 t 一個是從 t 加一開始看到大 T
這是 b 然後還有一個是 event 是 c 就是我在時間 t 要在 state  i 上面
這個是 c 我如果 given 這三個 event 的話那麼我要說是這樣子
就是 probability 的 a  b  c 的機率是什麼
就是 probability  of  a 跟 c 乘上 b  given  a  c
這個沒問題吧嗯我這個只是一個最簡單的條件機率的拆解
 a  b  c 要同時看到 a  b  c 三件事的話呢
我可以看到 a 跟 c 的然後乘上 given  a 跟 c 看到 b 的
那這樣的話呢這乘起來就是這個那如果是這樣的話那麼
當 b 跟 a 是 independent 的時候我們加一個條件
就是咳咳當 b 跟 a 是 independent 的話我這個 a 的條件可以拿掉
就變成 b  given  c
這個應該也是個很簡單的條件機率的的關係你應該記得就是
你學機率學過 given  x  given  y 如果它們兩個 independent 的話會怎樣
就是 x
這個 independent 的意思就是你給我另外一個東西的條件等於不給一樣
所以呢你根本就可以不要這個是你回去如果不記得回去查你的機率課本一定學過這個
所以呢如果是 independent 的話你可以把它拿掉那我這裡呢我說我們可以先假設 a 跟 b 是 independent
為什麼因為 a 是看到 t 的 b 是看到 t 加一以後的那麼我們現在並沒有假設
不同的時間上他們 o 有什麼關係我沒有講他們有什麼關係所以我們先假設他們是沒有關係的
讓他們是 independent 如果是這樣的話呢我這個就沒有了就跟剛才這個情形是一樣的
於是我就變成這樣當我變成這樣的時後你現在再來看這個是什麼
這個就是 alpha  t 的 i 那這個是什麼這就是 beta 　 t 的 i
何以見得你看看 alpha  t 的 i  alpha  t 的 i 的定義
就是這樣子那是不是 a 跟 c 就是 given  Lambda 裡面我看到一到 t 而且 t 在 i 上面對不對
就是 given 這個 lambda 我看到一到 t 而且 t 在 i 裡面所以呢就是 a 跟 c 這兩個在這裡的機率就是 alpha  t
那麼這個為什麼是 beta  t 呢那你再看 beta  t
就是我看到 t 加一以後的 given 這個東西
那就是我看到這堆東西 given 這個
所以是 b  given  c  ok 所以這個 b  given  c 就是這個
所以 alpha  t 的 i 跟 beta  t 的 i 相乘就是在這裡
而這個是什麼這個 a 這個 a  b  c 的機率就是我看到全部
也就是我底下的這個這個的這個或著這個就是我看到全部
看到全部裡面呢只有 t 的時候等於 i 別的沒有規定
我看到全部只有 t 的時候在 i 別的都沒有規定別的都可以那這就是 a  b  c
 ok 所以呢那我我用這個來講的話這個其實是在說推說剛才講的這個這個東西等於這個的意思
所以呢也就是這邊講有看到全部但是 t 要等於 i 或是說看到全部但是 t 要等於 i 的這個條件
那我等於是在講這個東西你如果是要推的話你如果不相信要用推的可以推就是用這個
那有一個條件就是 a 跟 b 要 independent
那麼你其實不要推也可以我們很直覺來看的話直接用這個圖來看幾乎就可以看出來是這樣子
好那麼到這裡我們可以回過頭來講我們現在再來看一下我剛才說 alpha 跟 beta 定義不太一樣是有原因的
第一個不太一樣的地方我們是說這個 t 在 i 的這個條件在 beta 是在這個槓的右邊
而在 alpha 的話呢是在這個槓的左邊為什麼會這樣你在這邊看就很清楚啦
這就是 c 嘛是 c 的這個東西在這裡的話是在算全部的機率是算進去的而這邊的話是算在 condition 這裡的
那麼這樣的這兩個相乘才會條件機率一乘才會乘到這邊來嘛
對不對嗯這條件機率兩個相乘會乘出乘出這樣來就是要讓一個在這個左邊一個在右邊這樣才會嘛
那就是 c 嘛所以當然要一個在左邊一個在右邊才可以那然後我們還有說一點就是還有一點不同是什麼呢
 alpha 的時候這個是 t 的話這個 t 有看到這個 o  t 在這個裡面
可是 beta 的時候呢如果這個 t 的話這個 t 不在裡面為什麼這樣
你這邊看也很清楚因為你這個只能算一次不能算兩次
所以那你這個 t 的時候在 i 的這個機率我歸到 alpha 來了我就不能再歸到 beta 去
只能算一次所以呢這個東西就只有在這邊不在那邊 ok
那這樣大概可以解釋我們剛才講的所有東西好於是我就得到這個了當我得到這個之後呢我其實可以做下一件事情就是
我剛才這個是 problem 　 one 要做的東西你如果看的話這是我的 problem 　 one 的要解的
我 problem  one 就是解這個嘛嗯就是算整個的 o 也就是所謂算整個的 o 就是我讓它所有的 path 所有 path 全部算進去
的機率或者所有的 state  sequence 全部算進去全不管怎麼走全部算進去那是什麼呢
我其實就是這邊講的我只要把這個東西 summation  over 所有的 n 就可以了
因為你現在這個東西已經是看到所有的我看到所有的 o 了但是呢我只有 i
時間 t 只有在 i 上面那我就把它 i 全部加起來嘛就是說我們剛才講我已經看到所有東西了
除了在 t 的時候呢這邊不可以這邊不可以只可以在這裡
那同理我也可以算另外一個 i 譬如這是 i  prime 我的是在這裡
我是在這裡那麼那我會得到一個 alpha  t 的 i 是這個 beta  t 的 i 是這個
那不可以的是什麼不不可以的是這個對不對
不可以的是這個那是另外一個那我把這個所有的 i  prime 全部加起來的時候就把這一行也都
每一個都可以了那不就全部都可以了嗎
對不對所以呢我現在如果把這個我現在如果把不管是這個也好或者這個也好那就是這個跟這個兩個式子
我都把它從 i 等於一到 n 全部加起來的話我把這個地方全部加起來的話那不就是全部了
就所有的都可能就所有 path 都算進去了所以我就得到這個那這個其實就是我們剛才 problem  one 的要解的東西
換句話說我現在如果有了 alpha  t 的 i 乘上 beta  t 的 i 這兩個都有了的話
我只要在任何一行去加都得到這個值你你回去看我們剛才 problem  one 的
的答案是怎麼做的 problem  one 是這麼做的我們剛才是
我如果算這個是 alpha  t 的 i 的話我是這樣一行一行算過來
等到最後一行最後一行算過來的時候我把它全部加起來
最後一行算回來是全部加起來就是我的答案
這是我們剛才 problem  one 的 forward  problem 呃 forward  algorithm 是這樣算的
那我現在說不見得要這樣因為我只要得到如果我現在這個表得到是 r 我我只要把 beta 算出來第二張表之後
把 alpha 跟 beta 相乘得到第三張表的話
那第三張表其實就是我們現在的底下的這個
當我有了這個第三張表的時候呢
我其實就任何一行 i 等於一到 n 去加 t 是那一個 t 是多少那一個 t 都沒有關係
任何一行的 i 一去加的話都得到它
所以呢我只要得到第三張表的時候
我隨便拿一行去加都可以假設說我的第三張表像這樣的話我在這一行全部加起來就是那個答案
那我也可以加這一行我也可以加這一行加起來都是那個答案
那就是我們這邊講的加起來都是這個東西因為如果是加這一行的話呢其實是說 ok
這一點要在這裡其他的都可以
對不對
那我只要把所有的通通加起來不就是全部嗎
那如果在這裡的話呢這裡的任何一點的話意思也是說這一點在這裡其他的都可以
對不對那我只要把它全部加起來就是嘛
就是全部所以我在任何一個 t 在 any  t 這個 t 隨便那一個都可以我只要把這個 i 通通加起來
就是我要這個東西
那也可以說就是只要我只要有 alpha  i  beta 這個 alpha  beta 相乘的這個表的話我任何一行去加
不管 t 是多少任何一樣去加都是這個答案
那如果是這樣的話你現在再回過頭來看我們剛才的 solution 其實就是什麼呢
我們剛才的 solution 就是當這個大這個小 t 等於大 T 的時候
當這個小 t 等於大 T 的時候我也可以嘛我們說任何一個 t 都可以嘛
所以小 t 等於大 T 也可以嘛
當小 t 等於大 T 的時候呢這個 beta 一律等於一阿對不對
這個 beta 等於一阿所以這邊都沒有啦就剩下這個
那就是我們剛才的那一式嘛
所以你看到我們 forward 的最後這個式子就剩下這個嘛因為那還有個 beta 大 T 等於一嘛
就沒有了嘛就剩下這樣所以這個就是我們剛才的那個
所以我們在 problem  one 的這個 forward  algorithm 的這個 solution 其實就是在這裡的我就是算最後那一行
等於是這個意思
那我們現在得到的是 general 的
呃得到是這個那其實每一行都可以
 ok 好那我們到此說了半天我們還沒有解 problem  two
我們現在只是在說這個這個 backward  beta  backward 的的這個 variable  beta
以及它怎麼算然後它跟 alpha 合起來有這樣的意思那有了這些之後我們現在過來看我怎麼求我要的這個 path
那我要求的這個這個東西其實是很直接
你不見得要像這裡講這麼麻煩我們可以回過頭來再看一次
這裡重畫一下嗯
我的這是 t 這是 i  alpha  t 的 i 是說
我這個在這裡
 beta  t 的 i 是說我這個在這裡
因此呢你 alpha  t 的 i 乘上 beta  t 的 i 就這邊底下所剛才那剛才所說的這個
那其實就是把這個連起來了嘛等於說是這樣子嘛
我全部都有了除了這兩塊沒有對不對除了這邊沒有以外我全部都算進去了
那這是 alpha  t 在跟 beta  t 的 i 那如果是這樣的話我在時間 t 到底是那一個 state 呢
我就看它嘛我每一個 i 都可以算我也可以算一個這個舉例來講我這個也可以算
我在就這一點這個 i  prime 而言我可以得到一個 t 的時候在 i  prime 的其他的都可以
其他的都可以可不可以這樣可以阿
那這個就是我看到了全部之後時間 t 在 i  prime 的機率那剛才這個呢是我看到全部之後呢時間 t 在 i 的機率
那我就在這上面看誰最大嘛誰最大的話我 t 就應該在那裡
那麼因此呢有一個很簡單的做法就是這樣子
就是我這邊講的就是 alpha  t 的 i 乘上 beta  t 的 i 也就是我剛才第三張表
那我現在就在這上面去看誰最大
在 i 上面 i 從一到 n 看誰最大最大的那一個 i 就是我的 q  t
就是我的最佳的 q  t 對不對那這個其實就是我們這邊講的這個 approach  one
嗯這個 problem  two 有兩個 approach 它第一個 approach 其實可就可以這樣解釋
就是我每一個 individually 去選擇最佳的 state  at  the  most  likely  state  at  time  t
在時間 t 上我的這一堆東西就是這個機率就是看到就是 probability  of 看到全部的 o
然後這個嗯就是剛才這個式子
就是我看到全部的 o 而且 t 在 i
而且 t 在 i 的機率嘛
那麼既然是這樣就是指我看到全部不過 t 在 i 上面
那我同樣我可以看到全部 t 在 i  prime 上面
我看到全部我 t 在這裡面每一個我就在這一行裡面去看誰最大
最大的那一個如果現在是 i 最大我就說 t 的時候應該是 i
然後呢 t 減一我也一樣去看這一行裡面誰最大
喔發現是這個最大就是這個在這行裡我又看誰最大嗯發現呢是它最大
就是這個那我把這些連起來就是我的 state 就是我的 path
我又我現在要這個 problem 就是要找它的我要找這個這個 state  sequence 所謂的 state  sequence 就就是這條 path 嘛
那我這條 path 怎麼找我就是這樣子找阿我就每一個時間 individual  time 我去算它是這個那個機率最大
那就是這個那然後呢這個在這個瞬間是它最大這個瞬間是它最大這個瞬間是它最大
我就把它連起來就是我要的機率那基本上就是我這邊講的的這一個 approach  one 在做的事
唯一不同的是
唯一不同的是在課本上它不只是這樣子而是它還除了一個東西
那它它做了一個 normalization 的過程然後 define 它叫做 Gamma 嗯
那這個其實是有道理的那麼因為在後面
我們後面還要再看一大堆就有這個 gamma 很有用 alpha  beta  gamma 都很有用了喔
後面還有用所以呢他就先這樣 define 了
那你看我這邊說我其實只要 alpha  t  beta  t 之後看就是在這一行裡面看
那一個 i 最大看那個 i 最大那個最大就是那個 state
就那個最大就就是在那個瞬間的那個最佳的 state 然後把它連起來就可以了
那麼在這裡課本上的寫法呢是不是這樣它這邊還多了一個除法
就是這邊呢再除一下除以 summation 的 alpha  t 的 i  beta  t 的 i 然後 i 等於一到 n
它是這樣子多除一個東西所以呢它 define 叫做這個
它把這個 alpha  t  beta  t 呢都除一個東西之後叫做 gamma 然後呢拿 gamma 再來看誰最大
那其實跟我剛才講的是一樣的因為你除的這個東西是個 constant 我在所有的 i 裡面
去算去看誰最大的時候他們都除了這個東西他們除的東西是一樣的這只是一個 normalization 的過程
它除了一個共同的東西所以所以我們剛才如果看白白色粉筆的部分
誰最大的話我現在用紅色來看還是誰最大所以這個並沒有改變任何事情
所以在課本上講的話呢它是用這個嗯用 gamma 來算誰最大那個最大那個就是那個 state
還有你如果去 check 課本的話他這邊寫成 MINIMUM 是錯的啦啊
在課本上這個寫 MINIMUM 是錯的這個是 MAXIMUM
那麼在課本上它是用 gamma 來算誰最大那其實這個 gamma 就是除上做這個 normalization
其實不除也一樣因為這個對所有的 i 都是相同的
那麼它為什麼要這樣做其實除一除也是有道理的因為除了有意義的
因為你看我把它全部加起來就是這個嘛就是我們剛才講的對不對
我們剛才就是說你把它如果把它全部加起來的話
把它全部加起來的話就是這個嘛就是我我看到整個 o 的機率
所以呢你這個把它全部加起來就看到整個 o 的機率所以分母呢
就是除以看到整個 o 的機率那分子呢是除了看到整個 o 之外我還看到 t 是 i 的機率
那這兩個一除就是把這個 o 放到條件來嘛那就變成是 given  o 的時候我 t 是 i 的機率
那有何不同這只是只是做一個 normalization我們現在要來把這個viterbi algorithm 今天要講完
我們來說一下怎麼樣解這個problem two 呢
最好的的solution 是這個viterbi algorithm 
那它怎麼解的呢它就是從頭開始
就是找一條最佳的best sequence 或者最佳的path 
那它的解它的解法是再define 一個新的東西叫做delta 
嗯這一段不斷的有新的的數學符號跑出來
那麼你如果已經搞得很頭大的話回去要看一下那個reference 
回去要弄清楚一下否則我們再下一堂課就聽不下去了
因為我們下一堂課講problem 三的時候會alpha beta delta gamma 全部全部用還有epsilon 
那麼所以你如果回去要看一下才會弄清楚的話就要回去看一下
先define一個新的variable 叫做叫做這個delta 
那它是怎樣的呢
它是說
我現在如果
它還是一樣是delta t 的i 嘛
所以是在任何一個時間t 任何一個state i 上面都有一個delta t 的i 這個東西
那它是什麼呢
我還是一樣我要given 這個lambda 我在t 的時候要在i 上面
我t 的時候要在i 上面
然後也是一樣我看到從一看到t 
但是我還有一個條件
就是到i 為止我有一個state sequence
而這個state 這個state sequence 是從一到t 為止的
而是所有的一到t t 減一為止裡面最大的那一個
也就是說在t 的時候是到了i 是沒有問題
從t 減一之前的呢
從t 減一之前到一呢
我都不是隨便的囉我現在是一定要有某一條
那一條的機率最大的那一條才是
別的都不算喔
因此我現在只有這一個
跟這一條
其他的都沒有這些都沒有
不像剛才我都是全部都算進來這邊都不算只算最大那一條
好這是不同的地方
也就是說你看我我也是一樣在t 的時候要等於i 
t 的時候要等於i 
而且我是看到從零看到t
這個都一樣
所以這邊跟剛才的alpha 是非常像的
可是我現在不再是像剛才alpha 說剛才alpha 說喔這邊前面前面都可以現在不是
我現在呢是有一個固定的一到t 減一的一條path
一到t 減一的這一條path 呢是所有的一到t 減一裡面的最大的那一個
其他我都不要了
我就把highest probability along a certain single path ending at state at time t state i at time t
那也就是說我就變成是這個之前我要先找好那一條是機率最大那一條就是那一條了
在這裡
這叫做delta
那除了這個不同的delta 之外其他是一樣的
那我還是一樣從頭開始
先想辦法填d 我就是現在這個這個表叫做delta 的
這個叫做delta t 的i 嘛
然後那我就是填這張表
是delta t 的i 變成一張表
我每一點去填
怎麼填也是一樣第一行
填了第一行之後呢就有下一行每一行就有下一行
每一行怎麼算下一行呢
這個基本的原理就是在這裡這個式子
那也就是說如果我這一行有的的話呢下一行其實不難算
我們舉例來講
如果這一行是就t 而言
我看我這邊是用ok 有了t 要算t 加一的話
我的這一行的t
我這一行在t 的時候我得到這一條path
那我在譬如說在這個上面的我有另外一條path 
那麼在這個上面呢我有另外一條path 
等
如果都有的話我這一行都有了
那我下一行呢
t 加一的時候呢的某一個j 
t 加一的某一個j 
它可能是哪來的
它的最佳path 呢可以是從這來也可以是從這來
也可以是從這來
depends on 誰最大
因為我只能找一條最大的
所以我就要看我我如果在t 加一的這個話呢
它可以從這過去也可以從這過去
要看那一條最大最大那個才是別的都不是了
所以呢我就要 del 這個delta t 的i 乘上a i j
去看誰最大
我在這裡也有一個delta 是這個機率乘上這個a i j 
在這裡也有一個這個delta 是這個機率乘上這個a i j
在這裡也有一個是這個機率乘上a i j 
我看誰最大
最大的那個才是
所以呢我要看誰最大
最大的那一個然後我再把後面的t 加一放進去
所以呢我們舉例來講
我算到最後之後發現是
這個才是最大的
發現這個才是最大的
所以呢我就要用這條路來算
ok 我如果是我把這些
它的delta 乘上這個a i j
它的這個delta 乘上這個a i j 
它的這個delta 乘上 a i j 看誰最大之後發現是這個綠色的最大
就把它畫成紫色
這個最大
於是呢我就應該是把這一個的最大那一個乘上a i j
然後我再把t 加一的這個o 的t 加一放到j 的機率裡面去
那就得到我下一個了
ok 就這個意思
所以呢看是是誰的delta 乘上a i j 最大
然後呢我再把我的最後那個t 加一放進去
那這樣的話呢我就得到我所要的
就是我下一個j 下一個delta 
但是我這樣算之後呢我得要記得我是從那來的
這叫做backtracking 
也就是說當我得到這個的時候
我得記得說它是從這來的
我得記得否則的話我後面就忘掉啦
我忘掉了這個是從那來的了
所以我要記得說現在這個得到這個delta 的時候
它是從這來的
就表示說我的前面的path 是從這邊過來然後這邊走它前面的等等
ok
那麼於是呢你可以想像如果我這一行t 加一都填滿的話呢
t 加二會怎樣
t 加二的這個的話呢我又會看它可能是從這來的
可能是從這來的可能是從這來的
看誰最大
不一定delta 大就會大因為還有a i j 對不對
我這邊是要把不是delta 大就會大
而是要你乘上a i j 之後再看誰大
是要這兩個所以你是它可能是任何地方過來
如果這個地方是從這地方過來的話呢我這回就要變成是我的最佳path 從這過去了
那這地方從這來的等等是這樣子的
對不對
所以呢你每一次向前推的時候呢
我又要重新去記得說我就把看它剛才從那過來的
那它是從那來的
它是從那來的它是那來的
我要這樣才知道我這個時候的最佳path 是那一條
ok 
那這個就是這個viterbi 的基本精神就是這樣
那這樣的話等我等我走到最後走完的的時候
最大最大機率就出來了那條path 就是最大機率的path 
恩
那詳細的在底下這一章其實我們仔細看看底下這一章就會很清楚
底下這一章是講我完整的怎麼算的
也是一樣先in initialize 就算第一行
然後呢就是我每一行怎麼算
第一行很簡單
我就是就是把每一個delta 的第一行
第一行因為根本沒有前面的path
根本沒有前面的path 
所以第一行其實就是把這些機率放進去
所以這個跟剛才的那個alpha 是一樣的嘛
就是我第一行我就是先把這個它會從第i 個state 開始的機率放進去就是pi i
然後呢我再把第一個o one 呢我把第一個o one 呢也放進state i 裡面去
那就是b i 的o one 
然後我就乘起來就對了嘛
所以這第一行其實沒有什麼學問很簡單因為前面根本沒有path 
所以第一行就直接就算這個在這機率在這機率就是了這是第一行
有了第一行之後
我們每一行怎麼算
given 前一行算下一行就是我們剛才講的了
那這個式子跟剛才是一樣的
所不同的只是這個寫起來有點不一樣這邊變成從t 減一到t 
也就是說呢
如果是t 減一的話
t 減一的話
它如果是在這裡有一個
在這裡有一個
在這裡有一個
在這裡有一個
在這裡有一個
的話
那麼到t 的時候的這一個從哪來呢
它可以從這來
可以從這來
可以從這來
所以呢我在t  的時候
我要從t 減一
就t 減一這一行我都算完了要再算t 的
i怎麼算
given t 減一的整行
要算t 在i 的那一點呢
啊t 在j 的那一點喔
一樣的t 在j 的那就一樣
就是
我現在
重畫一下好了
如果我t 減一的這一行都有了
我現在要算t 的
然後在j 的那一行
所以我現在要算t  的在j 的那一行
那是怎樣呢
那就是
我可以從這裡的每一個跳過去
但是要看它前面是多少機率
它是哪一個path
它是哪一個path
然後它們各的機率是多少
所以呢
我就是
把t 減一的那一行裡面的每一個i 
它就帶了
每一個i 它就代它前面的所有的path 的機率
那個最佳path 的的機率已經代好了
那麼
乘上a i j
乘上a i  j 看誰最大
那最大的那個a i j 就告訴我說它是從哪來的
於是呢你就知道說ok 
現在發現它這個過來最大
於是呢
我就是
算這樣子的
這些就沒有了
這些都不
它最大的話
這些就不用算了
那這時候再來看它是從哪條路來它是從這條路來的
那我就這樣連起來就到這邊的路
就這樣子
所以呢
我就是只要把前面一行t 減一分別乘上a i j
看誰最大的那一個
那個機率
再這時候我再把最後那個
o t 放進去那個j 
把這個放進去的機率
再乘起來就是我新的了
那於是呢我這個就可以這一行就可以算了
這一行算就下面就可以算等等一行一行算下去
所以呢我這個是算那一行
然後每一行一直算到最後那一行
那你算的中間呢
你都要不斷的記得
我剛才那個最大的是哪一個最大的
那麼因此是從哪裡過來的
換句話說你在這個過程裡面
你等於是在做這件事
我如果這一行都算出來之後
我的下一行的這一個
我必須去看是哪裡過來的
如果發現它是從這兒過來的話
那它呢又是從哪過來的它又是從這樣過來的
它又是從這樣過來的
這樣的話你才得到它最佳path 就是這條
所以這樣我一路走的話呢我一路都知道
我最佳的機率是哪一個
那到這個時候呢我又得到譬如說這點我發現是從這來的
而到這點我發現它最佳是從這來的話呢那我要走這條等等
所以你不斷往前走的時候你會發現每一點它的前面我都要
重新看看它最佳路徑是哪一條我都要重新算
那因此呢我就一路都記得我所有的最佳path 
等於是這樣的意思
所以呢這個phi t 的j 這個符號只是說
在t 的j 算出來的時候你要記得一下我剛從哪來的
是從剛才的那個是哪來的
那個就是這個最大的最大是誰
嗯的那個
i 是哪一個i
所以呢我就把那個i 記得
就這個意思哦
當那個i 記得之後
那我就
我就可以這樣一路它都記得它的最佳從這來的
它的最佳是從這來的我都記得一路都記得的話我就一路向一路向後走
當我走完的時候
最後看誰最大
當我一路走到底
這是大t
譬如說這個是delta t 的i
如果這個最大的話最後這邊我就看誰最大
所以在最後最後在大t 的時候呢我就所有的i 裡面看誰最大那個叫做p star
也就是說呢
這個得到的是
這樣走過來最大的那個機率是這裡
這邊所得到的是這樣走過來最大的那個機率是這裡等等
那因此我最後看誰最大
在我只要在這裡比誰最大就好了
如果發現是這個最大的話這個就叫做我的機率
這就最大那一個
所以呢我就所以我就在這裡去找最大那一個
然後呢
最大那一個找到之後
會最大的那一個就是我的q t 
所以顯然呢這就是我的最機率最大的path就是這一條
所以我的最後一個state 就是這個
就是這個
就是這個
所以呢就是最大的那一個機率的
它的誰是誰最大那一個i 就是我的最後一個state
有了最後這一個state 之後呢我就一路向前推
這個式子看起來有點頭大其實只是一路向前推的意思
剛才這個前面是從這來的
這個前面是從這來的
這個前面是從這來的
這個前面是從這來的
等等  
你就得到哦這樣子這一條就是我的path
那這就是我的機率最大的那一條
state sequence就是這樣子
來 ok 所以呢這個講穿了就是這麼一回事
那麼數學式子有點頭大不過意思是很簡單的哦
那我等於就是第一行先填
然後呢given 這一行就填下一行其實是很容易的事
因為每前面這一行的每一點都已經告訴我走到這邊為止的最機率最大那條path 的機率了
我再乘上a i j 看誰最大就是我下一個
當我找到下一個之後呢我看剛才那個最大是從哪來是從這來的
那
像這樣的話我就一路填了這個之後
我就填下面的每一個我都一路填每一個再一路填下去
填到最後的時候
看誰最大
因為它填到最後的這裡的每一點都是走到最後的
最大的那的那條機率
只不過是ending 在這裡還是ending在那裡還是ending 在那裡而已
看誰最大
如果是這個最大的話我們就知道這個就是我最大的path 的機率
然後呢那它是從這來的
它我一路倒回去
就知道這一條path 就是我最大的path 
那所以這個呢就一路倒回去我先看到
最大的那一個的機率最大的
然後最大的是哪一個i 呢
那個i 就是我的最後一個state
於是呢
從最後這個state 開始
一路去做剛才這個function 
就是去看前一個state 是誰
看它是從哪來的
那我就一路向前推
就是前一個
所以呢q 的t  q star 的t 加一
可以得到q star 的t 啊
這樣子
這樣我就於是一路往回推
我就得到我要的path
那這樣子的話呢那這一條path 就是我的答案
ok 
那這樣呢我的這個viterbi 就解出來了
那麼我要的path 
就得到那條path 也就是我的state sequence 
那麼viterbi
原來發明這個演算法的時候並不是做這件事的
那你在別的課可能也學過
viterbi它原來是做
這個convolution 的code 就是做數位通訊的那個error control code
在那裡面要解那個decoding 的時候有一個跟這個很像的問題
它當初的viterbi algorithm  就是解那個的
只不過後來做hidden markov model 的人發現其實可以拿來做這個
所以他就來拿做這個
所以你在別的課可能也會學到這個
那我們這邊是講把它用在hidden markov model 的時候的情形
就是
假設我現在要做isolated word problem 
現在就isolated word recognition 的話
你現在已經可以做了
譬如說我要辨識零到九的十個數字
零我有一個model 是零
一我有一個model 是一
k 我有一個model 是k
然後呢九我有一個model 是九
假設我要辨識零到九的十個音
每一個音有一個model 在那裡的話
我怎麼做
我可以算
這個
然後看誰最大
所有的k 裡誰最大
最大的那個
就是答案
ok
我就我就算這個嘛
算這個的時候就是我看到了一堆聲我看到我現在
給我零到九的十個model
我給我一個聲音
假如說這個聲音是七
我就把這個聲音七放到零到到九這十個model 裡面去
都可以算這個機率
那麼這個機率之後我看在哪一個model 裡面機率最大的
那個k 
就是我的答案
如果這樣講的話這個我是用我們剛才講的basic problem one 
這是problem one 
所以我可以利用什麼用forward algorithm 來解
但是呢
我也可以把它看成是for 這個problem two 
為什麼呢我可以算另外一個東西
就是算這個
這個這個p star 的這個probability 
這是表示最大那一條path 的
given 我的每一個model 我都可以算
看誰最大
那個最大的那一個
也是答案
ok
我現在我剛才是解這個problem one 
我用forward algorithm 去算這個東西
我現在是把它看成這是什麼呢
這是problem two 
我的solution 是解viterbi algorithm 
就是我們這邊講這個
嗯為什麼呢你仔細看
因為我如果是我對每一個model 我都找到一條機率最高的那條path
它有一個機率是這個
p star 是這個東西
我看這個東西到底對哪一個model 最大
如果你那個是七的話
如果這是七的話
照說會在七的model 這個機率最大
你這個七放到什麼六放到五裡面那顯然會比較低嘛
所以呢它在在七裡面最大
因此我可以用我可以用這個用這個機率看誰大
也可以
那這就變成變成problem two 的問題
變成解viterbi algorithm
那你注意到這兩個機率是完全不同的東西
不同在哪裡
這是算所有的path 
或者說就是所有的state sequence 
但是這個呢是一個單一的path
這是單一的path 
或者是單一的state sequence 
但是呢with 最高的機率
ok
也就是說
你給我一個聲音的時候
七
在那個任何一個六的model 的裡面的話我有無數非常多的path 都可以走的
那麼剛才左邊那個機率是把所有的path 全部機率通通加起來了得到一個
所有機率通通算進去得到一個機率
右邊呢是我只算機率最大那一條path
所以這個只算那一條path不過是算機率最高的
ok 
這個只算那一條path 機率最高的
而這個呢是算所有的path 
那這兩件事情顯然是不同的
所以呢你必須要知道這兩件事情是完全不同的
可是呢你如果去找他們最大的
去找這個arc
常常是一樣的
也就是說最可能的那個model 通常是同一個
擦掉了
最可能的model 常是同一個
所以雖然這個機率不等於這個機率
但是我現在不是要算這個機率
我是要算看這個機率誰最大
那最大的那一個呢常常這個機率也是最大的
也就是說雖然這個把所有的path 都算進去這個只算一個
其實這個dominate 嘛
這是最大的那一個
最大的那一個基本上dominate 那裡面哦
所以呢我只要算這個最大的通常也就是那個最大的嘛
所以你如果是算哪一個model 最大的話呢我是在求誰最大
那通常是同一個
雖然這兩個機率是完全不同的
ok
所以呢我們現在要講剛才講這個解這個isolated word recognition 的話呢我們可以這兩種方法都可以解
你如果去看這兩個演算法就是forward algorithm 跟viterbi algorithm 演算法好像也複雜度也差不多所以好像是都可以
是沒有錯這兩個都可以做
那麼但是我們到後面會講到八點零的時候我們會看到
畢竟其實二比較容易做
為什麼當你有無限多個詞有六萬個詞
每一個詞後面可以接無限多個六萬個詞的時候
你六萬詞後面可以接六萬個詞每一個詞後面都有六萬詞的時候你這個變非常複雜
你如果所有path 都要算的話不得了
我不如只算一個最大的path 哦
所以我們後面會看到你真正的problem 你如果是連續語音
不只是辨識十個音而是連續的有非常多的變化的話
你其實是每一次永遠只算最大的比較容易
你把全部都算進進來是很複雜的
所以我們後面用得多是viterbi 
那雖然在現在來看如果是算這個isolated word problem 的話呢
這兩個是一樣的
ok 
好我們viterbi 說到這裡
我們保留剩下最後一件事情要說的是補課
那麼下週停課一天啊抱歉
下週停課因為我出國
那包括我們上次二二八也停過一次課
已經停了兩次課我們進度已經落後很多所以我們現在
應該要找到兩次補課的時間
才可以補得回來哦
那我想我們今天至少要確定一個
如果確定兩個更好
我們上次說過
可以補課的時間是禮拜六下午
似乎是一個大家都可以的時間
所以我們就以禮拜六下午為目標
然後我們讓時間可以flexible就是說
所謂禮拜六下午
最早是十二點半到三點半
最晚是三點半到六點半
ok 那我們就儘可能調到大家都可以的時間
那另外呢我們這個錄影錄音的同仁告訴我們其實他們現在呢可以提供錄影錄音哦
因為這間教室的設備都做好了
所以萬一我們補課的時間你不行的話
那麼這個
應該可以有別的辦法看到那一段哦
只是說這個怎麼怎麼做我們後面技術問題再來解決不過應該是做得到
嗯不論是用什麼方式嗯
所以應該也可以解決哦
那ok 
那我現在看最可能的時間
三月十八號就是本週六
二十五號下週六
四月一號
四月八號
四月十五號
這都是禮拜六
再下來這週就是期中考週了
所以從這個觀念來看我們儘量往前面是比較好你到後面比較接近期中考了
最快的一次補課是三月十八號是本週六
如果是本週六的話因為我是下午六點的飛機
所以只能最早
就是十二點半
到三點半
如果是這個的話我們可以本週六補課
ok 
那如果是本週六這個不行的話我們就是下週六
下週六就比較flexible 一點
我想最早是十二點半到
三點半
最晚可以從三點半到
六點半
我們中間可以調
讓所有的人都方便的人都可以最好
對不對
這個是那個溫書假所以這個其實不太好
如果這個不太好到這邊已經接近期中考了可能也不好
所以我會prefer 是前面這兩個
ok 有沒有問題
我們說最快的補課是本週六的十二點半到三點半
春假的這個禮拜二我們基本上假設它是放假
ok 我現在講的補課補兩次是指一次補下週一次補二二八
對不對我還沒有補春假
恩
春假是有可能會造成另外一個問題因為我後面我還會出國一次
所以後面可能還要補
但是我想我們現在不要講太多嘛對不對我們現在如果是講兩次其實一次是補二二八一次是補下週
我們至少先找出這個補課的的時間
所以呢最快的補課時間是本週六有沒有問題
沒問題ok 
所以呢本週六的這個嗯十二點半到三點半好不好我們這個最早的這個時間
這個只有這個時間
然後呢我們要不要訂第二次第二次最快就是下週六
下週六的話可以早可以晚
我個人會主張早一點因為這樣三點半你下午還有整個半天嘛對不對
你對不對你如果是三點到六點這個你就把整個週末都搞掉了
前面也沒什麼時間後面也沒什麼時間對不對
所以我會prefer 就是早一點是比較好的
好不好
那我們就先訂這兩次好不好本週六跟下週六
把這兩次補下來我們的至少進度我們可以追上來
這樣子後面就比較好辦事
我們春假還要不要再補課我們再看這樣子我們至少先補這兩次
ok
好那就這樣子喔那我們基本上地點在這裡
如果不是的話另外公佈
ok
我們基本上我們來借兩個這個地方但是如果不是另外公佈
ok 
好
今天上到這
 OK 我們上次把 problem  two 講完
今天講進入這個 problem 三
那麼我們來看一下 basic  problem 三是什麼問題
那基本上是一個嗯假設我已經有一個 model 我知道這個 model 是
這有一點被切掉了這是那個完整的 model 參數這個 a  b  pi 這個 pi 掉了那麼我 given 這個 model 之後
我知道這個 model 是某一個音譬如說九那麼我也知道這個聲音是現在是一個新的九的聲音
我要把這個新的九的聲音 train 到這個 model 裡面去讓這個 model 呢裡面可以學到這個新的九的聲音
那麼使得呢那麼這個新的聲音的話呢那麼它可以這個的
當我把這個新的聲音放進去算的時候它的機率可以最大但是呢它不能不能這個
干擾到原來的那些訓練就是說原來這個 model 也是九的聲音訓練出來的啊
那麼它們的的這個所描述的那個變化呢不能夠被破壞但是我要把這個新的聲音也訓練進去
那這就是就是這個 problem 的意思那這個 problem 比較複雜一點這個比前面的一跟二都還要複雜那麼
它基本上是一個沒有 close  form 的的問題所以呢我們只能用 iterative 的方法來解它
所以它是一個沒有 close  form 
 solution 然後呢但是呢 can  be  solved  iteratively 
那麼這個解的方法呢就是所謂的有一個名字叫做 forward  backward  algorithm 
也有另外一個名字叫做 Baum  Welch  algorithm 
所以一般你在文獻或者書籍裡面看到所謂的 forward  backward  algorithm 或者 Baum  Welch  algorithm 都是指這一個
那麼 forward  backward 是比較講它的意思你可以猜得出來是因為用了我們前面的 forward 的 alpha 跟 backward 的 beta 然後它兩邊兩邊都在跑一邊 forward 一邊 backward 所以叫 forward  backward  algorithm 
那麼它也有一個名字叫做 Baum  Welch 這是兩個人的名字
那麼那它的這個詳細的基本的原理呢是有相當複雜的它的原理基本上是 base  on  E M 所謂的 E M  algorithm 
或者叫做 E M  theory 
也就是說我們底下講的東西它是從根據這個 E M 的推導來的
那麼 E M 本身是一個有一堆複雜的 theory 所以有的人稱為 E M  theory 
那麼它本身這個也是一個不能解的只能夠 iterative 的解
所以呢需要一套 algorithm 來解它所以有一堆人稱之為 E M  algorithm 
那麼我們現在講的這個方法是根據 E M 來解的至於 E M 是什麼呢 E M 比較複雜一點我們會留到學期中以後再講 E M 
所以我今天大概並不真的講 E M 
我只是大概講一下這個 forward  backward 的流程跟它大概的意思
我們了解怎麼樣就好了那有一堆詳細的理論我就留到學期中講 E M 的時候再來講它 
OK 好那麼要解這個問題我們剛才講
我現在知道某一個聲音它的 model 已經有了但是那個 model 是用一堆別的聲音 train 一堆那個聲音 train 出來我現在有一個新的聲音進來
我知道它是它的那個聲音我要把它的學進去使得那麼這個新的 model 呢可以算這個機率的時候可以最大
那麼怎麼辦呢我現在 define 一個新的參數叫做 epsilon 又來了那麼我們之前已經有了 alpha  beta  gamma  delta 現在再來是 epsilon 
那麼你看這個 epsilon 的定義裡面就把我們之前講過的 alpha  beta 這些東西都用進來
那麼它是什麼呢我們看一下它的意思大概就能夠了解我們仍然用之前的那張圖來畫
這個橫軸是時間
T 從一二到大 T 縱軸是 state 一二三到大 N 
我們用上次那個圖那麼它現在講的是什麼呢我要在時間 t 的時候在 i 時間 t 加一在 j 的機率
所以呢譬如說時間 t 的時候是在 i 
在 t 加一的時候呢在 j 
 OK 所以呢它的要求是時間 t 的時候在 i  t 加一的時候在 j  given 整個的全段的聲音我 given 從零到 t 一到 t 然後呢 given 這個 lambda 
那這是什麼呢怎麼算呢你看它這個分母分子這項其實是很容易看的
 alpha  t 的 i 是什麼就是我們講過就是在這裡的時候 t 的時候是 i 之前呢
通通都看到了但是沒有規定它要是什麼 state 這個是 alpha  t 的 i 後面到後面 beta  t 加一的 j 是什麼呢
 beta  t 加一的 j 呢我們說我是 t 加一要在 j 這裡但是我呢沒有看到它我是看到這個以後的全部
而且沒有規定它在哪裡所以呢這個是 beta 的 t 加一的嗯 j 那前面這個呢是 alpha  t 的 i 
那那你就馬上會發現會怎樣呢其實 t 加一我沒有真的看到這兩個算進來的時候
那麼我已經規定好 t 要在 i 了 t 加一要在 j 了這兩個條件已經我所規定的條件已經在這裡了
譬如說這兩個條件可是呢我現在沒有看到 t 加一因為 beta 的 t 加一的話呢只是看到 t 加二以後的
這點我們上週前一次說過了 beta 的定義是如果這邊寫 t 加一的話呢這 t 加一沒有看到是看到以後的
所以它是從 t 加二以後看所以這個沒有看到啊所以呢我現在要把這個 o 的 t 加一這個東西呢
放到這個 B J 來這樣子我才看到這個所以那就是這一項
 OK 所以呢我現在要把這個一旦放進來之後呢 OK 那現在我都看到那這個呢就是 B J 的嗯 b 的 t 加一的 j 
那當這個放進來之後呢那這裡有了那還差一個什麼還差一個這個的 transition 就是 a  i  j
對不對所以我還要有一個 transition 就是這個這個就是 a  i  j 
於是呢這麼一來的話呢我現在就等於是從頭到尾我都看到了
而且呢知道 t 在 i  t 加一在 j 而且呢一路這樣 trans 過來全部都有了
那麼因此呢這個分子這個東西呢就是我們這邊講的這個
對不對就是這個嘛那你看就是我看到全部的 o 了而且呢 t 在 i  t 加一在 j 對不對所以呢這是分子這樣做其實就是在做這件事 
OK 那麼分母的話呢分母我現在其實我要除的就是這個東西這個東西我們現在已經有很多種求法了
你記得我們在 problem  one 的時候的 forward  algorithm 也可以求它那 problem  two 的時候我們解第一種方法的解法的時候也求的出來
這個我們都講過了那它這裡的話你其實用任何一個都可以都可以做
但是它這裡的辦法就是把完全把上面這個照抄下來然後把所有的 i 跟 j 全部加起來也是一樣的意思
那你看因為我你看這個式子就知道它的分子長這樣它的分母呢長得完全一樣
只是我把全部的 j 全部加起來全部的 i 全部加起來那也就是說
我分子是規定它在這裡跟它在這裡分母的話我把這個的所有的通通都加進去嘛
那麼把這個也全部都加進去那就變成全部都是了嘛對不對那麼因此我就不再規定 t 加一在 j 
 t 在 i 這不規定了因為這邊我全部都加滿了所以呢我只要把這兩個這個 j 跟 i 從一到 n 全部加滿的話那這回這兩個條件就沒有了
於是呢就是就是這個式子所以呢這個式子又有一個新的新的求法是這樣求的那它的意思是完全一樣的 OK 
那我如果是把這個分子拿來 j 全部加起來 i 全部加起來就跟這個一樣就變成這樣子
那當我這個有了之後呢分子跟分母一除會怎樣就把這個 observation 放到條件這邊來就變成這樣子
那這個做法我們之前也已經看過求 gamma 的時候就是這麼做的我們之前求 gamma 就是這麼做的
所以呢你就把它這個一除之後呢就把這個 o 放到右邊來就邊成這樣子那這個就是 absolute  t 的 i  j 
也就是說呢你給我整個的 given 全部的聲音我都看到了然後呢我又 given 那個 model 的話
那其中 i 在 t 在 i  t 加一在 j 的機率呢那就是這個 absolute  t 的 i  j 
 OK 好有了這個之後呢我們再回想一下我們之前講的 gamma 是什麼
 gamma 跟這個很像只是少了一個 t 加一的條件而已
 OK 這個 gamma 是我們在這個二 problem 二的時候解過就是
這裡的 gamma 那 gamma 跟剛才那個其實是蠻像的我只是只是這個這邊 alpha 跟 beta 都是 t 然後都是 i 這個分子這堆東西呢分母也是把它全部加起來
等於是 normalize 一次那這個就是 gamma 那它的它的這個意義就是這個
我看到這個整個的 lambda 看到整個的聲音之後 t 在 i 的時間叫做 t 的 gamma  i 這個我們在上上次已經講過了
好那我現在就把這個帶過來看回到我們剛才說的地方就是在這裡
那麼我現在 gamma  t 是這樣的我已經有了
好那麼我現在要做的底下這兩件事情就是我把 gamma  t 的 i 從一到 t 減一全部加起來
以及我把這個 absolute  t 的 i  j 也是 t 到一等於 i 全部加起來
這兩個一起都加起來那這個是在幹嘛呢
我們再畫一次剛才的那個圖
這個橫軸是時間 t
縱軸是 state  n 
這是一二三到大 t 這個是一二三等等這個是 i 這個是 j 這個是 n 
好我們先說 gamma  t 的 i 是我看到全部的聲音之後在 state 在 t 的時候在 i 的時候譬如說在 t 的時候在 i 的時候
這是這個是 gamma  i 的這是 gamma  t 的 i 的機率那我現在如果要把它從小 t 等於一到 t 減一全部加起來會怎樣你可以看這個是 t 這個是 t 減一
這是 t 減一那麼你要把這個 gamma 從第一個開始第一個第二個第三個一路加起來加到 t 減一為止
把這個全部加起來那是什麼意思呢我們舉個例子來講
我假設說這個 t 等於兩百五十也就是說我這一句話的聲音總共有兩百五十個 vector 在這裡
那 t 減一呢就是兩百四十九
那麼於是呢在這中間會怎樣呢這個每一個都有一個機率
每一個 gamma  t 我們現在的這裡的每一個 gamma  t 的 i 都有一個機率那麼我們舉例來講譬如說這個呢
譬如說這個是零點嗯我們說這個是零點零零一
這個是零點零零三那麼到中間的時候呢譬如說呢這個變成零點一二這個變成零點二三
那再來呢變成零點一五到最後我又變成譬如說零點零零二等等等等
這個變成零點零零一
我舉例來講假設是這樣這什麼意思呢因為它是 t 在 i 的機率嘛
 t 在 i 的機率的話那你可以想像因為我們看到大部分的狀況我們這個 state 從這邊往上走對不對從這邊往上走走到上面來
然後所以呢在時間早的時候是在底下時間長的時候是在後面嘛那假設這個 i 在中間的話呢
那大部分的狀況其實是在中間的時候我這個會在這裡通過這邊就會通過的機會很少
這邊就會通過的機會很少那是為什麼你看我這邊寫的數字前面只有零點零零一很小很小
到後面多起來零點一二零點二三這邊是比較高的這邊比較會多一點然後到這邊的話呢那就又少了
嗯等等那意思就是說大部分的狀況它應該在中間通過到那邊去
好假設我把這些東西就是我的 gamma  t 的 i 的話那我現在把它全部加起來對不對這邊是講是加起來
加起來的話我們舉例來講我把它全部加起來的結果呢
那麼假設這些東西全部加起來
加起來呢是等於譬如說十點五假設說是十點五
那是什麼意思呢你那個十點五意思就是說你你可以想像在這兩百四十九次裡面 
in  average 那麼你有多少次這個 state  i 是 visit 也就是說你可以想像我現在總共有兩百四十九個 vector 嘛
那麼這兩百四十九次裡面平均有十點五次的時候它是在這個上面的等於是這樣的意思
對不對你再想一次因為我現在 gamma  t 的 i 是這個意思啊就是我現在已經看到這整個的我看到這整個的 o 了然後呢 t 在 i 的機率
所以呢 t 在這個一在 i 的機率只有零點零零一因為它一開始不太可能在這裡二在 i 的機率有零點零零三稍微多了一點
到了這個地方呢比較多了零點一二零點二那是比較多了那這樣到後面又會少了因為它不是在後面
那這樣最後加起來是十點五的話那等於是說在這兩百四十九次裡面我們可以說有十點五次
平均總共有十點五次譬如說這個在零點二三等於說有零點二三次的時候它會在這裡因為在這裡的話它會有零點二三次在這裡
那這裡有零點一二就表示在這個時候它會有零點一二次在這裡對不對那我把它全部加起來就十點五次在這裡
那麼因此呢你就可以想像成這個是我的 state  i 總共被 visit 的次數是譬如說十點五次
好那如果是這樣的話那還有一個 epsilon  i  j 呢我也可以照樣來做這件事
那 epsilon  i  j 是什麼呢你再回想一下我們剛才講的是這個啊那意思就是說跟剛才很像我現在看到全部了
我看到全部了但是呢 t 也是在 i 的但是我規定了 t 加一是等於 j 的所以呢就 j 而言呢是這個
這些東西但是呢是 t 的時候要在 i 那就跟底下這個條件完全一樣但是我多了個條件 t 加一要在 j 
所以呢就 t 等於一而言它講的是二多了一個二那麼它呢是譬如說我這邊是零點零零一
那麼 t 等於二的時候呢我們講的 t 加一是三那它呢是零點嗯我們說零點零零零一好了這零點零零零三等等
那到這邊的時候它可能也會多起來因為它這個會往這邊跳
那這個譬如說我們說零點零零點一一嗯譬如說等等
那麼等到 t 等於 t 減一的時候呢那麼 t 加一就是大 t 了那這個時候我有一個機率譬如說是零點零零零一
那我這邊寫的大小也是一樣譬如說讓它在中間比較大就像我這邊剛才畫的中間比較大的原因我剛才解釋過就是說這個
假設它這個是在中間的位置的話中間比較會通過它嘛機率比較高一點
那麼什麼意思呢意思是說這個零點零零一是這邊有零點零零一這邊有零點零零零一
表示說我從這邊會跳到這邊來的情形是這樣我有零點零零一的機會它會在這裡
那在這裡面的還有十分之一的話下一個會跳到這來另外十分之九可能沒有跳過去跳到別的地方去了或者怎樣
對不對你看這個我這邊底下是 gamma 這上面是 epsilon  gamma 的話呢只有講 t 
 t 的時候在 i  epsilon 的話就多了一個 t 加一在 j 所以呢你可以想像假設
零點零零一的時候是在這裡的話有這麼多機率它會在這裡那這裡面只有十分之一的機率它會下一個跳到這來
那就零點零零零一那另外十分之九呢它可能也許是停在原來的地方或者跳到別的 state 去或者怎樣它不在這裡
那同理呢那麼零點零零三在這裡那但是呢我這邊也應該多加一個零
我零點零零三的時候在這裡有這個機率在這裡可是呢它除了這個之外我同時再下一個 t 會跳到
 t 等於三的時候它會跳到這裡來的機率呢是零點零零零三那這樣又表示我有十分之一的可能在這裡等等
那如以此類推的話呢我現在把這些東西通通加起來
於是呢我這些東西我也加一次這些東西加起來譬如說總共等於一點二三
那是什麼意思呢那就是說
我平均我可以猜說我在這整個時間裡面那麼我會從 i 跳到 j 總共有幾次
總共一點二三次那你可以看到說我在這兩百四十九次裡面平均有十點五次它是在 i 上面
有十點五個 vector 有十點五個 vector 它在 i 上面但這裡面呢會跳到這個 j 的呢是一點二三個
那麼因此呢我們就有底下的這個式子就是下一頁的這個式子我們怎麼估計它的 a  i  j 就是這個除這個
也就是說那麼我的 a  i  j 呢
怎麼算就是這邊的十點五分之一點二三
那你說這合理不合理呢
意思就是我剛才講的這個意思那這個怎麼推出來的這個其實是用 E M  theory 推的我剛才
剛才寫在這裡 E M  theory 這個並不是這樣隨口亂講的那事實上是有 E M  theory 整個可以導出來最後證明它就是這樣子
那我們現在不去導那個東西所以我只是把答案寫出來而已所以這個是 results 我只有把答案寫出來那那我們現在再解釋說這個答案這個式子其實是有意義的這意義就是我剛才講的意義
那我們可以再看一次這意義再講一次就是說
我的分母是這個東西那也就是我假設這個 t 是兩百五十的話我現在是從一看到兩百四十九
我有兩百四十九個 vector 這兩百四十九個 vector 裡面它都有機會掉在 state  i 裡面不過呢有的時候機率非常小零點零零一啊零點零零三啊什麼
那你把這些機率全部加起來是十點五的話
你可以假設說就是這兩百四十九次裡面有十點五次它是在 i 上面
那這個是 gamma  t 的 i 它就是算 q  t 在 i 的時候那另外我再加這個 absolute  i  j 
那這個呢就多了一個 t 加一呢在 j 的條件比剛才就多一個比剛才多一個嘛所以呢這個跟這個相比的話你就知道它就是
除了這個要在 i 呢下一個還要在 j 的意思嘛
這個在 i 下個還要在 j 嘛那這個東西也就是我們剛才講的這個意思
就是你要用這個來算就是這樣算出來也就是我們剛才講的上面這個算法那麼所以呢你現在如果說是
把這個也算進去看的話於是呢我現在在這個這個嗯譬如說呢
我還是 t 等於 absolute  i  j 的時候我還是 t 等於一開始做但是 t 等於一的時候是指
 t 的時候在 i  t 加一就是二嘛就表示二在 j 所以說 t 等於一而這個這個一的時候等於 i 二的時候等於 j 的時候
二的時候跳到 j 的機率是多少呢我們說如果是零點零零零一的話是它的十分之一就表示說呢這裡面呢
我有我有十分之一的機率當我在這裡之後有十分之一的機率會到這來另外十分之九呢它可能回到原來的或者是到別的地方去了
那同理這個是 t 等於二的時候這邊是零點零零三那這個是零點零零零三又是有十分之一的機率跳到這來
另外十分之九呢到別的地方去了等等那我每一個都可以加那我這邊還是一樣從一加到兩百四十九
但是對 t 加一的 j 而言呢是從二加到大 t 這是我能夠觀察到的是 t 嘛也就是從二加到兩百五十
 OK 那麼於是我把這些東西全部加起來的話你就可以想像總共有一點二三次是會從總共有一點二三次在這個兩百四十九次裡面
總共有一點二三次它會從 i 跳到 j 去那如果是這樣的話那就可以做我們底下的這個式子你怎麼估計這個 a  i  j 呢
就是這樣估計你就是把這些東西找出來然後呢我總共有十點五次是在 i 上面
但是有一點二三次是從 i 跳到 j 那麼一除的話我的 a  i  j 這就是我的 a  i  j 那也就是我們剛才講的這後面講的解釋
那麼你譬如說這個加起來這個這個分子分母的話呢就是平均有多少次在 state  i 上面
也就表示平均會有多少個 transition 從 i 出去
總共有十點五個 transition 從 i 出去那這十點五個 transition 裡面有一點二三個到了 j 
所以呢這個東西呢就是那一點二三就是從 i 到 j 的總共的 transition 是一點二三次
於是你這兩個一除那我就得到這邊的 a  i  j 的這個公式
那麼 pi 怎麼求 pi 不難求因為 pi 只是一開始的
那就是說這個東西會在這裡的機率是多少這是 pi 的意思嘛那其實就是 gamma 的 t 等於一的時候的 i 嘛
所以呢就是這個嘛所以呢那這個我剛才 gamma 已經求出來所以我會有 gamma 嘛我就把這個東西放進來就是這堆 pi 
好這個是所以我有新的 pi 了我有新的 a  i  j 了
那麼 b 怎麼求呢這個其實不太需要了因為這是指 discrete 的時候用的
那麼我們姑且看一看這個很容易看所以姑且看一看其實我們要講的是底下那個 Gaussian 的 case 
那這個東西其實不難看它的意思跟剛才你從剛才看就很了解是一樣的你看我的分母也是這個嘛
分母幾乎是一樣把 gamma 加起來分子也是 gamma 看它掉在哪一個上面因為這裡講的這個 discrete 的 B K 的 j 的意思是這種東西就是
我那個 distribution 它不是用一堆 Gaussian 
這是 discrete 不是用一堆 Gaussian 來來描述而是什麼呢用一堆點它說每一個點各是多少
它說這個點是零點零零零六這個點是零點零零一三等等它用這個點零點零零七五
它就把一堆點用把一堆點的數值通通標出來的這種這是所謂的 discrete  Hidden  Markov  model 
那這裡的每一個點呢就是所謂的 v  k 這些點就是 v  k 
所以那它這個機率怎麼來的這種零點零零零六這個機率怎麼來的呢就是這樣子跟剛才一樣
譬如說你現在如果 state  j 的話就是指這個 j 嘛對不對所以就是這個 state  j 嘛那在這個 j 裡面
我就看這個 j 會有一個對這個 j 而言會有一個 distribution 就是這個 distribution 
這個 distribution 我就 specify 這個 v  k 的值是多少就好了那怎麼看呢那我就是看說
我的這個我我掉到 v  k 上面的有多少個那麼這是 total 對不對
譬如說我現在的分母還是跟剛才一樣那麼唯一不同的是我現在可以把第兩百五十個也算進來
唯一不同是這樣因為我這邊現在不要做 transition 了剛才是剛才為什麼是都是到 t 減一呢
為什麼是到 t 減一因為還要有一個 transition 我是在算 transition 所以最後一個 transition 是從 t 減一到 t 的
對不對這個所以這樣的話我只能算兩百四十九個所以剛才這邊都是到 t 減一我現在只算是這個的話
沒有這個問題了所以我就算到兩百五十所以我如果把這個我如果把最後的這個嗯
我如果把最後的這一點也加進來之後呢我得到一個呢是變成十點六三
 OK 那這樣子的話呢我的分子就是分母就是十點六三然後在這些裡面我看它們的 o 
有多少個可以應該可以歸到是算是這一點的
那其實它這個方法是要算 distance 然後根據這個 distance 來說 ok 它可以算是這一點等等看有多少個所以這就是分子就是這個全部裡面
你看這些這些裡面是哪些可以歸到這個的然後把它加起來那譬如說這裡面加起來如果是
這個總共是一點二三可是但是一點二三裡面很多那些 o 是在不同地方
那麼哪些的 o 是可以距離跟這個最近可以歸到這邊來的呢假設加起來只剩下你把那些個加起來剩下這些譬如說是零點零六於是這個就是零點零六
那這個呢就是我們的這個嗯我的在 v  k 的那一點那這個一除就是零點零零六那就是這一點嗯譬如說這個這樣子
所以呢這個沒有什麼特別只是跟剛才一樣我們其實這個它就這樣算出來
那這個是 discrete 的部分那我剛才講這些其實不是像剛才那樣用嘴巴講的它其實都是用 E M  theory 推出來的
不過 E M  theory 的推的過程我們留到學期中以後再說就是了那我們現在先只是講你看這個式子其實都可以了解它的意思
那我們真正 concern 的不是這一個而是哪一個是 GAUSSIAN 的時候也就是說呢我們真正 concern 的是這樣子
在 j 的時候不是這樣一個點的而是一堆 Gaussian 
因此呢我可能有一堆 Gaussian 
那我要算這堆 Gaussian 的 mean 跟 covariance 那才是我們要 concern 所以底下這個是 continuous  density  H  hidden  Markov  model 
那這個時候我的 B J 呢變成一堆 Gaussian 相加所以呢我要我要求的是 Gaussian 的 mean 跟 Gaussian 的 covariance 
以及那個 Gaussian 的 weight 這三組參數那這三組參數該怎麼求所以我要求的第一個是 Gaussian 的 mean 
譬如說這個 Gaussian 有一個 mean 這個 mean 是一個三十九維的 vector 這個我要求得出來第二個這個 Gaussian 的分散的程度
怎麼分的那就是它的 covariance  matrix 
然後第三個每一個 Gaussian 有一個 weight 加起來要等於一嘛對不對所以我要求這三樣東西那這三樣東西怎麼求呢在下一頁我又要 define 一個新的東西叫做 gamma  t 的 J K 嗯
這個東西有一點複雜不過呢其實蠻簡單你如果記得 gamma  t 的 j 是什麼的話 gamma  t 的 j 是什麼呢
就是我們剛才講的這一個
我們剛才一直在加這個東西嘛就是這一個
那它就是這個嘛就是我剛才加的這些東西就是 gamma  t 的 j 
那麼就是說我看到整個的聲音而我時間 t 的時候掉在 j 的掉在 i 的機率叫做 gamma  t 的 i 或者 j 
那我現在要再 define 一個新的東西叫做 gamma  t 的 j 跟 k 再多一個 k 那 k 是什麼呢
你如果仔細看一下的話這個 k 呢它是說就是包括我在第 k 個 Gaussian 的機率
那其實呢你只要看這裡就好了看這一項其中前面這一項其實就是我們原來的 gamma  t 的 j 前面這一項就是我們原來的 gamma  t 的 j 
那就是多了後面這一項那後面多了這一項就是把這個 k 算進來的意思
OK 那麼前面這一項就是我們我們這個之前講的 gamma  t 的 j 那基本上就是 alpha  beta 相乘 normalize
那這個東西呢也就是我們之前在 problem  two 的時候上次上課在講 problem  two 的時候
講的這個 gamma  t 的 i 這個是 problem  two 的時候第一個 approach 所講過的東西
那這個你看我們當時說過這個 gamma  t 的 i 就是這個就是我們這邊講的這個啦就是這個啦
那它的式子就是這樣算的嘛 alpha  t 的 i  beta  t 的 i 除以這個嘛那這個也就是我現在的這個式子
就是這個式子 OK 所以我就是就是這個 gamma  t 的 j 但是呢我後面多了一個 k 我剛才是只有一個 index  j 的
現在變成兩個 j 跟 k 那 k 是什麼呢 k 就是後面這個後面這個是什麼你仔細看看就是 Gaussian 
那麼上面是一個 Gaussian 底下是全部 Gaussian 的和什麼意思呢我們如果用 one 一個 dimension 是比較容易想像它的意思
就是說假設我有很多個 Gaussian 
那麼這是一個 Gaussian 這是一個 Gaussian 這是一個 Gaussian 這是一個 Gaussian 這是一個 Gaussian 
那麼今天呢假設第 k 個 Gaussian 在這裡
譬如說這是第 k 個 Gaussian 
這是第 k 個 Gaussian 
或者叫做第 k 個 mixture 這邊常常寫成 mixture 因為它們都稱為 mixture 所以一個 Gaussian 就是一個 mixture 
如果這是第 k 個 Gaussian 的話呢那麼所以呢分子就是第 k 個 Gaussian 所以這是第 k 個 Gaussian 的 mean 跟 covariance 等等等等
而分母呢是全部加起來的然後我他們都拿那個時間 t 的那個 vector 放進去
我現在講的是 t 嘛這個是 t 嘛所以我是把時間是 t 的那個 vector 放進去所以呢譬如說時間 t 的那個 vector 呢是這一個
這個是這個是 ot 的話放到這來的話呢在第 k 個 Gaussian 的時候在這裡
這樣我們就用這個好了這個是在 t 的時候在這裡可是如果放在全部的話呢它是一個它是一個
有這麼多 Gaussian 加起來它是一個可能是一個這樣子的
是一個這樣的東西所以呢那個呢是這個
所以呢我的分母就是這個 Gaussian 的這個全部的 Gaussian 的值分子是這個 Gaussian 的值 OK 所以後面這項是這個意思嘛
就是說我現在把我那個這是一大堆 Gaussian 的組合嘛對不對一大堆 Gaussian 的組合那你看你是要看一個單一的 Gaussian 還是看全部
那我如果是單一的 Gaussian 的話我把我的 ot 我時間 t 的 vector 放在那裡得到一個值
但是呢我如果全部加起來的話我也可以放進去也得到另外一個值看看這兩個的 ratio 是多少
那這兩個的 ratio 顯然告訴我的意思是說對這個 vector 而言我現在在算這個 ot 我現在是時間是這個 t 嘛
我對這個 ot 而言對這個 vector 而言那麼它在這一個 Gaussian 裡面
會得到多少分數跟在全部裡面的比例是多少那那個比例其實就是我們到時候就是要算這個
就是算我的這個 weight 就是在 state  j 在的時候呢第 k 個 Gaussian 的 weight 多少嘛
也就是我要算就這個東西就是
當我在變成這個的時候一大堆 Gaussian 加起來每一個 Gaussian 要有一個 weight 嘛那這個 weight 怎麼算這個 weight 就這麼算
所以呢我是要跟這個有關 OK 好那我們現在再看一次我現在算這個 gamma  t 的 J K 的目的是為了要算這個 weight 嘛哦
那我怎麼算呢我就是把 gamma  t 的 j 先算出這就是剛才講的那個
就是在時間 t 的時候它平均會掉在 i 上的機率是多少這個時候我是只要掉在 state  i 就好了
我沒有管它掉在哪一個 Gaussian 裡面對不對所以呢 Gamma  t 的 j 呢只是說在時間 t 它在 j 上面就好了
那麼沒有管它在它在哪一個 Gaussian 上面那所以在後面我再多乘一個東西就是要講它如果在哪一個 Gaussian 上面的話會怎樣
所以呢我就要算它在這個 Gaussian 上面除以它在全部的這個 ot 放進去它的 ratio 差多少 OK 
好那如果這個了解的話那這個就是我們講的 gamma  t 的 J K 的意思那如果有了這個的話那我現在就可以把它這個算出來就變這樣
那這個式子你再看看我其實就是把 gamma  t 就是這邊算了半天這個 gamma  t 的 J K 就拿來就是了
那唯一不同的是你看我分子就是我這邊算的東西從 t 等於一到大 t 我就是把這個 gamma  t 的 J K 從第一加到第兩百五十
我就全部加起來這就是分子那就表示說我在這個時候
時間是在 j 的時候那麼它在第 k 個 Gaussian 的 weight 是多少然後我全部加起來這樣總共是多少
那除以分母呢也是一樣的東西只是呢我每一個 t 的時候我把所有的 Gaussian 全部加起來了
對不對所以等於是做個 normalization 對不對你如果不看這邊如果不做這個這邊也不做這個的話這個除以這個就是 normalization 
就把所有的這個 k 等於一到 m 就是所有的 Gaussian 全部加起來所以呢我的分母跟分子是一樣的東西我只是把所有 Gaussian 全部加起來
所以一 normalize 之後就是它的 weight  OK 所以這樣的話我就這個 gamma 算了半天之後我的目的呢
第一個目的達到了就是我可以估計這個這個嗯在 state  j 的時候在 state  j 的裡面這一大堆 Gaussian 的裡面
那麼第 k 個 Gaussian 的 weight 怎麼算可以用這麼算那這樣講其實也是嗯跟剛才講的一樣就是這個式子其實也是用也是用 E M  theory 推出來的
那我們後面會講現在只是說他的物理意義這其實是有物理意義的這這樣的意思好那有了這個之後那再下來呢我們再大的一個問題是要算這個 mean 跟這個 covariance 我每一個 Gaussian 都有一個 mean 要算怎麼算
它有一大堆 covariance  matrix 怎麼算
那 again 我用這堆東西就是剛才算的 gamma  t 的 J K 就是這個東西
那這個式子是什麼呢這個式子我們可以這樣子看你如果回想 one  dimension 的one  dimension 的 random  variable  x 
那如果它有一個 distribution 是 p  of  x 
是一個 p  of  x 的話它的 mean 怎麼求是不是這樣求 x 乘上 p  of  x 然後積分
就是它的 mean 
這個沒問題吧這個是非常簡單的一個求 mean 的假設是一個 random  variable 只有一個 one  dimension 的 random  variable 
它的 probability  density  function 它的 probability  density  function 是 p  of  x 的話我怎麼求 mean 就是拿這個 x 的值乘上它的 distribution 然後全部去積分
等於說每一個值都可能是 mean 每一個值都有可能然後把它的機率都乘進去然後全部去平均起來這就是它的 mean 
如果你可以了解想像這個式子的話那上面那個式子意思是完全一樣的
它就是這個式子那何以見得呢
我們寫一下你就知道了它是分子是 summation 的 gamma  t 的 J  K 然後乘上 ot 
是從 t 等於一到大 t 然後分母的話呢就是這堆 t 等於一到大 t 的 gamma  t 的 J K 
對不對這是那個式子那你如果仔細看這個式子的話呢
那我說這堆東西其實就是 p  of  x 相當於那邊的 p  of  x 那這個東西呢就是我相當於那邊的 x 
然後這堆東西呢就是積分那這樣是不是就是一樣了那這個就是這個意思
我其實只是在求平均而已換句話說我現在看到從一到 t 這兩百五十個 o 
我要知道這兩百五十個 o 裡面的平均但是呢 well 這兩百五十個 o 是可以在每一個裡面啊
所以我得第一個 identifies 在這裡的不要把這些東西再算進去嘛所以我先要把這個東西算出來嘛對不對
那所以怎麼辦呢我就要去根據那我要這些 gamma 就是就是要 gamma 就是要在 t 上面的嘛
對不對所以這個 gamma 的定義就是它在它在這個 t 上這個 gamma 就是我要在 state  j 上面的
然後呢我的這個後面的 k 就是我要在第 k 個 Gaussian 的位置
所以把這個都算進去之後呢我得到的其實其實就是
我把這些東西去求平均但是我只要算它在這裡面的這部分不要算到這些地方去
只要算這部分我只要算它在這裡的 j 是說我只要算它是在 state  j 裡面的機率
然後呢這個 k 的意思是說這個 k 的意思是說我現在只要算這個第 k 個 Gaussian 的東西所以呢我是以這個 k 跟這個 j 來算
那這個其實就是一個 distribution 那麼那這個等於是說我每一個時間的時候它這是除以 total 嘛在每一個時間它除以全部佔多少
然後呢這個把它的這個 mean 乘過去然後全部積分全部加起來那這樣呢等於是在算這個式子
所以這樣的結果我就會得到這一個 Gaussian 的這個 mean  OK 那如果這個你可以想像的話
那底下這個式子的意思是完全一樣的現在求 covariance 了求 covariance 怎麼求
那就跟你平常所想像的 variance 怎麼求是一樣的如果對單一的 random  variable 跟剛才一樣哦
如果這是單一的 random  variable 的話我的 variance 怎麼求 variance 是這樣求的對不對
是我的一個 x 值減掉它的 mean 
然後平方然後然後乘上它的 probability  density  function 
積分嗯 D  X 積分
這樣的話我得到的是那個 x 的 variance 對不對這個式子應該是很容易講跟那個意思是完全對等的
就是我要求 variance 是這樣求
那你如果 variance 這樣求你可以想像的話那我這邊也是一樣啊那我現在這個式子跟剛才一樣我不再寫一次了
跟剛才這邊的情形是完全一樣的你可以想像中間這個就是 p  of  x 就是這一個這個除以這堆就是那個相當於那個 probability  density  function 
那後面這個呢就是相當於這個 x 減 mean 的平方
只不過因為我現在是 vector 現在都是都是 n  dimension 的 vector 所以呢我變成
這個要剪掉之後這個是什麼這是 transpose 
這兩個這個剪掉之後再乘上一個 transpose 之後的話就變成是這個每一個 component 相乘嘛
對不對就是說你現在因為是我這邊講的都是 vector 所以呢這一個是這樣子的
它減掉 mean 是一個這樣子的 vector 然後呢它的 transpose 是一個這樣子的 vector 
嗯這樣寫有點反了它很可能它把它當成是當成是 row 來寫的話
哦沒有錯沒有錯這樣對因為我這樣子的話就是每一個乘一個每一個乘一個每一個乘一個所以就乘出來會變成一個 covariance  matrix 
對的沒錯這樣子也就是說我這邊是減一個一個值減掉它的 mean 
但是我現在變成有一把值分別減掉它的 mean 然後呢那它的 transpose 的話是一把值分在這裡
那這個跟這個在做 matrix 的相乘的話這後面是它的 transpose 那這個一乘的話這每各自都可以乘一個 component 出來就變成一個 matrix 
然後我現在在求平均的時候就求出所有東西出來那其中的對角線上的這些東西呢就是你原來的每一個 dimension 的這個 variance
但是我還有這些對角線以外的這堆東西呢是它們的 cross 的 covariance 的參數
那因此我就可以得到這整個不過這裡面所有的意思都跟這個一樣就是你分別去某一個對不對
我們如果要寫得詳細一點的話就是把它寫成一個譬如說這個 xi 減掉它的 xi 的 mean 
乘上這個 X J 減掉 X J 的 mean 的這種東西去做平均嘛
這個是 covariance 的這裡面的每一個 element 的意思是這樣子嘛那這個跟這個的意思是一樣的
只是我把這個平方變成一個是 i 一個是 j 然後變成這樣子去做就是了
如果照這樣子來看的話呢當 i 等於 j 的時候就是這些東西對角線上的那就是求出它的它的這些 variance 的值
就是每一個各自的 variance 的值當 i 不等於 j 的時候就得到這些對角線以外的點就是它們之間的 cross  correlation 
它們的就它們的 covariance 的參數那這樣我就得到這個 matrix 那麼如果這樣看的話你如果可以想像那麼這個式子其實就是這個意思
所以你可以看成中間的這個一堆其實就是這個分子除以這個分母就是我這個 probability  density  function 
然後呢我右邊乘上這堆東西呢那就是相當於這個嘛或者相當於這種東西只是我現在做出來就是一個 matrix 
那然後這邊來積分嘛積分就是在求平圴嘛求 expectation 或者就是對機率來平均的意思
因此我這樣我就得到我的 covariance  matrix  OK 　好那麼這樣一來的話這才是我們今天真正用的最多的forward  backward  algorithm 是這一塊哦
也就是我們剛才講的當你算出 gamma  t 的 J K 以後
那麼我現在來算它每一個 Gaussian 的 weight 然後我來算每一個 Gaussian 的 mean 每一個 Gaussian 的 covariance 
那麼於是呢我這些東西全部都可以調了全部都可以調了以後那當然那還有的東西是前面的嗯譬如說這個
 a  i  j 的話就用前面這個嘛 a  i  j 是不受影響這個 pi 就這兩個所以這個 pi 呢就用這個式子就可以求
 a  i  j 就用這個式子就可以求那只有這個 B K 呢是這個式子是對 discrete  model 這樣求
對 continuous 的要有 Gaussian 就是這樣子求
 OK 所以 continuous 的話就是這樣求
於是呢我就可以都可以求出來了但是真正求不是這樣求的是一個 iteration 的 procedure 怎麼講呢
嗯應該這樣說我們舉例來講假設你是辨識零到九的十個聲音那你有零的一個 model 
一的一個 model 有二的一個 model 你有一個九的 model 假設這個是你的九的 model 你已經用了一堆人說的九的聲音所 train 好的一個九的 model 在這裡
現在有一個新的聲音進來你知道它是九我要把這個聲音再 train 進去
 OK 是這個 problem 是這樣子假設我已經有一個九的 problem 九的 model 已經在那裡了我用了一堆聲音 train 好一個九的 model 在這裡
現在一個新的聲音九進來的話那怎麼辦那我現在要把這個聲音 train 進去怎麼 train 呢那就用我們剛才講的這一招
那這一招是怎樣呢其實是一個蠻複雜的過程因為你要先求這一堆 alpha  beta  gamma  epsilon 
所以呢你看譬說說這個 epsilon 裡面要有 alpha  beta 所以你 alpha  beta 都要求嘛
所以你要一面要求 forward 的 alpha 一面要求 backward 的 beta 通通都要求然後你要求 gamma 然後你要求 epsilon 
當你這些東西都求出來之後等於是說你把那個 o 放到現在這個 model 裡面去了那個 model 是用別人 train 好的聲音
 train 好的一個 model 你現在把這個新的 o 放進去之後我得到所有的機率就是 alpha  beta  gamma  epsilon 等等都有了之後呢我現在用它來
估計重新估計這堆東西得到一個新的model 
 於是我的這些個 a  i  j 啦或者是這裡面的 mean 啦 covariance 全部都換新的我得到一組新的 model 
可是你不能相信這個是好的為什麼
因為這新的 model 怎麼來的我們剛才說我是把這個新的聲音放到舊的 model 裡面去然後呢得到那堆機率 alpha  beta  gamma 這些東西機率之後
我用那些東西來算出這些所有的新的值
嗯你很難說它會它會好為什麼因為原來這裡面的 model 是有我另外找了一把人
另外找了一群人的聲音 train 出來了現在就為了這個聲音你就把它全部都動掉了你動成這樣子了
一定對嗎這是有點懷疑因為你可以看到我這邊其實是在把這個聲音我把這個新的聲音去算它的 mean 
對不對去算它的 covariance 是這個意思那我把原來這麼多人都動掉了只為了去配合這個人的聲音
所以呢不見得這個真的是好的我們不敢說它是好的但是它只是說呢我已經把這個聲音又放到這個 model 裡面去求機率
又然後全部重新調了就是了那麼因此呢它考慮了這個東西也考慮了這個那這個沒有理由它會比較好所以呢我要把這個拿來重新放回來
再做一次再拿回來再放一次這樣經過一個一堆 iteration 當它會收斂的時候就表示它比較好
 OK 就這個意思哦也就是說你
我新的聲音來的時候我用它來得到一個新的但是我沒有理由說這個一定會有多好因為它顯然很參考我現在這個新的聲音
而對於舊的聲音好像比較不重視了那麼所以不曉得會怎樣所以我就把這個呢再拿來再來一次再拿來再來一次那麼等於說是
這個讓這個新的聲音跟舊的 model 之間的所有的差異性儘量地慢慢在新的 model 裡面越來越模糊越來越模糊那最後收斂的時候這個應該就好了
那麼在我們剛才講這裡面整套是用 E M  theory 來證明的我們現在沒有講在 E M  theory 是可以證明底下這個 can  be  shown 這個是用 E M  theory 證明就是說你其實
每做一次每做一個 iteration 的話這個機率是會提高的
那其實我們講的就是這個我就希望在這個 model 裡面看到這個的聲音的機率要大我要這個東西儘可能最大
所以呢但是同樣的我也要看到原來的每個聲音都要機率都要大我不能為了光把這個聲音大光把這個聲音的機率變大別的都變小也不行
所以呢我要我要一面要有原來所有的這些統計特性在裡面我一面要把新的聲音的考慮進來
那我那在 E M  theory 可以證明當你這樣做的時候你每做一個 iteration 其實這個機率是會變大的
因此大到最後他會收斂收斂的那個時候呢就是我們要的結果那這就是我們一開始講說這一個 problem 三是沒有 close  form  solution 的
我只能夠借助這個 forward  backward  algorithm 那麼經過 iterative 的方式讓它趨進我要的值
 OK 那麼到這裡呢那我們也可以解釋這裡還有一些重要的問題那第一個問題就是說我需要一個 initial開始
所以呢這個這個 problem 三的 forward  backward  algorithm
我們說它是一個微調的工作
我們還要一個 initialization 的過程
我還需要叫做 model  initialization
我要有一個好的開始的 model 我才可以算
就好像我剛才說 OK 　假設這是已經有一群人的聲音我已經做好了一個 model
我現在把這個聲音把它 train 進去
這樣講當然可以但是那個 model 怎麼來
你那個 model 總該有一個要有一個 model 才能夠做嘛對不對
所以一開始怎麼做這件事情
model  initialization 是一個很重要的問題
那麼嗯這個你可以想像這個工作呢是相當於我們講的粗調
而我們現在講的這個 iteration 的過程是等於是微調精調或者是微調
所以呢我們第三個 problem 講完我們只是講微調怎麼微怎麼調
但是 initialization 也就是粗調呢我們還沒有講
所以我們底下要講
怎麼做這個粗調怎麼做這個 initialization 
OK 那麼我們這邊講的是微調就是已經有了一個 initial  model 之後
怎麼樣去把這個東西去調慢慢調得更細更好一點調得更精緻一點
但是我還有一個粗調的工作就是 initialize 還是要做的
好那有了這個那我們又講另外一個問題就是這是所有的 EM
用 EM 的基礎來做都有共同的問題就是說它嗯它可它可能 converge 到 local  optimum
也就是說 EM 說了半天它其實只是證明這件事情
你每一個 iteration 都會提高
那最後它一定會收斂到某一個可以可以這個最大的值是沒有錯
但是它也許如果是一個這樣的東西你在這邊調的話
你如果從這裡開始調你調調調調調就調到這裡就停在這裡
你還會不會走到這邊來呢不會走過來了所以你就會停在這裡對不對
那你如如果從從這裡開始調的話你也是就回到這來
你必須要從這裡開始你才會到這上面來
所以這個時候呢是這個嗯
也就是說這個嗯 results  depend  on heavily 
depend  on initialization
depend  on 你從哪裡開始
因為它幫你往上走
但是你如果開始點不對的話就走到一個比較可能走到一個更小的地方去
那麼因此呢這個 initialization 非常重要
那麼同樣的一堆 data 讓兩個人去 train 它
那 depend  on 它怎麼做這個 initialization
最後 train 出來的 model 會不一樣
所以這是一個很難做的問題就是這樣子
因為你的 initialization 不一樣的話就會不一樣
那麼雖然同樣的 data 你會得到不同的結果
那麼事實上同樣那一堆 data 可以得到很多不同的 model 那會有不同的結果
那這就是你很容易 converge 到 local  optimum 的問題
所以呢我們 initialization 非常重要
好以上這段講的是 problem 三或者說是 forward  backward  algorithm
那它的目的是解這部分的微調的工作
那麼再下去呢我們要來講粗調
ㄦ粗粗調就是 initialization
這個應該是我的最後是不是這個的最後一章了
對這是最後一章了
OK 那麼要講到這個粗調 initialization 我們就要回到 power  point 的這個地方的
那一般的這個粗調的這個 model  initialization 都是使用 vector  quantization 的方法
那麼因此呢底下我們會先講一下 vector  quantization
這個東西是相當有用的在很多地方都很有用
因此呢你也許在別的課裡面有學過我們會講的快一點
不過基本上呢在我們這裡也一樣用它來做我們要做的這個嗯 initialization 的工作
OK 我們先在這裡休息十分
 OK 我們來講下一段
我們把 HMM 的那些複雜數學暫時丟掉了
我們先講到這裡為止
我們換一些東西不然一直搞那個是很頭大
我們底下講的是嗯其它的東西了
那第一個我們先說的是這個
這個我們要用 v q 來做這個粗調
我們簡單講一下 v q
那麼 vector  quantization 是在很多地方都很有用
那麼你可能別的地方都學過
不過如果你學過的話我們就算是一個複習就是了
那麼 v q 的用處在很多地方
一個常用的是說做為 data  compression
那麼當然還有另外的用途是 clustering
那這個底下我們就會解釋
那麼我們先從 data  compression 的觀點來解釋的話呢
那 vector  quantization 原始的來源是來自
這個數位通訊裡面的那個pulse  code  modulation 所謂的 pcm
那麼它的觀念就是所謂的 scalar  quantization
是從它衍生出來
那麼這個的觀念講起來很簡單就是說
假設我有一個 signal 我有這堆 sample
我怎麼把它送到遠方去呢
那一個可能的辦法是我把它整個的 range 切成若干格
然後每一格代表用一個 bit  pattern 來代表
舉例來講我這邊切成八格
如果切成八格的話總共只有三個 bit 就夠了
也就是這邊只有零零零零零一零一零零一一等等
於是我的第一個 sample 如果掉到這一格我就是一一零
表示說它是掉到這格
第二個呢還是這一格還是一一零
第三個呢還是這一格還是一一零
第四個呢還是這一格就一一零
第五個呢變成一零一
第六個呢一零零等等
那我就用這個 bit  pattern
來描述說我的這些點的位置在哪裡
那當然如果這樣做的話我這些點的位置其實只說明了它在哪一格裡
至於在那一格裡的位置它已經丟掉了
我們不管它在哪一格裡的什麼位置
只管它在哪一格就好了
當我把這一串零跟一送到遠方去的時候
接收端呢它其實無法判斷
它只知道它在哪一格不知道它那格在哪裡
所以呢它就會怎麼辦譬如說它就會把這些一一零呢
它就是一律以一一零這格裡面的中央那一點來代表
於是呢它就會看成這幾點都是一樣的
都是中央那一點
然後一零一呢也一律用中央那一點
一零零也一律用中央那一點
因此呢到你遠端它會把這個連起來
就會變成一個這樣子的曲線
跟原來會有點不一樣
那這樣子的過程呢我們稱為 quantization
那麼在中文當時通常翻做量化
那麼其實它就是把這個你該有的 range
切成若干段
每一段用一個值來代表它
然後這個段數總共是二的 r 次方
於是我就只要多少個 bit 就可以了
那這件事情就是我們這邊講的 quantization
那麼後來因為要把它變成 n  dimension 變成 vector
所以這個就叫做 scalar  quantization
一開始的時候這個就叫做 quantization
後來因為變成 n  dimension 以後 vector  quantization 這個就叫做 scalar
那它的意思就是把一個 single  real  number 用一個 r  bit  pattern 來代表
就像這本來是一個 ream  number
一個 real  number 但是我變成一個 r  bit  pattern 來代表它
那我做的方法就像這邊講的一樣我這個 range 就是正 a 到負 a
就是我這邊的這個正 a 到這個負 a
那我就把這個 range 呢分成大 L 個段
每一段叫做 j  k
所以呢我這邊所以我這個橫這個畫的橫軸就是我這邊的縱軸
所以這裡面的某一段
我就叫做 j  k
就是第 k 段叫做 j  k
那那裡面有一個代表值就是中間那個代表值
叫做小 v  k
那就是我這邊所畫的
所以呢我總共有多少個呢有 L 個
L 是我總共的總數
因此呢我就會有 L 個 j  k  k 等於一到 L
它們的聯集就是整個的 s
所以 s 就是整段的
整段的 real  number 叫做 s
也就是說這整段就是我的 s
它就是所有的這些 j  k 的聯集
那另外呢這些代表值 v  one  v  two 到 v  l
就這些中間代表值呢的集合呢叫做大 V
如果是這樣的話呢我的 quantization 不過就是一個從
大 s 到大 v 的 mapping  relation
這個 mapping 條件是什麼其實很簡單
就看你的 sample 掉在哪一個裡面嘛
譬如說這個 sample 是掉在這一格裡面
我就把它把那個值 map 到它的代表值
對不對就是我們這邊的意思
看它掉在哪一格我就用它的代表值來代表它
所以我最後就變成這種的就變成黃色的這個
也就是我把它的在那一格裡面的精確的位置都丟掉了
只留下它的代表值的值
那麼如果是這樣子的話呢這個
那我這個就等於是一個 mapping  relation 嘛喔
完全看每一個 sample 的值掉在哪一塊裡面
那麼我就用那一塊的代表值來代表它
那麼這個時候呢這個 r 的代表值呢我全部可以存在遠端都可以存好
所以我每一個每一每一個代表值我就只要用 r 個 bit  pattern
就可以代表了
所以我在傳送的時候我只要送這些少數個 bit 這個 bit 數目很少
就可以代表這些東西了
但是當然我也同時丟掉了重要的 information
就是這裡面的每一個點到底在這裡面的什麼位置是丟掉了
我只知道它是那個代表值的位置而已
那這個過程就是所謂的 quantization
那如果是這樣想的話呢
那麼嗯這裡面很重要的一件事這到底要怎麼做
我們這邊的說我把它等分成八塊
那當然沒有理由要等分
我可以做不等分的
那麼你最容易想的情形就是
把它分成譬如說中間比較小外面比較大
我這個隨便畫畫
中間比較小外面比較大這裡面你可以想像有幾個原因
第一個原因是說它也許有一個 distribution 是這樣子的
這我們剛才講的這個它有一個 distribution
這它的 probability  density  function
如果是這樣子的話
我就有理由
中間比較小外面比較大
為什麼因為這邊機率那麼大嘛
經常出現我就做得比較精細一點
因為我的我每次做的時候我就會把我的真正的值在那個裡面的精確的位置丟掉了
所以呢我如果這個值越細的話呢那我丟掉的東西越少嘛
越能夠精密
當變成這麼大的時候當然就搞不準一定會有很大的誤差嘛對不對
所以呢你這個越大的話我的 error 越大
越小的話我的 error 越小嘛
那麼中間這個機率那麼大我儘量用細一點
它這個這個 error 比較小
那外面的話反正難得發生所以呢給它粗一點大一點就算了
阿這個是一個很容易想的想法就變成這樣
那這個就是我們講的這個 probability  distribution  of  X  M
你可以根據這個東西來 design 怎麼做這件事
那同樣 error  sensitivity 是說呢
有的時候其實這個 error 大小是跟我們的感覺的敏感度有關
舉例來講呢你如果是在這些地方的話呢可能我的 signal 本來就很大
signal 本來那麼大所以中間如果多一點 error 可能影響不大
可是呢你如果是在這邊的話呢
它可能是很小的 signal 
這個時候你的一點點的 error 可能都很影響都很大
所以呢如果這樣來看的話我就會希望說我的 error 呢
在中間會很小在外面可以比較大
所以我是會讓外面這格比較大裡面這格比較小等等
那這就是所謂的 error  sensitivity
因此呢我們至少可以考慮我們至少 at  least 考慮這兩個因素
就是 x 本身會有 distribution
然後包括它的 error 我們可以容許的程度我們敏感的程度來 design 這個東西
那這個東西是什麼東西就是你如何來分
我不一定要是等分我可以分得大大小小
同樣我那個代表值 j  k 這個 v  k 的位置
也不是一定要在正中央我可以放在邊邊上
我可以偏離中央也沒關係
那麼 depends  on 怎麼樣比較好
那這些都是可以考慮的
那也就是說你每一個 J L 這個區這個劃分怎麼劃分
還有裡面的代表值 v  k 倒底怎麼取
這些東西的加起來就是我們講的一個 quantization 的 characteristics
也就是一個 code  book
那我們要想辦法做這件事情
那這個是所謂的 scalar  quantization有了這個之後我們現在可以把它 extend 到我們先說 two  dimension
如果 two  dimension 的這個變成 vector  quantization 意思是什麼呢
我們舉例來講就是我把相鄰兩個當成一個 two  d 的 vector
譬如說這一個跟這一個
這兩個合成一個變成一個 two  d 的 vector
這兩個變成一個 two  d 的 vector
這兩個變成一個 two  d 的 vector 你可以這樣子來看
那如果是這樣子來的看的話呢我就變成一個 two  d 的 vector 是相鄰兩個 sample
那麼於是他們的 range 就這樣不是一個 dimension 了是兩個 dimension
於是呢就會變成
譬如說這樣變成這一塊
那譬如說這個軸是 x 的 n
那這個軸是 x 的 n 加一
那它變成一個 two  d 的兩兩 two  d 的一個 range
我的 s 變成這一塊它是在它都是在正負 a 之間
所以這個 x  n 呢也是在正負 a 之間從正 a 到負 a 
x  n 加一呢也是從正 a 到負 a 之間
在這裡面了
那這個時候我仍然可以做相同的事情
就是把這個 range 我也一樣的分成 l 塊
不過現在每一塊是 two  d 的 region
我還是寫成這個 j 　
因此呢我舉例來講呢你可能可以說呢
 ok 這裡有一塊
這裡有一塊
那麼這塊呢叫做 j  k
這中間你有一點呢叫做 v  k
還是一樣
那我這樣這塊 region 我總共可以分成譬如說 l 個
 l 個 two  d 的小塊
那我讓這些 l 個 two  d 小塊的聯集
仍然是整個的這個 s 這個就整個的 s 整個的 range
然後每一個 j  k 呢裡面有個代表值是小 v  k
那麼我的小 v  k 的集合呢叫做大 v
還是一樣
如果這樣的話我仍然是一個 two  d 是一個 mapping  relation
還是從這個 two  d 的 s 對應到這個 v 　
跟剛才是完全相同的
那麼這個 mapping 的方式也是一樣
如果你的 two  d 的那個 vector 掉在哪一塊我就對應到哪一個值
對不對
譬如說呢我的 x  one 是這個值
x  two 是這個值
於是掉到這一塊
在這一塊我就用這個代表了對不對
那這樣呢我這個如果我總共有我總共有 l 塊的話
是二的 r 次方
我仍然只要用 r 個 bit  pattern 就可以傳送了
就可以代表了
那這個是 v q 如果拿它來看成是這個傳送 data 的問題的話
我們可以這樣子看這個問題
那這個時候你可能會想到第一個問題就是說為什麼要這樣子做
這不是多此一舉嗎
因為我其實當你看成這樣的話
我馬上想到就是說那我其實就是把 x  one 的這個軸也切成幾塊
 x  two 這個軸也切成幾塊
不就一樣嗎
於是我就得到這樣子一堆正方形的或者長方形的
它們不一定要一樣長
就變成這樣就是啦這不就是那個嗎
沒有錯如果你這樣做的話
就跟剛才的 one  d 的這個是完全一樣的
對不對如果那樣做的話就跟這個是完全一樣
這個就沒有意義就這樣就好了
那麼會要這樣的原因就是
你可以想像這個並不是一個最好的辦法
因為它等於把你自己限成一堆框框
這堆框框把你完全限制成這樣子之後你這樣做的話就是原來那樣做法
但是其實我現在不一定要這樣子嘛我現在就可以變成別的樣子了
對不對我就變成別的樣子了
就不是那樣子那效果就會不同嘛
那麼為什麼會不同呢我們至少有這些個原因
一個最容易想像的原因就是它們這兩個之間是會有統計上的 correlation 的
所以它的 distribution 會不一樣
那麼最容易想的一個例子就是
如果這個 x  n 跟 x  n 加一是如剛才所說的這兩個相鄰的 sample 的話
如果是這兩個相鄰 sample 的話呢
in  most  case 它們值是比較接近的
你要說一個在這個上面一個在這個下面的機會是很少的
通常是都是蠻接近的
因此呢你就可以猜得到它的 distribution 是怎樣的
它大部分的 distribution 會集中在中間這一堆
那麼兩端是很少的
如果說它的 distribution 都在中間這一堆的話
那我其實我就可以把中間分得很細
就跟剛才的情形一樣
我中間可以分得很細
但這邊的情形是不太容易發生我就可以弄得很粗
這一大塊就好了這一大塊就好了因為它不太會發生
OK 那麼因此呢我就可以得到這個比較精緻的做法
那麼這樣子的話我就不再受限於這堆框框而
你可以想像這堆框框其實是多餘的
因為你搞到這邊來
這邊還是有一樣精細的框框跟這邊一樣
這個框跟這個框一樣是沒什麼道理因為它們本來就不一樣嘛
那麼你在這點的話是說一個這麼正一個這麼負其實不太可能嘛
哦等等所以你這邊就可以很鬆等等
那如果是這樣做的話呢
我們舉個例子來講
如果這邊是這個是這個
 r 是八個 bit  per  sample 的話
如果這裡的一個值是八個 bit  per  sample 所以 l 是兩百五十六格
我這邊分成兩百五十六格那麼每一個 sample 我用八個 bit 去描述它
我可以得到一個精細的代表
那麼當我變成兩個的時候
我如果仍然用這個框框來做的話呢
我就是要變成這個兩百五十六的平方 l 對不對
於是呢我其實還是就是十六個 bits  per  sample
這樣就跟剛才完全一樣
我這個這個軸也分成兩百五十六格八個 bit
這一軸也分成兩百五十六格八個 bit
這樣的話呢我兩個 bit 要十六個嗯十六個 bits  per 兩個 sample
對不對每兩個 sample 呢
是要十六個 bits
所以結果還是八個 bits  per  sample 是一樣的
但是你如果這樣做的話呢
我可以把這中間分得很細外面分得很粗
搞不好這樣我就只要譬如說兩千零四十八個 region 就夠了
我這邊不見得要分兩百五十六平方這麼多
我搞不好只要這樣就夠了如果這樣就夠的話呢
這個就是什麼這就是二的十一次方
於是呢我就只要十一個 bits  per 每兩個 sample
於是我一個 sample 呢只要五點五個 bit
比剛才的八個 bit 就省了很多等等
那你可以從這個觀點來想的話這個它就有它的意義
那這就是我們這邊講的這個
你為什麼要這樣子做
這樣做為什麼要這個把這些框框丟掉
而我重新去於這些奇奇怪怪的 region
那是有原因的
那原因就是我要做這類的事情
那我的考慮包括呢我的這個
這些東西可能是有這個這個統計上的 correlation 在
就像我剛剛講的這就是他的統統計上的 correlation 在
然後呢當然我可以有更 flexible 的 choice  of 每一塊可以更 flexible
那然後呢我的這個
還有一種可能就是 error  sensitivity 可能是 depends  on 它們 jointly
換句話說
如果我的一個 sample 非常準
一邊非常準的話
那另外一個搞不好我可以允許比較大的 error 可能沒什麼關係
譬如說你可以想像
如果其中一個已經很準了另外一個可能可以差很多都不影響等等
那這些的話就造成我可以做的空間
那麼因此呢這樣的話我就可以得到一個比較好的
那比較大的問題還是這倒底要怎麼做這件事
那這個如果做的好的話呢這每一個就是
每一個就是 j  k
那這裡面的每一個代表值就是 v  k
那麼這些 j  k 跟 v  k 的組合
就是所謂的我的 quantization 的 characteristics
或者說就是所謂的 code  book
就是嗯碼書啊它們有時候翻做碼書啊
就是 code  book
那就是那在哪裡歸在哪裡這樣的意思
那麼到這裡為止我們大概可以想像做 vector  quantization
這個嗯為什麼是一個 data  compression 的方法
原因我們剛才在最前面的這一頁說
它是一個 efficient  approach  for  data  compression
我可以把一組 real  number 變成一個 final  number  of  bits
所以我的 data 大為 compress
而且我 data 數目可以減少對不對
我現在這個兩個 sample 我只要十一個 bits
不像剛才要十六個 bits 譬如說
那這就是我的 data  compression 的功能
那當然這個觀念是可以衍生下去
那麼我們就可以得到更複雜的
舉例來講這個是 two  dimension 的
那當然你也可以 tree  dimension
然後可以變成 n  dimension
那這個情形都完全一樣這個 formulation 跟剛才都完全相同
那麼當你有這麼多 dimension 當然我們就沒有辦法畫了
我們也許最多只能畫一個 three  dimension 的
如果是 three  dimension 的話就變成一個像這樣
那它這塊就在這個裡面
然後那你中間還是一樣把它切成一塊一塊等等
three  d 的話大概就是這樣子啦
那你也是一樣你每一塊 three  d 的 region
我叫做 j  k
它有一個代表值叫做 v  k這是一樣的
所以呢以此類推就可以變成
 n  dimension
所以我這個會有 n 個
然後呢我這 n 個構成一個 vector
那每一個值都有一個上下限正負 a 之間
然後呢我把它切成很多小塊
就是每一小塊叫做 j  k
每一小塊有一個代表值叫做 v  k
然後我仍然是一個 mapping  relation 
depends  on 你那 n 個值的 vector
掉在哪一塊裡面對不對
你那 n 個值的 vector 掉在哪一塊裡面我就用那一塊的代表值來代表
那到時候呢我就變成剩下 l 個代表值
所以我只要 r 個 bit 就可以代表它等等等等
那這個都一樣
那到這裡的時候你就可以發現其實
我可以變成 n 之後呢
那麼並不見得這個一定是剛才那譬如說相鄰的 n 個 sample
可以是任何的 n 的不同的參數都沒有關係
當你是 n 的不同參數都沒有關係的時候
那其實我們在做的事情是等於是在這個看這些 data
這些這些個 data 它們之間的關係了
那麼這個時候我們再回過頭來講
現在最大的難題就是
你倒底怎麼做這堆 code  book
倒底它應該怎麼求
那你可以想像它顯然有道理
但是它要怎麼做
你憑什麼到這個這個邊界怎麼畫定
然後這個代表怎麼設定
那這個其實是經過很多年
很多人一直想不出來
因為這個 v q 的觀念其實很早就有了
但是人家一直想不出怎麼做這件事
那麼真正做出來的時候是在七零年代的末期
那做出來的人是因為
到那個時候 computer 進步到一個階段
可以處理大量 data 了
於是他就想到說其實這個 code  book
這個code book是可以用大量的 training  data 來 train 的
那麼因此他就想出這個用大量 training  data 用 computer 來 train
讓它自己收斂 train 出一個 model 來的方法
那這是七零年代的末期所出來的
那麼要講這件事情呢
我們要先 define 任意兩個 vector之間的一個 distance
因為它是這個演算法完全是用 distance 來算的
那麼這個 distance 就是在假設這個是 two  dimension 的話
這上面的任意兩點我怎麼 define 它的 distance
如果這是 n  dimension 的話
這上面任意兩點我怎麼 define 它的 distance
我們先要 define 好一個 distance 之後
那它就可以用那個 distance 去去算這件事情
去做出這個去做出這個切割的動作
那這個就是我們這邊講的
所以它要先 define  distance 的原因
那這個 distance 怎麼 define 呢
它說其實你自己可以 define 照你的要求
那麼你只要符合這些條件
什麼是 distance
就是 s 乘以 s 對應到正的 real  number
也就是你你這裡面任意取兩個 vector 出來
任意取兩個 vector 出來都對應到一個正的 real  number
然後呢它有這些條件
就是這就是一般的 distance 所需要的條件
譬如說任意兩個 vector  distance 呢應該是正的 real  number
如果一個 vector 跟它自己的 distance 呢應該是零
然後呢它們是可交換的
你 x 跟 y 的 distance 跟 y 跟 x 的 distance 是沒有區別的應該是一樣的
然後這個呢是三角不等式
三角形的兩邊合大於第三邊對不對
就這個是 x 這個是 y 這是 y 這是 z 那這是 x 跟 z
那兩邊合呢要大於第三邊
這都是 distance 的意義
當你符合這些條件之後你自己可以隨便怎麼定義
那這邊舉幾個例子是一般常用的 distance
那這些 distance 都符合這些條件
那顯然你還可以設很多其實的 distance
那舉例來講這個就是我們所熟知的歐幾里得距離嘛
對不對每一個 dimension 都平方的 distance 都平方就是了
這是歐幾里得距離
那這個呢我不平方我做絕對值可不可以也可以
這是所謂的 city  block  distance
city  block  distance 什麼叫 city  block 呢
你可以想像就是在假設在街道城市的街道裡面它都是這樣子的
所以你如果要從這一點走到那點去怎麼走
你只能夠走這樣子過來
那這個距離就是你的 x 跟 y 分別都求 difference 取絕對值
那這就是所謂的 city  block  distance
那麼你沒有辦法做直線距離
這個就是 city  block  distance
那底下這個距離呢
嗯這個名字比較難念
不過它的你看它的長相就知道了
這個就是 covariance  matrix
所以這個長相其實就是 Gaussian  distribution 後面那個東西
你現在應該很熟悉那個 Gaussian  distribution
就是後面有 e 的 minus
前面有一個 vector 然後乘上一個 inverse  matrix
然後再乘上一個這個對不對
那後面的這堆東西就是這個 distance
就是這個 distance
那這個東西就是 inverse 的 covariance  matrix
那這是什麼意思呢
我們其實很簡單的解釋就是
你最容易想像就是假設是對角線的
假設是只有對角線其它都是零的話
在這個狀況的意思是說
我這邊的 x  one  x  two
都要分別 normalize  by 它的 variance 除以它的 inverse 嘛
就是它們這個 distance 對到 normalize  by 它們的 variance
那為什麼呢因為我不同的值它可能的 range 有大有小
譬如說假設這是某一個東西它的 range 都是正一到負一的
可是這個呢是從正一萬到負一萬的
如果是這樣的話這是正一到負一這是正一萬到負一萬的話
你算的 distance 馬上它 dominate 它
它的差距它的差異就沒有了
那你怎麼辦
它們都先 normalize  to 它們的 variance 之後
它們的差異就一樣了對不對
像這類的功能就是顯現在這裡了
那我現在再進一步我可以把這邊變成不是零的
那有類似的情形
那這就是
其實就是我們一般做 Gaussian 的時候後面這個東西本來就是這個意思
那這麼一來的話呢那這個就是所謂的這個 distance
那你也可以想很多別的 distance
譬如說
我們剛才講的歐幾里德 distance 是 xi 減掉 yi 的平方
你也可以為每一個 dimension 做一個不同的 weight
這樣也可以啊
如果你覺得哪個 dimension 比較重要你該 weight 比較多這也是一種啊
那同樣呢我們剛才講
我們剛才講你 design 這種東西的時候
要考慮的一個因素是 error  sensitivity
那麼假設你要做的東西是
耳朵要聽的聲音或者眼睛要看的畫面的話
看你聽覺視覺會感覺什麼
會怎麼樣都可以把那個感覺上差異的要求通通都放到這個 distance 裡面去
那我的目的就是希望
我看起來覺得跟原來的一樣好看
聽起來就跟原來一樣好聽都可以
如果這樣的話我就想辦法把
怎麼樣子讓我聽起來的差距顯現在這裡面我就把它放進去等等
所以呢這些不同的因素我都可以放到那個 distance 裡面去考慮
那有了這些 distance 之後呢那我就可以做底下的這個演算法
那這個呢就是所謂的
嗯它也有一個名字是 k  means  algorithm
或者是 low  and  max  algorithm
那你可以看得出來這是兩個人的名字
而這是它的原來的意思
那這個演算法意思很簡單
雖然不怎麼好用但是呢是容易了解
所以一般我們都會先講這個
那這個的意思是
像這樣我們舉例來講以 two  d 為例
假設我有一堆 data 在這裡
假設說我們要做的 data 是聲音
然後這是 two  d 就是相鄰兩個 sample
這是一個 vector 的話
我就把一大堆聲音的相鄰兩個兩個 sample 的 vector 通通拿進來
就變成一堆 data 在這裡
然後呢我做哪兩個 step呢
第一步是先 fix 我的代表值
然後去找它們的區域
譬如說我先假設假設有一個代表值在這裡
一個代表值在這裡
一個在這裡
一個在這裡
那麼我先設這四個值之後
我想辦法去找這個
那怎麼找呢那它就是根據每一點去看
每一點去找
它跟這四個代表值的 distance
剛才 define 的 distance 就用在這裡啦
我每一點都去看它跟這四個的 distance 跟誰最近
跟誰最近呢它就歸給誰
於是呢像這個跟它近歸給它
這個跟它近歸給它
這個跟它近歸給它這個跟它近歸給它等等
這個歸給它這個歸給它
你每一個呢都根據 distance 去算它跟誰近就歸給誰
當你把這個歸好之後
其實這個 boundary 就出來了
譬如說這些東西都歸它於是呢我的 boundary 就是在這裡
這些東西歸它那 boundary 在這裡
這些東西歸它 boundary 在這裡
那麼呢 boundary 在這裡
那這就是我的第一個 step
第二個 step 呢反過來
我假設我的 boundary 確定了
就我的每一個 j  k 確定了
我要重新找它的代表值
於是呢如果這一些已經知道這是一個 boundary 的話
這是一塊的話呢
這裡面到底哪一點才是最最能夠代表這些的呢
我要把這裡面的點重新去看一次
發現其實我如果把這一點
搬到這邊來的話
那麼每一個跟它的距離加起來才是最小的
它的 distance 才最小所以應該搬到這邊來
同理呢這一點也不是最理想的我應該把它搬到這邊來
這一點也不是最理想的我應該把它搬到這邊來
這一點也不是最理想的我把它搬到這邊來
那這樣子我就完成我的第二步
之後我就再繼續 iterate
我現在這個用新的這四個點做為代表之後重新把這個擦掉
boundary 擦掉重新再來
那麼這時候我得到新的 boundary 可能會變成在這裡然後在這裡在這裡在這裡等等
那樣子經過若干個 iteration 之後它會收斂
收斂的應該就是比較好的
那這就是所謂的 k  means
那你看名字就知道求 k 個 mean 嘛
那每一個就在求它的 mean 嘛這樣的意思
那如果說得詳細一點的話呢就是這邊寫的式子
所以第一步是怎麼做呢
就是我們剛才講就是把每一點都去求它的 distance看它跟誰最像
距離最近就歸給它
所以呢所謂的 j  k 是什麼
就是所有的那些個 x 它跟 k 第 k 個代表值的距離才是最近的
跟別的 distance 都比較遠的
那那些個 x 的集合呢就是 j  k
那這個目的很顯然就是讓我的 total  distance 最小嘛
我在 minimize  total  distance 啊
那我如果每一點都歸給跟它最近的那個 distance 的話
那這樣子它都歸它它都歸它的話呢
那這樣的話我的 total  distance 一定最小
所以呢我其實就是在 minimize  total  distance
而這個 condition 其實就是所謂的 nearest  neighbor
那麼我每一點看這幾點誰是我的 nearest  neighbor 就歸給誰
那第二塊的意思呢是反過來
那其實就是給我這堆之後呢裡面看哪一點最能代表大家
那很直覺的就是求它的 mean
那求它的 mean 其實是針對剛才那個歐幾里得 distance 而言
我們剛才講你可以 define 各種 distance
你可以 define 各種 distance
那如果是歐幾里德 distance 的話
那麼這個 total  distance 最小的條件就是
total  distance 最小的條件就是你取它的 mean
於是呢你每一個我都求它的 mean 就是最好每一個求它的 mean
那這也就是 k  means 這個名字的來源
那這樣之後當然我就讓每一塊各自都有最小的 distance
那這個 mean 其實就是所謂的 centroid 就是你的質心嘛
那麼這個那這樣這兩招之後就是我這個 iteration 不斷進行
那我怎麼收斂
因為你每走你每走一個 iteration
我的 total  distance 一定在減少
因為每一次都在 minimize 這個 distance
那這邊是把每一塊的 distance都在 minimize
那把每一塊 distance 加起來就是我的 total  distance
所以我就看 total  distance 應該是一路在減少
然後什麼時候少到不會少了我就給它停止
那這個就可以收斂了
那麼它一定會收斂為什麼因為我的 distance 是正的
我的 distance 是一個正值
但是呢我每一個 iteration 之後呢我的 distance 一定會降低所以它最後一定會收斂這是所謂的 k  means 的 algorithm
這個方法簡單而容易了解
它有一個很大的缺點就是不容易真的用
為什麼呢因為這個嗯我沒有寫在這裡
這邊只是說你可以你有一個夠多的 data 就可以跑了
那麼它一個最大的問題是在我們下一頁講的就是說呢
這跟我們剛才上面講的那個問題一樣
它 converge  to  local  optimum
而且它 depends  on  initial  condition
它的 solution 顯然不是 unique
也就是說你一開始的我們說一開始是你要設幾個代表值的點然後才開始跑嘛
看你那個設的好不好
你設的不一樣跑的結果就是不一樣你的那個 solution 不是 unique 
solution 不是 unique 然後它會看你 initial  condition 設的好不好就不一樣
你很可能會收斂到 local  optimum 去
就跟我們剛剛說的情形是一樣的你現在是要往下走
但是你不見得走到這來
你如果從這開始的話你就停在這裡就出不去了
你如果從這開始你還是停在這裡
必須要從這開始才會到這來
那麼這個是剛才那個演算法的缺點
那麼到後來有人想到一個辦法
這是真正vector  quantization 後來變成有用是有了這個
那這也是它最早最成功的一個演算法所謂的 L B G  algorithm
那麼 L B G 是什麼東西呢是三個人的名字
actually 這是 L 跟 B 是兩個研究生 G 是他們的指導教授
那麼他們三個人他們三個人發明的
那麼這個 L B G 的意思是怎樣呢
我們簡單地解釋就是它變成 iteration
然後呢每一個 iteration 重新來一次
那麼我們可以簡單地來看的話就變成這樣子
它就是說我們仍然以這個為例
我第一次的時候呢我只要 l 等於一
第一次我只要 l 等於一
所以也就是我只要求一個 vector 的 v q 那就是求它的 mean
所以呢我就是找出一個這邊的 mean 來譬如說 mean 在這裡
這是它的 mean 這就是我的第一步我只要等於一就好了
然後第二步我把它 split 變成二
由 l  split 變成二 l
我怎麼 split 呢就是從這個這個當初找到那個核那個代表值開始把它拆成兩個
這最常用的拆法就是這個方法
就是一個向一個一加 epsilon 一個一減 epsilon 這個 epsilon 是一個比一小很多的值
讓它讓它呢在這個核心的附近拆開來變成兩個
譬如說呢我就把這個拆掉變成一個在這裡一個在這裡
不過這兩個仍然非常接近核心
我就從這兩個開始跑
那麼這兩個開始跑這時候跑什麼呢就是跑 k  means 跑剛才那個 k  means
那麼於是呢它就會開始把譬如說把它這個切開來
這樣子然後呢這一堆呢重新求它的 mean 就會跑到這邊來
這堆重新求它的 mean 就會跑到這邊來
OK 那這樣之後呢我再跑 L B G 再來一次
於是呢這個時候我的新的 boundary 到這來了
然後呢我可以再來一次
那麼發現呢是這個比較好這發現是這個比較好等等
當我 l 等於二完成之後就會得到一個比較好的
這個 point 在哪裡呢你可以看到
他剛才這個辦法是他從核從一個整個的中心那裡拆成兩個還在附近
所以基本上它還在中心的部分這兩個都在中心從中心部分開始往外走
那根據它的 data 的分部的情形走到外面來
這樣的話大概這個這個 initial  condition 比較好
那麼這樣子之後呢
我就可以如果我覺得不夠我就再來一次
所以當我這個跑完之後我可以再回到 step  two
我再把 l 分成二 l 　
譬如說我這個可以再拆成兩個
這個再拆成兩個這個也再拆成兩個
然後讓它們再去跑
那麼再去跑的結果呢它就會把這個拆開來
於是這個往這邊跑這個往這邊跑
這個拆開來這個往這邊跑這個往這邊跑
那這樣的話它慢慢慢慢跑出來我的就會接近比較好的
所以這個比較會 converge  to  better  code  book
那這個精神我想很容易想像就是它為什麼會這樣就是我原來的 k  means 裡面太 depends  on  local  optimum
那麼這個時候 depends  on 你的太 depends  on 你的 initial  condition 了
所以呢很會收斂到 local  optimum 去
那用這個方法的話我等於是我先從一個核心開始
從它向旁邊一點走的話呢我慢慢再往外散
這樣的話它比較不至於會搞不好然後它大概都會接近它的 optimum  solution
那我們可以舉一個例子來來解釋
假設有一堆 data 長得很奇怪如我們剛才所說它專門長成這樣
那如果專門長成這樣你如果一開始的的點放在這裡放在這裡什麼的它就不太容易收斂的很好這個就很難
但是呢我如果是用這個方法的話呢你想我會怎樣
第一次得到的一定是在這裡嘛在中間
那第二次就算是它拆的時候拆到這邊來了
拆到這邊來之後變成一個在這裡一個在這裡
那它很可能會畫的一條線是這樣子的那也沒有關係
這個時候各自去跑 l b g 各自去跑 k  means 之後那它會怎樣
那它顯然會往這邊搬它顯然會往這邊搬
OK然後它再拆開來就算它再拆錯也沒有關係
它再拆成一個這樣子一個這樣子的話呢它最後還是會它最後還是會它往這邊搬它往這邊搬
它最後一定會它往這邊搬它往這邊搬
所以最後呢它們會得到一個比較好的 code  book
那這個是這個把它由核心去拆成兩個的一個辦法
當然還有另外一個辦法就是一個用原來的一個用最遠的也可以
那如果一個用原來的一個用最遠的話呢這個意思是像這樣
我們也舉個例子譬如說我的 data 也是這樣子的
我得到一個在這裡
那我現在怎麼辦我的兩個呢一個就用這個另外一個用最遠的
這個時候跟它最遠是在這裡 OK 我就在這裡
如果就在這裡的話呢我一開始就會切在這裡這個歸它這個歸它
但是之後我再來繼續跑這個 k  means 的話呢它就會往這邊搬
於是呢這個就會往這邊搬它就會往這邊搬對不對
於是你就會看到這個會向這邊移動這個也會向這邊移動這個也會向這邊移動所以它慢慢就會過來
那這樣也是可以的所以這是另外一招
不過這精神都一樣就是用這個方法來得到的就是這個就是這個喔這是 l b g  algorithm
那這個呢到這裡我們 v q 講到這裡
那麼我們可以回過頭來看剛才第二句話
我們在這裡做 v q 不是為了剛才講的 data  compression 是為了拿來做 clustering  large  number  of  sample  vectors
當我有一大堆 vector 的時候倒底怎麼樣把它們分成最合理的一群一群
然後每一個群分別有最有代表性的 vector
那等於是這件事情那我們很多時候做 vector  quantization 是這個目的
那這樣的話就像我們這裡其實是這個目的然後可以做這樣的事
好有了這個之後我們現在底下就可以講我這個 v q 怎麼拿來做這個 HMM 的 initialization
那就是底下這一段 OK 我們休息十分鐘 OK 
我們接下來講這個做 initialization 
那麼這是其實這 initialization 很多種方法那麼這邊講的是最簡單的一種也是最常用的一種
那這就是所謂的 segmental k  means  algorithm 
那什麼叫 segmental k  means 就是這麼做
那基本上是怎麼樣就是你你總要有一個 initial  estimate 
所有的 model 怎麼辦呢
最常用的辦法就是
分成這個 equal  lance 
怎麼講呢我們舉例來講如果我要 train 一個
某一個音譬如說
六
假設我要 train 四個 state 
假設我要 train 四個 state 的話
那怎麼辦呢我有
好幾個六這是一個六這是一個六這是一個六這是一個六這是一個六
有長有短
那麼於是我都一樣的都把它等分成為四段
這個也等分為四段
這個也等分為四段
因此呢我的最早的 initial 呢是
讓它們的
等分的東西去 train 一個 model 
譬如說它所有的這前四分之一
所有的前四分之一
拿來做這個 model 
所有的最後四分之一嗯做那個 state 
所有的最後的四分之一
做這個 state 等等
這樣子哦那你
這樣子之後那你這邊就有一堆
對不對這裡面是一堆 vector 
這裡面是一堆 vector 基本上它們大概都是屬於這個六的聲音的
前面的四分之一然後我就假設拿來 train 這個 model 
 train 這個 state 
那這些呢都是最後的拿來 train 這個 state 
那這個時候你這個 state 是什麼呢
我需要一堆 Gaussian 嘛
我舉例來講我如果需要十六個 Gaussian 
的話那怎麼辦
我就是把這裡的一堆 vector 一堆 vector 拿來做 vq 
做十六個
做這個 l 等於十六的 vq 嘛
於是呢我就
那麼於是我就會得到
這一堆有一個這一堆有一個
這樣總共十六個
那我就當它們是 Gaussian 
其實這每一個你當然沒有理有這是 Gaussian 了
它就是一堆 vq 
就把它們做成一堆東西然後有一個代表值有一堆有一個代表值我就那個
把那堆東西的 mean 跟 variance 
就當成是 Gaussian 的 mean 跟 variance 
這個其實寫在底下這個
我們一直都這樣子做哦
包括後面的是這樣子做一開始也是這樣
所以呢你就是說呢你
那這樣你才可以得到得到第一堆這些這些 model 裡面的 perimeter 
那麼因此呢你就可以說是這個
你總得要有一個比較好的 initial 嘛那怎麼來呢
你就是把你的聲音
先用等分為假設它們都 equal  lance 
然後你的前的四分之一做第一個 state 
最後四分之一做最後的 state 等等
那你這裡面一堆 vector 一堆 vector 嘛你就把它拿來做 vq 
那麼這樣子的話呢你可以得到
得到這些東西的這個
然後這堆呢我就求它的 mean 跟 covariance 等等
那就是那其實這個步驟跟前面是一樣的就是把這個
所有的這些 observation  vector 呢
那變成一個
用 vq 的方法
變成 m 個 cluster 
我這邊變成 m 其實就是剛才的 l 啦一樣的
你把它變成個
 l 個 cluster 不過現在因為我們前面講 Gaussian 的時候我們都說有幾個 Gaussian 有 m 個嘛
就是 m 個 Gaussian 
 m 個 Gaussian  distribution  
那麼你就把它變成這個 l 就等於 m 就是了嘛
然後呢於是呢你就可以把那一每一個 cluster 
第 n 個 cluster 到 state  j 就得到它的 mean 
然後呢每一個 cluster 裡面的
它的求 covariance 就是你要的 covariance 
你就把這些 data 去算它的 mean 去算它的 covariance 
就可以了
然後那你這個這個怎麼算這個是那個 weight 
 Gaussian 的 weight 怎麼算就數有幾個 vector 嘛
就是 number  of  vector 去除以全部的 vector 
那在這個 case 就是四分之一嘛
哦不是就是說你現在如果掉在這裡面了
這裡面我假設有一百五十六個 vector 
總共有七百五十個 vector 
那這個就是它的 weight 
它就這樣算嘛就是算 vector 的數目除以全部 vector 的數目就是它的 weight 
那其它的我這邊沒有詳細講那其它通常都用假設譬如說 A I J 怎麼辦
 A I J 我們通常都是假設假設一個值
那通常就是這個你只讓它這邊有一個這邊有一個
你別的都讓它是零最多再讓它多一個這個
其它都讓它是零然後你就設一個簡單的數字就讓它開始跑
所以這是一開始的時候我先假設一個 initial 的 estimate 
 of 所有的 parameter 用這個方法
那這樣的話我就有一個一開始的 model 
有了 model 之後呢
你 step  one 是幹什麼呢
跑 viterbi 
重新切一次
雖然說這裡有四個 state 
沒有理由每一個 state 各是四分之一
所以呢剛才只是因為你必須要有個開始
沒有開始沒辦法做所以
我開始先用四分之一做
四分之一做完我有了這個起始 model 以後
我就不能再去相信它是四分之一等分的於是怎麼辦
我現在就可以這些東西每一個都去跑
我們上週講的上次講那個 viterbi 
 viterbi 會幫我把重新切一次對不對
所以 viterbi 的結果可能會說
這邊的第一個 state 
 viterbi 是根據現在有的這些 state 裡面的這些參數
去切它會說這個可能這個是在這裡這個是在這裡這個是在這裡
那麼這個是在這裡這個是在這裡這個是在這裡
譬如說這樣子
它會重新切一次
重新切好之後你這邊就得到第二次的
這個來 train 它
同理呢這個也是得到一個第二次的
這個來 train 它
因此呢當你這樣 viterbi 切出來之後呢
我再重算一次
重新我現在用紅色這堆 vector 
重新跑一次 vq 
然後呢重新找它每一個
那每一個 vq 之後每一個就是一個 Gaussian 
算它的 mean 跟 covariance 
之後我又得到一組新的出來了
那我這個做完之後我就可以算一次這個機率因為我還是以這個機率為準
我就算這個機率看這個分數
看是不是提高了
有提高就回過頭來繼續做
有提高就回過頭來繼續做
我又回去呢
再切一次
用再跑一次 viterbi 再切一次
然後呢再重新做一次
這個 vq 然後再算一次 Gaussian 這樣子
那麼等到這個機率收斂為止
我的粗調就到此為止
沒有辦法再細調了
那這個時候我就把這個做出來的東西放到剛才的那個
 forward  backward 的 algorithm 的那個起始值去
開始用那個 forward  backward 來跑那個是跑得比較細
那個比較細之後可以得到一個微調然後可以得到比較好的結果
這個基本上就是
所謂的 segmental k  means 也是我們最常用的簡單的辦法就是這麼做的
那這個我後面有兩張圖是在講這件事
這個比較容易看就是我們講的這件事
就是說現在假設你已經有了第一個
一開始這個等分 train 出來已經有的話
你下一步怎麼辦就是跑 viterbi 
你的這一堆這一堆就是 viterbi 這一樣橫軸就是時間軸就是一個一個的 vector 
縱軸就是 state 跑 viterbi 就跑出一條
最可能的路徑
這條路徑也告訴我說這一堆 vector 到這邊為止
歸 state  one 這一堆呢歸 state  two 這一堆呢歸 state 三
於是我 state  one 的這個裡面的 Gaussian 怎麼 train 呢
就拿這些來 train 因此呢就是把這些東西拿來做
其實這個應該是做這個
嗯 LBG 啦 LBG 是一堆 k  means 所以寫 k  means 也沒有錯啦
就是做 LBG 啦所以呢你先做一個 global 的 mean 就是做 l 等於一的
 mean 在這裡然後把它拆成兩個小的就是我們剛才下課前講的把它拆成兩個小的
於是這兩個小的可以分別做成兩個
然後呢做好之後呢這個再拆成兩個小的這個拆成兩個小的就變成四個
如果你現在四個 Gaussian 夠了的話就你就拿這四個來做 Gaussian 
於是呢它就有它的 mean 
跟 covariance 跟它的 weight 都可以求出來
那這樣的話我就得到它第一個 state 的四個 Gaussian 
等等那這樣這就是我們剛才講的這個情形
那這個是 continuous 的做法
 discrete 是一樣的大概不需要講
因為我們其實不用了不過你可以想像它一樣的事情只是說呢
我現在就變成如果在那裡面你是不是用一堆 Gaussian 而是用一堆點來規定的話規定好一堆點的話
那其實那堆點怎麼來的它們當初在
七零年代八零年代的時候它們其實
八零年代的時候它們其實沒有辦法做 Gaussian 都是做這些點
那這些點怎麼做這些點都是用 v q 做的其實就是用 v q 做出來
然後看看它們有夠多 data 之後用 v q 來算它們大概有哪些點最具代表性
就用這些點然後這個每一點到底上面給它多少機率
給它多少機率呢就也是一樣用數的嘛
就是有多少個 vector 在做那個 v q 的時候被歸到這個來
好那就是就是我的機率嘛
那用底下這張圖講的也就是這件事就是假設你 viterbi 跑成這樣子
但是呢那我我這些 vector 呢我現在先看它 v q 到哪幾個點去
然後規在哪裡的時候就看在那裡面
舉例來講這個 state 裡面的話呢你會發現它有四個裡面
有三個都歸這個一個歸這個所以就是四分之三跟四分之一等等等等
這是 discrete 不過當然我們現在沒用就是了
好那這樣我們四點零就講完了
那四點零我們是等於是把這些 h m m 的最基本的數學模型跟它們的 training 的方法
講完那麼我們五點零還是在講 h m m
