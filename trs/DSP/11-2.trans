好那我們現在先看第一個
就是這個map 的principle 
那麼所謂的map 呢這個這個觀念其實我們已經講過很多次都知道
我們現在只是把這個這個maximum a posterior 這個觀念呢
放到再來做這個speaker adaptation 
那這個意思你可想而知就是我現在已經有一堆這個speaker independent model 了
那這個speaker independent model 就是我們前面說的我用了譬如說五百個男生五百個女生
所以train 了一個general model 
對多數人都可以用
只是正確率不高
那我有了有了這個model 呢
那我現在這個model 假設我有一堆tri phone 或者一堆什麼
我們假設是tri phone 好了
譬如說有五千個tri phone 
那麼這個m 就是五千
然後每一個tri phone 有一個hidden markov model 
它有a b pi 都有了
那不過這個是general 對一千個人所train 的
那現在呢這個新的speaker 來了
這個speaker 講了一句話是他的o 
那這個speaker 講的講的這句話o 呢你可以想像它裡面有很多phone 
他講的這段話裡面呢
它有一堆phone 你可以抓說ok 這個是這段是某一個tri phone 
這段是某一個tri phone 等等你可以去抓
因此呢這段tri phone 相當於這堆音
於是你想辦法用這個去adapt 這個東西
把這個呢把這個e 呢調到這邊來對不對
然後這個tri phone 相當於這堆音
所以呢我想辦法拿拿這個來
去去調這個
那麼就知道那個r 呢其實是這個嗯等等
那麼這個是它的基本的想法
所以我現在就是given 這個speaker 的adaptation data o 
裡面有一堆不同的縫
於是想辦法用它去train 它用它去train 它等等
那你你怎麼根據這堆聲音去調這個
跟這堆聲音去調這個呢
那它基本的原理就是所謂的map 
那這個式子沒什麼特別就是我們平常講的map 
也就是given 一個observed 這個data 
那麼base on 這個observed data condition 呢
我想辦法去調這個裡面的所有的model 
那麼使得我找其中的一組model 能夠讓這個機率最大
那麼使得我找其中的一組model 
能夠讓這個機率最大
那這個機率就是所謂的a posterior 的機率
a posterior 這個a posterior 的機率也就是在given observation 的條件之下
我調所有可可能的這個這個這個model 參數想辦法讓這個機率變得最大的那一個最大的那一組就是我要的
所以這個其實沒有什麼特別跟我們之前所講的所有的a posterior 的機率是一樣的
所以這個其實沒有什麼特別跟我們之前所講的所有的a posterior 的機率是一樣的
那這個式子也就是跟我們前面所講的所有的map 是一樣的
那現在後面也是一樣
因為這個機率我們不會算
但是我們比較會算的是反過來的
於是就把它倒過來這就是bayes theorem 
倒過來之後就變成這樣子寫
然後乘上那個的機率除以observation 的機率
那到這裡還是跟我們之前講的完全一樣的習慣的作法
就是這個時候因為我現在是調所有的lambda 參數
想辦法找一個這個機率最大的
所以呢
那麼我就把這個那麼底下這個倒是無所謂因為這個反正對所有的o 都是一樣的
對所有的lambda 而言我現在是要找lambda 嘛
那麼這這這個對所有的o 都一樣所以不用看
所以我只要maximize 上面這兩個
於是就變成那兩個
那到這裡的時候呢有個問題就來了
雖然這個機率我可以算
這個機率我們會算因為given 這些model 
given 這些model 可以看到這些observation 這個我們是會算的
h m m 就會算這個東西
可是這個是什麼呢這個實在不知道
這個model 的機率是什麼
我們really 不知道
你凡是所有的要做map 都會碰到這個問題
我們要知道怎麼算這個
那在這裡的話呢這是當初做這做這個方法的人他下了一堆功之後做了一堆assumption on 這個機率
因為這個不知道嘛
所以他做了一堆assumption 
有了這堆assumption 之後就可以推這個式子
他推了一堆數學
那堆數學是基基本上就是based on em theory 
那個這個em 就是我們剛才說在九點零我們會詳細說的
那現在我們先姑且先把它直接跳過去
那那一堆這個theory 的部份
我們跳過去等到嗯過兩週之後我們會講那個em theory 
到時候你就會就可以看在paper 裡面寫得很清楚
我們就不詳細去推它
他推了之後得到這樣子的答案
他有一整套答案我們這邊這邊只舉一個例子
我們說過從這裡開始
因為我們講的都是研究的課題
所以我們不再詳細地說每一件事
我們只是拿代表性的東西來說一下
那詳細的留給你做為這個各位的寫期末報告的題目
所以我們就不多說我們就舉個例子
它那樣推之後得到一個像這樣的答案
這個答案是他的答案裡面整套裡面的一個
我們拿一個來看
這個是什麼呢就是它的mean 怎麼調
假設這個是一堆一堆mean 
我們舉例來講我的某某一個
我的某一個tri phone 的某一個state 
某一個tri phone 的某一個state 它是一堆gaussian 
那麼於是呢這每一堆gaussian 有一個mean 
那麼這些mean 呢
應該怎麼調
我本來的tri phone 的mean 是這樣子
現在知道了這個聲音
這個speaker 是這樣子的
於是呢這堆聲音拿來調這個這裡面
於是呢我我的那個tri phone 的那個mean 呢
那個tri phone 在裡面的這個gaussian 這些mean 要調
這些mean 怎麼調呢
這個mu j k 就是這些mean 
是它的某一個tri phone 的lambda i 裡面的第j 個state 的第k 個gaussian 
ok 
所以mu j k 是一個gaussian 的mean 
第j 個state 第k 個gaussian 的mean 
那怎麼調呢它把它從mu j k 調成mu j k 的star 
那這個調的過程呢用這個式子
這是他經過做了一個assumption on 這個東西然後用em theory 去推推出這個式子來
那這個式子到底是什麼呢這個看起來有點複雜我們稍微看一下它的意思
它是有意思的
那麼這個mu j k 呢是某一個參數
所以這邊是那個mu j k 是那個參數
然後這邊是什麼呢有這個gama t 的j k 
這是什麼東西呢
這就是我們在四點零裡面
推hidden markov model 裡面的嗯basic problem 三的時候我們用過的
這個gama t 的j k 
那當時我們推過這個gama t 的j k 是相當於前面這些東西alpha t 跟beta t 的j 
這些是什麼這就是我們當時的foreword 跟backward 的variable 
那麼就是在時間t 走到這個state j 等等
那麼這個alpha t beta t 等等
那這兩個相除之後的意思
相當於我們當時說的gama t 的j 
那麼就是given 這個model 
given 這個model 
然後我現在看到這整個的observation o 的情形之下
在時間t 等於j 的機率
時間t 等於j 的機率是這一塊
就是這個
然後呢要乘上後面這個
後面這個是什麼呢
是我把現在這個時間t 的這個o t 
放在第k 個gaussian 上面
除以放在全部的gaussian 裡面
那這個的意思我們當時也說過就是你等於是
我現在如果有有一堆很複雜的distribution 
你把它看成是好多個gaussian 
好多個gaussian 
那裡面呢假設我現在要考慮的是第k 個gaussian 
第k 個gaussian 的話呢它是裡面的某一個
譬如說是這一個
這是它的第k 個gaussian 
那麼於是呢我現在就把我的時間
我先把我的時間t 的那一個o t 
放在第k 個gaussian 上面
這個得到的機率是多少
以及放在整個的這裡
那它變成這個是這個機率是多少
那這個機率除以這個機率
所以整個的是這個嘛
所以呢我把我現在時間o 的那個observation 放在這一個gaussian 上面的機率
除以放在全部的gaussian 的機率
那就是這個東西
那等於是說我現在在算的是
我不光是在時間t 是在state j 上面
在在這裡或者在這裡
而且呢我還把還算它現在是在這一個gaussian 裡面的機率等等
那這個是所謂的gama t 的j k 
所以呢這個這個是gaussian 的index 
這個是state index 
然後幹嘛呢它在這邊去做一堆summation 
t 等於一到大t 這是什麼就是我整個的observation 
那這裡講的這個observation 應該是指譬如說這一個
譬如說這一段我們知道它是應該去adapt 這個e 的這個tri phone 的
那麼因此呢這個呢就是我所謂的時間t 等於一到大t 
那用這堆呢去adapt 這一個
待會呢這個r 呢是這段
這是這是這是另外一個從一到大t 呢我去adapt 這一個
那麼這個時候我怎怎我怎麼辦呢
是用這樣這個式子
這個式子什麼意思呢
看起來有點頭大
不過我們可以用簡單的符號來想
它的意思呢就是這樣
這個式子你可以看成是一個
a 加上summation b t summation over t 
然後呢是a 的v 加上summation 的b t o t 的t 
我們這我這只是把符號簡化一點
這樣會比較好看
所以呢所謂的a 就是這個tau j k 
所謂的bt 就是這個gama t 的j k 
如果寫成這樣的話呢比較容易看
變成這樣子
變成這樣之後你怎麼看式子呢
我現在如果把它看成這樣的話
我現在如果先不把這個
它應該是這個括號在這裡啦我現在如果括號先不不括在這裡
我如果括號括在這裡
比較容易想像它是什麼
那這個時候其實就是這兩個相加分之這兩個分別除以這兩個
那這個意思其實就是
一一個譬如說一個alpha 乘上v 加上一減alpha 乘上o t 
那這就是一個內差嘛
就是一個內差嘛
換句話說
我今天如果原來這個v 就是我的mu j k 
就是我的某一個mean 
那我如果原來某一個mean 在這裡
現在呢這個人講的這個聲音
他的他的聲音不是exactly 在這裡他的聲音在這裡
那怎麼辦我就在這兩個中間做個內差
然後當成中間那個值
對不對
就是說我我原來的mean 在我原來的mu j k 的mean 在這裡
某一個譬如說e 的音它在這裡
現在這個人講了講了e 它它在這裡
因此我就取中間的那一點
那中間這點就是在做這個內差
那所以一個是alpha 一個是一減alpha 
那這個內差讓它這個靠近誰呢
由這個alpha 來決定
這個alpha 呢其實就是這個a 加b 這個b 分之a 嘛
來決定說這個比較靠近哪邊
它等等於是你如果這樣看是這個意思
那現在其實不是這樣
其實我們說這個括號不是這樣括的
這個括號是這樣子括的
那意思是什麼呢因為我講的不是只有一個聲音
而是我有一堆聲音嘛
它的這個它的這個這個e 有一堆從e 到t 呀
有一堆啊
那這一堆不是都一樣啊
因此呢你可以想像它其實不是只有一個
而是有好多個
它有好多個在這裡
這是t 等於一t 等於二一直到t 等於大t 
有這麼多個在這裡
所以它的內差呢是要要跟每一個分別去做
等於是這樣子嘛
我變成是從這點向這些每一個點去移動
對不對所以呢就變成這個所有的那這個這個b t 就是這個b t 就是我們這邊的gama t 的j k 
那這些個b t 呢告訴我每對每一個t 而言的那個o t 
它呢在不同的地方
那我到底應該各weight 多少
然後呢那我其實把它們全部平均起來得到一個
所以我基本上是從這點向這些點去移動
但是呢我把它weight 起來
最後移動一個值
那那個值就是這樣
基本上是是這樣算的
那比較像這個東西不是不是exactly 這個東西啦
並不是等於
只是說你可以想像成像這樣的東西但但但但是它一個一個都去移動之後平均起來得到一個
那等於是這樣的意思
所以呢這句話這就是我們底下講的這句話他說weighted sum 
把這個原來的mean 向向o t 的方向移動阿
那麼向所有的這些o t 
凡是它掉在第j 個state 跟第k 個gaussian 的這個條件之下
那麼向那就based based on 這些東西這些gama 
去向這些東西去移動
然後移到一個某一個合理的位置去
那當然現在這些b t 是沒有問題我們就有有gama 可以算
那tau j k 是什麼呢
tau j k 就是等於是這裡的一個weighting 
你可以看到它是一個parameter having to do with prior knowledge about mu j k 
通常呢是跟那個number sample use to train 這個有關
換句話說
你如果原來這個mean 是用非常多的data train 出來的話
我這個可能比較相信這個比較可靠
現在你這個人只講了這這幾個音我就把它調過去嗎有點危險
那在這個情形之下我就把這個weight 比較重
我就把這個值變得比較大
如果我這個是用夠非常多的data train 出來比較可靠的話
我就weight 它比較重一點
我讓這個值比較大
因此我就移動比較少
那反過來如果我原來train 這個的時候這個聲音本來就不夠多
我本來就data 不夠多所以不太可靠的話呢
我就weight 少一點
我就讓這個值小一點於是就比較靠比較向這個方向移動
等等ok 
所以呢這個移動多少這個alpha 是跟它們的相對大小有關嘛
那麼因此呢是跟這個地方跟這個這個原來這個mean 的可靠度有關
因此呢我就跟我的prior knowledge 有多少有關好
那麼跟我原來用多少sample train 出來有關
那這個其實這個參數就是它原來的假設這個prior knowledge 裡面的東西
ok 好那這樣我們大概可以解釋這個式子的意思
那它其實不光是這樣
它其實這個這個式子並不是這樣用嘴巴講講它的道理出來不是
它是完全用數學推出來它有一堆很很完整的的的的的theory 
根據em 去推推最後去推出這個式子來
只是推出這個式子之後我們可以看得出來它式子是有道理的就是了
那麼因此呢我們這樣做之後我現在這個mean 呢可以用這個方式來調
可以調到那麼你現在聽到它的那個聲音是e 的話我就可以調那些e 的那些model 
讓它呢比較像那個新的speaker 講的聲音等等喔
那這個不光是這個mean 可以調所有參數都可以調
包括這個gaussian 裡面的covariance matrix 
這裡面的covariance matrix 做的東西都可以調
它的weight 也都可以調等等
那我們這邊就不多不多講但是如果有興趣可以去看這個原始paper 裡面都有
那這個辦法有個最大的弱點
就是只有那些有data 的才會調
unseen model 就不會動
那什麼意思呢你可以想像我現在user 講的這句話裡面有什麼phone 我就調什麼
那沒有的phone 我就沒有調啊
也就是說呢你你今天真正的這個model 這整個的state 上這個整個空間裡面有所有的音的譬如說五千個tri phone 在這裡
那現在user 講了這句話之後那他總共只講了裡面的十個phone 
於是呢那十個講到的phone 可以調
這個phone 說到了它呢把它調過來
那這個phone 說到了呢那這個phone 它調過來
那這個phone 說到呢它調過來
這個phone 說到呢它調過來
假設我有五千個tri phone 在這裡的話呢它其實總共只調了這裡他講的這句話總共只有十個phone 的話就調了那十個而已
其他的就會全部都不動
阿那這個也就這邊講的就是只有有data 的才會動
unseen model 全部不動
那麼那這個其實是map 的基本精神因為map 就是這樣子
就是given observation 
那given 這個東西之後我調這個
那當然我沒有看到當然就不動啦
那因為這樣的關係所以呢它的一個最大的弱點就是你要有夠多的data 
你通常一句話只有十個phone 的話你只會調十個
那講了一百句話呢其實可能只有裡面並不是一百乘以十
很多常用的phone 已經出現很多次
沒有常用的phone 還沒有講到
那因此你講了夠多data 它可能還沒有調很多點它還是沒有調到
那麼因此呢它的performance 是你如果這個data 這個adaptation data 你這個speaker 講的話有限的話
它呢其實performance 進步呢會是比較有限的喔
這個是map 的方法的基本的缺點
這是原始的map 方法的缺點
那麼我們如果畫一個圖來看的話呢
就可以畫成這樣
這個是adaptation data 的量
那這個呢是我的正確率
那假設這個上限是speaker dependent model 
那這邊呢是speaker independent model 
也就是說如果你針對某一個speaker 跟它收集大量data 之後
你可以train 到這麼好
可是我們現在如果拿一千個speaker 的話不會太好
就會有個差距是在這裡
那現在你讓那個那個speaker 來講話
他講的講的話我這個正確率會慢慢從這邊慢慢上上來
基本上它是隨著你的data 越來越多我會進步
那就是我們剛才講的因為你講的一句話裡面有十個phone 我就調了裡面的十個phone 
你講了十句話裡面有五十個phone 了喔我會調裡面五十個phone 等等
所以基本上你你你講的data 越來越多的時候呢你這個會慢慢上去
那最後它應該會趨近於這個s d 的model 
它的上限是慢慢接近於這個地方
這是我們講的map 
那當然它的好處是說當你的data 夠多的時候它會趨近於這個地方
但是它的壞處就是說你一開始的時候它其實進步得很慢
這邊還差很多它進步得很慢喔
這是map 的原始map 的這個的缺點
但是它的好處它的它的這個map 的這個這個principle 這個maximum 這個a posterior 這個原理是非常精確的一個原理
所以這個式子是相當有道理的
那麼只是說呢它這樣做不了太好就是了
那這個map 的方法我就說到這裡
那麼你如果要詳細看的話就是它的原始paper 就是這一篇
雖然一九九四年已經十年多了哦
不過這個應該可以算是一個重要的經典
所以嗯所有的講到這個的paper 都要site 這一篇因為這個是嗯我們今天來看仍然相當不錯的一篇喔是值得參考的
你如果有興趣的話
