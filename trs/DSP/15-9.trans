那麼我們就可以進入下一面個呢就是l d a 的觀念
那麼l d a 是跟p c a 蠻像的一個東西
但是它比p c a 更進一步
有更多的更多的想法
啊
什麼是l d a 呢
這個也是在很多課裡面都學過的
不不論是pattern recognition 還是這個machine learning 裡面大概都有這個東西
就是所謂的這個linear discriminative analysis 
啊
那l d a 的基本精神跟這個有點像
但它比它更進一步
它的意思是說
假設假設我的
譬如說這堆點是ㄚ
然後呢我的這堆點是ㄨ
如果我知道這堆是ㄚ這堆是ㄨ的話
我現在這個軸是c one 
這個軸是c two 
你會發現呢ㄚ的distribution 是這樣
ㄨ的distribution 呢是這樣
overlapped 地很厲害
我在這裡切不開
那我如果用c two 來畫的話呢我發現呢
ㄚ的distribution 是這樣
那ㄨ的distribution 呢是這樣
也切不開
但是真的他們切不開嗎
其實我如果想辦法另外再找一個軸
我把它們儘可能找一個怎麼樣的軸才是能夠切得最開呢
那可能是在這裡
這個是e one
為什麼呢
你如果看看的話你會發現
譬如說呢
在這個情形之下呢
我的ㄨ到這兒來了
然後我的ㄚ呢是在這
那麼因此呢我原來在在在c one 或者c two 上面它們都overlapped 地相當厲害
因此你不管怎麼切都有很多error
可是我可能可以找到另外一個軸
它們能夠分得比較開
啊
那
那這樣的觀念就是所謂的l d a 
就是我我仍然在做linear 的transformation
跟這個很像轉一個軸嘛
還是轉一個軸
但是我找的軸呢跟剛才不一樣
p c a 這裡我並不知道誰是ㄚ誰是ㄨ
我只知道有一堆data 我我把這堆data 把它拉得開一點而已
我不知道誰是ㄚ誰是ㄨ
但是在這裡的話呢我是假設我知道誰是ㄚ誰是ㄨ的話
我要把ㄚ跟ㄨ拆開來
我重新再找一個軸我讓ㄚ跟ㄨ能夠拆得開來
啊
那這個觀念就是所謂的l d a
ok 我們在這休息十分鐘好啦
那麼l d a 的基本精神跟這個是很像的
只是差一點點而已
我們現在應該只剩下
啊
我們來說一下期末報告啊
我們這門課期中考已經考過了然後嗯有兩個題目你suppose 也已經做了
那麼於是你只剩下一件事情就是期末報告
那我們的不過我們的期末報告佔分很重了佔百分之五十啊
所以也就是說你前面的期中考也好
那個習題也好
總共只有佔百分之五十而已
那期末報告是另外的百分之五十
所以啊你得要花點工夫來做你的報告
那麼我們的deadline 我現在是訂六月三十號
這個是校例所規定的期末考結束以後的三天
所以應該是盡量給你盡量給你最多的時間啊
校例規定的期末考是六月二十七
我們再加三天那這天是星期五
然後我們訂五點
下班時間
你交到這個電資學院大樓就是這棟樓的五三一
是我們的語音實驗室
電資學院大樓五三一啊
交給我們的任何一位助教
不論是宮嵊益還是還是許長文啊
就是在這個以前的任何時間都可以你最晚最晚是在這個時候為止
超過這個時間就算遲交就是了
這是我們講的deadline 
那我想這樣應該有夠多時間了我希望你有夠多時間做它
是期末考校例規訂的考完之後還多三天
那麼我們可以可以做的這個style 應該是有
第一種是reading report 
我不要做任何程式做任何實驗我就是讀paper 就夠了
我們講了那麼多paper 
然後很多豐富的東西都可以都可以讀
讀了之後我可以寫一個心得報告我就純粹是讀paper 的也可以
當然你也可以做computer project 
因為我們講的每一樣東西幾乎都可以寫程式
然後都可以做你要做的事
所以呢可以做computer project 
你如果要做computer project 的話通常你需要data 
那我們兩個習題已經給你不少data 了
我們包括在第一題裡面給你很聲音的data 
第二題給你很多文字的data 
所以呢那同樣這兩個習題也給你很多工具
包括第一題的這個h t k 
跟第二題的這個s r i 
它們都有非常多的工具可以用
那你如果要做的東西比這還多的時候
你可以找我們的助教說你要做的哪一種東西不夠
所以想要多要一些data 或多要一些工具
應該也是可能的
那助教會過來跟我商量說哪些東西可不可以給同學啊
那我舉個例子我們講的eigen voice 
eigen voice 是可以做報告用的
可是你如果eigen voice 是可以很可以適合做報告的題目
可是如果你要做eigen voice 的話
你需要有夠多的不同的人都有夠多的data 
那我們給你的data 大概沒辦法做eigen voice 
啊
那麼因此呢在那個情形之下你就可以跟助教商量說我要做eigen voice 所以我的data 不夠多
那我會跟助教討論
拿一堆data 給你讓你可以做等等
啊
所以呢你如果要做computer project 的話
基本上從我們兩個習題給你的data 跟工具來著手
但是你如果覺得你的你的題目希望做更多東西
可以
啊
你跟助教商量
那助教覺得他有問題他可以來問我啊
然後當然第三種是combination 
也就是說你都有一點
我讀了一些paper 之後
然後把那些東西弄清楚之後我來做一個程式
我兩個都做一點也可以
哦所以這三種都行
那然後呢我可以是這個啊嗯嗯這個team work 
也就就是兩個人一組
可以啊
不要超過兩個人啊
兩個和尚挑水喝是剛剛好
但是如果三個和尚就會沒水喝
啊
所以呢就是
你如果要兩個人一起做是有它的好處就是說我們兩個人合作的話讀paper 的時候我讀一半你讀一半
那這樣的效果可能比一個人讀來得好我覺得是有道理的
或者說你讀paper 我來寫程式
啊
只要這兩個人可以密切合作的話
兩個人可以
但是不要超過兩個人
那這個時候你要註明每一個人的contribution 啊
就是說如如果我們兩個人合交一份報告的話
那麼你要裡面要註明清楚是誰我做哪裡他做哪裡
那這樣就可以
你如果沒有註明的話我就會以為是一個人護行另外一個人
因為可能是一個人把另外一個名字寫上去
啊
那這樣子就不行
所以你一定要註明說哪一部分是誰做的
那那不管是你既使做的是computer project 的話你還是會要交一個書面報告
就只是說你的因為你一定要有一個文字在描述你的程式嘛
所以只是說你如果是做computer project 的話你的書面報告會變得很簡單
這個少數幾頁就夠了
你的重點是那個程式了
啊
那你如果是reading report 的話你就你就完全是書面報告啊對不對
如果是computer project 的話你重點是那個程式
但是你還是要有書面報告
那不管怎樣呢這三種形態都有書面報告
只是書面報告多少的問題那書面報告交的時間就是這個時間
那你如果是computer project 裡面有computer 有程式的話
你可以交一個光碟或者交一個什麼
那當然你你如果想要demo 也可以啊
不是一定要
但是如果你覺得你寫的的程式適合demo 來呈現
那麼如果光是交一個交一個光碟不見得能夠感覺到它多麼有意思
demo 比較容易看得出來你可以說我要demo 
如果要demo 的話呢就是在交報告的這個時候
跟助教登記說我要做demo 
那到那個時候的話
我會來看demo 
啊
所以呢這最後面的五十分的話是我自己改的
不是助教改的啊
那
這個所以這五十分的報告是我自己改的
包括如果你有demo 也是你是demo 給我看的不是給助教看的啊
那所以這個大概是關於期末報告這個你可以想的事情
那我們到目前為止已經講了好些東西了而後面還有很多東西都都可以做為期末告的的思考的方向
好這是關於報告
那我們們現在回過來說l d a 
我們說l d a 的精神是
跟剛才p c a 不同的是
p c a 是我只是希望把他散開
l d a 是假設我知道這個是ㄨ這個是ㄚ
我希望把ㄨ嘎ㄨ跟ㄚ分開
那這是什麼意思呢什麼叫做把ㄨ跟ㄚ分開呢
那麼我們如果用比較精確的語言來講的話呢
是這樣講
如果說
這一堆是ㄨ這一堆是ㄚ
這就是分得很開的意思對不對
這樣我很容易區別
但是反過來呢
如果說
這一堆是ㄨ
然後這一堆是ㄚ
這就是分不開的意思對不對
那你想這個到底我們用什麼方式來說它們到底分得開還是分不開呢
那個最簡單的方法就是還是用我們的covariance matrix 
我們譬如說
就這個而言
這一點它有它的一個mean 
對不對
譬如說這是ㄨ的mean 
那這個呢是ㄚ的mean 
那除了mean 之外呢我還有什麼東西呢
就是它跟mean 散得多開
離mean 有多遠
那這個東西呢就是我們講的covariance matrix 
u 的假設我用我用這個這個u 來co 代表covariance matrix 
這一堆有它的covariance matrix 
這就是它的散得多開的意思
怎麼講呢
那你如果仔細想的話
所謂covariance matrix 是什麼
這covariance matrix 
它的每一個element是假設叫作u i j 好了
這它這是這個matrix element 
它是什麼東西
它是這個嘛就是這個x 減x i 減掉x i 的mean 
x j 減掉x j 的mean 
就是第i 個這我這個都是vector 
每一個vector 有第i 個component 跟第j 個component
這個x i 減掉x i 的mean 就是指它跟mean 的距離嘛
我分別減掉它的mean 
這個x i 減掉x i 的mean 就是指它跟mean 的距離嘛
x j 減掉x j 的mean 是在j 上面它跟它的距離嘛 我都是在算它跟mean 的距離
然後呢
我再這兩個相乘求平均的這個東西就是u i j 
這個matrix 裡面的每一個element u i j 就是這種東西
那你就可以想像得到我這個matrix 的對角線上的每一個呢
就是它的每一個component 自己的variance 
這個你很容易想像就是我們平常講一個gaussian 的肥度
對不對
一個gaussian 這個是是mean 
它的肥度就是它的variance 
對不對
這個variance 就是當i 等於j 的時候
當i 等於j 的時候就是指對角線上面的這個東西的時候
就是指每一個component 它自己的那個散開的程度
所以呢這裡的在對角線上的每一個就是它散開的程度
那同理
不是對角線上的也其實也是一樣的意思
只不過我現在i 不等於j 
是i 跟i 散開的是i 跟它的mean 以及j 跟它的mean 各自散開的程度的平均嘛
所以我變成用整個的matrix u i j 
來描述這東西散得多開
那你如果這樣想的話呢
那你可以想得到我這邊的話呢
就這個圖而言它們這個散得不開這個比較緊這個比較緊
而這個呢就是散得很開
譬如說這個一個這麼大這散得很開別外一個呢又這麼大
那我希望得是變這種而不是變成這種
那我就希望每一個class 
這是一個class 這是一個class 
我希望每一個class 的u 呢盡量小
那那個class 就是我們這邊的u j 乘以weight w j 
也就是說呢我現在我每一個class 每一個class 有它的mean
就是它的mean
有它的covariance matrix 
哦
那這個mean 跟covariance 就是我這邊講的這兩這兩個東西
我每一個class 譬如說ㄨ譬如說這個是ㄚ這個是ㄧ
那麼這兩個每一個class 有它自己的mean 有它自己的covariance 
那麼我希望呢它們都盡量
covariance 就是說它散從這個mean 散散開來的散開的程度
我希望它越小越好
那這就是所有的u j 的平均
那麼weight 是什麼呢w j 是什麼是number of samples 
假設我這邊有一萬個點
這邊有十萬個點
那我就這個乘以一萬這個乘以十萬
或者說這個是乘以這個十一分之一這個乘以十一分之十
那這樣就表示說我我我同時把它們散開的程度跟這個data 的量weight 進去了
那這樣呢這個叫作within class the matrix the scalar matrix 
就是指一個class 裡面到底它散得多開
我希望每一個class 自己不要散得開來最好越緊越緊好
所以我會希望這個u i j 的每一個element 越小越好
對不對
u i j  的每個element 越小越好就表示說我這個它的散開來的肥度越小
那就表示越緊
那這個是對每一個class 內部而言
我都希望它都能夠儘量收得很緊
這就叫作within class scalar matrix 
但是還有一個東西呢就是between class 
我還希望呢這個class 跟class 之間能夠散得開
對不對
我我不要這兩個雖然我我如果兩個排得很近的話還是會會混淆嘛
我希望它們儘量散得開
那這是怎樣呢那我就要有一個global mean 
啊total mean 這裡有一個mu 是一個total mean 
假設我有十個class 
那這個十個class 的total mean 在這裡的話
那我就把這裡的每一個mean 相對於這個mean 來做一個那個covariance matrix 
那那個呢就是這個
所以底下這個式子這個這個東西叫作between class 的scalar matrix 
就是指class 跟class 之間我要它們這個分得開
那我就是把它們的mean 拿來作做一個covariance matrix 
ok 
換句話說
這個不好為什麼不好
一方面是因為每一個class 自己散得開
一方面是class 跟class 之間排得太緊了
譬如說白色的這個mean 在這這裡黃色的這個mean 在這裡
它們這兩個mean 太緊了
所以呢一方面是我的within class 在class 之內我盡量要收得緊
一方面呢我是要class 跟class 之間我要盡量拉得開
像這個的話就是我class 拉得開又收得緊
所以呢我有這個between class 的matrix 
那這個其實是式子你看起來就知道它其實沒有什麼特別
就跟我這個covariance matrix 是一樣的意思
只不過我現在每一個就是各自的mean 
每一個class 的mean 分別減掉那個total mean 
那等於是這我們剛才是每一個點分別減掉它的mean 是一樣的嘛
啊我等於是每一個每一個class 的mean 去減掉它的total mean 之後
我來算這個covariance matrix 
那然後呢我我用這個covariance matrix 來代表我的between class 
那因此呢我變成是說我class 跟class 之間我要盡量散得開
而同一個class 裡面呢我要盡量收得緊
那相對於這個呢就是我同一個class 就沒有收緊就散開了
而class class 之間又又變得很緊所以這樣就是不好
這個是好的這個是不好的
啊
那麼因此我要怎麼做這件事情呢
我我有兩個matrix 
我要within 要越小越好
between 的要越大越好
所以基本上我是要把這兩個來相除
就是把我要的東西是between class 的除以within class 的
我這個東西要越大越好
因為這個大就表示是說我class 跟class 之間分得開
所以呢這是越大越好
而這個東西呢要越小越好
因為這個就表示說我的一個class 裡面是收得越緊越好
那麼因此呢這兩個相除的話呢大就是可以最大嘛
我如果這個東西是maximum 的話呢
那就是表示是說這個越大越好
這個比這個這個越大越好這個越小越好嘛
或者說我的class 跟class 之間的距離相對於class 內部的距離是越大越好嘛
對不對
那你也可以解釋成這好比是s n ratio 一樣
這個是class 跟class 之間的距離讓我可以拆開signal 的
那這個呢是是noise 
等於說是我每一個會把的noise 把它它的弄弄模糊的
所以這個有點像s n ratio 一樣的東西
所以我要的就是這個東西這個ratio maximum 
就表示說這個這個最大這個最小
但是呢不能這樣做為什麼呢這兩個是matrix 啊
這是一個matrix 這也是一個matrix 
matrix 哪能相除呢
不能相除啊
那怎麼辦
我們就加一個trace 
trace 是什麼
你可以回去查你從前唸的線性代數裡面的matrix 就知道有一個trace 
每一個matrix 我都可以做一個trace 
做個trace 之後呢
其實是相當於它的eigen value 之和
那其實就代表它的total scattering 
那麼換句話說
你可以想像我們的譬如說我們的covariance matrix 它的eigen value 其實是什麼
其實covariance matrix裡面的每一個eigen value 都代表它的那一個dimension 散開的程度嘛
就是我們p c a 講的嘛
它的散開的程度就是那個eigen value 嘛
所以我如果把eigen value 之和加起來是trace 的話
就代表它total 在每一個dimension 散開的程度
就變成一個單一的值啦
所以呢我這個matrix 我不能直接去求它不能直接去除它
而我必需這兩個matrix 都分別求它的trace 
當我這個也求trace 這個也求trace 之後
分子就代表它的class 跟class 之間散得多開
分母就代表每一個class 內部散得多開
那你這個希望越大越好這個希望越小越好
所以你是這兩個的trace 來相除我要maximum 
那在這個條件之下我現在要做的事情是什麼呢
我做的事情是要找一個軸
原來的這個c one c two 這個軸可能不好
我要找一組新的軸
那一組新的軸呢使得它們能夠散得最開
那這組軸怎麼找
就是用一組orthonormal basis 排成w one w two 等等
在k 個dimension 
然後呢
那我如果用這個w one w two 這k 個這個orthonormal basis 做成一matrix 
來做這個轉來做這個新的軸的話
那麼我的這個s 這這兩個這兩個這個scattering 的這個matrix 其實就是它的covariance matrix 會變成什麼呢
會變成前後各乘一個這個w 啊
所以這個數學式子是這樣來的
所以呢我現在我我真正要maximize 應該是這個嘛
我要maximize 的是這個嘛
但是呢我現在要轉一個軸
那個轉軸的那個matrix 呢就是w 
那這個w 裡面的每一個column 就是一個basis 
那當我用這個w 做個轉軸的時候呢
那你的那個s 呢就會變成左右各乘這個
這個s 也會變成左右各乘這個
所以我就變成說是我要左右各乘一個w 之後這是transpose 這邊沒有transpose 這是全部各乘之後求trace 相除
我要maximize 這個值
那我選這條所有的w 去找
哪一個w 讓它最大的那個w 就是我要的w 
那用這個方式呢得到的答案就是我們講的linear discriminative analysis 
那它的它跟p c a 有何不同
p c a 只是找出一些principle component 使得它在那些component 上面散得最開
但是p c a 並不知道誰是ㄚ誰是ㄨ
所以p c a 只是把它們散得最開並不知道誰是是ㄚ誰是ㄨ它並沒有把ㄚ跟ㄨ散開
但是l d a 是進一步
l d a 是知道這一堆是ㄨ這一堆是ㄚ然後所以我要把ㄨ跟ㄚ這兩個class 拆開來
所以呢l d a 使用的information 比p c a 要來得多
in general 在大多數狀況l d a 的效果比比p c a 更好
因此呢
所以呢我們l d a 呢是目的是要找出最最有具有鑑別力的
啊這個是鑑別力的意思嘛啊
我要找出最有最具有鑑別力的那些個dimension 
然後呢把它這樣子拆開來
那如果是這樣子的話呢
那我就得到這個這個最好的結果
啊
那這叫做discriminative linear discriminative 就是我有鑑別力的一個分析
那麼當我這樣做的時候這個solution 是什麼solution 其實很簡單
我還是找eigen vector 
不過什麼matrix 呢是這個matrix 
就是這個within class matrix 的inverse 乘上between class matrix 
這兩個相乘之後還是一個matrix 
這個matrix 的eigen vector 
然後找它最大的eigen value 的
那些個就是這些w 
那麼因此呢跟剛才跟p c a 裡面一樣
那麼eigen value 最大的那個eigen vector 是w one 
第二大是w two 等等
那結果我到時候在w one 上面呢我可以拆得最開
然後w two 上我拆得第二開等等
啊
那這就是所謂的l d a 
