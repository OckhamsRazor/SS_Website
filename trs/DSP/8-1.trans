ok 喔 我們今天要講的八點零就是這裡要講的最後一塊還沒有詳細說的
就是這個我們一直在還一直圍繞著這一塊在說
那麼我們上週的七點零講的是這塊就是 frontend processing 包括怎麼求這些 feature
六點零講的是 language model 是這一塊
然後呃五點零四點零講的是 h m m 是這一塊
那現剩下就最後這一塊那這一塊其實並不容易
因為你現在得把所有的這些東西全部整合你包括要把這些的 feature vector 這些個 h m m 跟 language model lexicon 全部要整合所以呢這個地方其實是相當複雜的一塊
那就是我們今天八點零要說的事
那麼這個我們有另外一張圖其實是一樣的意思那麼在二點零裡面
對那麼我們那時候在說的事情是這件事也就是說當你的acoustic model 給你一堆h m m
language model 給你一堆n gram 之後
你這個search 在做什麼事情你可以想像成是given given 一個這個sequence of feature vector x
你希望找一個word sequence w 那能夠maximize 這個機率也就是 m a p 的principle
那這個機率呢我們可以把它變成這樣子因此呢你要算這個機率是用h m m 算的這個機率是language model 算的
那你要找一個word sequence 這兩個乘起來最大
這個也這個也就剛才那個圖這樣講起來好像很容易其實並不容易
為什麼你只要想我這個word sequence 每一個word 假設我有六萬個word 的話辭典裡面有六萬個可以用的word
那麼你這邊有每一個都有六萬個可能
所以呢你一開始的第一個你的第一個word 就有六萬個可能第二個word 又有六萬個可能每一個word 都有六萬個可能所以呢你其實就有六萬的r 次方種
那其實應該講你的第一個phone 就有假設你有六十個phone 的話第一個phone 就有六十個可能第二個phone 又有六十個可能所以你如果要把所有的通通都這樣子來做一次的話來maximize 的話其實是非常難做到的事
那也就是我們現在八點零要說的那麼在八點零裡面我們倒底用什麼辦法來做這個search 這就是所謂的search process
好那我們現在來看八點零那麼這裡所說的呃這個其實是recognition 的一個一個chord element 因為你如果沒有這塊的話你其實是沒有辦法真的執行
那麼在所有課本裡面都會講這一段大概相當於這本課本的這些跟這些或者這些
我這邊都寫or 意思是說你只要選擇一個你覺得喜歡看的就可以了
那麼這裡面呃講得最完整的可能是這個
那我這邊講的大概也是從它裡面我大概也是以它為這個主要的reference 來講的我主要是從它來做的
但是呢它講的比我講的多很多所以我只是從它裡面抽一些少部分的我來講而已喔
那它的缺點是說它寫得多而不見得那麼好看
那寫得淺而比較好看的大概是這一個這是寫得淺而比較好看的
那這個呢它講得其實相當不錯只是說它畢竟不是一本教科書它只是他的演講稿所以呢稍微難讀一點就是了
不過這三個我想你只要選任何一個來讀都可以
四十一篇paper 哦是算寫得非常好的完整而詳細的一篇paper 把這邊所有東西都講得很清楚
呃缺點是說這篇不好念就是了相當長而且不好念但是你如果認真讀這是相當好的一篇
那五的這個倒是不是我們這邊講的search
而是我底下要講的這個我們一開始先講一個古代的東西
就是所謂的dynamic time wrapping d t w
那這個東西是在還沒有h m m 之前最成功的技術
那麼其實到今天仍然有它的價值所以我們稍微提一下
那它的觀念其實也就是在做search
所以呢我們可以可以把它看成是這個search 的基本精神可以可以從這邊來看
那麼因此呢我們會先講一下這個東西
那這個是現在用得比較少了但是其實呢相當值得學一學它們的觀念所以我們稍微說一下
那這個的reference 是我們剛才的這個
那這本書是比較比較早的書九三年的書所以我們現在不太reference 它因為它講的很多東西其實之前已經不太用了
但是它的這段其實講得非常好因為那個年代其實他們花了很大功夫做這些東西喔
在還沒有h m m 他們做做得不錯的所以你如果要了解詳細的話是可以reference 這一個
好那麼我們現在說什麼是d t w
我們說過這個是在pre 我所謂的pre h m m 的時代也就是說在古代h m m 還沒有成熟之前
當時已經有非常多的語音辨識的研究跟技術
當時用的最成功的方法就是這一個
那這個這個方法even 今天仍然很好
如果你只是做small vocabulary 跟isolate word recognition 的話
那也就是說呢今天仍然有一些其實有相當多的isolate word small vocabulary 的應用是用它做的
那麼因為用它比用h m m 簡單如果你是一個這麼簡單的problem 的話
那舉個例子來講你如果去看玩具有的玩具是可以聽聲音的
你說這個這個turn to the right 它就向右邊轉你turn to the left 它向左邊轉那 sing a song 它就唱個歌它能夠聽少數的字的這種就是isolate word small vocabulary
那它怎麼做的一片晶片在裡面
它那片晶片其實裡面做的是這個因為在這種狀況之下其實這個比h m m 要簡單
但是呢你如果複雜一點的話呢它就輸給h m m 了
當你的字彙很大的時候我們說過h m m 最大的好處是它可以把小的串成大的
所以你只要有有phone 的model 就可以串成word 的model word 的model 就可以串成sentence model h m m 的好處是可以直接串起來
那它這個就沒有這個好處所以你要連起來不是不能連連起來困難比較多而且比較做不太那麼好
那麼因此呢他要做continuous speech 就比較難它比較適合做isolate
另外呢它大概vocabulary 不容易大也因為這樣就不容易大喔那麼等等這是它的缺點
但是即使如果只是做這種小問題的話
它到今天仍然是跟h m m 一樣好
大概比起正確率是完全相同的是一樣好的
那它的基本精神是什麼
就是找一條optimal path 來match 兩個template with different length
這個講different length 有點簡單化了一點其實不只是different length 而是different sequence of events
嗯我們舉個例子來講譬如說san francisco
你要辨識一個san francisco 它可能這麼長
等一下你再念一次san francisco 的時候呢san francisco 就只有這麼短
那麼你你怎麼怎麼辨識這中間的的東西呢
那麼第一個它們長度本來就不一樣你誰跟誰去比呢
而且你要講這裡面的譬如說某一段音斯它在這裡它在這裡
它也比較長它也比較短那你怎麼知道它跟它比呢
另外一個可san francisco 的這個可在這裡那它在這裡
那你怎麼知道它在這裡它應該它跟它跟它比呢
那那如果你這個都不能知道的話呢我怎麼比呢
那我們知道今天h m m 怎麼做這件事情
h m m 做這件事情是是我們用的方法是就是它的state transition
因為這些都是random 的
對不對所以它可以state 在這邊比較長也可以跳下去
所以每一個state 的的都是可長可短h m m 是用這種方式來handle 這個問題
那它這裡怎麼辦呢它就是想辦法去找一個optimal path 來match 它們
那麼我們舉例來講假設你這個叫做reference 這叫做test pattern
那我的 reference pattern 在這裡假設是比較長
test pattern 在這裡比較短
那怎麼辦呢那你可以想像的是我如果拿裡面的某一個參數來說
假設某一個parameter p one 或者p k 它這邊的值是這樣子的
那那個p k 在這邊的值呢是這樣子的
那你大概可以可以想像那麼很可能應該這一點是對應到這一點的這一點呢是對應到這一點的那麼這一點應該是對應到這一點的
同理呢起點對應到起點終點對應到終點
那你真正要找的應應該是這麼一條path
我如果可以找到那麼一條path 那條path 告訴我哪一點對應到哪一點哪一點對應到哪一點哪一點對應到哪一點
那如果是這樣的話呢我現在就可以在這延著這一條path 來比對
那這個時候就知道它應該對應到它來比它應該對應到它來比等等
喔那這個就是這個d t w 的基本精神就是這件事就是找一個optimal path mention to template with different length
那麼我剛剛講不一定是different length 而且還包括這個這個different distribution of events
那這個event 在這裡這個在這裡你要對應到它們各自應該在哪裡等等
好那麼這個基本精神是這樣真的要做的時候其實是有非常多種方法
所以我們剛才看到在還沒有h m m 的年代呢他們下了相當大的功夫喔所以你詳細你要看的話是看這一這一段
喔那我這邊大概簡單的說一下我們舉一個例子這只是一個example
不見得是最好的但是呢我們稍微看一個例子看它是怎麼做的
那基本上呢你第一件事情就是我的一個是我就是我那邊畫的一個是reference 一個是test 喔
所以一個是test template 是y 然後reference template 是x 那我怎麼樣去去這個調它們呢
那麼這個是這個x 是x i i 等於一到m
所以呢譬如說這個是x one x two 一直到x m
這是我的這個喔reference 這是我的reference template
然後我的test template 是 y j j 等於一到n
喔所以這個軸是i 然後這個軸是j
y j 呢這是y one y two 一直到y 的n
那當然 m 跟n 是沒有理由會一樣它們是兩個不同的integer
那我現在要把它們來做剛才講的那件事情找那條path
怎麼找呢我現在在這個方法裡面這只是它有諸多方法裡面的一種在這個方法裡面它說呢我先想辦法
把它們都對應到一個共同的長度l 去
那麼因此我都做一個mapping 讓它們對應到l
換句話說呢我不論是這個test template 還是這個reference template 我都重新畫一根軸
那這根軸呢變成l
這邊變成一二到l
這個軸呢叫做這個x f x 的i 對應到m
所以我這邊就變成一個 m 軸這個是一個m 軸
那麼我重新把它調成長度是l 這個index 叫做m
那麼以別於剛才那個叫做 i 的現在是m
那同理呢y 這邊我也重新做一根軸我也是一到l
那這軸叫做n 以別於剛才的那個j
那這樣子之後呢這就是我這邊寫的這個
我有一個這個function 把i 對應到m j 對應到n 那m 跟n 都變成只有長度相同都是l
那這個時候當然你有一些條件你這個mapping function 怎麼定義呢就是這邊所講的那一對應到一最後都對應到l 對不對
我一都對應到一一都對應到一那這邊的話呢m 要對應到l 這邊的話呢n 要對應到l 那就是這一排所謂的n point constrain
之後呢所謂的monotonic constraint 是說呢這樣子
那這個意思是說當然你現在這個這個m 不見得等於l 嘛你可能是要拉長或者縮短嘛
那你如果要拉長或者縮短的話
你可以想像會變成這一點對應到這一點這一點對應到這一點這一點對應到這一點是可能的我中間丟掉一些這是可以的
但是呢不可以說是這個對應到這個之後這點給我對應回來這個不可以
好那就是所謂的monotonic constraint 那你看這個意思就是這個意思喔
就是你你如果是你你不能這個你可以往前對應過去如果它比較長的話你要丟掉一些東西嘛
反過來如果它比較短的話你也可以上面丟掉一些東西嘛阿你可以這樣子嘛這個都可以嗯
但是呢你不能說這樣子嘛這個是不行的嘛就是就是就這樣的意思喔
所以這個是monotonic constraints
那有了這個之後我現在都都對應好了然後呢它最主要的動作就是底下這一步
那其實所有的這個我們說這個做法千變萬化有很多種不過基本精神就是這個這是一樣的
那這個的意思其實跟我們在跟我們在嗯四點零講的viterbi algorithm almost 是完全相同的觀念
你如果回想一下我們那時候講的viterbi 是怎樣的
如果還清楚的話我們現在說這個就非常簡單因為跟那個是一樣的意思
你記得viterbi 是怎樣的嗎
我今天如果有一個這個在時間t 要在某個state i 上面我定義一個東西叫做delta t 的i
那是指說我在時間t 走到i 的時候呢這個的某一條path
那條path 走到這裡的時候我的分數是最高的那個的分數
那如果說我現在這點每一個都求出來了的話
那麼下一下一排的t 加一呢我就只要去算這些東西裡面倒底是誰過來的就好然後一路走往前走這就是viterbi
那你如果這點還記得的話我們這邊講的其實almost 是相同的事情
那麼他這邊的 d 的m n 是個cumulate 我這邊漏寫一個字哦應該是minimum distance up to m n 應該有個minimum 這是minimum distance
那麼換句話說呢我現在如果我走到走到某一點的 n m
m 在這裡n 在這裡的這一點是m n
那有一個分數叫做d
那它是到這裡為止的所有的path 裡面分數最低的
嗯分數因為它現在是在算distance 要找一條minimum distance 所以它現在是算distance 所以是minimum
我們那邊 viterbi 我們因為那邊是在算機率所以是要maximum 這邊是算minimum
也就是說這個時候我我的d m n 呢就是我走到這邊為止的一個minimum distance 的 path 某一條
是我的我的這個分數最低的一條
那如果這點知道的話呢那麼我的這個嗯現在是怎樣呢我現在要算一條新的m n 是對所有的m plum n plum
那我們可以假設這點是m plum 這點是n plum 這個是m plum 這個是n plum 的話
那我現在要算一個新的點
這一點呢是m 跟n 這點是m n
那同樣的這點可以來自很多點
它可以從這點這樣過來也可以從另外一點這樣子過去也可以從另外一點這樣子過來
那每一點呢都相對於它有另外一條optimal path 走過去
那看看是誰加到這邊是minimum distance
那這個精神跟我們那邊講的viterbi 是完全一樣的
所不同的是在viterbi 的時候我們已經變成是一排一排這樣一行一行向前走
在這裡的時候現在講的這裡呢它並沒有規定要一行一行走
你可以從前面的任何一點往前面走就這樣不同
ok 所以呢我現在在這些個m plum n plum
我都有走到那裡為止的minimum distance path 跟它的minimum 的分數就是accumulate minimum distance
那然後呢那我的到這邊怎麼算呢就是看從誰過來分數最低嘛是minimum
那這時候我就要算從m plum n plum 走到m n 的要多加多少就是多加這個
那多加的這個是什麼呢這個東西你可以看它其實就是從m plum n plum 走到m n 的所有的多增加的這個分數
從n plum m plum 走到m不管是從這個還是這個我都可以過來嘛那就看哪一點過來的分數加起來最少
那這裡面又要稍微算複雜一點
那就是說因為我現在呢我現在從這個n 這邊走到這邊我的每一點呢我都要先由剛才那個function 對應回去
所以這個f x 跟f y 的inverse 就是這些mapping 的先對應回去到 i j
因為在i j 可以算它的distance measure 在原來的d t w 的frame work 裡面最重要就是
你任何一個frame 的x 跟任何frame 的y 你可以算他們的distance 所以它就去算這個distance
但這個distance 定義在這個上面呢
所以你現在是要把這個n plum 我現在看看這點是什麼那這點跟這點是什麼
然後你拿這個的x 跟這個的y 去做去求它們的distance
好所以呢那也就是我們這邊所說的
你從這裡m plum n plum 這 plum 這邊呢你先inverse 回去得到在原來的i 跟j 軸上面各是哪個地方
然後拿這個的x 跟這個的y 才能夠算它的distance
那你算distance 的時候你還要算另外一個東西
這個所謂的w 的delta i delta j 是什麼呢是 weight for different types of move
什麼意思呢你從這邊或者這一點或者這一點往那邊走可以有很多種走法
你可以想像的是我可以這樣子我可以這樣子走我可以這樣子走我也可能是這樣子走那我可也等等哦
那你每一種你可以讓它們分數不一樣
你可以定義如果這個是一步的話這個是一步這個可能是一點五步或者這個是什麼你可以定義它們的weight 不同好
那這就是這邊所說的這個w i w j
也就是你這個地方到底向這邊走的是w i 嘛這個走的是w j 嘛
那你看w i w j 你看是走走多少看你是走哪一種move 那你可以有不同的weight
那這樣的話呢你就可以把從你的m plum n plum 走到n m 那中間一路這個走好幾步這樣走到這邊那麼這中間的通通加起來
但是weighted by 這個這個weighting factor 那這樣子之後呢你就可以那這個精神這樣做的話呢你這個精神就跟我們那邊講的viterbi 是完全一樣的
所以呢你可以假設一開始的那一點這點很容易開始
然後呢你每一次要往上走的話呢你就是看倒底哪一個哪從哪一點走過來是這變成一個iteration 嘛你可以看出來這就是一個iteration
那這個式子跟我們viterbi 的那個iteration 是完全一樣的
那你每一次你只要看說這個前面走到這邊了我下一步走到這邊的話應該從哪裡過來分數是最低的
喔跟那邊的情形是完全一樣所不同的是我們剛才講viterbi 的話我是進步到一排一排一排變成一排一排走下來
它這邊沒有它這個你往上面走都可以
那這個的精神你基本上可以看成是就是所謂的dynamic programming
那你如果清楚的話知道它是什麼那麼如果不清楚的話呢我底下幾章在這裡有有一個在講什麼是dynamic programming
它的基本精神就是說replace the problem by a smaller sub problem 然後formulate 一個iterative procedure
你看它就是在做這件事我們講的viterbi 也是這樣子
就是你本身你要viterbi 是想要找一條optimal path 這個很難找啊你怎麼找這個optimal path
那我其實是把它reduce 成為一個簡單的多的一個sub problem 是smaller sub problem
在這個problem 裡面我只是要算這個iteration
假設我已經有optimal path 到這裡了
假設我已經有optimum path 到這裡了那我現在看的是
下一個在這裡的話應該從哪來
我只要做好這一點就行了這是一個smaller sub problem
我有了這個smaller sub problem 之後呢我就變成一個iteration 就把就可以把它做出來
那這個意思就等於是dynamic programming 的基本精神就是這樣
那我們的viterbi 其實就是在做這麼一件事
那同理這邊在做的這個也是這件事
那我的sub problem 也是一樣
如果我已經有了這些個點的minimum distance path 到這裡的話
那下一步怎麼辦我就看誰走的最近我只要做這個這一步就好了那這一步就是所謂的smaller sub problem
好那如果是這樣來看的話呢那其實大概這就是d t w 的基本精神
雖然詳細做不一定要這樣子做啦這只是這裡面這裡寫的只是諸多的方法裡面的一種而已喔
那這裡面呢其實還有很多別的譬如說呢有所謂的local constraint 跟global constraint
什麼叫local constraint 呢就是我們剛才說ok 你可以這樣子走可以這樣子走但是你可能規定說我我一次最多不能走多少步
這邊最多不能走多少步然後遠到多少程度就不能走了對不對我一次只能夠跳多這一類的就是所謂的local constraint 你可以做下各種規定
那global constraint 呢是說我們通常會規定說這邊有一個限制吧在這個範圍之外不能走
你就是說你如果走到這裡的話是有點問題走到這邊的話好像是說這整個的聲音聲音都歸給它一點點
你走到這邊來就它這麼一點點要要涵蓋所有這不太通所以你應該是最底下是不能走
同理最這邊也不能走好你如果在這邊也不通的
所以呢你會有一個global constraint 說我整個的我大概在哪個範圍之內才可以走
好這是所謂的global constraint 等等
那這些就構成所謂的d t w
我們說這個是沒有h m m 之前的古代最成功的方法
那它有很多當然它有它的缺點
一個缺點就是缺好缺少一個training 的方法你這會怎麼train
那麼其實它們怎麼train 的呢它們就是說你今天如果有譬如說san francisco
你念個二十次的話有san francisco 有長有短怎麼辦你就用這個方法把它們都warp 到一個長度
對不對你如果有二十個二十個san francisco 的training data 你就拿其中的一個當reference
其它的通通都都跟它對應成相同長度用這個方法得到一條path
然後就把以那條path 為準把它們全部平均起來變成一個做為reference
他們就當當是這樣做的也因為這樣所以它本身沒有一個統計模型
它本身欠缺統計模型所以它比較這個這個pattern 比較train 不好
那當然還有一個大問題就是它不是它沒有辦法做這個continuous speech
可以做做不太好你怎麼把它連起來你怎麼把一堆pattern 連起來你還要能夠找這條path
那變成有點難做他們當時其實都做了那但是做的不如h m m 做得好就是了
ok 關於這個d w d t w 我們說到這裡
那麼它基本上你可以想像它是一個它是就是我們剛剛講的這個dynamic programming 的一個具體的實現的做法
