我們要進入五點零只不過五點零講的是另外一回事
就是我們把它當成是用來處理聲音
我們剛才四點零為止沒有當它是聲音
我們只當它是數學模型然後就是一堆 arbitrary 的 number 在那裡跑
沒有當它是聲音
但是其實它是聲音啊
所以呢我們必須要想一想在聲音的狀況之下它會怎樣
那就是五點零講的
那這種東西我們稱之為 acoustic  modeling 
那這部分牽涉的內容稍微廣一點從基本的聲學語音學一直到後面怎麼樣做
 model 還包括什麼什麼很多東西都在裡面
所以 cover 東西比較多
那麼比較好的 reference 是課本的這些章節
那我們其實後面最重要要講的是
它的最後這個最重要的方法
就是 training  TRI  PHONE  model 的這件事情
那這個這個也就是今天最主流的方法
那這個的原始 paper 來自這一篇
那這一篇其實是我們台大的一位校友的博士論文
非常了不起的所以後來變成這個經典作品大家都在用所以我們也引這篇有興趣可以讀一讀
那麼我們前面先簡單講一點我們
我想我可以稍微再講幾分鐘
我們禮拜六禮拜六補課是蠻辛苦的所以我們也不要太辛苦
不過
我的飛機快要飛了所以我們要
 OK 那麼一開始的時候我們先第一個問題是說我們可以做 HMM 但是倒底要用什麼來做
 HMM 可以短可以長可以大可以小
那麼我們我們這個說過
 HMM 的好處就是它可以小的可以兜成大的
你如果有一個基本音的 model 是這樣子
這個基本音的 model 是這樣子
那麼它們串起來可以變成一個比較大的
譬如說詞或者什麼東西是可以這樣做的
所以呢從這個觀點來講好像是越小的 model 越好因為它可以串成比較大的東西
但是你也不能小到太小
你要能夠好做等等的原因
那麼基本上來講呢
嗯我們可以最小是所謂的 phoneme 
 phoneme 就是最小的基本音這待會解釋
不然呢再來就是 syllable 
 syllable 就是我們通常講的音節
譬如說這個 minimum 
就是這是三個音節
那麼這個
 same 這是一個音節等等
那麼這個是 syllable 我們講是音節
然後 word 這就是一個 word 
對不對或者 phrase 是比 word 還要大的幾個 word 構成一個 phrase 都可以做
都可以做這個 HMM 那倒底什麼比較合適
那我們先要了解一件事情我們這邊在講 phoneme 這個是一個語音學上一個專有名詞
這個東西是什麼我們來解釋一下
這個 phoneme 是一個語言裡面的最小的單位最小的聲音的單位
它可以幫助我們區別 one  word  from  the  other 
就是說我們 word 跟 word 之間的區別是靠這些東西來區別的
那它的最小的單位叫做 phoneme 
舉例來講像一個以英文而言這個是 bet 這是 pet 
那差別就是這個 b 跟 p 
所以呢這個 b 或者 p 呢的音呢就是一個 phoneme 
同理呢這個是 bad 這個是 bed 那這兩個差別是什麼就是這個音
那也是一樣這個它就是 phoneme 
從這裡你就可以了解其實我們講的 phoneme 
這是一個語言學上非常講起來好像很好聽的話其實簡單的講就是每一個子音每一個母音都是一個 phoneme 
那跟這個 phoneme 很像的叫做 phone 
那在語音學上來講 phone 跟 phoneme 是有區別的
那麼這個 phoneme 是這個東西叫做 phoneme 是一個最小的單位
但是 phone 呢是 phoneme 的 acoustic  realization 
這是什麼意思呢是說一個相同的 phoneme 可以有很多不同的 realization 
舉例來講這個 set 跟這個 meter 這兩個 t 是同一個 phoneme 這個 phoneme 就是 t 
是同一個 phoneme 如果講這個的話講這個的話是同一個
可是它們是兩個不同的 realization 
你也可以想像這兩個聲音本來就不一樣 set 的這個 t 跟 meter 這個 t 
是顯然不一樣的是兩個不同的 realization 
這是語音學上它們的解釋這兩個是區別的
但是從九零年代以後因為一大堆 engineer 在做這些東西
那 engineer 始終讀不太清楚語音學然後他們最後就把這兩個混為一談
因此呢在後來今天在看的很多文獻跟書裡面
其實 phone 跟 phoneme 已經混為一談
那麼因此呢有的課本乾脆就說在我這本書裡面這個 phone 跟 phoneme 是一樣的隨便講哪個都可以
那這一點就是 engineer 跟語音學家始終不一致的地方
那語音學家始終認為你們不懂的人在胡搞一通
但是 engineer 覺得我們做出來可以 work 就好所以這是不一樣的地方
那在這個情形之下很重要的我們需要知道的就是 context 
什麼是 context 呢就是相鄰的 unit 在左邊右邊
譬如說如果這是一個 phoneme 的話這是它前面的這它左邊的 context 
這個呢是它左邊的 context 這邊是它右邊的 context 這叫做 context 
因此呢那麼很重要的一件事就是我們必需了解在聲音裡面
存在一個現象叫做 CO ARTICULATION 
什麼叫 CO ARTICULATION 呢就是
我的聲音是受到左右的 context 的影響就它的 neighboring  units 
左邊右邊 unit 影響我的聲音就變了
這是很重要一點就雖然這是同樣一個音
這同樣一個音這邊是接這個後面是前面是這兩個因為左邊不一樣它就會不一樣
那這兩個是同樣的音這左邊不一樣它就不一樣這右邊不一樣它也不一樣
那為什麼會這樣那其實說穿了很簡單因為我們人發音的任何一個音的區別主要是靠口型跟脣齒舌之間的關係
你這樣關係你的啊ㄜㄧㄨㄩㄕㄙ都是因為你的口型跟脣齒的關係所造成的
那你發這個音需要這個口型發那個音需要那個口型的時候你不可能在一瞬間變成另外一個
你一定是 continuously 變過去的
因此你要從這個音變成那個音的時候互相都會影響所以前面會影響後面後面會影響前面
因此呢你可以想像譬如說我們講八跟逼這兩個波是一樣的嗎顯然不一樣
因為這個你也知道這個八當我發這個波的時候其實後面已經是要發八了所以那個八跟這個逼顯然是不一樣的
就好像 tea 跟 target 
這個顯然不一樣
 tea 我後面是接了 e 了這個 tar 我後面是接 ar 　所以這兩個本來就不一樣
那這個呢就是 right  context  dependency 
我受到右邊的影響這個也是這個也是右邊的影響
這就是所謂的 right  context  dependent 
就是說我的右邊一定受同理左邊也是
譬如說 it 跟 at 這兩個 t 有一樣嗎會不一樣
因為這個 t 是從這個過來的這個 t 是從這個過來的這兩個口型不一樣所以會影響到這個 t 不一樣那這個呢就是
這個 left  context  dependent 它受左邊的影響這個是受右邊的影響
 OK 這個是受右邊的影響這叫做 right  context  dependent 
這個受左邊的影響這叫做 left  context  dependent 
所以呢我就會有這個 right  context  dependent 跟 left  context  dependent 的區別
當然我們真正的狀況應該是 both 
真正我們發的每個音是受到左右兩邊的影響
你可以想這裡面的每一個音其實它都是受左右兩邊的影響
那只是說呢你如果左右兩邊都考慮進去太複雜了
有的時候難度比較高所以我要怎麼樣子做到比較有效
這個是這個 context  dependency 
那麼也就是我們這邊講的 CO ARTICULATION 
就是我們發聲會受到它的 neighboring  unit 的影響
會受到左邊跟右邊的 context 的影響
那還有呢我們也可以說它是這個 intra  word 跟 inter  word 這兩種
怎麼講呢就是所謂的這個 intra  word 就是假設我的影響僅限於一個 word 裡面
假設這個 word 裡面它的左右都會受影響
但是呢我的影響不跨過一個 word 的 boundary 這叫做 intra  word 
就是說在一個 word 裡面它們前後互相影響
可是我的影響不會跨假設啦都是 engineer 在做這些問題的時候
做一些假設因為你你不做那些假設你有時候很難做
你如果假設它們影響不會跨過一個 word  boundary 
所以這個音不受它的影響
這個音不受它的影響這個呢叫做 intra  word  context  dependency 
那如果變成 inter  word 的話呢就變成讓它可以跨越 word  boundary 
於是呢它可以影響它它也可以影響它的話呢那就是 inter  word  context  dependency 
所以呢如果講真正從語言學家來講的話它這個所有的都是存在的所以應該是 inter  word 
應該是 both  right 跟 left  context  independent 
但是從 engineer 來講呢 engineer 就會希望說我們如果能夠做一些 assumption 
讓這問題簡化我可以得到比較好的答案的話呢
看它 work 不 work 嘛如果 work 就好了這個是這個這些東西的來源
那麼就我們講的國語而言
那麼那麼你知道我們的每一個字就是一個單一的 syllable 
就是中文的特性就是每一個字都是單一的 syllable 
所以你察注音符號檢字表裡面的巴拔把爸逋不補布一直到淤於雨育每一個都是一個 syllable 
那每一個單一的 syllable 對應到的是
一個或者一個以上的字
這個是字那麼通常都會以一個以上的通音字
那麼因此我們有這個 many  to  one 的一種這個 mapping  relation 
但是反過來也是 one  to  many 因為有這個還有破音字嘛等等
所以有同音字還有破音字這是我們的特有的一個現象
那對每一個 syllable 而言呢
傳統的聲韻學
它們分做聲母跟韻母
那麼在早年的語音學家它們把它翻成英文就叫做 initial 跟 final 
所以呢我們後面也會以這個為例
我們這門課絕大部分的內容我們都以 language  independent 的東西來解釋所以講的都是以英文為準
那麼這些課本啊 reference  paper 大部分都是以英文為主所以大部分都是用英文
但是碰到有中文的地方我們會說一下中文的部分
像這個 case 的話呢我的這個我們就會說到後面會說到一下聲母跟韻母
什麼是聲母跟韻母呢其實就是我們的每一個音都會分成兩個
譬如說八這個是聲母這個是韻母
逼這個是聲母這個是韻母
那麼如果說是天的話呢那這個是聲母這個是韻母
那麼如果是六的話呢那這個是聲母這個是韻母
所以基本上聲母是前面那個字音韻母是後面這些東西
那這個但是呢這個
這是用聲母韻母來分這是聲韻學上的分法
那麼如果用它們西方語言學的 phoneme 來分的話就不一定是這樣子了
譬如說這個天呢你可以想像是ㄊ一ㄢ它是四個 phoneme 
ㄊ一ㄚㄢ天那麼六的話是ㄌ一ㄜㄨ六
那麼你可以看成是四個 phoneme 兜起來的
所以呢這是如果用西方語言的 phoneme 來分的話可以看成是這樣
那麼其實這三個兜成一個韻母這三個兜成一個韻母等等諸如此類
這樣你大概了解一下我們講的東西
那我們比西法語言多一個就是聲調
那你知道就是我們這就是所謂的四聲加輕聲
那麼我們有聲調不是說西方語言沒有
只是說我們用聲調來辨異
西方語言不辨異
辨異
所謂辨異就是說你不同的聲調代表不同的字代表不同的詞有不同的意義
那麼西方語言也有聲調只是它不拿來辨異
它可以說 how  are  you  today  how  are  you  today  how  are  you  today 都可以
那意思其實是差不多的只差一點點不能說完全一樣但是只差一點點
但是我們如果把聲調一變的話就是不同的
那這是我們一些不同的地方就是了
好那我想也許我們今天就停在這裡好不好
好這個週末補課大家辛苦
喂喂喂
喂喂
有沒有聲音有齁
喂ok 好
我們上次我們補課在講從講五點零開始阿
這個週末補課大家都辛苦
我是今天早上八點降落在桃園機場
經過二十六小時的飛行才降落的
ok 我們上週在講說這個喔四點零講的h m m 的時候我們只當它是一堆數學模型
沒有當它是聲音
那我們在五點零是把他看成是聲音的話
那麼看看有何不同的情形
那麼這個時候那麼我們要考慮的因素就包括
到底要用什麼樣子的單位來做h m m 
當然可以是phrases
可以是words
可以是syllables
可以是phoneme
那麼所謂的phoneme 是我們所講的最小的一個聲音的單位
也就是我們所說的基本單位的音
那麼最簡單解釋就是每一個母音每一個子音都是phoneme
那麼我們常常把phoneme 跟phone 混為一談
那麼很多人他們其實都很偷懶就是phoneme 太長了我就唸前面
就是phone 嘛
那所以他講的phone 其實就是phoneme
那麼雖然語言學家說phone 跟phoneme 有點不一樣等等阿
那麼這裡面影響最大的一個問題應該是所謂的co articulation 
也就是說每一個phone 的發音其實是context dependent 
也就是說它跟它的左右的neighboring units 或者說跟它的前後音都有關係
那麼這就是所謂的co articulation
也就是我們發的聲音會跟前後音不同而不同
那麼我們說過呢因為你發的每一個音都是因為某一種口形
你的ㄚㄨㄧㄕㄙㄕ都是你的某一種口形所造成的
那你那個口形不可能在一瞬間變成另外一個口形
所以它前面要發什麼音後面要發什麼音這個口音一定是變過來的
因此你的聲音自然就受前後音的影響
這就是所謂的co articulation
因此我們的聲音顯然是受到左邊的影響
受到右邊的影響
那就是所謂的left 跟right context dependency 
那麼就我們國語而言
我們的每一個音我們的每一個字都是一個syllable 
那一個syllable 都可以分成聲母和韻母
還有就是第幾聲的聲調阿等等
好我們底下要講的就是那麼到底要用什麼來做
做這個h m m 的unit 呢
基本它有幾個最重要的考慮
這三點
第一個要accuracy
也就是你如果要用那個聲音那個單位來拼聲音的話
顯然它必須要足夠精確來描述真正的acoustic realization 
好也就是說你如果用每一個音這個這是一個這是一個譬如說一個p 是一個h m m 
r 是一個h m m 
i 是一個h m m 如果這樣拼的話他們拼起來真的能夠變成一個primary 嗎
那這個字是不是能夠精確的用那些音拼的出來
第一個問題就是accurate 就是accuracy 
這些小音是不是真的能夠精確的拼出那個音來
第二個問題是train ability 
也就是指裡面有沒有辦法有夠多的data 去train 那些model 
換句話說你如果說是這個你必須要考慮到每一個音都能每一個model 都要有夠多的data 
它都是統計的model 
如果沒有夠多的data 的話你沒有辦法真的train 那些聲音喔
所以要有夠多的data 
那第三個也就是所謂的generalized ability 
也就是你的一個新詞必須要有夠多的任何一個new word 都要能夠用已經有的unit inventory 拼出來
我們不能因為在我們語言是活的
語言是活的所以永遠有新詞
永遠有新new word 
你不能因為有個new word 你必須要去train 一個new 的model 
那麼你不能為了一個必須是要用已有的unit inventory 去拼才可以
這是所謂new word 的問題
那麼new word 在不同的語言它的新詞出現的比例高低是有不同的
那麼就中文而言我們中文是新詞出現特別多的一種
原因是我們的每一個音都代表很多字
每一個字都有意思
這些意思很容易拼成任何的新詞
所以我們很容易創造新詞
我們舉個最簡單的例子
四不一沒有這就是一個新詞阿
這個廢統還是終統這都是新詞
所以我們隨時都可以創造很多新詞
反分裂法
這個都是新詞
那麼中文算是因為我們每個字都是有意思所以很容易湊成新詞
同樣的鼎泰豐也好是
這個什麼小福還好也好什麼
我們很容易創造很多新詞
在英文裡面新詞比起來是比較少的
但是也有很多阿
最多的就是專有名詞嘛
譬如說microsoft 那就是一個micro 跟一個soft 兩個字拼起來
當它創了那個公司的時候它就有那個新詞
intel intel 是比較不同它不是一個in 跟一個tel
它是根本就是一堆字母臨時就是拼出來的阿等等
所以那他們也有一堆新詞
不過他們的新詞顯然比我們少
中文是新詞最多的阿
那麼因此你不能因為有個四不一沒有你趕快就要就要當成一個新詞
然後去找一堆人去唸四不一沒有然後才能夠辨識這個詞呀
你必須要是已有的單位能夠拼出來的才可以
這就是所謂的generalized ability
當你同時考慮這三種因素的時候
那麼我們可以舉例來說明他們哪些會怎樣
那麼例如說呢這個words 
就英文而言呀words 
就中文而言呀就是詞
到底好不好呢
它最大的好處是絕對accurate 
舉例來講你如果是primary我就真的用primary 來做成一個model 的話
你就用夠多的聲音primary primary primary 去train 它
它當然會變成很精確的描述這個primary 
這是這是這絕對是accurate 阿
只要有夠多的data 
所以呢用word 來做單位的話絕對是夠accurate
但是呢它trainable 可能只限於small vocabulary
你如果是辨識兩百個詞兩百個words 三百個words 還可以
你如果要辨識比較英文常用的words 的數目大概至少是三萬以上吧
當你要三萬個words 你都要每一個word 你都要用夠多的data train 的出來就很累了
所以呢它不見得是trainable 
你除非是small vocabulary 的應用
譬如說你的這個辨識人名阿
你的那個這個name dialing 
手機上可以說我說amy 它就幫我接到amy 
這種這種是這只是少數
是small vocabulary 是trainable 的
可是如果vocabulary 變大的話就會變的很難
那麼最大的問題是它不generalized 
出來一任何一個new word 沒有辦法做了
除非那個new word 再做一個新的model 
重新唸一堆聲音給它它才能train 的出來
所以這是word 的問題
那麼最急最想到最小單位就是phone 嘛
這裡講的phone其實就是我們之前講的phoneme
好我們沒有把phone 跟phoneme 區別開來
就這邊講的phoneme 就這些子音跟母音
用這些來做好不好
當然好
第一個它trainable
為什麼trainable 
因為它的總數有限嘛
你任何一個語言的phone 的數目phoneme 的數目大概是不到一百個
幾十個吧
所以數目不多
所以是trainable 
然後generalized理論上我都可以拼成所有的字嘛
所有的word 都是用它拼出來的嘛
所以generalized 嘛
可是有個最大的問題就是不容易變成accurate
因為什麼因為context dependency
換句話說你如果每就是我們剛才舉的例子你說這個每一個p 有一個h m m 
r 有一個h m m 
這個e 有一個
阿每一個都有一個它們真的拼起來真的可以變成一個primary 嗎
這是一個很大的問題
那麼為什麼是一個很大的問題
是因為你每一個每我們說過就是剛才講的這個co articulation
也就是說每一個音都受到前後音的影響
發音就不一樣了
因此你不要以為那個e 在這裡
我只要有一個e 的model
這個e 在這裡它就可以變成e 
其實不然
因為這個e 只要前面接的是不同的音後面接著不同的音它的音都不一樣的
同樣這個m 
前面接個不同的音後面接個不同的音都不一樣的
所以呢它你如果有辦法把這幾十個phoneme 通通都train 出來
變成model 把它拼起來可以嗎這是鐵定不可以的
它不容易變成accurate
因為我們每個音都是context dependent
那麼因此呢phone 有它的困難
那syllable 如何呢
syllable 非常depends on 是在哪一種語言
因為不同的語言裡面的syllable 數目差很多
因此可行性就差很多
舉例來講
日文總共只有五十個syllable 
這個算是一個非常容易control 的一種單位
那麼各位學日文就知道
ka ki ku k e ko 
就是它就有五種母音
再加上十種子音
五十個syllable
這是在很容易control 範圍之內所以在日文而言
syllable 是一個非常好的一個單位
來做這些事情的
中文就比較麻煩了
你知道我們的國語有多少個syllable 呢
大約一千三百個這個
數目其實是不大
比起日文來是大很多了
那這一千三百個是指說你去查國語辭典的
注音符號檢字表
從八拔把罷撥柏跛播逋不補部一直數數到淤於與欲阿
大概是一千三百個而已
不算多
為什麼說不算多
你跟英文比的話
英文的syllable 是超過三萬個的阿
比我們多很多
英文為什麼它的syllable 會超過三萬呢
是因為英文有非常多的前後都可以帶的子音阿
我們隨便舉個例子就可以知道
譬如說prompt
這是一個syllable
一個syllable它只有一個母音在這裡
可它前面可以帶兩個不同的子音
後面可以帶三個子音阿
那譬如說script 
它只有一個一個母音在這
但是它前面可以帶三個子音
後面帶兩個子音
阿等等
那像這些東西都是一個一個不同不同的syllable
所以這也是一個syllable 這也是一個syllable那這樣syllable 就多了很多
那中文只有一千三百個為什麼這麼少
就是因為中文沒有那麼多複雜的子音
它的母音前面只有一個子音
後面常常沒有子音阿
我們中文譬如說ㄅㄨ
這是一個母音前面帶一個子音
後面沒有
基本上就是差不多是這樣子的
那麼後面有的情形很少很少
那麼那當然這個是指國語而言
如果是方言的話不同的語言又不一樣了
譬如說閩南語的話
閩南語的話這個數字超過兩千
超過兩千
那原因是它了很多
而且包括它後面可以帶一些子音阿
閩南語的是有一些syllable 後面可以多帶一些子音
那麼國語比較沒有等等
所以這都不一樣
所以呢這個syllable 好不好作單位呢
這也是depends on 也是作哪一種語言等等
那turns on 呢
最理想的最理想的應該是什麼呢
那麼最理想的似乎應該是所謂的tri phone
tri phone這個名詞是在九零年代末期
人家想出來的
那麼雖然後來很多人說這個名詞取的不好
不過人家已經都叫它叫tri phone 了
tri phone 聽起來好像是三個phone 連在一起叫做tri phone 
其實不然它是指一個phone 
阿tri phone 是指一個phone 
但是呢它前後只要接的音不一樣就算不一樣
譬如說這個是一個這是一個phone 
就是這個這個喔的這個音
它前面帶的是r 後面帶的是m 
所以這是一個tri phone 
是指這個音前面帶著前面是跟著r 的後面是跟著m 的
那其實不是三個連在一起
而是前面後面都不算阿
它只是算中間這個
但是因為前面受到它影響後面受到它影響
所以呢這個它是單獨一個
你換另外一個的話是不一樣的
你如果前面是接這個
後面是接這個
那當然是另外一個阿
你只要前面不一樣後面不一樣都算不一樣
但是它講的其實是講中間這一個
ok
那這個叫做tri phone
那麼我剛才講後來很多人認為這個名字取的不好
是因為這個一看好像是三個連在一起
其實它是只有一個
它並不是這三個連起來叫做tri phone 
它tri phone 是指中間這一個阿
是指中間那一個但是前後不同就算不同
那麼這麼一來的話呢
它的好處它是一個最小的單位
它是一個最小的單位
那麼喔你就把左邊右邊不同都算不同
但是呢壞處是數目很大
假設某一個語言
現在多數的語言大概它的phone 的總數大概是這樣子的order 嘛
六十的話
六十的三次方就是這麼多個
這麼多個之後數目非常大
更大的一個問題是說很多時候你找不到夠多的data 來train這個
很多時候你找不到夠多的
譬如說你一定要這個phone 是前面是這個的後面是這個的
這樣的phone 可能就不多了阿
那很多的phone 可能你根本找不到哇
阿這是最大的問題
所以呢tri phone 的好處是very good generalized ability
因為你如果真的有一個tri phone 的一個set 的話
你可以拼成任何的一個文字嘛
對不對任何的一個word 都可以拼
所以你如果真的有tri phone 的話你是可以拼成所有的
所以是最好的generalized ability
但是呢另外兩個條件都不太好
trainable是有問題的因為它train 不好
因為你數目太大了然後很多很多的tri phone 根本找不到
所以呢
你就不容易train 
然後呢
也因為不容易train 的結果其實就就不容易accurate
因為你如果所有的音都能夠train 的很好的話顯然是可以accurate 
但是因為你train 不好
所以就沒有辦法accurate
那麼怎麼辦呢
那麼他們通常的辦法就是作所謂的parameter sharing 
什麼叫parameter sharing 
就是指說
有些tri phone 實在沒辦法我們就讓一些個tri phone 雖然不太一樣我們把它拼成一個
當成一個算了
阿那我們底下的一頁的兩個例子就是這一纇的叫做所謂的parameter sharing
那麼第一個例子就是
像這樣
假設這是一個tri phone
這另設是另外一個tri phone 
他們都是相同的東西
但是呢就是最後面接的不一樣
就譬如說他們我們說這兩個好了
它兩個都是這個
所以呢母音本身是一樣的
前面那個子音也是一樣的
只是最後這個是接這個這個是接這個
所以因此他們是兩個不同的tri phone
他們雖然是兩個不同的tri phone
所以呢你看到它它畫三個state 的話呢
最後一個state distribution 不太一樣
就是因為最後一個state 是因為後面接不同的東西的關係所以最後state 會不一樣
前面兩個state 是很像
但是因為我找不到夠多的data 去train 這兩個不同的tri phone
那我就乾脆把他們merge 成為一個
把它合成一個算了
那麼這一個呢它最後就它把它們混在一起
就把相同的tri phone這個寫錯了喔這是p h o n e 阿
把相同的tri phone 
類似tri phone 把它當成一個
然後呢就用共同的data 來train
譬如說這個總共只唸到三次
這只唸到兩次
那至少有五次總比三次兩次好嘛
阿那我就把他們兜在一起
當成一個model
恩
那這是一種辦法
這就是所謂的generalize tri phone 
他們稱為generalize tri phone 就是指說有一堆tri phone
我實在沒有辦法各自train 
我就當它們很像
合成一個算了
那麼這樣的話呢我可以至少至少這樣的話呢這兩個可以獲得
但他們就不夠accurate
那另外一個情形是像這邊的這種
那我有另外一個辦法就是
這個至少前面這兩個state 很像
我就把它們當成是同一個
只有最後一個state 
那不像就拆開來
那這樣畫的意思其實是就是這樣的意思嘛
知道就是
那也是像這樣
是同一個phone
前面是接一樣
只有最後後面接的東西不一樣
使得它們最後一個state 不一樣
既然這樣的話如果這個音只有三次這個音只有兩次
那我至少前面這個就我就用五次一起train
所以我這兩個state 呢至少是用相同的讓它們有
所以這兩個我讓它們有相同的distribution 至少train 起來比較好一點
那最後後一個就讓它們不一樣好了
阿這這是另外一種方法
這是所謂的shared distribution model
那麼前面這個情形我們說是sharing and model level 
就是這兩個model 一起share 
那這邊呢就是指state level 
就說在state level 讓它們一起share 
那也就是說如果它們這個state 確實不太一樣的話
就讓它們不merge 起來
讓它們拆開來阿
那這種都是所謂的sharing
所謂的這個這裡所謂的sharing 就是我們剛才所說的意思
我們剛才說呢
parameter sharing 
其實所謂的parameter sharing 就是它們有共同的
有共同的parameter
或者說有共同的mean 跟covariance
共同的gaussian parameter 
阿那所謂的共同parameter 其實也就是有共同的training data
如果這個有三次這個有兩次我我就用五次一起train 嘛所以有共同的training data 阿
那麼這樣的話呢
這個tri phone 的最最大的好處是generalized ability
你如果完整
真的能夠做到的話你可以做的最好
但是最大的問題就是不好train
然後呢不容易拼的好
因此呢我們就是就是要用這個parameter sharing 的方法
那麼這樣sharing 之後呢是在train ability 跟accuracy 中間作一個trade off
或者作一個balance 
這個意思是說
你如果是越讓它sharing 越多的話
這種sharing 越多
當然就越好train 哪
可是train 出來就越不精確嘛
這當然不精確嘛對不對
這兩這兩個怎麼會當成那一個呢
這當然不精確嘛
所以你這種這種狀況這種sharing 發生的的越多你一定越不精確嘛
所以accurate 一定比較差
但是這樣之後這個比較trainable 
對不對所以train ability 跟這個accurate 是互相矛盾的
同理那邊也是一樣
你的sharing 做的越多的話你越不accurate
但是你越是trainable 阿
那麼因此呢就是剛才講的
那麼因此呢就是剛才講的就是你這個train ability 跟這個accuracy 之間是一種balance 
或者是一種trade off
也就是兩者是無法得兼的
好那麼有了這樣的了解之後
底下我們這個五點零整個在講的都是在講這件事
就是如何來train 這個tri phone 阿
那麼tri phone 是今天所有的語音系統的主流
也就是最成功的model 
一律都是用tri phone train
但tri phone 都有共同的問題就是說事實上你是train 不好的
因為一定有一大堆tri phone 是看不到的
那是根本沒有data 或者data 太少
是train 不出來的
那麼因此呢你顯然是需要用一堆方法
來做到這個accuracy 跟train ability 之間的balance
這就底下我們要說的事
那麼在我們之前的這個reference 這邊講到
unseen tri phone 阿
所謂的unseen tri phone 就是你永遠有一大堆tri phone 你的data database 根本就沒有
你根本就看不到阿
這所謂的unseen tri phone 
那麼哦以這個以我們的經驗而而而立
譬如說這個哦我們要train 一一套tri phone 的model 
很可能會發現裡面有一半的tri phone 根本沒有data 
也就是說你蒐蒐蒐集了幾十小時的聲音
發現它只中間只呈現了一半的tri phone 
另外一半它們根本就沒有出現
那些根本沒有出現的tri phone 呢就是所謂的unseen tri phone 阿
這所謂的unseen tri phone 那怎麼辦
我們要有辦法來做它阿
那就是底下我們五點零整個都在講這件事
就是怎麼樣來做這樣子的tri phone
那麼為了要做這件事情起見我們就開始講底下一堆事情
因為我們後面真正要做的最成功的方法是就是用這個所謂的這個cart 也就是classification and regression trees
用這個來做
而這個的基本的原理是用所謂的information theory 裡面的entropy 的觀念阿
那麼因此呢我們這邊要從information theory 開始說起
那麼就在說這段
那麼information theory 本身是一個博大精深的學問
那麼在古典通訊原理裡面是一個非常重要的基礎
那麼你或許在別的課也學過等等
那麼也許沒有學過也許學過
不過我們這裡只用到裡面一個最基本的東西
所以我們利用這個機會在這邊簡單的說一下它的一個最基本的觀念
就是在古典的通訊原理裡面的information theory 
裡面所說的所謂的information 的major 
那麼那麼這個東西turns out 是非常有用的
那麼用在非常多的地方
那麼包括用在我們這邊的要train tri phone 也是用這個方法
所以我們簡單的來說一下它是
那麼假設有一個information 的source s 
那麼它送出一系列的symbol 
那麼每一個symbol 都是一個random variable
譬如說m one m two m 三到m j 等等
其中m j 就是指在時間j 或者說是第j 個的那一個symbol 
那基本上它們每一個symbol 都是一個random variable 
那個random variable 都有一定的可能的值
譬如說有大m 個可能的值
是x one 到x m 
ok 那麼因此呢
我們說這個是這個information source 
它所送出來的的所謂的information 就是一系列的symbol 
那麼在在時間j 的時候的第j 個symbol 就是m sub j 
那麼它呢是一個random variable
它可以有大m 種可能的值
就是x one 到x m 
那麼這個random variable 呢它基本上是有一定的機率的
所以呢第x i 個第i 個呢有一個機率就是p 的x i 
那麼那當然囉那就就表示說這個阿第j 個m 的它turns out 它的值是第i 個x 的機率
就是p 的x i
而這些所有的p x i 的加起來
i 從一加到m 的話應該是正一
每一個是機率所以都是大於零的值
加起來是一
這樣樣講有點抽象
那麼我們用最具體的例子來講
那麼也許就是零跟一吧那假設說
我這個送出來就是一個一系列的零跟一的話
那其實這裡的每一個就是
第一個bit 就是m one 
第二個bit 就是是m two
第三個bit 就是m 三
那每一個m j 其實就是一個bit 
那那個bit 本身是random variable
它可以是一可以是零
所以呢它基本上呢它的x one x two 呢就是兩個
就是一跟零
然後它們的機率呢各是二分之一
那也就是我們這邊講的p 的x one 等於p 的x two 等於二分之一
它們加起來要等於一等等
那這樣子也許是最具體的一個簡單的例子
那這邊講的當然是比較一個general 的說法
這個比較general 而這個呢比較specific 
當然這個specific 例子呢我們也不是一定要這樣
我也可以把它複雜一點譬如說我兩個兩個當成一個symbol 
我如果兩兩當成一個symbol 來看的話
我的我就變成是有四個積x one x two x 三x 四
我我每兩個bits 是一個symbol 的話
我有四種symbol 
分別是譬如說零零零一一零一一
那如果是這樣的話呢
我的機率各是四分之一
各是四分之一
那這就是我的四個
這樣子也是可以
那你如果這樣看的話呢它仍然是在這樣子的model 之下
那就是我的每一個symbol 是有四種可能
是有四種可能它們機率各是四分之一等等阿
這就就是我們這邊舉的這個這個情形
那在這個情形之下呢
那麼在information theory 裡面
它希望為你所看到的每一個symbol
不管是一個還是不管是這邊的一個bit 還是這邊的兩個bit 的一個symbol 
你為每看到一個symbol 定義一個它到底它給我多少information
這個在這個在這個information theory 裡面他們稱之為information 的major 
它到底給我多少的量information
的major
那麼也就是指quantity of information 
到底這個event 到你看到一個m j 等於x i 的時候
當你看到這個bit 等於一的時候
或者當你看到這個bit 是零這個這個symbol 是是零一的時候
也就是說當你看到m j 等於x i 的時候
到底獲得多少的information 
那麼在information theory 裡面它仔細的分析說這個information 怎麼定義呢
它說它應該要有這些個我們所希望有的probability
第一個就是當你看到一個symbol 出來的時候
當你看到一個information 的時候
它是譬如說m m two 是等於一的時候
這個bit 等於一的時候
或這個bit 等於零的時候
你得到information 絕對是正的
所以你的information 絕對應該是正的
它就把這個叫information 量嘛
i of x i
就是指你看到一個event 
它是x i 的時候
某一個m j 等於x i 的時候
那麼你得到的information 量它那個量應該是正的
那第二個呢如果那個的機率趨近於一的話
你得到information 應該是零
這話什麼意思呢
這個如果那個information 如果那個x i 的機率是一的話
你得到的information 的量應該是零
我發現我現在走路
ok 好謝謝
如果我不能走動的話那一半的黑板就不能用了
ok 那麼喔什這話是什麼意思
簡單的解釋就是說你如果你的那個那個bit 那個symbol 出那個出來的全部都是一好了
如果它永遠是一的話
這個時候我的一的機率就是一
這個時候它還帶有information 嗎
應該沒有帶information
因為我都可以猜下個一定下個一定是一嘛
我都可以猜得到一定是一嘛對不對
我永遠猜得出來它是一嘛
所以看到一的時候有沒有看到information 沒有看到嘛
那麼因此呢它就有這個有這個definition
也就是說我的要求就是應該是如果這個x i 的如果這個x i 的機率是一的話
它就應該沒有帶information 
那第三個條件是說呢如果是機率越小的
帶的information 越多
機率越多的帶的information 越少
所以呢如果x i 的機率小於x j 的話
x i 的information 就大於x j 
什麼意思呢我們我們說都是一未免是太太誇張一點
那我們說是這樣子
它是一很多零很少
那在這個情形之下呢就是一的機率比零的機率大很多
如果這樣的話
你想我看到一個一所看到的information 
比看到一個零所看到的information
一樣多不一樣多
顯然不一樣多
因為我這邊幾乎可以猜下一個是一八成都會猜對
所以呢再下一個我再猜是一八成還是會對
所以呢這個給我一個一的話這個information 是量是很少的
反過來呢我今天如果看到一個零的話
這個是給我非常豐富的information 
為什麼因為我絕對不管誰不敢隨便猜它是零
要要猜零會八成會錯嘛
我要猜零而會對的機率是很低的嘛
所以今天告訴我那個是零的話這是給我非常豐富的information
所以這所以在這個case 如果一的機率大於零的機率的話
那麼我一所帶的information 呢
應該是比零所帶的information 要少很多ok 
那麼也就是說我如果看到一個零的話
應該是看到非常多的information 
看到一個一的話
大概是沒有看到太多information 因為我都猜的出來
看到零的話我是不敢隨便說它是零的
所以呢它顯然是給我比較多的information 的
那麼因此呢這就這個這是一個decreasing function of x of 這個probability 
當我機率小的時候我的information 量是大的
機率大的時候information 量是少的
第四個條件是說它們是additive 的
這個比較難解釋什麼叫做information 量是additive 的
你只能講就是說哦當我不斷的增加獲得新的information 的時候
我的information 量應該是不斷地在增加嘛我不斷看到新的東西出來
我的information 量應應該是不斷地加起來的它是additive 的阿
就是我就一路我一路看到新的東西我就是得到information 量應該是一直可以加上去的
那麼用我們這邊的例子來講的話呢
簡單的解釋應該應該可以說是你如果看成是一跟零零跟一各是單獨的一個event 的時候
那麼你看到它有多少information 看到它有多少information 的話
跟你現在把一一當成是一個symbol 來看的information 應該是一樣多的
你在這邊可以兩個看成一個你
如果兩個看成一個你所那一個能夠給我多少information 
跟每一個看成一個它給我的information 應該是就是兩個加起來嘛
ok 所以呢我如果看到這個零跟看到這個一
得到多少information 
我把它們當成是individual event 來算的話
看到看到一個零得到多少information 
看到一個一得到多少information
跟我把零一當成一個event 
看到它多少information照說應該是一樣的
所以呢這個給我的information
這個給我的information 
跟這個給我的information
照說應該是一樣的阿
那這是所謂的additive 的意思
那麼當有了這四個條件的時候
那麼當初發明information theory 的人
很聰明他就想了一個方法他說很簡單就是log 
你只要取這個機率的log 的倒數
就符合這四個條件
那麼那麼機率的這個這個這個分之一的log 
其實就是負的這個log 
那這個呢就符合這四個條件
這也就是information 量的quantity information 
或者是information major 
的算法就是取log
那這是什麼呢
這個我們簡單的畫一個圖就知道
你知道log 是這樣的
log 是這樣的
但是呢我現在是我現在橫軸是log 是什麼log 它它的log 是機率呀
那機率的log 是最大到一為止嘛
所以他沒有上面沒有沒有這些呀
它只有這一段
只有這個這邊沒有啦
這沒有只有這個啦
然後呢你現在是負的嘛
現在是負的嘛
所以倒過來你就得到一個這個圖
就是這樣子的
這個是一點零
橫軸是那個機率p of x i
縱軸就是它的information 的量
那這條曲線就是這個log 的這條曲線
變成負的
就變成這樣子
那這個的意思其實很簡單就是符合剛才講的這四個條件
第一個呢它永遠是正的
它是一個function of probability 所以呢它是在零跟一之間
在零跟一之間
在零跟一之間它永遠是正的
那麼當你如果是趨近於一的機率趨近於一的時候它就是零嘛
當你機率趨近於一的時候它就變成零
那也就是說當我如果是永遠都是一的話那個一沒有給我information
所以它就是零
那麼同時它是一個decreasing function
機率越大的information 量越少
機率越小的information 量越大
所以呢這是這這是一個一個這樣倒過來的這就是剛符合剛才的關係
那最後一個所謂的它要additive 的這一點呢
其實我們如果從這樣來看就很簡單
因為你的機率是相加的相乘的
當你譬如說你要看到你要看到到這兩個零一的話
那麼就是它的看到它的機率乘上它的機率
如果你看成是independent 阿
我們姑且假設independent 比較容易解釋
independent 的時候呢看到這個的機率跟看到這個機率相乘才是看到這個的機率
所以這個機率是是二分之之一這個機率是二分之一的話你看到這個機率是四分之一嘛
那那因此呢你如果取log 的話
就是兩個相加嘛
所以呢當你把它當成這一個的時候
你看到它的機率就變成四分之一嘛
那麼因此呢這個取log 之後就是這兩個的log 相加嘛
ok 所以呢你如果這樣來想的話這個這個additive 這個也可以解釋啦
就是如果是independent 的話
如果這些m m j 彼此都是independent 的話那麼它們的發生的機率
就應該都是相乘的
取log 就是相加的所以就是additive 的
那麼這麼一來呢我們就有這個information 量的這個定義出來了
有了這個定義之後我們就可以再繼下一步定義這個東西
就是平均到底每一次會出現多少
什麼意思呢
就以這邊為例
假設說我這個一出現機率很高在這裡
p of 一等於零點九
就在這裡
因此呢它的量很小很小
那麼零的機率呢只有零點一
它的機率很大很大
它的它的這個帶的這個information 量很大很大
那我真正到底門我每次看到一個bit
那個bit 就是m
m j 啦我們這邊講的m j 
我每次看到一個bit 到底它給我多少information 呢
那應該是它可能是零點九
是一可能是零點九
但是呢它的機率只有喔不對喔它可可能可能是這個值
就是i 的這個一
i 的一是是這個值
但是呢它這個值很小
但是它的它的機率是零點九
然後呢這個零的帶的information 量很大
但它的機率呢是只有零點一
那這樣子平均起來才是我的那就是這裡的這個東西
那這就是這個的意思
也就是說我現在有有零有一嘛
那麼零出現的機率很小
所以我帶的information 量很大
這個量很大
可是因為出現機率很小阿
我十次才出現一次呀
那麼一的時候出這機率很高
但他帶的information 很少
這個很小可是呢我十次裡面有九次是它呀
所以那你平均你每看到一個bit
在這個例子而言你每看到一個bit 到底看到多少呢就是平均嘛
平均一下的結果
那麼這就是我的average quantity of information 
就是average in average 你每你每看到一個m j
到底看到多少information 呢
那麼你看到的應該是它們的平均
所以就把它們平均一下
所以呢當你看到的是p 的是x i 的時候
它的這個information 量是這樣
但是它的機率是那樣
你要把機率平均一下阿
所以就是這個i of x 的這個平均值
那這就是你所要的你所看到的看到一個m j 的時候它的information 量
這個就是這個i 的這個也就是這個h of s 的定義
它有一個名字叫做entropy 
那麼為什麼叫這個entropy 的名字這個我們底下會再進一步的解釋
不過我們現在先這樣說
這是 entropy 的定義就是平均你每次看到一個bit 或者一一個symbol 或者一個m j 
那麼平均你看到多少的information 的量
或者說就是quantity of information carry by 這個event of 一個random variable 
這邊等於一個都等於是一每一個都等於是一個random variable
因為它有一堆random 的值
它有一堆值然後都有機率
所以每一個都是一個random variable
那麼到底你看到一個random variable 是多少的時候
到底得到多少information 
那麼平均可以得到就是所謂的entropy
那麼這這樣講有點抽象我們底下舉一些例子來看喔
對了還有漏掉的我們要講一下
這邊講到說取log
取log 當然很重要的是你到底log 的基底是取什麼對不對
你這個是log 是什麼呢
那麼在課本裡面它會說你depends on 你取什麼
最常用的是基底是取二
如果是log 是二的話
那這個單位就叫做bits 
那麼這個有它的道理的我們底下就會解釋為什麼是這樣
但是呢這個是最常用的一個一個基底
當你不一定要要用二你用其他也可以
譬如說那個用自然對數的e 也可以
你用log 十也可以等等
但是最常用的是二
當你用二的時候呢
算出來那個數字的單位叫做bits 
那麼這裡有點有點confuse 
因為我們平常也常常用bit 來講另外一件事情
我們常常講的bit 就是講這一個這一個零或者一叫做一個bit 
那麼這個bit 是binary digit 的簡稱
binary digit 叫做bit
那麼這是一個零或者一叫做一個bit 
但是這裡不是
這裡的bit 是information 的量
是取log 出來的值
它可以是任何的real number
這裡一定是整數
你如果講這一個bit 就是指一個bit 沒有半個bit 嘛阿
所以呢bit 一定是一個整數嘛
可是這裡不是這裡bit 是一個real number 阿
是一個real number
那麼為了區別起見我這邊凡是後面寫個括號of information 
就是指這個bit 是information 的量
而這個我如果沒有寫of information 的話呢
就是指它是我們所熟悉的bit 的意思
那為什麼會這樣子搞得一個bit 有兩個意思呢
這個其實都都是有原因的這個我們底下就會解釋
好那麼底下我們就舉一些例子來看這些東西這樣會比較清楚一點
那麼最簡單的例子就是我們這邊所熟悉的零跟一
如果說我我送出來的如果說我的這個就是零跟一
而且它們機率各是二分之一
這是個最容易想像的case 
就是那麼零的機率是二分之一
一的機率也是二分之一
就是我們這邊講的這個case 
在這個情形之下你很容易算
你怎麼算它的information 你就是把這個機率各取log 嗎對不對
因為我們剛才講我就是把那個機率把那個機率取log 嘛
取log 就是我的這個information 的量
所以我就取log 你就會發現那個零給我的是一個bit 的information 
一也給我一個bit information 
平均起來它還是給我一個bit information 
所以呢也就是說呢
譬如說這一個這一個binary digit 
我們把它寫清楚這樣比較不會混淆
這一個binary digit 給我多少information
一個bit of information 
對不對
這一個binary digit 給我一個bit 的information 
同樣呢這一個零也是一樣的
這個零也是一個binary digit
也給我一個bit information
所以呢這些既然每一個bit 都是給我一個bit information 
所以呢我就是每一個bit 就是帶一個bit information
ok 所以呢平均起來我每一個symbol 
或者每一個bit
每一個symbol 或每一個給我的就是一個bit 的information 
那這裡你比較容易可以想像為什麼它要用information 量也要要這樣樣取然後也叫做bit 
因為它就是一個bit 
這一個bit 就給我一個bit information
那頭第二個例子呢是我如果有四種的話
那麼就變成我有四種
那麼我有這四種四種的時候呢我們說它各是四分之一的機率
m 一m 二m 三等等等等
那他呢可以有四種
也就是說有x one x two x 三x 四有四種
如果這四種的機率各是四分之一的話
那你可以想像這四種既然有四種不同東西
我可以其實它就可以分別代表零零零一一一一零
代表這四個兩個bit pattern 
那麼因此呢如果它們各是四分之一的話
我也很容易算它們每一個給我的information 量呢就是四分之一的log 嘛
我一樣的就用剛才的那個式子
去算四分就算那個機率的log 嘛
我取log 之後我得到的
就是兩個bit 嘛
所以呢我每一個symbol 
這個symbol 呢
給我的是什麼
它帶的是two bits of information
它這一個帶的就是兩個bit information
剛才根據我們來算就是算它是四分之一嘛
它的機率各是它它機率各是四分之一的話它就是帶了兩個bit information 
那麼那麼因此呢那我我現在每一個都是都是兩個每一個都是兩個bit 所以平均還是兩個
於是呢平均起來呢每一個都是帶了兩個bit information
那麼因為它帶了兩個bit information
所以呢你也可以很容易想像
欸我就是其實就是每一個相當於一個two bit pattern
那它就是等於是這個是零零這這是零一
它每一個就是帶了兩個bit 
那所以從上面這兩個例子來看的話
是比較容易想像它為什麼要取bit 的這個名字
然後它為什麼要用log 二
為什麼用二來做這個log 的基底哦
為什麼要用二來做這個log 的基底然後叫做bit
這樣比較容易想像
因為這樣就跟我們平常的binary 平常的這個binary 的這些哦想法是一致的
不過當然剛才這兩個case 都很簡單
因為他們都是是機率都是一樣的
就是二分之一或者四分之一是完全一樣的
如果不一樣會怎樣呢
我們底下的第三個例子就是如果不一樣的話
如果不一樣的話我們舉例來講
像這個例子就是不一樣的
那麼如果說一個是四分之一一個是四分之三
那這裡應該會多一點零
這樣子
那麼我零的機率是四分之一
一的機率是四分之三
如果是這樣的話呢你也可以算算看那麼零給我幾多少的information 呢
因為零是四分之一呀
這就如我們剛才所說的因為零比較少
所以我如果看到一個零的
話我的information 給我看到information 是比較多的
那麼因此呢你這樣一算的結果你發現那個零給我就是兩個bit information
所以呢譬如說這樣應該講說這一個binary digit
也就這一個bit 
零是一個bit 阿
那這一個不過這個是一個binary digit
它呢它給我多少
兩個bit of information 
因為這個比較少看到難得看到一下
它其實給我的是兩個bit information
那反過來呢一比較多呀
所以呢你如果看到一的話呢
一的information 就少啦
譬如說這個一的話呢
它只給我多少呢
這一個binary digit 
這一個bit 這是一個bit 這也是一個這個bit 呢只給我多少呢
你用四分之三去算的話呢
發現取四分之三的log 之後只有零點四二個bits of information 
ok 所以呢這個呢是只有它的information 就很它只有零點四二bit 
它是兩個bit 
它有零點四二
它是兩個bit 
零點四二兩個bit 零點四二等等等等
這樣子
那這樣子有的帶的多有的帶的少
那平均帶多少呢
那你就可以算它的平均就是這個h of s 
那這個h of s 其實就是
零是兩個bit
可是呢它的的機率是四分之一
一是零點四二個bit
但是呢它的機率是四分之三
對不對
你這樣算起來它平均是多少呢
算起來是零點八一
ok
我這個這個零的話呢它有兩個bit information
不過呢它的機率只有四分之一
一的話呢只只有零點四二bit information
但是呢它有四分之三
平均起來是零點八一
那什麼意思呢是變成說呢
現在的in average 那麼每一個every bit 這個bit 我們寫清楚是binary digit
每一個binary digit 呢只給我多少
零點八一個bit of information
換句話說平均起來每一個bit 只給我零點八一個bit information 
跟剛才每一個bit 給我一個bit information 是不一樣的
在這邊的話
二分之二分之一的時候呢每一個bit 就給我一個bit information
我現在呢因為一個是多一個是少
結果每一個bit 呢只給我零點八一的information
就差了阿
那這個就說明它為什麼這樣define 
它其實是在說明你的每一個bit 
每一個binary digit 這個bit
它到底給我的information 是不是充足
還是是不是efficient 
那顯然呢當你是各二分之一個時候
一個bit 就是是一個bit information
可是你如果不是各二分之一而是一個多一個少的話呢
一個bit 帶了info information 就不到一個bit 了
那麼從這個引申下來
其實就有一個非常有名的在information theory 裡面有所謂的叫做binary entropy function
我把這個擦掉了哦
這個應該知道了沒什麼難的
所謂的這個binary entropy function
這個意思我們待會也會再解釋不過簡單的講就是說假設我現在這個假設我現在的這個送出來還是只有零跟一
然後呢譬如說一的機率是p 
零的機率呢是一減p
p 現在是一個random variable 哦
p 是一個variable 從零到一之間
一個是一一個是一減p 
那麼這個時候它的這個h of s
就是我們這邊講的這個東西
平均每一個symbol 給我多少information 這個東西呢
你就會發現它就是我們講的p log p 
加上一減p log 一減p
就是這個
這個東西p log p 加上一減p log 一減p 
也就是這個log p 是一的機一所帶的information
乘上一的機率
這是零所帶的information
乘上零的機率對不對
那其實這個就是我們剛才在算就是在算這個式子
那你如你如果把這個整個都畫出來的話呢
就得到一個這樣子的圖
一個這樣子的一個對稱的圖
畫的有點不對稱不過應該是對稱的
零到一
橫軸是p
縱軸呢就是在零點五的地方
是一點零
那這個縱軸就是這個bits 
就是information 的量阿of information 
縱軸又是bits of information
那這個就是我們剛才的剛才講的所有的example 都是這裡面的圖這裡面你一個點
譬如說我們剛才擦掉的就是零跟一各是零點五
就是這一點
當p 等於零點五的時候
一減p 也是零點五
所以呢也就是零跟一各是二分之一的情形
當零跟一各是二分之一的時候呢
每一個binary digit
每一個bit 帶的量
就是一個bit 的information
就是一個bit information
那我現在剛才這邊的這個例子是四分之一跟四分之三
一是四分之三零是四分之一
所以呢是相當於p 是四分之三
零是四分之一
那就是這一點
零點七五
那這樣子算上來的話呢
你得到的是多少呢就是零點八一
就是我們剛才那個零點八一的那一點
是不是零點八一
就是這個零點八一的這個這個例子
你如果一個是p 是四分之p 跟一減p 一個是四分之一一個是四分之三的話就是零點零點七五的地方
或者是這邊也可以
零點二五也可以
都一樣都是零點八一
那同樣呢你也可以猜得到
這點是哪一點
這點是零
p 等於零也就是說呢根本就沒有一
全部都是零嘛
也就是根本就是零零零零零一整串都是零
一整串都是是零的話就是我們說它不帶information
因為我都猜的出來後面一定是零
每一個我都猜得到它是零
根本不要看了都是零
所以你給我一個零我沒有給我任何東西
所以都是零都是沒有information 的
所以呢它的總共的info information 量就是零
所以在在縱縱軸上就是零的位置
同理呢這一點呢是p 等於一
就是整串都是一整串
都是一的話
這個任何一個一也不給我任何information
所以它給我information 量是零
ok
那這樣我就不知道一個一個整個的一個一個function
變成是一個function of  p
那麼於是呢這個時候剛才的這些這個例子跟這個例子
都是它的一個special case
對不對這個例子是指各二分之一的時候
每一個就是一個bit information
那這個例子呢是說一個四分之一一個四分之三的時候呢
給我的是零點八一個bit information 等等
那這樣就可以發現呢在這個case 而言
那麼只有有零跟一各是零點五的時候
是最efficient
因為每一個bit 就給我一個bit information
所以呢這個是在是在它的頂端是最高的那一點
你如果不是各二分之一而是一個多一點一個少一點的話呢
in average 你所帶的information 是減是降低的
是比較不efficient
所以呢只有在最只有在零跟一各是二分之一的時候它給給你帶的information 是最efficient
然後一個bit 就是一個bit 阿
這是為什麼它叫做bit 的的意思
它取了這個取了這個這個量我用bit 來做information 的量的原因
那就是這樣的意思
那麼也因此呢我們這邊剛才看的這個例子如果有四個的話呢
你如果各是四分之一的話就相當於兩個bit 阿等等
好那如果這點可以了解的話我們可以再進一步衍申出更多的東西出來
那麼一個最容易想像的情形就是這樣
假設我有三個好了
我有三個會怎樣
譬如說我的m j 我會有三種
x one x two x 三
如果我有三種可能的話
那我說x one 的機率是p
x two 的機率是q 
然後呢x 三的機率呢就是一減p 減q 
如果是這樣的話我我也一樣可以像剛才一樣算這個算這個function 
只不過現在剛才是一個是p 一個是一減p 
所以是一個function of p 是比較好算的
現在我有一個p 有一個q 有兩個變數怎麼辦呢有
兩個變數我沒有不不太容易畫那樣子
所以我可以想個辦法就是把p 固定
p 固定的話呢就變成q 變成variable
所以q 呢我就讓它從一減p 哦從零到一最小可以是零
最大是到一減p 
如果這樣的話我可以畫一樣的可以畫一個q 的圖
我仍然可以算這三個三種symbol
這三種可能的symbol 所造成的平均每一次看到一個東西的的時候它的information 量是多少呢
這個答案我們不去算了不過答案你可以猜的出來也是這樣
跟剛才那個圖是一樣的
所不同的是minimum 那點不是零而已
這個就是零跟一減p
也就是說我的q 的值
是在從零到一減p 的中間
那麼q 的值在零到一減p 的中間
那麼這個那麼在什麼時候最大呢
是在二分之一一減p 的時候最大
那什麼意思呢跟剛才那個完全一樣
譬如說這點呢就是x one 的機率是p
x two 的機率呢是二分之一減p 
x 三的機率也是二分之一減p 
那就是這一點
那如果這一點是什麼呢
這一點就是q 等於一減p 啦
那麼因此呢這個x 三就沒有了
所以呢就變成一個是p 
一個是一減p 零
這個根本就沒有了給我個x 三根本不會發生
只有這個x one 跟跟x two 會發生x 三根本不會發生了
這就是在這裡
那反過來呢如果在這裡的話呢
那是p 零一減p 是x 二根本不會發生
x 二根本不會發生
x 二根本不會發生就就是x 三是一減p
那在這裡面你也可以發現呢這個簡單的現象就是說
我如果把x one 的機率固定的話
這個最可以給我in average 每一個symbol 給我最多information 是在二跟三機率相同的時候
在二跟三機率相同的時候給我最多的information 
那如果說是什麼時候最少呢
根本就沒有三
或者根本就沒有二
這時候就只有兩種東西了
這裡根本沒有三的話只有兩種
這裡根本沒有二也只有兩種
如果只有兩種的話我的information 就少了嘛阿
所以它就會減減少就變成這樣
那這個圖的情形呢畫的呢
其實跟那個是完全一樣
只不過我們這邊因為有三個變數
所以呢有兩個變數所以我們只固定其中一個來畫
好如果這個圖可以想像的話
那我們可以再進一步衍生
你就可以想像哦如果說是我這邊是假設x one 機率是固定
這個時候機率最大是發生在二跟三機率相同
都是二分之一減p 
同理我也可以把q 固定
q 固定的話呢它跟什麼時候這這這個information 量最大呢
是它跟它相同都是一減q 的時候
我也可以把它固定
那麼什麼時候最大呢是它跟它機率相同的時候等等
以此類推你就可以猜的出來
真正information 最大的時候
就是這三個各三分之一的時候
對不對
也就是應該要這這三個機率各是三分之一的時候
如果各是三分之一的時候它們機率是最大
那如果可以這樣子衍生的話
我們就可以這些都是可以證明的
不過在information theory 裡面都有證明
不過我們這邊就不證
我就這麼用嘴巴講一下
那麼我們就可以得到一個重要的結果
那就是這邊所說的底下這個這個講的就是這就是這件事情
也就是說
你如果總共有m m 是什麼
m 就是我所總共的總
我這邊x 一x 二x 三總共有大m 個嘛
大m 是這裡symbol 的總數
所以呢剛才這邊的這邊m 就是二
我們那邊舉例的時候m 就變成三
這m 就是這個東西
那麼如果m 是這個的話呢
那麼你什麼時候這個就是h of s
就是我們這邊所在算的這個所謂的這個source 的entropy 的這個東西
的這個東西這個平就是平均也就是h of s 就是這個entropy
也就是我們所說的平均每一個symbol 到底給我多少information 的這個東西
那麼它的值呢應該是它的maximum 發生在什麼情形之下
發生在我們每一個機率都是m 分之一的時候
那麼剛才在這邊看到的時候呢就是什麼時候它的它的peak 在哪裡
peak 在我這兩個都是二分之一的時候
因為我大m 這裡在我這個case 大m 等於二嘛
在我這裡大m 等於二所以呢
就是它的peak 發生在各是二分之一的時候
那我們這邊講如果這個這邊舉舉的case 是大m 等於三
這邊講的例子是大m 等於三
於是這個peak 發生在什麼時候
我去同時調p 跟q 的話
會在p 跟q 各是三分之一的時候
那個時候會最大
那如果是四的話呢
那我我最大的應該就它們各是四分之一的時候
那就是等等
那麼因此也就是不管是大m 等於幾
它的這個上限都它的peak 就是這一點
這一點
或者說是這一點
都發生在它們equally probable 的時候
或者說呢就是h of s 
is maximum s when all symbols are equally probable
也就就是說我的p 的x i 
都等於大m 分之一
那麼我的每一個symbol 它們的機率都一樣
就是這邊的的這m 個symbol 呢它們機率要完全一樣都是m 分之一
當它們的機率有都完全一樣都是二m 分之一的時候
它的這個它的這個這個entropy 呢是maximum
那個時候呢就是各是m 分之一
那如果是m 分之一的話呢那這很容易易算
我們帶回去算帶回去這個式子的話
你就知道
每一個的機率是log 的m 分之一
然後呢它們的然後平均的話呢
這個這個然後平均我我我一樣都是都是乘以個m 分乘以m 分之一嘛
然後加起來加m 個嘛
所以平均起來就是這個值
平均起來就是這個值
因為每一個每一個都是m 分之一嘛阿
因為每一個就是log 就是這個就是log m 嘛
負的log m 嘛
每一個就是log m
然後再平均起來還是log m 嘛
所以呢我所得到的就是log m 
所以它的上限就是log m
就剛才而言就是log 二就是一點零
那如果是三個的話呢
那這上面就是log 三
就是log 三
如果四個的話呢就是剛才這個例子的話呢
那這個例子的話呢就是log 四就是二嘛等等
所以呢我的上限就就是log m
發生在什麼時候發生在每一個symbol 都equally probable 的時候
就是上限就是log m 
那它的下限呢就是零
你只要有其中有一個的機率是一別的都是零的話
像這一點
它全部都是這就是這個全部都是零的情形
它根本就沒有information
這點就全部都是一的時候它也沒有information 等等
那其實你這個m 個裡面只要存在某一個x j 
存在某一個x j 它的機率是一
而其他的所有的j 都是零的話
那它就會變成零
那麼因此它的上限下限就是這樣子
那這其實是information theory 裡面非常重要的定理
是該證明的不過我們不證它了
我們就這樣講一下
那這個在這個情形之下的話
那你會說ㄟ那這邊不是零呀
這邊有有一個不是零的值
是因為我我永遠有p 嘛
我這邊已經讓p p 等於這個有一個p 在這裡
所以這個其實就是你加起來不不是零就是因為有個p 的關係
那你所以像這個情形的話
一個是p 一個是一減p 一個是零
所以在這裡並沒有任何這三個裡面沒沒有任何一個機率是一的
所以它不會在零的地方
這裡也是一樣
沒有任何一個機率是一的
所以呢並不符合剛才的這個下限的條件
所以它們都不是零
它是一個有一個非零的值
有一個非零的值因為它沒有一個是機率是一的
那麼剛才在這裡是零是因為它真的有一個是機率是一的的關係阿等等
好那這個是一個非常基本的
關於information 量的一個這個簡單的這個這個這個說法
那有了這個之後我們底下就可以用它來推我們底下要做的事情了
好我們先在這裡休息十分鐘
ok 
我們剛才在解釋這個這個式子
就是這個entropy 這個東西
那麼它的上限是log m 
下限是零
上限發生在這個狀況
它們是equally probable 
下限發生在只有一個是一其它都是零的情形
這是什麼意思我們如果再衍生一下的的話
就可以這樣子來看
我現在如果把這個看成是一個機率p of x i 
這是x one x two 等等到x 的m
那麼這個就是我的我的這這m 個我們之前講的這m 個可能的值它們的機率就是p 的x i
那麼這樣子畫是什麼case 呢
這樣子畫的case 就是我們剛才講講的發生上限的情形
也就是也就是在嗯這裡
那麼如果他們各是m 分之一的話
那這個時候我的我的這個所以每一個機率各是m 分之一
那這個時候我的entropy 呢其實就是log 的大m 
那這是它的上限
下限是什麼呢下限是只有其中的一個是一
對不對其中那個是一
其它全部都是零
其它根本不會發生那
如果只有那個是一的話呢其實它不帶任何information 因為你猜都猜得出來這樣子
所以這是x one 這是x m 
那只有其中的某一個x j 是一
那這個呢就是它的的entropy 就是零
那就是我們這邊所看到的下限
那在這個上限跟下限的的中間那當然還有很多個case
你可以想像的是哪些case 呢我們舉例來講
它不再是完全一樣m 分之一而是有多有少
但是都有
有多有少
這樣我總共這是x m 這是x 一
它有多一點有少一點這樣子的話呢
那它是在比它那這是什麼情形
就好比我們剛才看到的
從這點向下移動嘛對不對
這點pick 的這點就是剛才的上限
就每個都是m 分之一的話等於是在這點上面
那你現在有有有有多有少它就在這邊來在這邊來
所以以呢平均起來就在這些地方
所以那就是這個情形
那再來的話呢
它可能慢慢集中到譬如說說若干個這附近的
譬如說這裡有幾個或者這邊有幾個
其它的很少很少
這個是x m 
這個是x one
如果它集中在少數幾個而多數都變成零了的話
那就更少一點
那如果再過來的話呢
它可能就只集中在在這附近有一點
其它都沒有了
這是x m
這個x one 
ok那你可以想像差不多是一個這樣的情形
就是由完全的uniform distribute 的
簡單講就是uniform distribute 的
或者是equally probable 的時候
這是最大的
然後慢慢的它開始有大有小了
然後慢慢集中在一個比集中在少數的地方了
然後別的地方會變成零了
最後集中在一個了
這就變成零
所以這就是它的這個分佈的情形
那你如果是這樣子看那你就可以猜的出來
它這個東西其實有非常豐富的意思
它可以拿來做很多用途
其實我們講了半天我們底下已經也就是要用這個
用這個的意思你可以看成是在最底下的是什麼是純度最大
純度最高
什麼叫純度最高就是說它的distribution 集中在一點上面
其它都沒有
這非常純的
這純度最高的
那當然然在上面就是純度最低的就不純了嘛
就完全的是完全沒有有純度可言
那在這上面呢這什麼我們可以說是亂度最大
所以這個可以描述一個distribution 純度或者亂度
那麼這邊是最亂的嘛
最完全最混亂的
你每一個都有一樣的機率
而這邊是最不最不混亂最純的
完全就是一種情形
那你也可以說這個是是這個不確定性最高
因為它的它的每一種都都一樣的可能所以最不確定嘛
這邊是最確定嘛
這個這個底下呢這是確定性最高
那我也可以說是這個它的distribution 的這個分這個嗯等等
那所謂的這個就是uncertainty 
那這所謂的純度就是purity 
對不對
這個那所謂的純度就是最高就是指highest purity 
那亂度最大就是highest randomness
這是random 的情況最高
所謂不不確定性就是uncertainty 最高對不對
那然後我也可以用這個來說就是一個這個the spread of a probability distribution 
我也可以說它就是代表the spread of a probability distribution 也就是說它的分散集中的的程度
一個機率的分的分分佈的分散集中的程度
分散或集中的程度
ok
那麼因此呢我們我們後面會一再的用到這個東西
其實是就是在用這件事
就是我只要算出這個東西來的話
其實就是給我一個distribution
給我一個p of x i 這個東西的話
那我來算這個東西
其實算出來就可以告訴我either 是算它的randomness
或者是算它的uncertainty
或者是算它的purity
或者是算它的spread
那這個是最不spread 嘛這是最集中在一點上
那這是最spread 的情形
所以算它的這個這個分散集中的程度等等
那我都用這個來做
好有了這個之後
底下這一頁倒是沒有太多複雜的東西
這個看起來很複雜其實倒只是一些簡單的數學
它在講兩件事情
這都是後我們後面要用的現在利用這個機會就講的
那第一個所謂的jensen's inequality
這個沒有什麼特別只是我們在算的那個東西其實就是p log p 嘛
你現在如果再回去看剛才這個式子
我算的這個東西哦ok 我們還漏了那麼為什麼這個東東西叫做entropy 
那我們現在取取這個東西的名字叫做entropy 的意思就很清楚
entropy 原來是熱力學裡面的一個名詞
那你如果去看熱力學裡面的講它也就是在講那些分子它的這個混亂的亂度
所以entropy 其實就是在講亂度
那麼因此呢
那麼在熱力學裡面的那個entropy 它它計算公式也就是這個
就是這是一個機率
p of x i 這是一個機率的distribution 就是這個東西
這個東西就是p of x i 
那麼你如果把p 的x i 乘取了log 之後再乘以p x i
然後去加起來的話
就是p log p 嘛
p log p 然後summation over i 的這個式子
在熱力學裡面
也就是用這個式子來算
的這個entropy
所以這個就把熱力學那個公那個名詞借過來
那麼這邊的意義其實跟熱力學裡面一樣
也一樣是在描述亂度跟純度
描述它的uncertainty 等等
都一樣的的意思
那麼因此
我們這邊才會有這個說法
就是說呢
它也叫做degree of uncertainty
它也可以說是quantity of  information
也就叫做entropy
那對一個random variable 而言呢
有一個distribution 我都可以這樣算
然後都可以得到這個東西來算它的這些東西等等
好那麼有了這個的話呢
那這個jason inequality 只是我們後面要用的一個不等式
那沒有什麼特別就是這個p log p
就是我們算entropy 的式子
給我一個機率
一個機率的分佈一個distribution 就是這個東西
我取log 之後分別對每一個p 去加起來
這就是p log p
所以這個式子其實就是我們剛才的
這個h of s 這個entropy 的式子就是這個式子
那麼它說呢
你如果把這個p 換成另外一個q 的話
一定會比較大
嗯就這樣子意思
我們算entropy 是p log p
可是你如果換成另外一個distribution q 的話
一定會比較大
q 是another probability distribution
所以q 一定也是要加起來等於一的
加起來要等於一它也是正的
所以是另外一個機率的distribution 不過不是p 就是了
那你只有把這個p 換成一個q 的話
這個p 換成q 的話都會比較大
什麼時候等號成立
當q 等於p 的時候等號成立
這個叫做jason inequality
那底下沒什麼特別它這只是在証明
這個証明其實很簡單這個証明其實就是用這個式子就是log x
永遠小於x 減一
這個是沒有什麼特別
你在學數學的時候就學過這個
就所謂的log x 永遠小於x 減一
其實有很多種証明的方法
然後等號成成立是在
只有那一點等號成立
它就這樣了這個是log x 
這個是x 減一
x 減一是一條直線
那然後呢它們兩個只有在一點等號成立
就是x 等於一的時候等號成立
那別的時候它都比它小
那就用這個式子
那麼我現在的x 呢
就把它寫成q 除以p 
q 的x i除除以p 的x i 
當成是x 的話
那麼log 的這個x 呢就會小於等於x 減一
然後我分別都乘上p 的xi 去summation over i
它仍然是小於等於它
ok
然後呢
你右邊的這個式子就是等於零的
原因是這個這個兩個一消掉了嘛
就是q q 的x i 減一的summation
那個加起來這個q x i 加起來也是是一嘛所以就等於零了嘛
所以呢
左邊這個式子就小於等於零
左邊這這個式子就是p log p 減掉p log q 嘛
嗯p log q 減掉p log p 嘛
那就得到上面這個式子哦
所以這個沒有什麼這只是一個証明而已
它就是用這個log x 小於等於x 減一的這個式子
然後只有一個等號成立就是x 等於一的那一點
它用這個
然後把x 代q 除以p 代進去
然後就這樣可以証明它等於這樣的意思就是了
那它的簡單的解釋它的說法就是說呢
你本來算entropy 應該是p log p 的
但是你如果把那個p 算做另外一個機率的話
你弄錯了用另外一個機率來算的話呢你entropy 一定是增加的
或者說呢你用一個估計不正確的distribution 去算
你得到的degree of uncertainty 是增加的
你可以用這個方法來解讀這個式子的意思
不過基本上這只是一個一個不等式我們後面會用它來証明一些東西就是了
我們只是這樣而已
那同樣的有了這個東西我們也可以寫成另外一個式子
你看這個底下這個式子的話呢這個p log 的的p 除以q 
其實就是這個一樣的不過倒過來寫就是了
那我們剛才証明這個東西小於等於零現在這個倒過來就這個大於等於零
那這個p log p 減掉p log q 
它一定是大於零的東西
這個東西呢我們給它一個名字
叫做cross entropy 
或是relative entropy 
就是如果你現在有兩個distribution 一個叫做p 一個叫做q 的話
那我們就可以算它們的這個cross entropy
或者relative entropy 
就這樣算
那這個其實是在算兩這是算兩個distribution 之間的distance 
這個有一個名字就是所謂的k l distance 
那麼換句話說其實
你給我兩個distribution 我可以算很多東
算很多種distance
假設這是一個這是一個p of x
那我給你給我另外一個q of x 
它是另外一個
這是q of x 
那麼q 跟p 之間是有是有差距的
到底它們的差距是多少
怎麼算p 跟q 的distance
這有很多種算法你如果去查
查書的話
given 一個q 跟一個p 怎麼樣算它的distance
有很多種distance 的算法
有很多種definition
各有它不同的意思
其中的一種叫做k l distance
就是這個
kull back leibler 
這個這個distance
那這個distance 算法就是用這個來算
那其實即使是這個distance 也有好幾種算法
那有很多個版本
那這個distance 那你看你可以看得出來它的意思就是說
你去算它的entropy 
因為給我一個distribution 本來就可以算entropy
那我如果算entropy 的話那我當把其中的一個
當成另外一個的時候
它的它的會entropy 會差多少
就表示它們之間的就表示entropy 會差多少嘛
就是p log p 跟p log q 會差多少
那就是它們之間的差異
所以這個呢是等於是這個它們的它們之間的這個這個uncertainty 的差異
那麼這個
那麼做為一個它們之間的這個這個distance嗯
這個也是我們後面會用的就是了
那麼嗯在這裡的話呢當然我們這邊都是用discrete 的版本來解釋
當然這個都可以變成continuous distribution
你現在給我我一個continuous distribution 我也一樣可以做這件事情
那這個我們就不多講就是了
好關於這個我們就就說到這裡
底下我們就要來講我們真正要用的是
用在底下這裡
那這是另外一件件事情就是所謂的cart 
就是classification and regression trees 
這個在嗯這是另外一個領域裡面一個重要的方法
這個領域嗯這個東西在嗯either 是這個data mining 裡面
or 是machine learning 裡面
or 是pattern recognition 裡面
它們大概都會講一堆這種這種東西
那我們等於是說把它拿來用
那麼嗯這邊舉的是一個非常簡單的例子來說明這個在幹嘛
那底下我們是要用它來做tri phone 的train tri phone
而train 的時候要用什麼
要用剛才的entropy
所以我們在講的這堆東西是這樣來的
好那我們先說這個是在幹嘛我們們這個這個是用一種tree 的structure 
像這樣一種tree 的structure 來幫助我們來這個把一大堆的data 把它如何把它結構化
讓它的structure 清楚哦等等
這是什麼意思我們舉這個例子來說明這個例子其實是課本的例子
你如果看課本的話在我這邊的給你的這個課本的reference 裡面哦
哦在這個裡面
它就是就是就有這個例子
那我其實就把課本上的例子拿下來講就是了
它說假設說這個區公所裡面拿到一群人的data 
這個區公所裡面有這一區的這一鄰或者這一里的居民的data
那它要根據這一群的data 把這一群人呢分成根據他們們的身高分成五群
最高的一群
普通高的一群
跟中等身材的一一群
跟稍微矮的
跟最矮的
分成五群
那區公所的這一群人的data 它有很多很多資料
譬如說他的年齡他他的性別他的工作等等等等
就是沒有身高
那你就要根據你所知道的訊息譬如說年齡啊性別啊工作等等
根據這些去判斷說沒有身高那你現在要根據身高來分成五群怎麼分
那這個時候我們要做的事情其實是跟剛才講的是很像的
是這樣的一件事
那麼我們姑且可以想像成說
ok 這些人的身高也是有一個distribution 的
那這個distribution 也許不太容易看
我們就把它當成是discrete 
那麼最矮的人的身高是叫做 x x one 
最高的人叫做x m
那麼他們各有一定的distribution 在這裡
那我現在要把這些人的身高
所以所以這些這些就就是它們身高的distribution 
我要把它們身高分成這五群
那最高的這一群在這裡譬如說
這是最高的大t 
然後這邊呢是是小t 是第二高的
中間這些是中等身材的m 
然後這邊是比較矮的
叫做s 
然後呢這邊是最矮的這群叫做大s 等等
我等於是要做這件事情
我要把它們的這個這個根據他們的身高分成五群
那我有很多data 
有的是說他的年齡有的是說他的性別說他的工作等等
就是沒有身高
我有根據這些其它的data 來分他們的身高
那怎麼分呢
你想個辦法做一個tree 
那就他這個case 而言他說呢ok 
我先說他的年齡是不是大於十二歲
這個理由是說假如小於十二歲就是小孩
小於十二歲的話大概是小孩小孩總是最矮的
所以呢凡是呢所以呢如果說凡是小於十二歲的話呢
就歸成最小的這一類最矮的
那大於十二歲以後不表不表示他一定最高
這時候怎麼辦呢我們再來分
那第二個呢他就說呢我根據什麼來分呢
根據他的職業
如果他是職業籃球隊的哦那顯然是最高的
所以呢只要他是職業籃球隊的我就分到就是屬於這個
那就是最高的
那麼如果那這是把最矮的跟最高的分出來了
那中間這堆人怎麼辦呢不知道
不過呢他有一個data 他說呢每週喝多少牛奶
ok 那supposed 每週喝牛奶喝很多的會比較高
所以呢每週喝牛奶多的呢就屬於第二高哦就屬於這一層
那那每週喝牛奶喝完之後怎麼辦呢
那剩下呢他就說看他是男的還女的
那這一群人可能男的比較高女的比較矮
所以呢他就如果又不是小孩對不對
又不是小孩所以不然如果小孩就男生可能特別矮
又不是打職業籃球隊籃球隊的話女生也會很高對不對
那到了這群人的時候那大概就是ok 男生會比女生高
所以呢男生的話呢就是屬於中間這群
女生就屬於稍矮的這群哦這樣子
那它這樣就構成一個tree 
也就是說我在這個tree 的每一個note 是根據一個question 來分
說它是屬於哪邊這樣把它分出來
那問題是你這個tree 夠不夠好
怎麼樣才能做到一個有效的做法呢
那一個可以做的方法就是用我們這邊講的這個entropy 
那這個entropy 是幹嘛呢其實就是在做我們這邊講的事情
那你可以想像成
我們剛才那個tree 是怎樣
就是把一個node 拆成二個node
不斷地在做這件事情
假設我原來這個node 是叫做n 
我把它拆成a 跟b 的話
它不斷地在做這個用一個question 或者用一個criterion 來判斷
把那群人拆成兩半
它就不斷在做這件事情之後構成一個tree
最後把它拆成剛才看到這五群
那它怎麼樣做這件事呢
那基本上這個你可以想像
最理想的情形是說
我們如果畫小一點它這樣子拆成這樣子
剛才這樣子拆法
我一開始上面的所有的人都在這裡
但是我會希望最後拆出來的時候呢在這邊的是什麼
是某一群
就剛好是譬如說這一群
其它都是零
剛好就是這一群
那這邊呢剛好是另外一群
其它都是零
對不對
那這邊剛好是另外一群
最好是這樣子
就是這一群其它都是零
也就是說如果要這樣子做的話呢
很可能就是我希望每一次一拆的時候
就是從這邊切開來或者從這邊切開來或從這邊切開來
我每一次如果我每一次的這個split 這個node 
這個node 如果這樣子split 的時候每一次都是可以這樣子切這樣子切這樣子切的話
那它就一塊一塊就切出來了
可是我如果是做得不好會切成這樣子的話呢
就不好了
這意思是說我如果每一個這樣split 的時候呢我是從這裡再這裡這樣切或者這樣子切的話
都有效我這樣的話就就就就就可以把它這樣一次這樣切一次這樣切把它切開來
就得到一群一群的
可是我如果這個node 做不好的話我如果弄了一個這樣子切的話呢
這樣一切就一點用都沒有
等於白切了
那麼舉例來講
它如果區公所用的一個是譬如說這個學歷
學歷可能就跟身高沒有關係
它這邊用的都是剛好跟身高會有關係的
所以你這樣一切的時候看我儘可能我儘可能是希望是這種的對不對
我如果弄個學歷的話搞不好大學畢業的有最高的也有最矮的
那麼幼稚園畢業的也有從最高到最矮都有嘛
那所以你這樣如果用學歷這樣一切的話
可能就沒有意義了
那當然實際上你可能雖然你要避免切成這樣切法
但你要真的這樣切可能是做不到的
那你真正會做成怎樣的你很可能會做成這樣子
就是我切一個可能是這樣子切的
譬如說那麼於是呢比較多的在這邊
這邊比較少
但是你不至於說完全沒有啦
嗯就是就是你可能切成一個這樣子的
那那我們怎麼來想這件事情呢
那我們就來define 我們要做的事情其實就是要算這個entropy
因為所謂的你希望直的切下來
你希望每一個直的切下來是意思是什麼
就是我希望純度
就是我希望這個純度啊我要
我原來是亂度
對不對我一開始的時候是亂度最大上面這個case 嘛
我一開始是最上面的是最亂的
然後呢我希望把它如果這樣子一刀一刀切下來的話呢
越到底下我越純
越純的話呢就是越好
那從entropy 來講呢就是我的entropy 要越變越小
我entropy 的越小的越多就是越純嘛
越小的越多就是越走向純嘛
所以呢我就是要把entropy 降得越多越好
因此怎麼辦呢
我就是為每一個node 去算在這裡的entropy
我這個可能是在這上面這裡
我就可以算在這裡的entropy 
然後今天如果有有有一有一個辦法讓我切一刀的話
那我不知道我這一刀切出來到底是切成這樣還是切成這樣
還是切成這樣
那我就是算這兩個的entropy 
那就是底下這一頁所講的事情
也就是說這一個node 我算它的的這個什麼東西p 這個p log p 
其實就是entropy 
這裡的不太清楚這這個c sub i 這應該有一個下標是i 
c sub i 除以n 就是percentage of data sample 
for class i at  node n 
也就是說
也就是說嗯在這裡
這裡每一個就是一個每一個cross 就是一個i 
它的data 的它的有幾個人
我們算人數好了就是data sample 的數目就是有幾個人嘛
那就是c sub i 
那我總數呢就是n 
total 的人數是n
嗯如果我們用剛才的人數來算的話
total 的人數是n 
這是第i 個人數的cross 是c sub i
你如果這樣子看看的話呢那麼這個c sub i 除以n 
其實就是一個機率嘛
所以它說percentage of data sample for class i at node n 
嗯嗯嗯等一下
這個n 不是總數
n 是指這個node 的意思
c sub i 是這裡的data 是沒有錯
然後n 是指這個node 這個node 
所以呢所以這個n 呢就是在這個node n 這邊在node n 這邊的所有的data 
然後我就就去算對cross i 的人數的percentage 就是一個機率嘛
所以呢就是這些機率
所以這裡的每一個就是所以這裡的每一個機率其實就是p 的c sub i 
at node n 就是它的機率
因此呢我這邊所謂的這個p 然後乘以log p 再加起來這件事情
就是在算這裡的entropy 
當這裡有個entropy 這裡這樣算之後我現在來切
用某一個方法
根據某一個criterion 把把它切成兩個的時候
那個depend on 我怎麼做對不對
我們說最理想的是這樣切
這樣的話呢就左邊跟右邊這樣是切得最乾淨的
但是你很可能做不到這樣
那你做到的可能是一個這樣
譬如說這樣
於是呢你左邊呢可能會變成是這邊這樣然後很小了
右邊這個會變成是這樣然後這邊很小了
這邊都不是沒有
只是少而已
那這樣也不錯啦這也是進步啦對不對
所以呢我這邊可以得到一個這個b 的
這邊可以得到a 的
啊都一樣
這就是在node b 這邊有一個distribution 
我也可以算它的entropy 
這邊的在node a 這邊有一個distribution 我也可以算它的entropy 
那我希望這個entropy 從這裡到這邊呢是降下來的
那降得越多就越好嘛
那降得越多就表示我這一刀切得越接近這樣子切法
對不對你如果這樣子切的話就等於沒有降低
那就等於是白做了ok 哦
那麼因為這樣的關係我現在就可以定義這個delta 的entropy 就是entropy reduction 
就是在node n 減掉node a 加b 的
就是我本來在這裡的時候entropy 是多少
變成這樣經過這個criterion 切下來之後
變成這兩個那它的entropy 是多少
那這個時候呢這兩個entropy 的差就是我的delta 的delta 的這個這個的這個entropy 就是entropy reduction 
看看我減多少
那麼於是呢我就可以選擇一個在每一個地方我都選擇一個最好的切它的方法
就是一個所謂的question 
當然不見得是所謂的question 應該就是一個criterion 你怎麼切它的一個切它的一個criterion 
不過在我們剛才的例子裡面我們都用entropy 都用question 來解釋
譬如說年齡是不是大於十二
他的的他的職業是不是職業籃球隊的
它每週是不是喝這麼多牛奶
這都是用一個question 哦
我們就稱它為question
那其實不見得是一個question 就是一個criterion 就是了
當我把它用用這個一堆我有一堆存在的criterion
但是到底用哪一個criterion 在哪裡切
我也可以一開始就先選男生還還是女生
我也可以一開始就看根據它的牛奶
我也可以一開始就根據它的職業
嗯你憑什麼要先選哪一個呢
那就根據看我在根據這裡來看
我從頭開始
到底我手邊有的data 裡面
哪一個讓我一拆開來
entropy 降低最多
就表示我的程度增加最多
那到了這裡我再來看還有什麼東西讓我降低最多的
我就從那裡來拆
這裡我再看哪個讓降低最多我從那裡來拆等等
所以我每一次都選擇我每一次都選擇entropy 降低最多的那一個
來做這件事
所以呢我這個h n  的entropy 減掉a 跟b 的entropy 
做為我的 entropy reduction 
然後它是一個function of q 
就是我的question 
或是我的criterion
因此我每一次呢就在所有的q 裡面去選擇那個降低最多entropy 的那個question 從那裡開始切哦等等
那這裡還有一個小問題
就是你如果光是這樣做的話會發現不對
為什麼不對呢
這個我們舉個簡單的例子
如果說如果說一開始是完全uniform 的
總共是m 個
那這個就是log m
這個entropy 就是log m 
假設我選了一個非常笨的方法
它這一刀是這樣子切的
那切完之後呢在這邊也仍然是一個log m 
在這邊也仍然是一個log m 
對不對所以這邊也是log m 
這個也是log m 
這應該表示說這個這一刀是白切了
entropy 完全沒有改變
可是你現在如果算一算的話呢
這個h a 加h b 是兩倍的log m 
上面是一倍的h m log m
嘿它還增加了
這邊是一個log m 這邊變成兩個log m 這不太對吧
那為什麼不太對呢你仔細想一想呢其實是應該怎麼樣
我這邊假設這邊有一百個人
其實是六十五個人到這來三十五個人在這裡的話
應該是這個乘以零點三五
這個乘以零點六五
這樣比較合理嘛
對不對
也就是說假設這邊有一百個人你這邊後來是六十五個人到這裡三十五個人到這裡的話呢
你應該是這邊兩個雖然都都等於白切了
是兩個都是log m
你不可能變成兩倍的log m 嘛那變成還變大了嘛
沒有變大只是一樣而已等於沒有做而已
那其實這邊的entropy 應該是它乘以零點六五它乘以零點三五加起來
那就是跟它一樣
表示沒有表示沒有做任何事
那麼因此你後面還要乘這個東西
還要乘這個比例才對
那後面的這個比例就是這個p 的n
ok 所以p 的n 呢就是prior probability probability of n 
也就是說有在你那個node 那裡
到底out of total number of sample 有多少
就像total sample 是一百個人
你現在有六十五個人在這裡三十五個人在這裡的的話呢
這個零點六五跟零點三五就是這個比例
就是這個p 的a 跟p 的b 
這個就是p 的a 這個就是p 的b 
那這樣的話呢乘在這個裡面這樣就不會錯了
嗯所以呢我們這邊還多了一個這個東西
所以這個就做weighted entropy
我還加一個weight 
那這樣的話呢就合理了
那底下這段沒有什麼特別
它只是說呢這個delta entropy 就是這個entropy reduction 這個東西呢你可以証明它其實是這個式子
那這個東西是什麼呢這個東西就是我們剛才所說的那個cross entropy
我們上一頁不是說過這個嗎
嗯再上一頁
就是這個東西
你今天如果給我兩個distribution 一個是p 一個是q 的話
那麼它們的cross entropy 就是p log p 除以q 的這個東西
或者p log p 減掉p log q 的這個東西叫做這個東西
你現在如果那這只是一個式子它說你可以証明它
那麼這個entropy 的reduction 是跟這個有關的
那這個a of x 就是到了這邊的的distribution 
b of x 就是到了這邊的distribution 
那麼它們分別跟n of x 就是
原來在這裡的distribution 
它們到底改變了多少
這個就是剛才講的那個那個entropy 的這個cross entropy 
或者說是它的k l distance
從這邊變到這邊到底distance 改變了多少了那個distance
那你其實就是這兩個distance 
那你這個這個可以想像
這個物理意義是合理的嘛
就是我原來原來這邊是n 啊
原來這個就是n of x 
當我這樣一切之後這個叫做a of x 
這個叫做b of x 
那因此呢我就是算a 跟n 差改變多少的k l distance 
跟b 跟n 改變多少的k l distance 
然後分別weighted by 它們的這個p a p b 就是這個東西
就是它們的weight 
那這就是我真正改變的entropy
那麼如果是這樣子的話呢那那那我就一路去把它降低就對了
那它這邊也提到說我這邊都每一次都算這個p n
我這邊算p a p b 就算這個零點六五零點三五這個東西呢
它有另外一個意義也是很合理的
就是我其實是是把reliability of the data 算進去
我量越多的數目越多量越多的這個這個統計特性越可靠
所以我就weight 比較重嘛
weight 比較數目比較少比較不可靠我就weight 比較較輕嘛
所以這個其實也是把把這個statistics 的reliability 算進去
那這樣的話呢你就可以發現我現在每一步
都在讓我這個tree 不斷不斷地這個repeatedly reduce 我的entropy 
那也就是說我等於是每一次如何split 用什麼方法來split 都是選擇一個 entropy 降低的最多的那個來拆
然後這樣一路拆下來一路拆下來
所以呢我每拆向下走呢我的entropy 都在降低之中
那這棵樹
整棵樹的的entropy 呢
就是它的最後的terminal 的entropy
那最後一路拆拆拆拆到最後的這個
在terminal entropy 就是我整棵樹
那當然我這個terminal 如果如果到最後的每一個純度最高的話entropy 一定最小
那這樣是最理想
所以呢這就是這樣的意思
好那如果這一點比較了解了的話
那我們現在可以來看我們現在怎麼來做tri phone
那麼要做tri phone 的時候呢
這個好像該休息哦我們休息十分鐘吧哦
 OK 我剛才漏掉說一件事就是 哦
就是這個 cross  entropy 的這個東西是不對稱的
嗯這 ASYMMETRICAL 
也就是說
這個 p 跟 q 的位置如果是反過來是 答答案就是會反嘛
你會看到因為這是 p log p 除以 q 嘛
所以如果這兩個倒過來的話就變成 q log q 除以 p 是不一樣的東西
哦 所以它是一個不對稱的叫做 cross  entropy 這是一個不對稱的 measure 
那麼 但是呢有人把它拿來當做 K L  distance 當成 distance 的時候呢 distance 常常覺得應該是對稱的啊
哦這個 a 跟 b 的 distance 好像應該 b 跟 a 的 distance 應該是一樣應該是對稱的啊
所以你當然也可以去定義一個對稱的也可以
那麼最常用的一種辦法就是讓它兩兩個加起來除以二嘛哦
那這樣就會變成對稱嘛
在這個定義裡面它是不對稱的
但是你可以定義另外一個東西就是 d 這個 d 的 p  of  x  q  of  x 
加上 d 的 q  of  x  p  of  x 
然後除以二對不對
你只要定義一個這樣東西的話它就變成是對稱的
所以你如果要一個 symmetric 的的 distance 也可以
就這樣就變成 symmetric 了
那至於說嗯我們這個地方剛才講說這裡這個當你一個 n  split 成為兩個 node  a 跟 b 的時候
它們的這個這個 entropy  reduction  turns  out 剛好是這個式子哦
那這個式子基本上其實就是數學推出來的
那我剛才講過這個你意義上也可以解釋
就是你是這個是這個這個嗯 n 跟 a 差多少
n 跟 b 差多少對不對 嗯
那這個地方如如果你覺得有點這個其實這個整個的式子的推導是來自這篇 paper 
你如果詳細去看的話
在那個 paper 裡面會講得很清楚
就是這一篇啦哦
這一篇嗯我在上次講有提到過
這一篇 paper 的作者其實是我們台大資訊系的一位學長從前的博士論文
當初是他做的
後來變這個變成經典之作
所以這邊拿來講就是
值得你去看一下你如果要看的話哦
那剛才那個推導或者那裡面相關的討論在那個裡面有
那基本上你可以看成就是我的 distance 我的這兩個 distribution 
a 跟 b 跟原來的 n 差多少的一個關係就是我這個 entropy  reduction 嗯
好那麼再來我們來講我們真的怎麼做這件事
我現在不再講那些東西而是說我們現在真的要要要 train 這個 TRI PHONE 
我們真正回到我們原原來問題是要 train 這個 TRI PHONE 
TRI PHONE 為什麼會有 會會難 train 呢
你記得我們一開始說過 
TRI PHONE 的問題是在它的數目太大了
那麼我就算只有三只有六十個 phone 的話 
TRI PHONE 就是二十一萬六千個
而且很多是看不到的
是很多很多 TRI PHONE 裡面 data 跟本就沒有 嗯
就是所謂的 unseen  TRI  PHONE 
就是你的 data 裡面沒有的
並不是任何一個 phone 
並不是任何一個 phone 
這個左邊接一個什麼右邊接一個什麼你在 data 裡面都可以找得到
可能 data 裡面都找不到
那麼因此呢這就很大的麻煩哦
那以我們國語為例
我們國語的其實很多 phone 不會連起來
所以真正會真正有的 TRI PHONE 大概四千六百個左右
比這個少很多啦哦
我們真正大概四千六百個左右
但是你如果拿一個二十小時 database 來數一數裡面的 TRI PHONE 
大概只有兩千個左右
根本一半都不到
有一大半根本就沒有 嗯
所以那就是這這是真正的問題
就是一大堆 unseen  TRI PHONE 就是你沒有的 TRI PHONE 怎麼辦
那這個辦法就是我們剛才講了半天的這些東西的辦法
那怎麼做呢
那麼我們先看它它它基本上就是什麼呢我我每一個 bass  phone 
包括所有的 possible  context  dependency 的每一個 state 去做一棵 tree 
譬如說我如果總共有五十個 phone 的話
每一個 phone 是五個 state 的話
我總共就是兩百五十個 tree 
什麼意思
譬如說 啊 這是一個 phone 
那麼這個 phone 的話呢它其實左邊可以接很多東西啊很多很多
右邊可以接很多東西啊很多很多
對不對那那光是這個就有很多很多個 TRI PHONE 
那這樣子那裡面有有的 TRI PHONE 我就就是沒有我我我無法 train 
那怎麼辦呢那我現在不要那麼多了
我現在就就只把把所有的啊拿出來
不管左邊什麼右邊什麼我通通都放在一起得到一個啊的
那麼假設這個啊我要五個 state 的話
五個 state 的啊的話我假設說裡面的我要 train 它最後這個 state 怎麼 train 
我就把所有的啊的最可以做為最後一個 state 的所有的 data 拿來
在這裡做一棵 tree 
在這邊長這個 tree 等等等等哦這是這個意思
那我現在就是這個啊
不管他是我現在不管它左邊是什麼右邊是什麼
我左右的 context 都不管我只管它中間這一個
然後呢如果光是光是管中間這個的話我要五個五個 state 構成一個 HMM 的話
我就做五個 state 
然後呢每一個 state 我就做一棵 tree 
那到時候每一個 tree 後面會產生某一些個 TRI PHONE 的這個 state 長怎樣
哦我們底下看到的是這樣
所以呢這這這這是第一句話講的意思
就是說呢我我所謂的 bass  form 所謂的 bass  form 就是包括所有的 context  dependency 
不管左邊接什麼右邊接什麼我都不管了
我就只管這個 bass  form 
這個 bass  form 裡面假設是要一個 HMM 的話
那麼它的每一個 state 我去做一棵 tree 
我就 con  construct 一棵 tree  for 每一個 state  of 每一個 bass  form 
因此呢假設我像我啊總共有五十個這樣子的 form 的話
我就做了就有五十個 HMM 
每一個如果要五個 state 的話
我就做了兩百五十棵 tree  
OK 好那這兩百五十棵 tree 怎麼來呢
我們就舉例來講假設是這個啊的這個音的最後一個 state 
這個 tree 的話我就是把所有的阿 所有的啊的聲音要拿來 train最後一個 state 的所有的 data 都放在一起
那這個時候呢這些 data 放在第一個 node 這裡然後我要開始長這個 tree 
這個時候因為這裡面它可能是所有的這些跟所有的這些都有影響嘛
因此呢它這裡面的這個時候它的 distribution 是非常複雜的
現在不再是我們剛才那邊畫的那麼單純
只有一個 dimension 說有 m 個不是那樣子
現在是三十九維
所以是非常複雜的一堆
尤其現在是因為各種狀況它可以接它可以接各種各樣
以這個為例我們如果放它說它是最後一個 state 的話
最後一個 state 顯然是比較受到後面的影響
後面有各種各樣的的 phone 都在這裡
所以它就會各種各樣
所以呢雖然說是這三十九維這裡面是非常複雜的 distribution 一一團亂
那我希望能夠把它變成一個 tree 之後
到最後能夠不要那麼亂
當然我不可能做到說像這邊純度那麼高那麼純
大概是做不到
但是我至少要能夠做到就是比較清楚一點譬如說呢
這個是這樣子的
那這個呢是另外一個樣子的
這個是這樣子的
它們各自不像這邊這麼一團亂
而至少呢都有一定的純度
而可能譬如說某些個 TRI PHONE 就用這個
某些個 TRI PHONE 就用這個這樣子 
OK 我的目的是這樣子
也就是說因為不可能每一個 TRI PHONE 都會有夠多的 data 去 train 
所以顯然是是有幾個要共用這一個
那它們的 data 都拿來 train 成這一個
那另外有幾個要 train 成這個
它們 data 要拿來變成這個
那問題是到底哪些個可以該兜在這裡共用 data 共用共同 train 一個
哪些個該在這裡共用這個呢
那我就看誰跟誰比較像
這麼複雜三十九維的我怎麼知道誰跟誰比較像呢
就用這個辦法
就去看這個 entropy 由這邊慢慢變到這邊來怎麼樣子 entropy 降到最低
就可以假設這裡面是最純的
這個時候如果有三個 TRI PHONE 共用這個的話
那至少這三個 TRI PHONE 是蠻像所以它們兜在一起純度還是蠻高的
有兩個 TRI PHONE 共用這個的話
它們它們兜在一起純度還是蠻高的
表示它們應該是像的 
OK 所以我的目的是這樣所以我要把這個三十九維的東西這樣這樣子把它把它這個這樣做下來
那我每一次到底用什麼 criterion 
那麼我們剛才在這裡的時候是用一堆 criterion 來判斷
然後來判斷哪些人的身高怎樣
那我現在到底是用什麼東西來判斷呢
那其實這個這個這想的非常好的辦法就是所謂的 phonetic  knowledge 
什麼是 phonetic  knowledge 就是去找語言學家
根據語言學家來說哪些東西會影響我的前後
哪些呢我們舉底下這個例子你比較了解
這是一個這是一個我們從前做中文的做國語的例子啦哦
就是假設我是要做ㄅ的第一個 state 
這是講ㄅ的第一個 state
啊 我這個例子不太好不一致
就是這是ㄅ的第一個 state 
這晡後面接ㄨ的後面接ㄨ的那個ㄅ的第一個 state 
這是一個 Hidden  Markov  model 的這第一個 state 
我把所有的這個ㄅ的 data 拿來我要用它來做一棵 tree 
這樣子那這個時候我用什麼樣的 criterion 
這都是從語言語言學裡面學來的
因為既然是這樣的話呢最主要的問題是這是ㄅ的第一個 state 
最影響它的是它到底左邊接什麼
左邊接什麼才是一個關鍵
那左邊的接什麼是會什麼呢
會影響的是語言學上的發音的道理
所以它左邊是一個母音嗎
還是子音
如果左邊是母音的話
那麼母音又可以分成這什麼什麼這是語音學上的分法
這是根據語音學裡面 你那個母音發音的時候 它的 發音的位置或者共振的位置或者什麼
它是在後面一點還是在低一點還是在遠一點這口這什麼什麼等等哦
那這些東西 really 就會影響
那你可以想得到就是說我的我我如果說是這個ㄅ的最左邊的這個這個ㄅ的第一個 state 的話會影響它其實最主要是左邊的
那麼它的左邊會是一哪怎麼樣的一種音
是母音還是子音是哪一種母音哪一種子音
然後那種母音那種子音它的口型會怎樣
它發聲會怎樣
那些 really 影響到這裡
所以呢它就去找所有的這種語言學上的這些區別
用這個來做這些個 question 
所以譬如說在這裡的話呢你先問什麼呢
它左邊是不是母音
如果左邊是母音的話就歸到這裡來
左邊不是母音的話呢那就是 silence 
那因為在在國語而言
這個ㄅ什麼東西前面不會有不是母音的嘛
那如果不是母音的話它它就是就是沒有就是 silence 
這就會變成這樣子
然後如如果左邊是母音的話那再來看三十說左邊是不是一個 low  vow 
如果是的話大概是屬於這幾個
如果不是的話是屬於這些
那這樣子一路再分下去
那麼看它左邊是什麼右邊什麼再看它是是屬於這個還是這個
於是分到最後就會發現
譬如說這兩個可以共用同樣一套
因為它們走到最後它的 distribution 是像的
那這兩個可以共用一套
因為走到最後它們是像的
那這個呢它不跟別人搞在一起它就自己一個 嗯
那它這個自己一個它跟別人那個等等
那這幾個都會都會很像就跟它們搞在一起哦等等
差不多是這樣的意思
那麼因此呢這樣你比較容易想像
那這樣子總共大概這個原來是最早是它們做英文
大概用了兩三百個 question 哦
就是說完全就是根據根據這個語言學家語音學的知識去分
左邊會是什麼右邊會是什麼
左邊有沒有什麼右邊有沒有什麼等等
這個語言學大概有兩三百個 question 
然後你就從這個一路走下來
那麼根據到底要從哪一個開始問起
就是看你問的是哪一個 question 我的 entropy 降得最多我就用那個 question 然後這樣一路走下來
那在這些 question 裡面其實都是比較簡單的
你如果仔細看的話這些 question 就是 yes 還是 no 
嗯是 yes 還是 no 所以是比較簡單
嗯是 yes 還是 no 所以呢只是說根據語言學的知識它它是不是什麼東西
嗯你如果是前面那個式子其實還更難
譬如說十二歲為什麼是十二歲
那其實你也可以是十三十四十五十六都可以嘛
十九八七六都可以嘛
所以你還要再選那個 threshold 是多少
像這裡你也是要選 threshold 是多少
嗯 那在這個例子反而不需要
因為在我們真正做的時候其實不需要
因為其實都是在根據就是不是 threshold 
只是在看說
你是不是哪一種具有哪一種左邊右邊是不是有哪一種特性的子音或者母音等等之類的東西
那這樣總共大概有兩三百個 question 
然後就用這個來做這個選擇
於是你可以想到這件事情其實是嗯它是 both  data  driven 跟 linguist  knowledge  driven 
它靠兩樣東西同時在操作
第一個是語言學的知識
因為你可以想像會影響它的 TRI PHONE 
左邊右邊會影響這個東西的其實是在它的左邊跟右邊是接在哪一種音
然後那個音的發音會怎樣
所以呢我就用我的 linguist  knowledge 來做我的 question  set 
然後一路來選
所以這個是 driven  by  linguist  knowledge 
但是另一個另一方面我是 driven  by  data 
因為根據現在我把這堆東西都都拿來的時候它是這麼亂的
然後我到底用哪一個 question 可以把它拆得比較清楚
我是根據 data  driven 
嗯所以根據這個算它的這個 entropy 來算
這是我在算它的 data  driven 
所以呢它是同時 data  driven 跟 linguist  knowledge 
我一面用人的知道一面用 data 這兩樣組合來建一個這樣的 tree 
那於是呢我這個 tree 呢就從頭相 開始向下走
那我用所有的 available  data 
就是譬如說我現在要 train 這個 
train 這個啊的最後這個 state 的所有 data 我都拿來
然後就開始用它的這個長像非常亂的一個長相開始來算它的 entropy 
然後根據這些 question 來看它怎麼怎麼建這棵 tree 
那麼然後呢我一路長這個 tree 呢
當然我最後我要有一個這個停止長 tree 的一個停止再 split 下去的一個 criterion 
分到哪裡應該要停住呢
那當然你可以想到第一個就是entropy  reduction 不小到一個程度對不對
我在哪兒再分下去 entropy 沒有沒有再變小的話就不必要再分了
entropy 的 reduction 小到一個程度就不必了
另外呢就是我的 data 量少到一個程度也不必了
對你可以 define 我這個 data 量嘛 對不對
我的 sample 如果少到一個程度
已經不足以 train 出一個東西來當然也就不要了
所以呢你可以這裡用這個方式來 define 你的這個這個這個 stop  criterion 
於是呢這棵 tree 會長到哪裡
在什麼地方停住
在什麼地方停住是 depends  on 在那個地方的狀況
看它的 data 的量看它還會不會再降低 entropy 等等
於是呢當你這個 tree 長好之後
所有的 unseen  TRI PHONE 你就從頭開始
這個這個延著這個 tree 向下走就好了
走到哪裡就歸誰
我我我現在這個長的這個這邊都是看到的 data 
我這個 training  data 看到哪些
我把看到的 data 拿來放在這裡 然後讓它一路長下來對不對
我讓一路長長完這都看到的
那還有一堆一大堆沒有看到我們剛才講有一半的 TRI PHONE 沒有看到怎麼辦
沒有看到的譬如說這個左邊要這個右邊要這個
嗯就是沒有
沒有怎麼辦
我就讓它從這上面往下走
那每一個地方因為它都它它它的問題都是說我是左邊是怎麼樣的右邊這個都是語言學的知識
所以我可以根據語言的知道來判斷它應該往哪邊走
所以我就可以
雖然沒有看到沒有 data 的 TRI PHONE 
我完全根據這棵 tree 上面的語言學的知識
就走走到它該走的地方
最後就是這個 traversal across tree  by  answering  the  questions  leading  to  the  most  appropriate  state  distribution 
你凡是沒有的
沒有看到的你就你就根據它左邊跟右邊的語言學知識來走
走到哪裡就歸那裡
你就用那個 data 當成那個 TRI PHONE 
就這樣子
所以呢這就是所有的 unseen  TRI PHONE 都有位置都有 model 的個辦法
嗯 那麼嗯這樣子做的話呢
我的 Gaussian  mixture 最後呢就是
哦 就是凡是有相同的 linguist  property 的就會 tie  together  sharing  the  same  data 然後 same  parameter 
就是說你你到到時候這一堆會會長在一起嘛
就像譬如說這兩個就會長在一起
那沒有什麼理由
就是一方面就是我我如果這樣一路走過來的話
表示說它們左邊右邊的這些音
它們的這個 linguist 特性就是很像的
那一方面呢那如果是這樣的話呢那是有理由說是這個
一方面你也等於是說它們的純度是最純的
或者說它們本來就長得像
那那那麼因此呢這麼一來的話呢
那麼這些個 Gaussian 就是所謂的 tie  together 
所謂 tie  together 就是我用同樣的 training  data 最後 train 出這一組來
最後 train 出一組來
那麼這個時候呢就是所謂的 tie  together 的意思
那麼因此呢它們就是用相同的 training  data 來 train 
所以是 sharing 用相最後就用相同的 parameter 啊
這就是 sharing 的意思
所以這樣的時候到到了最後 leaf  node 
到了 leaf  node 這裡的每一每一組 Gaussian 
它們都是一組大家一起 share 的
那麼這是這個一個非常簡單的解釋
那其實要做這個是很有學問
怎麼樣 train 得好還是有很多很多進一步的問題
舉一個例子來講 
tree  PRUNING 就是說你這個 tree 
你有的時候長長得太長得太茂密了
可能最後分得太細不見得好的時候
你有一些 criterion 把它砍掉一點
把它砍掉一點讓它這個比較不要那麼茂密
麼可能效果會更好
這是所謂的 tree  PRUNING 
另外呢你的 question 也可以是所謂的 composite  question 
所謂 composite  question 就是像這樣子嘛
假設你可以把這個這個是 and 這個是 or 嘛
嗯你可以把左邊又是這樣右邊呢又不能那樣這有一個 這個 bar就是否定嘛 對不對
左邊要這樣右邊不能那樣這是 and 
然後呢或者是嗯左邊是這樣這都可以
你這樣就變成一個 composite  question 
那你也可以用這類方式來做
所以這中間的學問還有很多
那我想我們這邊並沒有詳細的講
我這邊只是把一些基本的原理大概我們大概解釋
大概是這樣一回事
那這個 TRI PHONE 的這樣的方式的的 train 是最重要的主流
也就是說我們今天絕大多數的最成功的系統都是用這個方法來 train 的啊
那這個嗯到這裡為止我們大致把這個最主要的 HMM 的 training 的大概都已經說完
那麼我們其實沒有說得很清楚
我們大概只是選擇裡面幾個重要部分把它說得清楚一點
然後還有很多中間有一些地方是沒有說清楚的
那不過嗯我想應該 OK 
我們在後面大概下週或者下下週會給各位做第一題習題
第一習題就就是在做這些事情
那這個習題倒是這你不用擔心這個程式要寫起來不得了
不用你寫因為都有現成的 toolkit 哦
所以你只要是用這個現成這個去上網 download 下來
然後這個主要的那個怎麼操作那個 toolkit 
那麼大部分的的助教都已經告訴你怎麼做
所以不會太難做
你如果認真得去做那一次的話
大概就會了解中間所有東西
現在看是還有很多問題因為很多功能其實沒有講得那麼清楚
因為真的要講清楚是是太複雜
那我覺得我們也不太可能真的把每樣都講得那麼清楚
但是後面就會給你一個這個習題
你如果認真去做一次你大概就會知道裡面所有東西 
OK 好我們今天就講到這裡
ok 我們補課的時候在講的事情就是
怎麼樣做這個tri phone 的training 
那麼做tri phone 最大的問題就是有一堆unseen event 我們說過就是因為有很多個unseen 的tri phone
你所需要的很多東西其實根本就沒有data
所以呢必須要跟別人去share 
那麼share 的辦法呢就是用我們這邊講的這個這個tree 的結構來做這件事情
而這個tree 的發展的這個過程之中呢我們就是讓它每一步都找一個有最大的這個entropy 的變化的那個地方的那個question 來分這個tree 等等
那這個基本原則是來自我們前面講的這個information theory 裡面的entropy 等等
那這部分詳細的這原始paper 是這一篇
所以你如果想詳細地看可以看這篇
那在這本課本裡面這一段其實也在講這件事情是一樣的
那所以呢你這邊都這邊都可以看得到
那前面這邊它是在講一些關於我們這邊說的一些嗯像是這個phoneme 啦
像這些個co articulation 等等的現象它有一些說明所以都是蠻不錯的一些reference
是可以參考的
那麼到這裡為止呢我們大概把嗯所有的h m m 怎麼train 
然後這些東西我們等於是講完一次
那這些東西其那你如果仔細想一想我們其實從第三四點零開始
整個的h m m training
從頭到尾其實是一個非常複雜的過程
中間很多東西譬如說我們講講這個basic problem 三是在講這個用用這個iterative forward backward algorithm 讓它作微調能夠train 得更好
我們後面有講另外一個是segmental k means 是其實怎麼做initialization 等等
那我們這邊講的tri phone 是另外一塊
那我們並沒有真的足夠的時間把整個都講那麼清楚
我等於只是挑裡面的幾塊把它說清楚而已
那你自己可以去想中間怎麼link 起來
那有些地方沒有說得很清楚
那麼我們後面會給各位一個習題
就是把這個東西做一次
那嗯你不用擔心如果這個習題如果這個這個程式如果你自己寫的話會寫死人
這個這個程式複雜到難以想像的程度
不過現在都沒什麼問題因為我們都有現成的工具
所以其實你只要用那個那個工具都可以download 下來
自己上網就可以download 下來然後你就自己可以train 
那嗯所以呢我不曉得今天助教會不會準備好如果準備好的話今天就會給你那個習題
那這個你嗯你只要從頭到尾仔細得走一次
那它的那一整套的htk 的程式就是所謂的我們給你用的htk 是今天國際語音界最普遍使用的一套程式就是h m m toolkit 
那它有一整套的manual 
很厚你如果把它印出來是很厚一本畫一樣
它詳細說裡面所有東西怎麼樣怎麼樣怎麼樣
你如果真的下工夫的話你想要真的弄清楚的話仔細走一遍
裡面所有東西
就其實是可以很清楚裡面所有程序
如果你沒那麼有興趣的話就照我們習題裡面給你的那些scrip 走一次
大概也會了解中間的程序
那嗯應該是這樣講就是說
我們今天在做這樣的事情的時候
那麼如果打個比喻的話好像是我們要蓋一棟房子
那麼當我們蓋一棟房子的時候
嗯如果說是譬如說我們需要有冷氣
我們就去冷氣行裡挑一個我們需要的冷氣放在這裡就拿來用了
我們需要一個爐子我就去爐子的店去買一個爐子來放就可以用了
我不需要從頭去
自己去做冷氣怎麼做把它做好然後爐子怎麼做把它做好其實是不需要
我們做的事情是蓋房子
因此呢像這類基本的程式
嗯我們可以不要自己寫我們都不要自己寫
然後都用現成的
然後這個但是我們要做的事情是要如何把這些現成的工具兜起來做到我們想做的事情
所以這是我們現在的工作是是這樣子
這個跟十多年前是不一樣的十多年前我們做語音的時候因為沒有這種東西所以我們每個學生都要自己寫一套h m m 的程式
那時候很累很累那現在都不需要了這是我們要做的事在不同就是了
那嗯這個htk 的這個程式是這個原始程式是英國的劍橋大學的學生寫的
英國的劍橋他們在九零年代的時候開始很認真地做hidden markov model 
那麼嗯他們覺得做得不錯他們也就去參加美國的這個比賽
結果一比美國人看不起他們覺得他們一定不會
美國人認為h m m 是他們發明的
認為英國人一定很爛一定不會
結果一比果然是最後一名
嗯結果大家就笑說啊英國人實在很爛
結果英國人回去就很認真地從頭寫這個程式
第二年再去比賽就是第一名
打敗美國所有的團隊
包括i b m 啊包括這些a t and t 全部都被打敗
然後它變成第一名
從那個以後這個嗯美國人也就承認英國人這方面最厲害
所以後來他們就把他們的程式變成一套變成一套軟體
當時開了一家公司是賣這套軟體的很貴
當時htk 一套大概是換算成新台弊是幾十萬才買得到
那後來到了九零年代的末期的時候
這個微軟把這家公司買下來
然後就把這個把它這個軟體放在網路上讓大家使用
所以後來這個程式就變成大家都可以用
那我們用的就是這套
那這個那當然事實上微軟放在網路上的是htk 裡面的最基本的版本
是最基本的版本所以這個嗯不是最好的版本
最好的最有效率最快的真正可以拿去賣錢的可以這個的
那一套它其實留著沒有放在網站上
那放在網站上的是效率不太好的
但是是夠夠用的就是了
所以呢這個雖然慢效率不好
但是是是夠豐富所有我們想要做的事情都可以做
所以我們也用這個來做為這個嗯習題的教材
所以如果either 是本週or 是下週
但是因為發現下週又放假了我們實在是很頭大
好容易才把進度補起來
下週又放假了所以呢我比較希望今天如果助教趕得出來就今天把這個習題給你們
ok 好底下我們利用這個機會稍微說一下國語的部分
那這個我們不要多說大概稍微提一下
我們基本上我們說在這門課我們講的所有東西我們都假設是
英文或者是language independent 的東西
但是我們碰到跟中文有關我們稍微提一下
那麼在中文而言那麼我們中文是一字一音的結構
我們的每一個字都是一個所謂的一音是什麼就是一個syllable
一個syllable 也就是裡面有一個母音
那我們總共有多少個syllable 
這個大約一千三百多個
這個括號裡面的數字就是總數
但是這個總數不是很精確
因為不同的字典不同的人說法都不太一樣
因為永遠存在一些音是
有的人認為有的字典算進去有的字典不算
譬如說油捱這類的音
倒底算不算呢不一定
所以呢就會多一點少一點
不過我們這個syllable 的總數
大概是一千三百多個
那麼這裡面呢我們可以分成有四聲的tone 就是我們的聲調
跟沒有四聲的
所謂的沒有四聲不是說沒有四聲是說我們的每一個音都有聲調
但是如果你不記聲調的話是四百多個
也就是說我們有八拔把爸吧
五個對不對
八拔把爸吧
你如果當成五個的話
那這個總數是一千三百多個
你如果只當成是一個
只是它的音調有變化當成一個的話
那大概就是四百多個
這個數字也不一定
最多大概人家講四百一十六個
所以就是四百多個
那另外呢我們有所謂的聲調
聲調就是這四加一四就是這四聲
一呢就是輕聲
那為什麼不是五呢
在語言學上來講輕聲跟四聲不一樣
那四聲是各有一個固定的pattern
輕聲是沒有
就是說你譬如說第一聲是這樣第二聲是這樣你知道第三聲
第三聲是這有所謂全晌是這樣
半晌是只有到這裡
第四聲是這樣
那所以呢它的這個這個四聲是有固定的pattern
至少長一定的長相
輕聲基本上呢被認為是沒有長相的
也就是說depends 它在什麼地方它會不一樣
根據它們的標準的語言學的說法
那麼輕聲最主要是depends on 它前面是第幾聲
它就會不同
所以來吧去吧走吧吃吧
你的這個前面是第幾聲你的那個吧就是不一樣的等等
所以呢輕聲在它們語言學上叫做neutral tone
這個neutral 的意思是中立嘛對不對
也就是它不屬於這四個裡面的任何一個
它是跟著前面的人走的
所以是叫做neutral tone
所以我們把它說成四加一而不是五
然後呢這個那麼所謂的base syllable
前面加一個base 的意思就是不計聲調
的時候叫做base syllable 
那每一個base syllable 呢我們又可以分成這個我們提過了就是聲母跟韻母
那麼聲母其實就是子音ㄅㄆㄇㄈㄉㄊㄋㄌㄍㄎㄏ這種都是聲母
它們都是子音
總數是二十一個
這倒是確定的
不過呢有的時候有的人說是二十二個
那是因為還有一個所謂的空聲母
因為我們有一些音是不要聲母的
譬如說愛
這是沒有聲母的
你如果是敗的話
就是有一個ㄅ就是有聲母的
它可以沒有聲母
那這種情形稱之為空聲母
如果把那種情形也算進去就是二十二個
那麼韻母的話呢
這個一般而言韻母的核心部分所謂的nuclear
是它的最重要的部分
前面跟後面的這兩個東西是所謂的optional
可以有可以沒有
什麼叫做medial 呢
medial 是我們有一些音是三個注音符號拼出來的
那三個注音符號拼出來的中間那一個注音符號叫做medial
那有三個就是ㄧㄨㄩ
那你知道一的話我可以由譬如說ㄒ一ㄝ些
ㄨ的話有ㄉㄨㄢ斷譬如說
有什麼ㄐㄩ捐譬如說這樣子
那中間的這三個ㄧㄨㄩ叫做medial
所以我們的medial 總共有三個
那我說它是optional 是說有很多音它沒有這個的嘛
我那這些其實它自己可以做為自己可以做為nuclear
譬如說我也可以ㄉㄨ都對不對
那這個ㄨ就不是medial
這個ㄨ是屬於nuclear
那我也可以ㄅ一必
ㄅ一必的話這個一也是屬於nuclear
也不是屬於medial
medial 只有在這三個注音符號的中間的時候這叫做medial
所以我們有三個是medial
那ending 是什麼呢
我們有兩個ending 
那就是n 跟n g
那麼n 跟n g 出現在哪裡
出現在我們注意符號的ㄣ跟ㄥ或者是ㄢ跟ㄤ
那ㄣ跟ㄥ你知道所謂的ㄣ跟ㄥ其實就是前面接一個ㄜ嘛
如果我前面接一個ㄜ然後再接ㄜ
應該說是這前面接一個ㄜ的話
ㄜㄣ就是這個ㄣ
那麼ㄜㄥ就是這個ㄥ
那如果我前面不是接ㄜ我前面是接ㄚ的話呢
ㄚㄢ就是ㄢ
接ㄚㄥ就是ㄥ
所以我們其實是有這四個韻母
這四個音它裡面是帶著這兩種ending 的
那麼這兩種ending 
所以呢我們總共有兩種ending
那也是只有這四個case 
才有的跟這三個case 才有的
所以這個三個medial 跟兩個ending 是optional 
不見得有
那麼這個其它有所謂的nuclear 
那這樣加起來的總數
總共有十二種不同的phoneme 在裡面
那你說加起來超過十二嘛沒有錯超過十二因為有的是重覆出現
譬如說這個ㄨ
也算medial 
也算nuclear 
這個一也是在這邊也算在這邊也算等等
所以加起來是超過十二種
那我們總共有十二種
那這裡面呢絕大部分都是母音
只有什麼case 是子音呢
就是這兩個ending 
這兩個ending 是所謂的鼻音
這是na nasal ending 
這是鼻音
鼻音其實是子音
所以這兩個是子音等等
那這些東西就是所謂的phone
那這裡面也有重覆
所以呢加起來大約是三十出頭差不多是這樣子
這是我們的所有的音
那當你有了這些音的時候我們也可以用這些東西做單位
來拼我們的tri phone 
所以我們可以做我們的tri phone 
那同理我們也可以做其它的東西
因為今天我們如果是要做國語的話
嗯你可以用的你當然可以用phone 來做然後可以做tri phone
那這裡面問題就是說到底什麼單位叫做phone
那麼其實不同的語言學家有一些不同的說法
那麼因此呢它們有另外一個名詞叫做phone like units
也就是說呢這個我不太能夠define 什麼是phone 或者phoneme 
那我乾脆就自己define 一堆東西然後就說就跟phoneme 很像就對了
那麼我們舉例來講呢我們剛才講的這個這個線
那你如果真的要拼的話可能應該是c e a n 
這四個這四個phone 
先
但是不同的人也許有一些不同的意見
他認為不是這樣變成只有三個也不是說不可以哦
所以這個沒有一定
不過這個基本上這就是所謂的phone 跟phone like unit 因為有的文獻裡面
嗯其實在西方語言或者是其它各種奇奇怪怪的語言你知道全世界有幾千種語言
所以各種語言裡面到底以哪些做單位其實並不見得是有一致的意見
我們隨便舉個例子
你拿一個什麼土耳其話你會一個什麼突厥語什麼的
那它們到底怎麼哪些單位其實很難講的
所以呢有所謂的phone like units
所以你可以用這些單位來做tri phone 
　那你也可以用聲母韻母來做
那通常我們嗯比較喜歡用這個
因為這個數目這個比較簡單
因為聲母韻母其實非常清楚
我們上次說過
所謂聲母韻母就是你總之第一個是聲母
後面兩個後面的兩個或者一個叫做韻母
那麼所以聲母韻母是比較容易切得清楚的
那麼聲母韻母也是一個可以用的單位
當你選擇好單位之後這是你選擇單位
當你選擇好單位之後
你要用怎麼樣子的context dependency
那我們說過就是你可以只看左邊也可以只看右邊
那就是right context dependent 和left context dependent 
或者兩個都看
那就是像tri phone 就是左右兩邊都看
那麼我們後我這邊只有寫只看右邊而沒有只看左邊的是因為我們做後來做過實驗都知道
你如果只看一邊的話看右邊比較好
看左邊比較差
那為什麼呢你也可以猜得出來是因為
其實我們的音受右邊的影響比受左邊的影響來的大
那麼譬如說ㄅ一必跟ㄅㄨ不
這兩個ㄅ顯然不一樣
是因為它的右邊或者後面的那個逼或者是逋
那個逋就不一樣了
可是你前面是譬如說這個前面是逋必
還是前面是這個嗯大逋
前面是啊還是嗚對它的影響是比較小
不是沒有
就是說前面對它的影響是比較小
這個不同對它的影響比較小
後面這個不同對它影響是很明顯的大
那麼因為這樣的關係所以是右邊的context 是顯然比左邊重要
你如果只算一邊的話我們通常喜歡算右邊
是有這個原因的
那麼或者當然比較好是兩邊都算嘛
那還有呢就是intra syllable 跟跟這個intra plus inter
也就是我們一個syllable 裡面
你譬如說這個早上
你的這個早裡面的ㄗ跟ㄠ之間的影響
這是intra syllable 
那麼這個上
如果是這個ㄠ影響到ㄕ的話這個影響是跨越了一個syllable 的邊界的
跨越了syllable 邊界的影響我們稱為這個inter syllable ok 
所以呢你一種情形是說我只算我只算syllable 以內的
我讓它它影響它它影響它那
它影響它是右邊的影響那它影響它是左邊的影響
你可以只在這個裡面做
如果只在syllable boundary 裡面做的話呢
那這種的context dependency 是所謂的這個intra syllable only
那當然比較好的是說我把這個都考慮進去
我也讓它影響跨過syllable boundary
但是如果是這樣的話當然你的影響就更複雜
所以呢總之呢你可以有不同程度的context dependency 
那這個就影響到你後面真正你要train 的tri phone 或者什麼東西它的總數會有多少
然後那些因為如果你某一些不算的話就會比較少嘛
那你如果都算的話就會很多那你就會有更多unseen 的
那更多unseen 的話你就更需要靠一些東西來補
等於是這樣意思
那在這個情形下就有很多種做法哦
那我們這個是隨便舉例
不一定只有這些還有很多
舉例來講我們有所謂二十二個聲母我剛才有講過
我們前面講的是二十一個嘛對不對
這二十一個
那為什麼變成二十二個
就是把空聲母加進去是二十二個
那麼一種最常做的做法就把二十二個聲母把它extend 變成一百一十三個
right context dependent 的聲母
那也就是說後面接譬如說這兩個就是不同的ㄅ嘛
那這個接一的ㄅ跟接ㄨ的ㄅ是不一樣的
所以呢這個是逋的那個ㄅ
這個是一的那個ㄅ等等
那這樣就一堆不同的ㄅ
ㄆ有一堆不同的ㄆ等等
那這樣一來呢我可以總共變成一百一十三個
那這是一個簡單的做法
我也可以用phone 來做
那麼倒底有多少個phone 呢這個也不一定
那麼一個做法大約三十多個啦
那三十多個其實是屬於剛才這邊你把這堆加起來差不多是這樣的數字
那這三十多個裡面呢
我如果只算intra syllable 的right context dependent 的話
大概可以變成一百四十五個左右
你如果把intra 跟inter 都算進去
那就大概是四百多個將近五百個
那也有另外一個辦法是說你syllable 跟syllable 之間是韻母跟聲母去銜接嘛對不對
譬如說早上
像早上這個case 是說
這個ㄠ後面要接這個ㄕ嘛
那你這個是所以inter syllable 之間的銜接的context dependency
你可以看成是在這個syllable boundary 的前面的韻母影響後面的聲母
那如果是這樣的話呢
我們這也是當時曾經做過一種辦法就是你把這個你把這個韻母分根據它們的結尾的音分成十二個group 
有些韻母的結尾其實是差不多的
譬如說這個彎是一個韻母這個這個嗯
單的ㄢ也是一個韻母
這兩個韻母其實是很像的嘛等等
所以呢那這兩個韻母的結尾可以算是相同的
我們就可以用同一個
就像我們剛才那邊講的另外一個case 是譬如說結尾都是這個ㄣ
那一個是ㄣ一個是ㄢ
ㄣ跟ㄢ的結尾都是ㄣ所以我就算它是同一個group 等等之類的
那如果是這樣子的話呢
那我韻母可以根據它的ending phoneme 
分成十二個group 
那聲母可以根據它開頭的那個發音的情形
那分成七個group 這樣子的話它們之間的銜接的狀況可以分成八十四個八十四種
用這樣子來分群這我們當時也做過這樣等等
所以這個是有很多種變化depends on 你要怎麼做
那當然最複雜的情形是做tri phone
這個也是效果最好的一種
那麼我們如果把這些tri phone 通通算進去的話的話呢
國語的tri phone 沒有那麼多
因為不是所有的組合都會出現
那麼這樣算起來會出現的組合大概tri phone 大概是四千六百多個
那我們上次提到過
我們要train 這四千六百多個tri phone 的話你拿一個十多小時的data base 來數一數
會發現大概只有一半有只有一半
有聲音另外一半根本不在data 沒有data 等等
那就是要用那個decision tree 來做的
那我想剛才也許漏掉一件事情是說
韻母有三十七個怎麼這麼多
注音符號我們總共才差不多這麼多嘛
沒有錯
我們的韻母有很多是所謂的這個結合韻母
就是像這種嘛
像這個的話
耶啊淵啊這種都是這種韻母很多嘛
所以呢你把這些東西統統算進去的話
ㄦ這種都是韻母
那這些東西統統加起來我們是有這麼多個
好以上大概是一個簡單的介紹
那這個是很多年以前我們做的實驗我想只是一個例子
說明說你用不同的unit 
然後你用不同的給它不同的context dependency 的話
可以得到不同的結果
所以呢這depends on 這個非常depends on 你用什麼data 去什麼database 去train 它
然後你用什麼database 去測試它
就會得到不同結果
所以這個你不需要把它看成是一個絕對的正確數字
而只是一個相對高低的參考
那麼用不同的data 去train
用不同的data 去測試就會不一樣
那麼我們如果是本週或是下週給你那個習題也是一樣
那個習題也是只是一個測試
那麼這個不同的狀況得到的答案就不一樣
那麼在這個情形而言
這邊的好像正確率很低才三十四十五十六十
那其實也是嗯包括另一個重要的原因是
我們沒有用辭典也沒有用language model
那我們知道是你加了辭典之後
辭典會告訴我哪些音才會兜成哪些字
什麼詞
所以這些音兜起來沒有一個詞的話這些音就不會存在嘛就一定是不對的嘛
所以辭典是有助於找到正確的音
同樣的language model 有助於連哪些詞連起來
那如果沒有language model 那當然也少了很多知識
所以我們這個是沒有用辭典也沒有用language model
純粹就是辨識這個syllable 對不對
所以正確率會比較低
那同樣的我們給你的習題也是這樣子
就是只是辨識syllable 完全就是看那個h m m 辨識syllable 的情形
那所以比較低是嗯ok 的
那在這裡我們可以只是簡單看一下它們之間的這個相對的高低的關係
譬如說這個呢就是我只用phone 為單位
c i 就是context independent 
我不考慮所有的context independency 的話
那它只有最低
這是最低的
我同樣的情形也是context independent 
可是我用聲母韻母為單位
立刻就高了不少
那從這個觀點來講也就是說這個這個聲母韻母來自古典的聲韻學
那麼這個古代的人他們就有所謂的聲韻學
他們就分析這個中文的聲音的時候他們就是用聲母韻母是有他的道理的
那這樣子我的總數其實這個總數其實還多一點
但是事實上這個結構可能還更清楚
更清楚的分這個每一個syllable 有所謂的聲母韻母這樣的關係的話呢
似乎是一個更好的單位
所以呢你可以看到它它這個我同樣都是context independent 
那麼用phone 的話呢低的多
用聲母韻母if 就是initial final 
我馬上就高了很多
那然後呢我如果把context dependency 考慮進去的話
我這兩個是跟這個來比
我如果還是用phone 為單位
但是我呢讓它左邊有context dependency 
還是讓它右邊有context dependency 
那你馬上看到呢你看左邊就是比右邊低嘛
不過不管左邊右邊都比不做要高很多嘛
所以你是context independent 差嘛
你加了一邊的context dependency 就高了很多
那這裡面的右邊加得比左邊多嘛
這就是我們剛才講的就是這個右邊影響你的右邊影響你的音厲害影響得大
前面影響比較小後面影響比較大
那所以我們如果只算一邊
大概都只算右邊那等等
那麼然後呢這邊我所謂的inter
加了inter 兩個字就是指做了inter syllable 的
做了inter syllable 的context dependency
那就是所謂的加了inter 的
那如果沒有寫就是用phone 為單位
那麼因此你可以看到呢這邊用了inter 之後我都會更高一點
那最高是什麼是後面這一群裡面的最後那幾個
那這一群都是tri phone
那麼tri phone 其實是有很多種不同的
你如果詳細講它train 的過程裡面有一些不同的做法
會不太一樣
那這個細節我們都沒有說你如果有興趣的話自己可以去查文獻可以看得到
那tri phone 這有不同的做法
那最好的是在這裡tri phone
那麼這個的話呢它是六十一在這個case 比這個三十一幾乎高了一倍
所以這是tri phone 是turn out是確實是最有效是沒有錯
就是我們這邊講的
tri phone 永遠是比較好
而且呢是這個嗯你有加inter syllable 都會比較好
然後呢這個但是你倒底怎麼train tri phone 是有是重要性
depends on 你怎麼做
你做到最好就會最好
差不多是這樣
