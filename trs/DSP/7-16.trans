我 做到這邊之後我現在再transform 回去到這邊來
那這是inverse discrete fourier transform 
那你本來transform 回來應該是得到相同點數
你這邊有m 個點
這邊應該得到了 得到m 個點才對
那我們通常會丟掉最後的這些點
保留比較少的點
這個為什麼我們底下也會解釋
那是為什麼我們後來這邊只有十三個嘛
十二或者十三個
那本來這邊就是二 二十三或者二十六個
二十多個為什麼變成十多個
其實我們丟掉了一些 阿
那丟掉的原因我們待會會解釋
那同樣呢 我原來這個地方的是 frequency 
我再transform 回來是什麼呢
那個是 這我們這邊叫做j 
叫做quefrency 
那這個東西其實 阿 是一個很奇怪的東西
那麼你可以想像我如果沒有做這個log 
沒有做這個的話
其實transform 回去就是time domain 
就是你本來是transform 過來inverse transform 回去
應該就是time 才對
但是因為我現在在上面做了一堆奇怪的東西我又取了絕對值又取了log 又平方什麼東西
之後這個東西已經不是原來的frequency domain 的東西了
是一種 而且你看 我是這樣子一個一個 嗯 是另外一種東西
所以transform 回回去的時候呢
這個其實不是原來那個time 
是另外一種東西很像time 但不是time 的東西
那當時的 發明這個的人
它就自己取個名子他說我不知道如何取它
我就把frequency 的四個字母倒過來 阿
其實這個就是原來的frequency 
這個f r e q u e 
把這兩個字母倒過來
他就叫做quefrency 
那它只是這樣的意思
那這樣過來之後的這個東西呢
那他也取另外一個名子
那本來的這個frequency domain 這個東西我們有一個名子叫做spectrum 
這個字spectrum 
我們中文通常翻做頻譜
就是指我的訊號在頻率上分佈的情形
這是所謂的頻譜
那當然你這樣轉回去的時候 這又不曉得是什麼東西
它已經取了一個很奇怪的名子叫做quefrency 了
那這東西它也取了一個名子呢
它也一樣 就是把它就叫做 cepstrum 
那這所謂的cepstrum 也也不過就是這前面這四個字把它倒過來
他把前面這四個字母倒過來就變成cepstrum 
那是這個字的由來就是這樣子
那麼 喔 所以我們現在所謂的m f c c 
你如果回憶起來我們當初講m f c c 它的全名是什麼
就是mel frequency cepstral coefficient 
mel frequency cepstral coefficient 
這是它的全名
這是m f c c 的全名
那麼這個 這個字是什麼東西
這個字就是這個字
那它只是 他只是把他字母倒過來而已
那這個東西 當時在十多年前 
我們在想這個字應該怎麼翻成中文
因為spectrum 叫做頻譜嘛
那這個字應該怎麼翻
喔 不知道
阿那曾經有人建議我們就叫他譜頻
我們就叫他譜頻
那這樣的話呢就跟他的原意好像很接近 嗯
那 那 不過當時也有另外一個人有有另外一個翻法叫做倒頻譜
它加個倒字阿
前面加一個倒
那意思是一樣我們把它倒過來就是了啦 阿
那你就為什麼叫做倒其實也就是因為它是 它是字母倒過來的就是了 阿
那麼 不過後來好像用倒頻譜的名的的比較多就是了
所以所以呢 所謂倒頻譜也就是指這個cepstrum 
也就是指這個m f c c 啦 嗯
這個解釋一下這個名子的由來
好 那底下我們現在來說我現在為什麼要做這個inverse d f t 呢
就是做這件事
那麼基本上呢這個 基本上我們在做的事情是inverse 的d f t 
但是其實呢
這個inverse d f t 呢會變成一個discrete cosine transform 
那麼為什麼呢 因為我的log power spectrum 本身是real 而且symmetric 嗯
那關於這一點我想細節我們不說
你如果有學相關的數學那些東西的話就知道他這裡面講的意思
如果沒有學不了解 也就無所謂阿
我們就是了了解這這件事就是了
就是說我這邊做的其實是d inverse d f t 
就是這一個inverse discrete for fourier transform 
就是這個這個轉回去的轉回去的這個過程
但是我其實因為我現在這上面所做的東西
其實是 不是這樣子畫的random 的
而是它有一定的特性的
這個特性就是我們這邊說的他是 real 而且是symmetric 
在這兩個特性之下那個inverse d f t 呢
會變成一個discrete cosine transform 
什麼是discrete cosine transform 呢
跟fourier transform 是很像的
只是我的basis 就直接是cosine 嗯
也就是說我我們這邊講的時候我這邊的每一個東西
都是用 我這邊的每一個
我都都說它其實所代表的是
一的j omega one t 等等
都是這個東西
然後我的transform 呢 也都是把他寫成
譬如說x of t 
e 的minus j omega t d t 我的
積分都是這樣積的
或者說我的summation 都是我剛才寫的這邊的啦 哈
就是x n 
e 的minus j omega n 
summation over n 
就是這種東西
那基本上呢我都是以e 的j 這種東西
作為我的基本的basis 來做這些事情的
那cosine transform 唯一的不同
只是這些東西我都變成cosine 就對了
那其實我們說過這些東西你如如果取實部就就是cosine 嘛
那它就 根本不 這邊就 這邊就直接改成cosine 了
我這邊就直接改成cosine 
我如果直接把cosine 放進去的話
這種東西就是所謂的cosine transform 
那cosine transform 跟這個 跟這個原來的這個fourier transform 之間
是有非常密切的一堆關係的
那我想不在我們這門課要說
你如果有興趣的話在相關的文獻相關的課本修相關的課會學到
那我們這邊不講我這邊只說這句話就是說呢
阿 你去 你去查paper 可以查的到
它可以證明
這個cosine transform 有個很大的特性
就是可以把這個 這個中間的correlation 降到最低
使得你得到的最後得到的東西是highly un correlate 
那麼換句話說你如果做了這個cosine transform 的 的話
你所得到的這些
最後這些東西他們彼此之間 的 幾乎是沒有太多的correlation 
它們幾乎都是 非常independent 的component 
那這樣有什麼好處
這樣最大的好處是
我到後來我的那個gaussian 那個gaussian 裡面
我都可以用對角線的matrix 
這是指什麼
你記得我們的 我們的hidden markov model 裡面 h m m 裡面
譬如說 這個state 裡面是怎樣的
我們說是一堆gaussian 對不對
譬如說這是一個gaussian 這是一個gaussian 這是一個gaussian 
我用一堆gaussian 來來描述說
當我的訊號在這個state 裡面的時候它的distribution 是這樣的
那這每一個gaussian 你如果去寫的話
它是會有什麼東西
e 的minus 什麼東西
那這裡面會有這個 譬如說 這一類的東西
那它有一個covariance matrix 
我現在這個東西是三十九維
所以應該是要有一個三十九維乘三十九維的covariance matrix 
那 那個matrix 非常複雜
那在它的對角線上
是相當於每一個自己的variance 
但是這外面的每一個點呢是他們各個component 彼此之間的correlation 
那麼我現在如果說這些東西都變成highly un correlate 的話呢
那他們等於說你可以想像
我大概比較可以假設他們是zero 
我如果假設他們是zero 的話呢
那就簡單很多 我這邊都是零
於是我這個matrix 只有三十九個參數
你否則要三十九的平方的參數就很多很多 喔
那事實上我們後後來真的在做的時候
在很多的情況之下
我們都會做這個假設
說 它的每一個gaussian 的這個covariance matrix 
我們都說它是 我們都說它是diagonal 
你就讓它是假設外面是零
那這個假設為什麼可以成立
就是因為我這這邊是做了d d c t 
等於是經過了這個d c t 之後
我們比較可以做這個假設
因為它transform 出來的東西比較是un correlate 喔
所以呢我們可以 可以做這個假設說他們是un correlate 
所以這些東西呢就可以假設它是零
在很多時候是可以這樣做的
當然在某些情形你如果要做特別精細的話
你可能還是不能做這個假設
你還是要讓他這個所有的維都跑進去
但是很多時候我們是這樣做這個假設
好那這個是 這個 阿 阿 這個 我們做這個 這個 dis 這個d c t 的其中一個原因
那還有這個地方我想有一點寫錯
喔 因為我們其實應該是 你如果看這個圖就知道
我們應該是這個y t 的m 
取絕對值平方 取log 
再做這個cosine transform 
所以呢看起來這裡應該是
取絕對值平方應該是掉了一個平方
有了平方之後再取log 
之後這個東西
乘上一堆cosine 加起來
這個就是cosine transform 
就是我現在把這個東西改成cosine 嘛
就這個東西乘上一堆cosine 加起來嘛
所以應該是 這個地方應該是有一平方的漏掉了
ok 那麼底下的這件事情
是跟我們前面的 連 連在一起的
我們跟前面這件事情連在一起來解釋
我我們休息十分鐘好了喔
喔 我說一下我們的這個期中考的schedule 喔
我們現在今天是四月
今天是四月十八
下週是二十五
再下週是五月 二號 九號 十六號 
我們期中考的範圍是到第八點零考完
那麼我估計大概今天可以把七點零講完所以下週大概可以把八點零講完
所以大概是 期中考的範圍大概到這裡
是大概講完
所以呢過一週兩週
合理的期中考時間可能是在九或者十六
那我的我的這個plane 是在
這一週 因為這一週我出國
那麼如果在我出國的時候考期中考
可能是最不需要補課的一個狀況
喔 所以呢我現在的plane 是
就是我們在今天把七點零講完下週把八點零講完
這個時候我們把basic 全部結束
這是我們期期中考範圍到這裡為止
ok 
我們現在來解釋剛才這邊沒有說的一個就是這個嗯
嗯在做log 的時候我們底下這一段話在說的東西跟
這邊的我底下也有這段話
那這兩段話在講的意思是什麼呢
那就是我們剛才在這邊所講的啊已經擦掉了
就是我們的這個
你可以想像成是
這個你可以想像成是一個進去是u n 
然後出來是x n 
那這是我的vocal tract 我的發聲的這個脣齒口型的變變化我們叫做g n 
那其實x n 呢是u n 跟g n 的convolution 的一個數學關係是這樣子的
那麼這個意思你等你可以想像呢是說
我們所聽到的每一個聲音
是有兩個部分的現象
一個部分的現象是這個excitation 裡面發生的
一個現象是在這個g n 裡面發生的
那麼那麼這個g n 裡面的東西呢等於是在描述我的口型的那些東西
而excitation 是在描述我我進去的這個氣流的
那這兩種現象呢在我們的聲音裡面其實最後是混在一團
對不對
經過這個運算混成一團
在time domain 上得到一堆像這樣子
或者說frequency 的到一堆像這樣子
那都是把它們這兩個混在一起了
那麼你如果是這樣子來看的話
那麼在time domain 上是一個這就是u n 跟g n 
就是這個是這個excitation 的氣流
跟這個g n 是我的那個那個喔vocal tract 
那這兩個是混在一起變成我的訊號
所以我的訊號是這兩個混在一起的
那這個時候你如果從frequency domain 來看的話呢
這兩個會變成相乘的意思
如果他在frequency domain 上面是用這個來呈現
它是用這個來呈現的話它也可以用這個來呈現
他們是相乘的關係
在frequency 上面是相乘的話呢
妳取了絕對值還是相乘
取了log 最後會變成相加
那這個意思是說呢
你如果是看在這個地方的話
我其實是有取絕對值有在取log 
所以雖然在原來的訊號裡面他們是有這個convolution 的關係
到了這邊之後呢其實是已經變成相加的關係了
所以呢在這裡的時候其實他們已經是一個相加的關係
那這個時候呢雖然是相加不過還是兩個混在一起
加法仍然是混在一起的
所以你可以想像呢
就是兩種東西一種是描述我的口型的
一種是描述我的excitation 的
這兩種東西呢是加在一起
加在一起之後譬如說一個是這種東西另外一個可能是這種東西
它可能會還是加在上面
但是基本上呢
雖然是加在一起可是呢
我如果是看他們變化的速度的話呢
一個是vocal tract 變的很慢
而excitation 是變化的很快的
那麼也就是說呢你如果在看他的這個譬如說你以這個unvoiced 這個為例
這個是變化的很快的他的這一瞬間跟這這不一樣的東西的
或者說你看這個的話呢其實他的音高一會高它是有變化的是蠻快的
所以基本上來講他們是變化快的東西
而vocal tract 呢是變化很慢的東西
因此呢你這個時候如果做個inverse d f t 的話會怎樣呢
會把它們換到兩個兩段去
喔
也就是說我如果回到剛才的那一張來看的話
這裡
我我做inverse d f t 回到這個quefrency 的這個domain 的時候
這是那個j 就是quefrency 的那個domain 的時候呢
那基本上在這上面的時候他們是加在一起的還是加的啦
就是雖然一個變化慢一個變化快還是加在一起的
可是經過這裡之後呢
一個變化快一個變化慢就會跑到兩個不同的區段來
就是這個就是在我的這個j 
就是這個quefrency 的domain 上面
在這個上面的話呢那麼
你變化比較快的一堆在這裡
變化比較慢的一堆在這裡
那麼其實呢
這堆呢跟這堆因此可以拆開來
那其中呢你可以想像成
這一堆的information 就到了這邊
ok 
然後呢這上面的這些information 
就這樣的意思
sil
那麼
當我這邊所畫的是把它畫在frequency domain 上
那其實呢我們是從是從這個domain 轉過來的嗎喔
那基本上在這個上面的時候我們講在這上面的時候其實這兩種現象仍然是加乘在一起的
但其實加乘在一起的時候他們其實一個是快一個是慢的
所以呢當我transform 的時候
其實會把快的transform 到比較高的j 比較大的地方
那麼這個慢的會transform 到j 比較低的地方
所以會變成這個兩段
基本上他們就somehow 雖然還是這兩個加在一起啦
你可以看成是還是加在一起不過就是說一個是在這邊嗎一個是這邊這邊比較比較沒有
一個是在這邊這邊比較沒有
好你是是是這兩個東西
還是都是整段啦還是相加的啦
只是說一個比較都在前面那一段那這邊沒有
一個是比較在這一段後面沒有
因此你這樣transform 回來的時候其實是這兩個是幾乎可以拆開來
那麼這是為什麼你在我們在這裡後來我們說
我這邊是取m 這邊只取j 
這邊本來是二十多個二十多個filter 二十三到二十七個filter 
可是這邊我只取十二十三個為什麼
其實就是在做這件事
我就取這個就夠了我這個就不要了
因為我們現在要的是這個formant structure 
我們真正要分辨的是在這個formant structure 我要要的是這個東西
這個東西就是被這些毛我們講這上面這些毛搞的很亂
所以你不太容易抓的到它的formant 在哪裡
他這個毛非常亂
所以呢你不太知道他那個peak 在什麼位置
但是呢你如果其實peak 在這裡譬如說
那你如果把這個毛拿掉的話
就毛在這裡我把這個毛拿掉了之後
剩下這個東西的話
就得到裡面這個
就得到這個
你反而清楚知道ok 這個f 這個你的f f f 三在這裡或者什麼
sil
那等於是這樣的意思
所以呢
我們大當這裡這裡做inverse d f t transform 的時候
是有這個原因就是為什麼我只取前面後面不要了
那其實後面因為後面more or less 就是那堆毛
那麼我其實可以把它丟掉後前面反而清楚
我可以得到一個比較清楚的東西
那這個解釋我們剛才講的就是說你中間為什麼取log 
這邊是在解釋為什麼取log 
取log 是有這個好處就是取log 之後可以讓這兩個東西由convolution 變成加法
那麼convolution 是把整個的把整個東西完全混在一起
可是變成加法之後
是有可能像我們這邊所畫的這個情形
就是你只要他在不同的區段的話其實是拆的開來的
sil
這個就變成這樣子於是我拆開來
所以他這個有一個log 除了我們前面這些原因之外還有這個原因是把
乘法變成加法
把convolution 變成加法
那麼這邊講的也就是這件事情所以你可以把interference of excitation on formant formant structure 把他拿掉
也就是說我要的其實是這個formant structure 就是我們這個紅色的
這個紅色的這個東西
那你這堆藍色的東西毛呢你可以看成是一堆interference 
是我的這個excitation 在上面的那一些破壞
我可以把他拿掉
那這樣子話呢我就可以比較清楚的得到我所要的東西喔
這個是這邊講的意思
好
有了這些之後我們現在m f c c 就出來了
就是回到剛才這張圖
就是回到剛才這張圖
那這樣子我這邊得到這個y t 的j 就是我的m f c c 了
那我們很多時候
同時也取energy 
