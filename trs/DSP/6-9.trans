底下我們來講下一個topic 
就是說language model 另外的一個常常常使用的很有效的方法就是所謂的class based
什麼叫做class based 呢
意思是說其實你真的一定要算這麼複雜的這種n gram 嗎
好像不盡然
因為明明很多個word 是屬於同一個class 
所謂屬屬於同一個class 就是指它們在語意上或者在文法上其實是很像很像的
舉例來講譬如說john saw a dog 
如果john 可以saw a dog 的話當然mary 也可以saw a dog 
然後其他所有人的名字都一樣可以saw a dog 
那這些人有區別嗎好像沒有區別
如果要你如果要算這個john 後面要接這個saw 的這個bigram 的話呢
只要你其中一個人算得出來
換誰都一樣嘛
就這樣一個想法
那麼如果這樣來想的話你其實是可以把很多東西
凡是屬於它們很像的
either 是語意上很像
or 是文法上很像都變成一個class 
舉例來講你可以john saw a dog 當然也可以可以saw a cat 嘛
那麼cat 跟這個dog 好像沒有什麼區別嘛都一樣嘛
那麼因此呢好像可以放在一起哦
那你如果這樣子來想的話呢
那麼如果這些人可以幹嘛的話
那he 跟she 也可以嘛
如果he 跟she 可以的話my father 當然也可以my sister 當然也可以嘛
那麼可以是father 當然也可以是sister 啦對不對等等
那如果這樣來看的話當然我也可以drove a car 
如果可以drove a car 的話當然也可以drove a bus 嘛對不對等等
所以這麼一來的話你可以把凡是相同的一群
變成一個所謂的class 
如果變成一個class 的話呢
那我的n gram 就可以變成class based 的n gram 
這所謂class based n gram 的意思
你現在就不需要去算這裡每一個word 這個word 後面這個這兩個word 後面接這個word 機率
你可以算這個是屬於那個word 屬於哪個class 
如果你每一個word 都屬於一個class 的話呢你現在可以算這個class 的n gram 
就是你如果前面看到這兩個class 的話
後面會接哪一個class 
這個想法非常的單純啊很很容易想
但是呢你這個這個class 完了之後
這個這兩個class 之後接這個class 機率算出來之後呢
你應該還要算一下在這個class 裡面真的會看到這個word 的機率
換句話說這一堆人名
還有很常出現的跟很不常出現的
你如果john 是一個很常出現的話
換一個很不常出現的人名的話
他的這個可以不一樣嘛對不對
嗯那麼你你所以你可以再多一個這個
就是你看這個人名之後
在那個人名你看到這個人名之後
那人名裡面會看到他的機率是大還是小
你可以算可以算進去
嘛如果是這樣的話呢
我這樣子的class 的trigram 
再乘上看到這個class 之後會看到這個人名的機率
這樣子這個東西呢就可以取代我原來的這個機率
那麼這麼一來的話
我的其實這個這就是所謂的class based language model 
那class based language model 基本上它有smoothing 的效果
為什麼有smoothing 效果
那也是一樣把原來的unseen 都放在這裡嘛
你本來譬如說有一堆很很少人見的人名那些人名我根本不看到的
我都給他歸在這裡
那他也就有機率了嘛對不對
那麼我我我特別去算這些人名裡誰的機率高誰的機率低
那機率再低還是有機率嘛
所以這個就做到相當程度的smoothing 的效果
那如果這個可以做到smoothing 效果的話呢
那麼它跟我們剛才講的用lower order 啊lower order 就是我們講的back off 
就是我們之前在這裡講的back off 
就是用lower order 
那它跟這個方法常常是互補的
也就是說你有的時候可以用back off 到lower order 的方式來算你的n gram 
那有的時候其實你如果是可以是class 的話呢
我就用class 算也可以嘛
所以它跟lower order 這個是常常是一個互補的方法
常常是一個互補的方法
那麼也因為這樣子所以我class 的數目也可以大為降低
所謂class 數目大為降低的意思是說
你現在n gram 不要那麼多了因為class 很少可以少很多嘛
那麼以這個最常用以我們常習慣的中文為例
我們說中文的常用詞六萬個
我們中中文常用詞是六萬的話
你n trigram 有多少個機率
是六萬的三次方
你的trigram 有這麼多
然後你就要算這麼多個機率出來然後存起來
然後每一次去找這這麼多個
那如果我用這個方式的話中文的六萬個詞可以分成幾個class 呢
這個我們過去做的經驗呢大概在七百到一千五百之間就夠了
你你的分的class 大概分成七百個到一千五百個
大概夠了
以一千五百個為例你的trigram 變成這麼少嘛
這個比這個少很多啊
嗯那這就是指我的那我做起來什麼都方便了
這就是這個parameter 就大為減少
那在這個情形之下呢
可能最大的問題是那些word 放在一起變成一個class class
那這就是所謂的這個word clustering 的問題
你如何把什麼叫做語意上文法上相近這是什麼意思
哪些word 真的可以變成一個class 
這變成一個真的困難的地方在這裡
那這裡面舉了講了兩個例子
那這個是limit domain 的例子
這個是假設我要做的事情是一個limit domain 的話
其實很容易
基本上就用人來做啦
舉例來講譬如說這是一個買飛機票的一個dialogue 系統
你就專門給人家打電話進去買飛機票
那那個不是一個旅行社的一個一個agent 而是一台電腦
那你打進去問說tell me all fights of united from taipei to los angeles 
那你其實你可以想像所有的航空公司的名字變成一個class 
所有的城市名字變成一個class 嘛
你這個用人做就好了根本不要去
那我我講的就是這些東西嘛
所以你只要把所有的城市名變成一個class 
然後sunday monday tuesday 變成一個class 等等等等的話
你總共就這麼多個class 嘛
那這個時候呢你就直接用人手做這些class 之後
你你只要train 這些language model 
就是這個這個class 在這些後面的機率
這個class 這這個class 接在from 接在to 後面的機率等等等等這樣就好了嘛
那這樣而且有一個好處就是new item 不用去train 
直接加進去
今天有個新的航空公司長榮
那你就加進去就好了跟本就不用去train 嗯
你有一個新的航點飛到里斯本
ok 就加進去就好了
啊根本不要train 
那麼你大不了就算一下里斯本在所有的city name 裡面它的機率是多少
最多多算一個這個機率
那這個機率可能是以航班的比例來算
或者以旅客的比例來算你可以加一個機率在這裡
那這樣的話就是這個機率那這樣就好了嘛
所以你基本用人手來做而且呢你都可以不用data 
新的新的item 直接加進去就好了
這是limit domain 的情形
但是如果不是limit domain 的話呢而是general domain 的話那比較難了
你可以想我們我們這麼多個word 
英文的常用詞word 的數目大概跟我們中文的六萬差不多同樣的order 
你這麼多word 到底是誰跟誰是同一組
你不能全都用眼睛看嘛哦用人來看是可以看啦
但是你完全用用人又分不出來所以這個時候呢general domain怎麼做
那就要有一些所謂的這個word clustering 的方法
基本上是用這個這個這個這個啊data 以data driven 為主啦
你就是要用大量的data 去算
底下我們舉兩個例子
這個其實這種就是在general domain 裡面的這個general domain 裡裡面的這種這個大量的詞到底怎麼樣去做這個word clustering 是一個很有趣的問題
那麼在大概九零年代八零年代幾零年代是有非常多的研究在做這件事情
那麼確實他們做了不少非常好的language model 就是這樣class 的
