那這個自動抽keyword 是基本上是用文件文章的文字的比較容易抽keyword 
你如果文字不容易的話如果是語音的話是比較難
語音的話呢比較常用的辦法是我去做個recognition 
做了recognition 之後呢變成文字之後再來抽
但是這個時候OOV 啊什麼已經已經掉了
所以呢如何如果是語音的文件
要如何讓裡面的keyword 出來這是蠻有學問的事情
這可以做我們今天都有技術阿
但是呢我們不見得有時間來說它
但是你基本上可以想成就是我得要有好的辦法
我不是是直接做recognition 
否則的話很可能OOV 全部都丟掉了
所以我要想辦法讓這裡面的keyword 能夠抓出來等等
底下這一章是在舉一個例子講
中文的文字怎麼抽KEYWORD 
那麼中文的文字面跟西方語言最大的不同
就是你even 不知道詞在哪裡
那你怎麼知道keyword 在哪裡那
英文而言的話呢他的keyword 不會太難抽的原因是
第一個就是你有boundary 
你所有的空白就是boundary 
你已經知道這是一個空格這是一個空格所以這是一個word 
這是一個word 每一word 都define 的很清楚
就是由這個boundary 所define 的
然後呢所有的專有名詞都有大寫
那麼所以呢通常的專有名詞都是大寫
有大寫的轉有名詞常常是keyword 
所以你先把這個有專有有專有名詞的大寫抓出來啊
有大寫專有名詞抓出來那你八成那個keyword 已經抓到一堆
然後你再用什麼TFIDF 那些東西一算的話
你那keyword 就可以出來
那中文的問題是說你它是一串字
你不曉得詞在哪裡
然後一大堆OOV 
所以你怎麼知道哪裡是一個詞
然後他該不該是一個keyword 
這就比較難
那這有很多特別的方法我們這邊並不細說
這裡舉一個舉一個例子
那這個例子是說你怎麼知道這幾個word 應該是一個keyword 呢
你先要看它是不是一個完整個pattern 
要看它是不是一個完整的pattern 都不容易
那在這個例子而言它是說OK 
譬如說這個是呂秀蓮副總統
那麼你拿掉最後一個字
或者拿掉兩個字
呂秀蓮副總
或者者呂秀蓮副
你會發現他不會單獨出現
他出現的時候一定是在呂秀蓮副總統一起出現的等等
那這樣就表示應該是這整個才是一個term 
或者是一個word 
而不應該是拿掉一兩個字的等等
那就是這邊講的就是你這個W S 如果always appear as part of W 的話
或者你前面拿掉一個字
呂秀蓮副總統跟秀蓮副總統
那你沒有這個秀蓮副總統你每次就是一定有呂的
那這樣的話呢那這個時候呢就應該是一個完整個term 
應該是這整個的等等
那另外呢你就是你除了這個之外你還要看後面會接什麼前
前面會接什麼
那麼也就是說如果它是一個完整的term 的話
後面應該會接不同的東西
那前面也會接不同的東西
那如果說他們後面老是接相同的喔
喔就是說如果你W 後面永遠接一個固定的東西的話
那他們可能是連在一起的
譬如說行政院長游錫坤
那你你如果發現他老是連在一起的話
他可能應該是連在一起才是一個term 
那反過來當然會不一樣
譬如說自動化
或者說OK 合法化
合法化的那個化你會發現合法自己常常出現的話
那麼合法跟化就應該拆開來合法才是一個term 
所以這裡面有一堆這類的問題
然後你看哪個是個term 
然後你再來看他重不重要好等等
那這個這是中文的key term 的的keyword 裡面有一些相關這類的問題
好那那我們這邊先把這個大致先說到這哩
那這個是我現在這個十四點零的部份
那跟這個相關的有另外一件事情我們在這裡順便提一下
就是我們剛才講這個L S A 
用這個十二點零的L S A 的時候我可以做這一類的concept matching 
那跟這個很像的就是加機率
那這個在我們十二點零的時候我們就提過這件事
在十二點零的最後一頁的時候
我們說過就是就是像像這個十二點零都是在講把這些term 
跟這些個文件之間
想辦法去找中間的那八百個concept 
那其實呢這個方法不算是最好的
因為他用用一大堆matrix 
但是它沒有機率的沒有太多機率的觀念
那到後來的時候就有了這個它就有一套全新的formulation 就是所謂的probabilistic L S A P L S A 
那他就重新建一套完全是意思還是很像的我這邊就是所有的文件
那邊是所有的term term 或者所有的word 或者什麼
term 啦就是我們這邊講的所所有的term 
或者所有的indexing element 
那麼你中間是什麼
是你真正的topic 
那你可以建這中間的的機率關係
因此呢對每一個文件而言呢
假設我這邊有八百個topic 的話
我每一個文件呢你可以分析說它是它是在講哪一個topic 的機率
都有一個機率
那對每一個topic 而言呢
每一個term 會出現有他的機率
那麼於是呢我這個時候某一個文件裡面會出現某一個term 
不是在這裡數他出現幾次的count 
而是算成一個這樣子的機率
就是這個文件他會講哪一個topic 的機率
以及在那個topic 裡面這個term 會出現的機率
然後呢我把所有的topic 加起來把八百個全部加起來
那這樣的話呢我可以拿來跟我真正數count 的時候
這個裡面的有多少我來比比看
那我可以maximize 這個likelihood function 
做為來train E M 的model 的方法
所以呢我用這個我要maximize 這個東西來用E M 就可以train 出這個model 出來
那這個東西的的的精神跟我們之前講的第十二點零講的這個這些東西是非常像的
可是他這個時候呢我就完全用機率方法來model 之後呢
我就可以用機率的各種方法來做它
那麼這個效果呢terns out 是比L S A 還要好
那同樣我這個可以做這裡的我可以做這裡的的的
這個retrieval 完全可以拿來做嘛
因為你你現在這個user 輸入一個query 
你也一樣可以把那個query 看成是一堆term 
然後那你也可以因此可以算他是他會在哪一個topic 裡面的機率等等等等因此
此你的都可以這樣算所以呢這些東西可以完全拿來來做做我們這邊講的retrieval 
那terns out 這個效果比L S A 通常是會好一些
這個到這裡呢相當於我現在把這個痾十三點零講完
十三點零我們這邊的這個reference 裡面
我們剛才講
因為information retrieval 是一個很大的領域
那最多的資訊來源應該是A C M 的這個special interest group on information retrieval 的這裡面
它每年有很多paper 
那麼然後呢底下這一篇是就是用H M M 來做的原始paper 
就是這個A C M sig I R 就是這個的就是這個
那底下這兩個都是這裡面的嘛
那咳那這個是用H M M 來做的
這個就是probability 
用就是我剛才講的用probabilistic 來做的L S A 的
那這兩個都是在他的原始paper 都出現在A C M sig I R 的一九九九年
那當然你現在看的時候這兩個原始paper 不見得是最好看的
那在這個後面都還會有好多篇的paper 
會寫的比較完整比較好看的
那你如果要看的話可以再從後面去找阿
這是講十三點零好
