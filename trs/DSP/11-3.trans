那這個方法有它的弱點我們剛才講了
因此呢底下我們來講下一個方法就是如何克服這個弱點
那麼後來就有人想了這個方法
所謂的maximum likelihood linear regression 喔m l l r 
那它的意思是什麼呢
我把這個gaussian 
先把它分成一堆class 
然後呢為每一個class 建立一個transformation 
喔現在不是這樣啦
假設我現在的這一堆所有的tri phone 的那些不同的音的mean 
我們先說mean vector 好了
假設它們在這裡
那剛才我們說如果是map 的話呢
我現在是聽到什麼聲音我會調這個
聽到這個聲音調這個沒聽到的我全部都不調嘛
那這樣的結果呢我我我只有所有的unseen model 都看不到嘛
對不對我們剛才的問題就是這個
所有的這個這個unseen model 我都沒有辦法調嘛
那它現在的辦法呢這個這個maximum likelihood linear regression 最大最大的目的就是我要unseen model 全部都要調
你只要講一句話我就開始全部都動
那怎麼可能呢
我我我只看到我只聽到那幾個音我憑什麼可以全部去動呢
他說我現在把它分分群
舉例來講譬如說這一群其實都滿接近的
我我把它叫做c one 
那這是一群
這群滿接近的我都叫做c two 
這群比較像的聲音我把它叫做c 三
然後我我為每一群定義一個transformation 
就是這裡面的每一個這個mu mu j k 還是一樣
就是第k 個第j 個state 的第k 個gaussian 
那我現在怎麼調呢
都有一個公式就是a 乘上這個加上b 
所以呢譬如說c one 的話我就會有它的a one 
跟b one 
使得告訴我說這一群全部怎麼調
都有一個共同共同的方向
都向這個方向調
那c two 我也有一個a two 跟b two 
它給我一個共同的方向說是這樣調
c 三我也可以求出一組參數就是a 三b 三
它給我一個共同的方向是這樣調等等
那我如果可以找得出這些來的話呢
我就直接調了
舉例來講假設我今天這個還是一樣這個user 說了這句話
這裡面呢這一段是某一個phone 
這段是某一個phone 
那麼根據這些個phone 的話呢
啊turns out 它是這裡這裡的某一個
這裡的某一個
這裡的某一個
那於是c one 裡面呢我我聽到的是這些
別的都沒有聽到
但是我根據這個聽到的呢
我就根據這聽到的這這些聲音
我就求出整個的a one b one 
於是我整個一起動
那同理呢我如果這邊我這邊有聽到譬如說這個那裡有什麼聲音
這邊有個什麼聲音
它那剛好是在這裡
跟這裡跟這裡ok 
我就根據這些東西呢
我就調出一個a 三b 三來
但是a 三b 三不是只調這三個而是我整群一起調了
那麼以此類推
我雖然user 只說少數幾句話
我只要每個class 裡面都有說到
於是我就整個一起動了
這是它的基本觀念
所以呢它就define 一個這樣的transformation 
那這個a mu 加b 這個transformation 是一個非常簡單的linear transformation 
當然是比較粗的
跟剛才不一樣
你知道剛才這裡面的它是用這個去算的
用這個去算所以它是完全在算機率然後去調那些東西
那我現在這裡沒有
它這裡只是給它一個很粗的
所以所以這個是一個比較粗的transformation 
那這個a mu 加加b 的這個東西其實就是multi dimension 的linear regression 
那你記得我們從前講的我們從前講的linear regression 我們在七點零的時候說過這件事
就是什麼是linear regression 如果two dimension 的話呢就是你給我一堆點
我想辦法找一條直直線
這條直線是y 等於a x 加b 
然後我希望有了這條直線之後所有的所有的點呢
跟它的距離是最近的
那這個是所謂的linear regression 
在two d two two dimension 的平面上的時候這是所謂的linear regression 
那現在這個一樣
不過變成n dimension 
變成multi dimension 的時候呢我不是a y 等於a x 加b 而是什麼呢
是整個的n dimension 裡面的的那個vector 是乘上一個matrix 加上b 是一樣的意思
所以這叫做linear regression 
ok 那麼如果我現在用這個linear regression 的方式
來為這一群一群的class 都找出他們的transformation 的參數來
這樣子的話呢那這個怎麼找
每一個class i 我都要找它的a i b i 
那憑什麼呢
憑這個
那它用這個這是什麼這是likelihood function 
也就是說如果你給我lambda 是原來這一堆原來這一大堆的model 叫做lambda 
現在你你現在給我a i b i 之後
ok 原來這一堆
給我這個a one b one 之後呢
我的新的新的model 就變成這個lambda 裡面的所這邊c one 裡面的所有的mean 
都用a one b one 去調它
調完之後的那個model 
我要看到我的這個observation 裡面的這些個聲音
的機率是最大的
ok 所以呢就是說譬如說我這個c one 用這堆a one b one 去調之後呢
調完的model 
我要看到這些掉在這裡面的這些聲音的機率是最大的
那這個機率呢其實就是likelihood function 
given 某一組model 之後看到聲音的機率
這個是這個likelihood function 
所以呢我要它是最大的
然後去找最大的那組a 跟b 就是我的a one b one 
ok 所以呢我就在調所有的a 跟b 裡面去找
讓這個機率最大的
那那這個呢就是maximum likelihood 
因為我現在是這個是likelihood function 
我要maximize 這個東西所以這個是maximum likelihood 
那那麼這樣做的話呢所以我現在這個名字就叫做maximum likelihood linear regression 
這四個字是這麼由來的喔
這個linear regression 是指這個公式
是一個很粗的transformation 
那它本身是一個linear 的
transformation linear regression 
那maximum likelihood 是指說這兩個參數怎麼求
是用maximum likelihood 方法來求的
那當然你要求要maximum 這個方法當然不容易
那個詳細的數學推導也有一大堆
那根據什麼還是一樣根據em 
喔那這個em 我們留留到後面會說
那基本上呢你可以想像這個em 是很重要
因為像這類都有同樣的問題就是你給我一堆observation 我就要去找這個參數
跟前面是一樣的
前面的這裡也是一樣
你給我一堆observation 之後我要去找這裡面一大堆參數
那怎麼找我們都是用em 的方法喔
那這邊也是一樣用em 的方法來找的
那如果是這樣的話呢我們就就是這邊講就是說我我所有的gaussian 
我在同一個class 裡面的話呢
我都我都用同樣的一組a i b i 去調它
所以這就是parameter parameter sharing 
或者adapt data adapt adaptation 的data 的sharing 
也就是說我現在只要聽到這些個聲音
那麼它們這幾個聲音聽到之後
我所有的這些model 這些個mean 
都share 了共同的這些個data 
都share 了共同的data 
所以是這個是data 的sharing 
同樣呢我用這些data 求出這些參數之後呢
它們share 了共同這些參數
所以是這些個model 的parameter 的sharing 
我這個都是sharing 的觀念
於是這樣的話呢我沒有看到的model 也都可以跟著調了
沒有看到的model 我都可以跟著調
那這個時候很大的一個問題是你怎麼分群對不對
這才是問題
到底哪些個該變成一群然後它們用同一組參數
哪些個該變成一群變成同一組參數呢
怎麼分群呢
當然你可以想像兩個原則
一個是data driven 
一個是knowledge based 
也就是說呢所謂的knowledge driven 意思是說我們可以有一些knowledge 
譬如說這裡這一堆都是ㄓㄔㄕㄖㄗㄘㄙ大概是比較像的我們給它們一群
這一堆是這個ㄅㄆ　
這個這個ㄉㄍ比較像的ㄉㄍ給它一群ㄅㄆ給它一群等等
母音給它一群子音給它一群等等　
這個是可以完全根據knowledge 就可以分群
但是更重要的是什麼呢data driven 
也就是根據data 去算
通常我們去算gaussian 的distance 
這個常用的辦法是算gaussian 的distance 
也就是說你你每一個gaussian 你可以算
嗯對不對你如果這裡有一個gaussian 
這裡有一個gaussian 
你可以算它們之間的distance 
那麼根據這個distance 來算說凡是distance 比較近的那一群
那麼它們在一起的
我我假設它們是共用的喔
你可以算gaussian gaussian 的distance 這樣來做
這是data driven 
那通常是可以這兩者並用就是你一面用data driven 的方式
一面用一些knowledge 
這樣子來分群
但是問題是到底應該分多少群才好呢
你可以想得到的是你不能分太多群
也不能分太少群
為什麼呢
如果分太多群的話你就沒有用了
你如果這個一群這個一群這個一群這個一群那你每一群都要一組a i 都要有data 
那你結果等於等於每一個自己都要調一樣的
所以顯然你要有夠多的在一起一群
夠多的在一起一群那麼群的數目不能太多
這是第一個原則就是你不能太多群嘛
你如果太多群的話就每一群都需要夠多的data 才能做你這樣就不行了
所以呢群數不能太多
反過來呢也不能太少
因為因為你群數很少顯然太粗嘛
如果我這邊總共只分三群的話
很顯然是說這一大堆不太像的通通都變成一群了
都用同一條顯然不好嘛
所以呢你也不能太太少群
太少群就會太粗
所以呢我一定是要這個不多不少
那這個東西原則是什麼呢
基本的原則就是說我要有夠多的data 就可以給它一群
那什麼意思呢
就是如果說你這裡面明明有相當多的data 了
假設說這些也有data 這個也有data 這個這個也有data 這個也有data 
如果data 夠多的話明明這個data 夠train 兩群的話
那我寧可把它分成兩群對不對
我這群找出一個a i b i 來
那這個可以變成另外一群
對不對我只要我的data 夠多
如果我data 夠多到可以得到a one b one 跟a two b two 
兩組參數的話我寧可分成兩群嘛對不對
那麼我我只要data 越多到讓我可以把它分得細我寧可分得細比較好
我如果那麼多data 結果只弄一個比較粗的一群的的調是比較不不理想嘛
所以我的這個基本的principle 是應該是我基本上是它們一定要像
這個所謂的similar property 就是要像
就是要像底下那樣我根據他們的data driven 跟knowledge 來判斷它們是應該是一群的
要夠像
一方面呢如果它們有夠多的data 就自成一群
那這個是我們講的分群的原則
可是你想這怎麼做呢
我怎麼知道哪哪些又像又有夠多的data 呢
因為user 顯然它隨便在說不同的話
他不斷的話不斷的說進來
你怎麼知道誰哪些是剛好是一群而且有夠多的data 呢
那比較好的辦法就是一個tree structure 
那這個所謂的tree structure 是怎樣呢就是我想辦法先把它們之間的關係先建好一個tree 
我們舉個例子來講
假設這裡面所有的所有的gaussian 的mean 
我們都是在最底層
是一群
是一系列的
然後呢如果它跟它比較像我們可以用它跟它比較像
它跟它比較像
那它跟它呢比較像
那它跟它比較像等等
那麼於是呢我們可以得到一個像這樣子的tree structure 
譬如說我們得到一個這樣的tree structure 
那這個時候呢完全我就告訴它們這個tree structure 告訴告訴我們它們之間的相似性
這個tree 怎麼建的這個tree 就是根據這個data driven 跟knowledge driven 想辦法建這個tree 
然後這個時候depend on 這個user 說了什麼話
它什麼話進來我去看它的data 在哪裡
我們舉個例子來講
假設它的假設它的聲音進來的時候呢
這個data 很多
多到它自己以為它自己知道怎麼怎麼調的話
我根本它自己就是一群
可是呢這些都沒有data 
當這些都沒有data 的時候呢
那那我很可能就是把這這些東西合在一起
看成是這一個
於是呢這個也是一群
那麼假設這個量這個data 量夠多到可以得到一組a two b two 的話
那你可以想像的是對對於對於這個而言
我其實完全根據這個我就知道它怎麼調
可是因為其它都沒有data 嘛
我就這整個的呢我就也用我我整個就都用這個來來調了
但是呢我也很可能是是另外一種狀況
是說其實這裡面的這裡有一些data 
這裡有一些data 
它們兩個加起來的data 夠多到可以train 一個
如果是這樣的話呢
那我也許就讓這些個這些個變成另外一個
然後呢得到一組等等ok 
所以呢就是說我我完全depend on 這個data 進來的狀況
我如果有一個tree 已經建好的話
depend on 我的data 進來的情形
那麼你發現說這裡有一點data 
但是不夠train train 一個這個a i b i 
這裡有一點不夠
但是那那這邊沒有
如果這邊沒有的話呢它們share 一個還是在這裡還是不夠
它們沒有它們share 一個在這裡還是不夠
但是呢我這個跟這個加起來夠了
於是呢到這邊為止我在這裡夠了
於是呢我就變成這一群我可以得到一個
等等喔
那也就是說完全那就是就是這邊講的就是說這個我可以dynamically adjust class 
當你摸了data 當你的data 不斷進來的時候
那麼那麼你你可以建一套這個把這個tree 建好
然後我有一個演算法
然後當我的聲音進來的時候depends on 我現在你說的是什麼話哪些音掉在哪裡
然後我去看每每一個地方
到底哪些地方構成夠多的data 可以train 嘛
可以train 出這個a 跟b 出來
我就在那邊呢看到高到什麼層次嘛對不對
如果這邊的data 不夠這邊的data 不夠但是它們加起來的話呢到這邊才夠
於是呢其實這邊就共用一個了
那麼於是呢我我就可以說是這個這個每一個狀況是完完全全是depends on 這個data 進來我隨時在調
那麼我我也很可能說是這個每每一群完全看狀況來決定誰誰變成一群
所以我的聲音當user 的聲音不斷說進來的時候我隨時在調這邊的東西
然後看這個嗯哪些夠多了可以調成一群我就調成一群
然後呢你繼續下一段話再說進來幾句話的時候我這個就會又變了
我又可以不斷地調因此我可以不斷地調的比較好
那那就這邊所講的
我我dynamic 來來調所有的class 
當我越越說越多話的時候
然後呢那這個原則呢就是這個node including minimum number of gaussian 
但是呢有足夠的data 的就變成一個class 
那一方面呢就是我們講為什麼要minimum number of gaussian 
就是要細嘛對不對
如果它們已經夠了話我就它們自己變成一個
這樣這個比較細對不對
那這邊因為沒有啊
沒有我就只好跟別人一起合用一個對不對
所以呢當我沒有data 就跟別人合用一個難免比較粗
凡是有細的地方我就把它變細喔
所以呢就是minimum number of gaussian 
但是有夠多的data 就可以做
那如果這樣子來的話呢那我就可以達到我的目的
那麼有一個tree structure 之下
看data 進來的狀況
然後我隨時調中間的東西
我去隨時調它的參數
那這個想法呢嗯獲得了相當不錯的結果
那你可以想像它的它的情形
跟剛才的map 比起來最大的不同就是它現在的curve 會變成這樣
就是我一開始還是從這裡開始
但是它的斜率比較高
它會比較快
它的它斜率會比較快
因為我我現在data 不斷進來它馬上就正確率會提高
但是有有個問題就是說它它會它會比較快saturate 
那為什麼會saturate 因為畢竟它這是一個是一個比較粗的model喔
那麼嗯就是說這句話就是ma 就是faster adaptation 你調得比較快
你你會進步得比較多喔
你只要有much less data 你就可以調所以它進步比較快
可是呢它有一個很大的問題就是這個saturate at low accuracy 
你你這個再多data 也沒有用了
為什麼因為它是一個比較粗的model 
它的model 本身不夠精細
因為它只是一個a mu 加b 
這個東西只是一個linear model 不是一個很好的model 
所以呢你現在不管怎樣都是是這樣用這個a 跟b 在調是一個比較粗的
所以你不太可能可以調到那麼好喔
所以呢你如果是這個m l l r 的話呢是像這樣的
那麼我我開始比較快
可是呢我沒有辦法像map 可以一直上去
map 可以這樣一直上去
可以趨近這個真正的你的你的這個s d s 
它沒有辦法
它到了一個地方它就saturate 
它跑不上去了
這個是m l l r 的情形
那那這個東西呢他還有一個地方可以進一步做就是什麼呢我這個這個a 呢
可以是full matrix 
也可以reduce 到diagonal 或者block diagonal 
什麼意思呢就是我這個a 
基本上這個a 是你可以看到是三十九維假設我這這裡是三十九維的參數的話
那這個a 是三十九乘三十九的一個matrix 
如果它是三十九乘三十九那是很大啦
那變成是一個這麼大的matrix 
那這裡面三十九乘三十九要參數很多
那我如果data 只有那麼少可能沒有辦法調那麼多
那怎麼辦呢
第一個辦法就是我假設它只有diagonal 
只有對角線才有值
其它都是零
我如果這樣的話呢我只要三十九個
我只要三十九個參數就可以描述這個a 
當然你假設它這邊都是零的話是有一點這又是又是一個簡化的假設
因此呢你如果這樣做的話呢你假設它是diagonal 的話
你可以reduce 到diagonal 你的你的這個需要的data 量就會少
因為我只要調這些就夠了
所以呢我需要的data 量比較少我就可以調出這個a 跟就可以求出這個a 跟b 出來
所以我需要的data 量比較少
因此呢你可以得到的情形是
你得到的情形是這個會更快
這個我如果是diagonal 的的a 的話
它會調得更快
會更快上來
因為我只要我只要那三十九個參數
本來是三十九乘三十九我變成只要三十九個就夠了
所以我比較少的data 我就可以把那個調好
會進來會進來更快
可是呢我會我會更快saturate 
我會我會這個進步得更快可是我會更快就就上不去了
因為那個更粗嘛
對不對你可以想像因為它更粗
更粗所以它有更大的問題就是它上不去
所以呢這是reduce 到diagonal 
那這個折衷的辦法呢就是block diagonal 
所謂block diagonal 呢是說呢我現在把它變成中間是一塊一塊的
那麼不是零的不是只有對角線而是這一塊這一塊
也就是說讓它們相鄰的這些東西有關係
但是別的地方讓它是零
當我變成這樣一塊一塊的時候
當然我的參數是介於這個跟full 的中間
我需要的參數比三十九要多
但是比三十九乘三十九還是少很多
所以那個是介於中間的
那你如果把它變成block diagonal 的話呢你得到的情形大概也就是在這個中間
這兩個折衷的辦法在這個中間這樣子
那這些東西呢就是構成所謂的m l l r 
那這個東西在嗯這個方法在嗯相當長的時間
很多人在不同的系統裡面使用
效果都很好
所以這個是恩另外一個非常重要的被普遍使用的方法
在我們這裡就是底下的第三個reference 
是它的我這邊都只給這個第一篇哦就是它的這個原始paper 第一篇是在這裡
那在這個之後有一大一大堆人在作跟這個以這個為基礎在發展
就像那個也有
那那個我這邊就不列了你自己可以去找喔
那所以呢這個m l l r 的這個原始paper 是這一篇
是九五年所以大概也已經十年的歷史
那當然當它的這個方法出來的時候
得到這個現象
就是mr這個m l l r明顯比map快的時候
那map的人就覺得說不服氣了
他說啊你你會這麼進步是因為你分群
是因為你分群你做這個tree structure 
其實我map 我也可以分群啊
那你可以想想看map 是不是可以分群可以啊
我其實map 這裡我也可以以群為基礎來做
我不要以每一個mean 來做
我也可以以群來做我也可以做一個tree structure 
所以後來就有tree structure 的分群的map 
那如果如果是那樣做的話呢那map 這條這條曲線的差異它也可以它這個也可以向上向上shift 
那它的好處是它最後還是可以收斂到最上面去
所以它有它的有各種不同的方法喔
那我們這邊都不多講就是說你如果有興趣自己去找都可以找得到
那麼map 我也可以用tree structure 做也可以把它弄上來喔等等
那嗯那當然就是說這個不同的方法他們自己各有不同的狀況
那麼有的時候是a 比較好有的時候是b 比較好
這個看情形
那在這裡的話呢
這個恩我們這邊只給我這邊只所列只是最原始的paper 而已你如果去找的話後面還會有很多篇
怎麼樣改進
這後面也還會有很多篇怎麼樣改進
