ok 以上是我們上週結束前下課前講的十一點零
底下今天我們主要的工作是要進入十二點零
十二點零的主軸是這個language model 的處理
那麼這裡面一個重要的方法是所謂l s a 
就是latent semantic analysis 
那我們來說這個東西
那其實這個在幹嘛其實還是一樣在做我們這邊所講的adaptation 
只是說我現在從acoustic model 的adaptation 調變成language model 的adaptation 
那麼什麼是adaptation 
就是要調它
調到你所它所適合的某一個條件
那我們我們之前在講speaker adaptation 的時候是說
我這個個hidden markov model 裡面的每一個gao 每一個state 裡面的這些mean 
每一個state 裡面的的這些mean 我都要調
因為如果是從針對某一個speaker 的話
針對某一個speaker 的話它的這些東西都會不一樣
不同的speaker variance會不一樣的
所以我最好要調這些東西
調到針對每一個人
這是我們上週所說的十一點零所說的speaker adaptation 
那我們今天在這十二點零所說的其實是language model 的 
那麼language model 有什麼好調的呢
因為你說今天後面接天氣
這個好像不管誰說都一樣嘛
這個language model 是在講這件事情
譬如說這是bi gram 
那麼今天天氣後面接好這是tri gram 
你如果這樣看這個是誰說都一樣所以language model 的的adaptation 不是為了調不同的人
而是調什麼呢調不同的domain 
不同的topic 
也就是說我們之前說過
在講language model 的時候我們說過
你如果是在談氣象的跟談政治的
用的詞彙是不一樣的
談體育的談財經的
用的詞彙是不一樣的
因此它的句型也是不一樣的
因此你如果今天當我在談不同的topic 
或者談不同的concept 
你如果講的是不同的concept 或者不同的topic 的話
我用的詞彙會不一樣
那麼它們的n gram 是不一樣的
那麼我們們在講六點零的時候曾經說過
那麼你如果是財經新聞或者體育新聞或者是這個嗯政治新聞
它們的n gram 是不一樣的
那我們現在講的是這件事
所以我language model 也是需要調的
但是不是在調不同的speaker 
是在調不同的topic 
如果我知道你是在講哪個topic 的話我應該要調到那個topic 去
然後使用那個topic 的language model 
這是所謂的language model adaptation 
那麼language model adaptation 有很多種的方法
正如我們十一點零說adapt這個speaker adaptation 也有非常多的方法一樣
這個也是一個很重要的主題有非常多的方法
那我們這邊所講的只是是其中之一
那麼倒不見得說這是最有效喔不見得
但是它是最可以算少數最general 
可以apply 到很多地方去
它等於是一種基本的的觀念
然後從它可以衍伸出很多東西出來
然後它也可以apply 到很多種不同的的應用上去
所以它有它的重要性
所以我們來說這這一個
並不表示language model 它就一定要用它
它有很多別的方法
那麼嗯我們所謂的linguist processing 也不限於是指language model adaptation 
你知道所謂的linguist processing 就是我們講的的在linguist 那一層所做的任何處理
包括詞字句的各種分析處理
都是我們所謂的linguist processing 
那麼舉一個例子
我們在六點零說過詞分群
因為詞可以分群之後做class space 的language model 
那怎麼分群呢
它的的分群本身就是一種linguist processing 
那麼所以linguist processing 只是只要在linguist 層次
那麼做的任何事情都是我們所謂的linguist processing 
那麼這個l s a 呢也是其中的一種方法就是了
那麼什麼是這個latent semantic analysis 呢
這個latent 這個字的意思我們已經從前看過了
就是潛藏的
那semantic 是什麼意思semantic 是它的語意
也就是指你裡面真正exactly 的意思是什麼
那麼因此你其實是在講一句話的時候我們希望知道它真正潛藏在裡面的意思
那些意思其實就是我們所說的是什麼topic 
跟什麼concept 
所以其實是要再分析是什麼concept 跟什麼topic 啊
這是我們這段要說的事情
那這段我這段大部分講的東西是based on 第一篇
那麼我底下主要以這個為基礎
那它算算是寫得最完整清楚的是這篇
那麼後面是些什麼我後面會再解釋
那麼這個的想法是怎樣呢我們也許用一個簡單的說法來講
那麼你今天如果說在一句話裡面看到布希
你大概會猜說它大概是講跟美國政府有關的東西
你如果看到一個另外一個詞是白宮
大概也是跟這些事情有關的
那因此你不管是布希或者是白宮可能是在講類似的東西
那你看到另外一個詞譬如說李安
那又是另外一件事它可能是跟電影或者是跟奧斯卡有關的等等
那麼你如果看到另外一件事情譬如說這個九二一
哦這是一件地震
這是又是另外一件事情等等
所以某一些詞彙可能告訴我那裡面講的concept 
或者說是嗯那個topic 
但是光是這些詞彙其實不容易讓我們了解憑什麼來分析這個
那在這個l s a 這裡
它想的辦法是說我另外找一堆文章
就是所謂的document 
那這邊呢我們可以說是word 
光是一堆words 
其實我們可以猜一堆word 是在告訴我某一些個concept 
只是我不太容易光用這些word 來分析
但是有一個很重要的東西存在就是document 
什麼document 
我上網去抓就可以抓千千萬萬篇文章下來
每一篇文章有它自己的concept 跟topic 
假設說我這個叫d one 這個叫d two 
我有我總共有大n 篇文章的話
那我有這麼多篇文章
那麼我就可以分析這些文章跟這些詞之間的關係
雖然每一篇文章我用人去看是可以說ok 這篇是在講什麼topic 
這篇在講什麼topic 
可是我我我如果不是人去看的話很難講它是什麼
正如這個詞一樣
我用人去看可以知道它是講什麼它是講什麼
可是如果沒有人去看的話我憑什麼分析呢
那它在l s a 這裡它想的辦法就是
我靠文章跟詞這兩件事情的相互關係來分析它們之間的關中間的topic 
怎麼講呢
譬如說如果是這這裡有幾篇文章都是在講美國政府什麼的話
它們可能都有白宮都有布希
那反過來呢這裡裡有幾篇文章是在講電影啊文化
搞不好他們都有李安
等等
所以呢你可以從這邊來看說誰有哪些詞誰沒有哪些詞
你也可以從這邊來看說這些詞在哪些文章裡面
你就靠這中間的關係來想辦法把它區分出它有哪些個concept 
因此呢我想辦法在中間找出一堆東西來
這些東西就是我們所謂的concept 
或者說是topic 
那它可能譬如說電影是其中一個
如果是電影的話
那就可能這邊就是李安
這邊就是有李安的文章
那如果是美國政府
那可能是另外一個觀念一個concept 
那它很可能這邊就有布希有白宮
那這邊就有這些文章章等等
那因此呢它等於說是我我在computer 我可以直接抓到data 是一堆詞跟一堆文章
那我可以算哪些詞在哪些文章裡面
哪些文章裡面有哪些詞
用這個關係去分析
抓出中間到底是哪些東西
那些東西就是topic 
那有了這個topic 我就知道今天如果這個人講話講了一堆話
應該是這個topic 的話
我就會猜它後面講的的詞還是電影有關的詞
那這個時候電影有關的詞的分數就可以跳高出來
如果那個人講的那堆事情我發現它是在講這些所以是在講這個的話
它再來應該會跳出都是跟美國政府有關的這些詞彙等等
那這個就是我們這邊所講的用l s a 來做language model adaptation 基本的觀念
也就是我們這裡要說的事情
那因為這樣的關係所以它現在做法就是
我要用用一堆word 跟一堆document 來建構中間的關係希望把這中間找出來
好有了這個背景的了解那我們現在來看這件事
它就是建一個所謂的word document matrix 
那是什麼呢
就是我這個matrix 每一個row 就是一個word 
就是這邊的每一個詞
然後每一個然後每一個docu 每一個column 呢就是一個document 
就是這邊所有的文章
那就變成一個matrix
所謂的word document matrix 
那講清楚一點的話就是我有一個辭典
這個辭典裡面有所有的詞
w one w two w i 到w 的大m 
其中大m 就是我的詞的總數
w i 就是第i 這個辭典裡面第i 個word 
那舉例來講譬如說大m 等於兩萬
假設我考慮一個兩萬詞的
這邊有有兩萬個詞所以這個matrix 是這個row 的數目是兩萬
然後呢我另外去上網抓了一大堆的文件
就所謂的document d one d two d j d n 
d j 是第j 個document 
那總共多少呢有大n 個
這大n 個呢就是譬如說說n 譬如說是十萬我抓了十萬篇出來
那就構成一個matrix 
那這個matrix 的裡面的每一個element w i j 是什麼呢
w i j 有一堆複雜的東西是這樣寫的
不過最核心的部分就是這個c i j 
c i j 是什麼
就是number of times w occurs in d j 
也就是說就這個word 而言就這個word而言它在這篇文章裡面出現幾次
我就可以數一下
那它在每一篇文章裡面出現都可以數一下
所以呢它在這裡出現幾次在這裡沒有出現這裡是零
這個word 是零這個是五十這是二這是三這等等
這樣我就可以把它全部排出來
所以我每一個word 出現在每一篇文章裡的次數給他排出來變成一個row 
換句話說你也可以看一個column 是什麼一個column 就是這篇document 裡面哪一個word 出現幾次
它不出他沒有出現就是零次
有就是一次兩次五次十次
這樣呢每一個這就是每一個document 
那你如果這樣子看的話基本上我這個c i j 就是最基本的一個這邊的word 對不對
這就是w one w two 到w m 
那麼這些個word 跟這些個d one d two 到d n 這些個最基本的關係
就是這個c i j 
就這個c i j 
只是說你如果光看這個c i j 
嗯其實它已經有相當有意義
因為你可以猜得出來如果這個word 跟這個word 譬如說一個是布希一個是白宮的話
它們這兩個row 可能很像
如果布希布希會出現在哪些文章裡面的時候
白宮可能同時會出現
那麼因此呢這個row 跟這個row 會很像就表示這兩個東西是蠻像的
反過來如果這個row 是布希那個row 是李安的話
那搞不好它們兩個row 完全不同
那麼它有的地方它沒有對不對
它是零的地方它有一堆數字
它是零的地方它有一堆數字
如果一個是布希一個是李安的話可能沒有什麼交集
等等
所以它們哪一個row 像不像
你其實在這裡已經看得出來了
那同理呢兩個column 的話也是這樣子
每一個column 代表一篇文章
如果這篇文章是在講奧斯卡
這篇文章是在講九一一恐怖攻擊
那顯然它們幾乎很少交集
除了有一篇有一部電影在演九一一恐怖攻擊之外它們幾乎沒有交集
那反過來呢如果這篇是在講恐怖攻擊那篇是在講在這個攻打伊拉克那搞不好這邊很有關係了喔等等
所以呢你這也是一樣你可以用它們出現的這個詞的頻率的的位置
就知道說誰跟誰比較像喔
所以呢這邊就是說每一個row 它是一個n dimension 的feature vector 代表每一個word 
每一個row 等於是一個那個word 的n dimension 的feature vector 
那這個只是說它每一個dimension 是什麼每一個dimension 就是相對於說它所有的文章
每一個dimension 是那相對於那篇文章的出現的次數
每一個column 呢反過來是每一個document 的feature vector 
等於在描述那篇document 它的特性
所以是它的column 的feature vector 
那麼然後呢它也一樣它是用每一個word 來做每一個dimension 的關係
可是如果你光這樣做的話其實是不夠的
那麼我們說除了c i j 之外呢我們還要做一堆這些東西
這些東西其實就是在做normalization 
我們希望把這這裡面光是這樣數的話其實有一些問題
所以我們要再做一些normalization 
第一個normalization 就是除以n j 
n j 是什麼
n j 是total number of words present in d j 
看這篇文章裡面有多少詞多少個word 
舉例來講假設說這個詞是陳水扁
他在這篇文章裡面出現兩次
在這篇文章裡面出現二十次
欸在這裡只有兩次在這邊只有有二十次那是不是表示一定這篇文章跟陳水扁的關係比較少
這篇文章跟陳水扁的關係比較大呢
不一定
要看這兩篇長短如何
對不對如果這篇文章總共才三十個詞
裡面有兩個是陳水扁
這篇文章很長有三萬個詞
裡面陳水扁才出現二十次的話
那誰跟陳水扁關係比較大
恐怕是這篇而不是那一篇
因為這篇可能很很短這篇可能很長啊
所以呢我們應該要對它的長度做一次normalization 
那就是這邊所做的事情ok 
所以斯 n j 是total number of words 
在那個文章裡面
所以我要除一除
那我這回才比較像了
所以我這回等於是說這個這個陳水扁在三萬個詞裡面出現佔百分之多少零點零三次的比例
這個在這三十三十個word 裡面出現百分之多少
那這個時候就比較有意義了
所以這個是這個除以n j 的意義
那麼前面還有這個一減epsilon i 是什麼東西呢這比較複雜一點
epsilon i 是這個式子
它是有t i 
t i 是什麼是c i j summation over j 
換句話說我是把剛才的這個c i j 對所有的j 加起來
橫的加起來
也就是說是等於是說total number of word present 
不是應該是total number of times 這個word i occurs in t 
t 是整個的document set 
也就是說呢我這個word 譬如說陳水扁
它在這邊出現幾次在這邊出現它在整個的十萬篇文章裡面出現了五千次
ok 那個五千就是t i 
就是我總共出現五千次
這五千次裡面呢
那麼我現在來看它的這個我用c i j 除以t i 呢
就代表說在這五千次裡面它在這篇文章占多少
這是什麼意思呢
那你如果仔細想一想
這個其實就是在算entropy 
那麼如果說這是那個matrix 
這是某一個word i 
這是某一個document d j 
這邊是它的次數是c i j 
所以呢我現在的這個ti 呢是把所有的c i j 全部加起來over j 
就是我這邊所有的數目全部加起來
如果這個是陳水扁的話
那麼它在所有的文章裡面總共出現五千次
在這裡出現二十次
那就是二十除以五千
在這邊出現五次就是五除以五千
等等
那它分別代表說
我這個word 在全部的word 裡面佔百分之多少
它有多少然後這邊是零零零等等
那等於它等於是某一個機率p i p j 的意思
等於是某某一種機率
如果這是這個c i j 除以t i 是某一種機率的話
那你看這個式子c i 這個機率乘以log 再乘以它這個就是我們講的entropy 
也就是summation 的p i log p i 
這不就是entropy 嘛
那這個entropy 是什麼意思呢
你可以想像某一個詞譬如說陳水扁
它會在我們如果這是d one d j 到d n 的話
它的這個機率會是怎麼分佈的
它可能會在很多文章裡面會出現一些
很多地方沒有出現
這個可能是跟選舉有關的
這個可能是跟外交有關的
這個可能是跟民進黨有關的
那這個可能是跟民進黨有關的這個可能是跟修憲有關的
但是還有一堆譬如說是什麼呢但是它就沒有了
那我如果是李安的話會是怎樣呢
它可能都沒有
只有某一些有
其它都沒有了
因為就是這些跟電影有關的才有它
否則就沒有了
如果是李安的話
那反過來我換另外一種詞譬如說非常
或者是今今天
如果換成這種詞的話會怎樣呢
很可能全部都有
那這裡面的這這個最極端的可能就是這個詞
的
如果是這個詞的話呢那全部都一樣
幾乎是完全相同的
全部都有
那因此呢這個p log p 的這個entro 這個代表什麼呢
其實就是它的分佈的情形
你可以看得出來這個其實是什麼這就是entropy 的差別
哪個entropy 最大
這個entropy 最大
然後呢這個entropy 比較小
這個entropy 最小
所以呢像entropy 最大的
就是這種非常的
這種東西其實不告訴我它是在講哪一個topic 
從從我們要分析它是講哪一個topic 的觀點來講
你如果碰到一個今天
其實沒有告訴我任何topic 
那麼因此呢這種東西我應該儘量不要算才對
那麼反過來我碰到一個李安
它非常清楚的告訴我它的topic 跟這個有關
那麼因此碰到這個的時候呢這個就很重要了
所以呢我可以用entropy 來判斷說他告訴我是哪一個topic 的重要的程度
那就是這個pi log pi 
也就是我們這邊的這個
c i j 除以除以ti 其實就是這個pi 嘛
那這個entropy 就告訴我這件事情
只不過呢我現在這個entropy 本身它的range 可以很大可以很小啊
怎麼辦
我就做一個normalization 
那你知道我現在總共的word 數目是大n 
所以呢如果這個機率完全相同的時候
是它的entropy 的上限
就就是log n 
這就這就是我entropy 的的極大值
所以呢我就除以log n 
當我除以log n 之後呢就會變成這個entropy 是介於一跟零之間了
ok 
所以前面除以log n 只是normalize 一次讓它變成介於零跟一之間
所以呢它叫做normalize entropy of 某一個word 在整個的corpus 裡面
在整個的document set 裡面
他顯示我的topic 的鑑別力
我們說如果是李安
這個鑑別力是非常明顯的表示它的topic 是電影
那麼如果這個word 是今天或者是的
它很明顯的沒有什麼鑑別力
它沒有告訴我任何topic 的訊息
喔這就是所謂index in power 
那麼因此如果你這樣做你就知道什麼時候這個epsilon i 會變成零
就是如果它只有一篇文章出現
假設某一件事情只有一篇文章有
其它的完全都沒有的話
那這個是entropy 最小最小的時候就是零
那麼這個這個如果存在的話可能是表示某件事情
譬如說這個某有一個科學上有一個新的發現
發現一個什麼什麼外太空有一個什麼星
只有一篇文章其他都還都還沒有任合人都還有在講那件事
那這個時候只有一篇文章有它
那如果你講的那個什麼什麼星座的話
那個顯然就exactly 就是指那件事了
所以它的鑑別力應是最大的
這個時候說epsilon 等於零的時候反而是鑑別力最大的
所以你你要用一來減
反過來呢什麼時候是epsilon 等於一呢就是真的就是的這個字
像的這個字的話呢就是每一篇都一樣有
那這個時候呢
我的就是它是等於這個t i 除以n 嘛
對每個都一樣
這個時候呢我的我的entropy 就是log n 
所以你一normalize 就是一
而這個東西一減一就變成零
那這些就是沒有鑑別力的像的這種東西
ok 所以呢我現在乘上這個一減epsilon i 是這樣的意思
那麼因此呢我們可以說是
我們雖然這裡的每一個element 是以c i j 為基礎
不過我們做了兩個normalization 
一個除以n j 呢等於是對於這個軸上面我們先做一次normalization 
然後呢這個一減epsilon i 呢
可以算是在這個軸上做一個normalization 
當我這兩個都做過之後
那這回它是比較清楚地描述
這個之間的關係我們底下要用這個來做了
那這個怎麼做呢在l s a 裡面
它的做法是拿來做一堆matrix 的運算
那麼這些matrix 運算是什麼呢我們來解釋一下
那麼這邊的想法其實非常接近我們上週說的eigen voice 裡面的p c a 
是很像的
所不同的地方我現在這個matrix 不是正方形的
你注意到我這個matrix 這邊是w one 這邊是w m 
這個m 是詞的總數
那縱軸是d one 到d n 
這個n 呢是文章的總數
沒有理由它們會一樣啊
因此它是一個長方形的matrix 
那麼我們上週講的那些個p c a 是一個正方形的matrix 你可以求eigen vector eigen value 
長方形的不能做了
那怎麼辦
我們可以這樣子做
就是把w 乘以w 的transpose 
這兩個一乘的話
就會變成是一個正方形
如果這個是w 
那這個是w 的transpose 
那這個是大m 乘以n 
這個是大n 乘以m 
那我乘出來就會變成一個什麼呢
變成一個正方形的m 乘以m 
當我變成一個正方形以後這就是這邊講的w w transpose 
變成一個正方形以後我又可以做這就是eigen vector 的分析
於是我就可以做eigen vector 的分析
就可以變成三個matrix 就跟上週一樣了
我變成三個matrix 相乘
這三個分別是什麼呢
我拆成三個之後
第一個是我的eigen vector 排起來的
e one e two 到e m 就是m 個eigen vector 
因為我現在是m 乘m 的matrix 嘛
所以我有m 大m 個eigen vector 
所以第一個row 就是我第一個eigen vector 
第二個row 就是我第二個eigen vector 等等等等
我可以排到第m 個eigen vector 
那這這m 個eigen vector 構成一個m 乘n 的matrix 
就是這個大u 的bar 
就是我這邊寫的這個東西
就是這個matrix 
那右邊這個也一樣你把它橫的排起來
這個就是e one 第一個eigen vector 
e two 就是它變成row 了
然後呢我有大m 個
把它這樣排
那這個就是大u 的transpose 
就是這一個
所以呢我這邊就是我的大u 
這就是我的大u 的transpose 
那中間是什麼呢中間就是eigen value 所構成的
那我們說呢它只剩下對角線有值
其它都是零
那對角線上的每一個值就是所謂的eigen value 
這個eigen value 我們現在故意把它寫成s i 的平方
所以譬如說第一個呢就是s one 的平方
第二個是s two 的平方等等等等
那為什麼寫成平方其實我們後面會有原因
不過我們現在先這樣寫
換句話說
你要把它的什麼是s one 
是它相對於第一個eigen vector 那個eigen value 的square root 
它的square root 叫做s one 
所以它的平方是它的eigen value 等等
我把每一個eigen value 都寫成他的寫成一個平方
然後我也按照大小順序排列排下來
這跟我們之前講的意思是完全一樣的
好那麼我們現在先要問這個matrix 到底是什麼
你如果看這個w w transpose matrix 是什麼東西的話
它其實告訴我word 跟word 之間的相似度
什麼意思
譬如說在這個在這個matrix 裡面的某一個
這個是第i 個
跟這個第j 個
嗯這個是第i 個這個是第j 個的這個element 
這兩個相乘的第i j 個element 這個東西到底是什麼意思
你看這個值其實是什麼
這個值其實是這邊的第i 個row 跟這邊的第j 個column 
兩兩它乘它它乘它它乘它
兩兩相乘加起來的
那其實這個第j 個column 是什麼
不就是這邊的第j 個row 嗎
對不對
這邊的第j 個column 就是這邊第j 個row 啊
所以其實是什麼就是這兩個在做內積嘛
其實就是第i 個row 跟第j 個row 在做內積
其實也就是第i 個word 
這是相當於第i 個word 
這邊是相當於第j 個word 
這兩個word 像不像嘛
內積就是它像不像嘛對不對
我都已經裡面都已經normalize 過了所以就是說它像它的相似度嘛
因此呢我這個這個matrix w 跟w transpose 裡面的第i j 個element 
其實就是第i 個跟第j 個row of w 它們在做內積而已
也就是說它們之間的相似度
好那麼有這個意思之後我們現在來看我把它拆開來是什麼意思
這個拆開來的意思呢
我們其實我們的目的就是底下要講的這件事
這個跟上週我們講的eigen voice 的意思是完全一樣的
當我用eigen value 把它拆開的時候呢
很清楚地我照大小順序排列之後
我可以把重要的值大的放到上面去
當我把重要的值放到上面去之後
我可以抽前面最重要的
譬如說前面的這r 個
這是我m 乘上r 這r 個
然後這邊我也只抽r 個
這是r 乘上r 個
我這邊也只抽r 個
那這就是r 乘上m 個
那麼我如果只抽這個的話
乘起來會almost 跟這個是完全一樣的
那你回想我們這件事情在上週的eigen voice 裡面是在說怎麼樣的一件事
我們上週在說的eigen voice 是說
我把一大堆的把每一個speaker 的所有的參數做成一個大的vector 
這個vector 可能多達它的dimension 可能四百八十萬個dimension 
那麼於是呢我在這四百八十萬個dimension 上的每一點
其實都代都代表一個speaker 的那一堆model 

不過這個dimension 太大了怎麼辦
我想辦法找一個它的subspace 
譬如說這個是一個它的subspace 
當然我現在沒有辦法畫那麼high dimension 的空間
我只能畫三度空間
於是它的sub subspace 變成是一個兩度空間的
那麼於是呢我真正做的事情是把這裡的每一點通通投影到這上面來
那到時候我會發現其實這個這個subspace 呢譬如說只有五十個dimension 
那所有的點投到這五十變成一個很小的空間只有五十個dimension 
而上面每一點呢都代表原來的每一個speaker 
所以我這一點都可以對應回去
這是我們在上週說的eigen voice 在在在做這件事情
那現在要做這件事情是很像的
你現在雖然我現在每一個word 是有譬如說大n 
大n 是十萬篇文章所以它原來是十萬個dimension 的這麼多東西
但是其實真的要這麼多嗎不見得
我現在這個matrix 
仍然代表這裡面所有十萬個word 裡面所有的的關係對不對
我們已經說了這裡每一個都是代表i 跟j 之間的關係
所以這個matrix 就是代表這十萬這兩萬個word 裡面所有的word 的關係
而它的關係是用這個十萬篇文章來描述的
是一個這麼大的一個matrix 
但是其實我可以把它縮減成為我這邊只取r 
這個r 是多少呢
我這邊有沒有寫嗯我們有有在這
我們通常做啦我們做過我們如果是以新聞來做的話
我用各種新聞來做的話
這個r 大概八百就可以了
通常r 大概八百做到一千五百都差不多
這不需要再大了
所以雖然我這邊有十萬篇文章甚至於一百萬篇文章
我其實大概r 只要八百個到一千五百個就可以了
因此這個所有的的word 之間的relation 其實可以reduce 成為只有這r 個這r 個
這八百個這八百個這八百個
那麼你其實把這三個這個綠的matrix 乘起來呢
跟這個是非常像的
原因是剩下這些都很少
這些都是非常常小的值了
我這邊的時候已經是這個最大的值都在這裡了這裡面非常小的值你可以丟掉
所以後面這堆eigen vector 跟這堆eigen vector 都是可以丟掉的
那麼這畫這就是我們底下這邊所講的
那麼這個本來是m 乘n 
這個是m 乘n 乘m 
那也就是這兩個matrix w 跟w transpose m 乘n 跟n 乘m 
那麼我現在呢
我可以簡化成為只有這個u 
那我這樣的寫法的意思是說
我如果只剩下r 個r 個column 的話這就是u 
上面沒有bar 
ok 那這個就變成這也變成u 上面沒有bar 
所以呢我凡事沒有bar 的u 
就是其實只有m 乘上r 
然後呢這個是r 乘上r 
然後這個呢是r 乘上m 
這是u 的transpose 
我這邊有寫bar 的
就表示是dimension 是大m 
沒有寫bar 的就是reduce 到只有r 個dimension 
其中這個呢就只有r 個eigen vector 所構成的
那麼如果是這樣的話這個意思是什麼呢
那我們也許應該去了解一下
你如果回去看matrix 的數學的話它會說
其實這些eigen vector 告訴我是這樣的東西
也就是說你每一個eigen vector 跟它的transpose 相乘
再中在scaled by 它的eigen value 
其實就是一個matrix 
我們拿它的第一個eigen vector 而言
我這個是一個e one
這是一個column 
那這個e one 的transpose 呢
是一個row 
這兩個相乘是什麼
就是一個整個的matrix 對不對
它跟它相乘是個整個的matrix 
而這整個的matrix 它的weight 給它一個s i 的平方
就是給它第一個eigen value 
那就是這個的這個東西
所以你可以想像呢
我任何的一個eigen vector 跟它自己的row 跟column 去相乘
就得到一個component matrix 
e i 跟e i 的transpose 
就是一個component matrix 
我們給它一個weight 
那個weight 就是它的eigen valuesi 平方
那如果是這樣的話呢
那麼這一個就是相當於這一個eigen vector 所構成的那一個component matrix  
那因此呢我的這個這個matrix w w transpose 這個東西呢
你可以看成是這一大堆加起來的
那這一大堆加起來裡面那它的weight 就是這些個eigen vector eigen value 的值
那我們說這個eigen value 的值它會把大部分的大的值都集中到上面來
我們把它照大小排列
大部分東西擠到這裡擠到這裡
到後面變成很小很小
所以後面這些個就不重要了嗎
所以我這些雖然是全部的i 加起來才會等於原來的
但是你只要加前面的大r 個
譬如說r 等於八百的話
你只要加前面的八百個幾乎就是原來的了
那八百以後的那一大堆
一直到十萬個其實都不重要了因為它非常小
所以你就可以拿掉了
於是呢我現在這個所有的word 之間的relation 
我就可以reduce 到用這三個
小的matrix 只有八百dimension 的來描述它綠色的這塊
那就是我們這邊所用的這個東西
這也就是我們講的一個dimensionality 的reduction 
我只要選擇r 個eigen value 
夠大的eigen value 值就好了
那麼我們底下會說其實這八百個就代表八百個concept 
或者說就是它的語意潛藏的concept 
或者說就是它的topic 
那這點我們底下再解釋
那麼如果說是這樣的觀念
你可以想像的話
那麼我們反過來也可以做另外一件相同的事
就是我現在做w 的transpose 再乘以w 
那這個呢是完全相同的情形
但是我反過來
我先把它transpose 
所以我得到一個這樣子的東西
這是w 的transpose 
它是n 乘m 
然後乘上w 
是這樣的一個這個是w 
這是m 乘上n 
所以這兩個乘完之後變成一個什麼呢
變成一個n 乘n 的正方形
n 乘n 的
那這就是w transpose w 
跟上面剛好反過來
那它也是一個正方形啊
所以我也可以做eigen value 跟eigen vector 
那麼因此呢
那同樣的情形你也可以看這裡面的第第i 個跟第j 個
的這個element 的意思是什麼呢
這邊的第i 個跟第j 個element 
相當於是這邊的第i 個row 跟這邊的第j 個column 
去它跟它相乘它跟它相乘去相加
那這個第j 個
那這個的第i 個row 是什麼
就是這邊的第i 個column 嘛
這其實就是這個嘛
所以其實是在這兩個column 在做內積
所以呢我們說它的i j element of 這個是什麼
其實就是第i 個跟第j 個column 這個w 的第i 個跟第j 個column 在做內積
其實就是這兩個document 之間相似的程度
對不對
所以呢就是說我現在這兩個document 之間有多像
就是這個
所以同樣的
這項剛才的這裡每這個matrix 裡的每個element 是在描述兩兩word 之間有多像
它們兩兩word 之間的關係是在這裡
那這邊是在描述兩兩document 之間的關係是什麼
那你如果兩兩關係document 之間的關係有了的話
那你現在就是得到這個matrix ok
所以呢這個這個matrix 裡面它的i j element 意義跟上面這個是完全對稱的
它的是對word 我這個是對document 
好如果有了這個的話我下一步也一樣我這個也一樣可以拆開來做
三個eigen value 
那麼於是呢我的第一個呢就是
那它的所有的把它的所有的eigen vector 排起來
我這邊寫做e one prime e two prime e 三等等
一直到e n prime 
這就是它的n 個eigen vector 
所以呢我這個e這個e i 呢就是嗯應該是有寫在哪裡
這個e i prime 就是它的orthonormal 的eigen vector 喔
我這邊講的剛才這邊的e i 是orthonormal 的eigen vector 
也就是說我這個eigen vector 求好之後都把它normalize 
變成單位長
變成normal 過的normalize 過的都是這個normalized 的eigen vector
而且呢你可以證明所有的eigen vector 是互相orthogonal 的
所以它們是orthonormal 的eigen vector 
那我這裡也是一樣e i prime 也都是orthonormal 的eigen vector 
那所謂orthonormal 的意思呢
就是它們這個它跟它的transpose 它transpose 跟它相乘會變成identity 嘛
對不對
這個就是我們剛才說的它們都是orthonormal 
所以它跟它如果直接去做它的transpose 跟它做的話呢
就變成這個跟這個去做內積
都是只有它跟它自己做內積別的都是零
所以它們的這兩個相乘變成identity喔 
這個式子的意思是這樣子嘛喔
就是這個東西跟這個東西相乘的話
u 的transpose 就是這個再乘上這個的話
你如果這個東西乘上這個的話
其實就是這裡的每一個跟這裡的每一個去做內積
那它就變成是identity 
這就是它的orthonormal 的意思啊
那我這邊也是一樣
好那這樣之後呢這是我的第一個matrix 
然後第二個matrix 呢是所有的eigen value 
也是一樣s one 的平方s two 的平方等等等等別的都是零
還有第三個就是e one e two e one prime e two prime 等等
喔一樣的
這邊已經黑板不夠大了所以我們就不多畫
不過你可以曉得就是跟上面一樣的意思
這是e one prime e two prime 等等的一個一個row 
等等一直到e n prime 
當我得到這樣之後
這就是我的這些個vveigen vec eigen vector 跟它的eigen value 
不過這這裡有一點很有趣的地方是這些個eigen value 是一樣的
所以我都寫成s one 的平方s two 的的平方跟這邊是一樣的
不像這裡的話我寫e one 這裡寫e one prime 
表示是不同的vector 
e one prime 跟e one 是不同的vector 
這邊是一樣的
嗯一樣的但是呢它們的dimension 不同
因為這邊是r 
這邊是這個m 乘m 嘛
這是m 個
這個呢是n 乘n 是n 個
那怎麼回事呢
應該是說在m 跟n 裡面的那個minimum 的值之內的
它們是一樣的
那麼超過的話呢就都是零喔
換句話說像我這邊所畫的這個大n 大於大m 
所以你可以想像呢
在前面的這m 個而言
的這個就是這一個
後面這些就都是零了
這些eigen value 都是零了喔
那為什麼會這樣這邊都是matrix 數學我這邊不在這裡講這些數學
但是你可以想像是因為這個是w w transpose 
這是w transpose 這是同樣的東西嘛
這兩個其實是同樣的東西只是都是同樣的那個w 所產生出來的東西
所以結果它們的只是說我都拿一個transpose 去乘乘在前面跟乘在後面的不同而已
所以呢它們大小因此變得兩個不一樣
但是它裡面真正的eigen value 的數目是相同的
那什麼數目什麼相同法呢
就是看誰比較小的那個是相同的
超過了就是零ok 
所以對i 大於m 跟n 的minimum 的那個地方的話它都是零了
就是這個意思
那除了這個之外我上面一樣按照大小數目來排列
按照它的數值大小按照eigen value 的大小來排列
那麼因此呢其實這個第一個就是它的第一個
第二個就是它的第二個
一直到第m 個就是第m 個
後面就都是零了
那麼於是你就可以想到其實這個e one prime 的這個eigen vector 
跟這個e one 其實是有關係的
因為它們都相對於同一個e one s one prime 的eigen value 啊
等等這是我們底下要說到的
那麼於是呢我就得到一個這樣子的關係
那麼那麼我現在中間這塊呢就是我這邊所謂的s one 平方
中間這個就是我的s one 平方
就是指這個matrix 
那中間這個呢我這邊叫做s two 的平方
就是指這個matrix 
好當我做到這步之後底下這些事情也是一樣
我一樣的可以發現我只要取r 個
就夠了
我再取這裡面的r 個個最大的eigen value 
譬如說r 是八百個
我只要取這r 個就夠了
那麼因此我這邊也就取r 個eigen vector 
這邊我也取r 個
那這三個相乘這三個小的相乘幾乎就跟這個一樣了
那麼這就是我這邊所講的我這個叫做v 跟v transpose 
這是大寫的v 的有一個bar 的
那這上面這個叫做v 的bar 的transpose 
那麼我當我寫了這個v 的bar 的時候是指全部的
就是v 的bar 是指這整個的方的matrix 
整個的方的v 的bar 的transpose 
那當我把這個bar 拿掉的時候呢
我就只抽了裡面的八百個ok 
當我把這個bar 拿掉之後只抽裡面八百個所以這個剩下的呢
就是我的v 的剩下八百個
就是這邊的n 乘上r 
因為我這邊現在只有只有這個八百個了
那同樣的呢這個是我的
這個這個就會變成我的v 的transpose 
也是只有r 乘上n 
我也變成只有八百個
就是這邊的v 的n 乘上r 跟v 的transpose r 乘上n 
那中間這個變成r 乘上r 
所以這個s two 就變成變成這個是s two 的的r 乘上r 
就是這個s two 的r 乘上r 
就變成只有這個變成變成只有這個只有r 個了
而這個r 乘r 跟這個是完全一樣所以two 可以就根本可以不要寫了
那這個跟這個是一樣的所以呢就就變成只有這r 個eigen value 了
那這就是我們在這邊所講的這兩件事情
我們的這個dimensionality reduction 
那這個意思也是一樣的
就是說我的這個w t w 的transpose 乘上w 的這件事情
我等於是把它拆成很多個component matrix 
就是e i prime 乘上e i 的transpose 
這裡的每一個eigen vector 跟它的transpose 去相乘
就是一個matrix 
那它的weight 就是它的eigen value 
所以你可以寫成這麼多個component matrix 去相乘加起來就是它
你如果這樣子寫的話
那麼後面這些eigen value 都很小
我都可以丟掉
於是我就變成這樣子
那當我變成這樣之後呢這回我可以做什麼事情
這回可以做的事情是我們下一頁所說的
那這就是在matrix 的代數裡面很重要的一件事情叫做singular value decomposition 
我們真正得到的是這個式子
這個式子是什麼呢是
你如果回頭看我們這邊的式子的話
意思是我取左邊的一半
我上面這個關係我取取左邊的一半
就是這個u 跟這個s one 這個s 的兩個
我本來是我取這邊的u 跟這邊的s 
取左邊的這個
我左邊取這個u 跟s 
我右邊取這個s 跟這個v transpose 
ok 我右邊取這個這邊的v transpose 
左邊取它的u 
那中間這個我也只取一個
這邊都是平方哦
這邊本來是平方我現在只不要平方我只取一個
我現在不要平方我只取一個
那就是我下一頁這邊的這個情形
你如果仔細看的話
左邊取的就是u m 乘r 
u 的m 乘r 就是這一個
我左邊取的這個u 的m 乘r 
也就是這邊的這個u 的m 乘r 
右邊取的這個是v 的transpose 
就是這個v 的transpose r 乘n 
就是這個這個那也就是這個r 乘n 
那中間這個呢我只取一個
剛才這邊我都要平方
這個是相當於s one 的平方嘛
這個是s two 的平方嘛都是有個平方的嘛
那我現在不要平方了我都只取一個
所以本來這邊寫s one 平方s two 平方我現在都不要平方了
所以呢就是s one s two 沒有平方了
那這就是s 的r 乘上r 
我就這三個相乘其實就approximately 就是原來那一個
ok 我們再看一次
這個意思是說
我剛才的話呢是要我必須要把它跟它的transpose 相乘
我才有辦法做這個eigen value 的eigen vector 的的的分解
但是這個其實都是兩倍的嘛
因為這兩次方的意思因為它它它跟它兩個它跟它自己相乘嘛
對於是一個平方的東西嘛
那這個也是一樣我也是transpose 相乘所以其實也是平方的意思我可以這樣做嘛
那因此我現在如果只做一個我不要平方的話呢
就是這邊只取左邊
這邊只取右邊
中間只取一個
不是平方
那就得到我們底下這張圖所說的
這邊只取左邊的
所以就是取這個
對不對
所以你可以想像我現在是左邊嗯不對左邊是取這個
中間取這個
右邊取這個
那中間的這個你也可以算成是這個也沒關係因為這個跟這個是一樣的
對不對
所以呢你如果看左邊這兩個的話
看左邊這兩個相當於是它乘上它
你如果看右邊這兩個的話相當於是它乘上它
不過中間這個就是中間這個
它都是一次方只算一次了
我剛才是兩次現在都沒有了那個平方都沒有了
喔這裡平方都沒有了剩下一個s one 跟s r 
那就變成這樣子
那這三個相乘你可以證明它其實就跟剛才這個等這個approximation 是一樣的
你這三個相乘的話
是跟原來那個w 很像的
不是exactly 一樣
是把後面這些丟掉了
是我把這堆東西
後面的這堆東西都丟掉了
或者把這些東西都丟掉了
我把這些個不重要的eigen vector 所代表的那些dimension 都丟掉了
我剩下一個比較簡單的了
那麼這樣子做的把一個長方形的matrix 拆成這三塊
是相當於很像原來的正方形的matrix 拆成這三塊
這非常像的
只是因為它是長方形所以必須拆成這樣的拆法
那這樣的拆法是所謂的s v d 
就是singular value decomposition 
那這個時候你所得到的這些東西叫做singular value 
那麼跟eigen value 有一點不像啊對不對
它是原來這個東西eigen value 的square root 
那叫做singular value 
不過我還是一樣按照大小順序來排列
那這些東西呢左邊這個u 呢叫做我的左邊的singular matrix 
右邊這個v 的transpose 叫做我的右邊的singular matrix ok 
那這樣的話我現在就把它展開就變成這樣子了
當我變成這樣之後這回我們底下就會看到有很多豐富的意義就出來了
那麼其實為什麼這樣子可以得到它裡面的concept 
這底下我們就可以看得到嗯
那嗯我們先停在這裡休息十分鐘
ok 我們下週期中考啊
下週我出國我不在
所以我們除了考試之外不做別的事
因此呢我們考一百二十分鐘就是後面的一百二十分鐘從十點十分到十二點十分
那麼只考那一百二十分鐘
那麼前面不上課啊
ok 我們現在回過頭來說我們這邊講的這些東西到底在幹嘛
那麼我們剛才說這個上週我們講的這個這個eigen voice 的觀念是
是說我有一個四百八十萬dimension 的一個很高維的空間
然後呢我想辦法把它reduce 到一個五十dimension 的的空間
那麼本來這個四百八十萬維的空間裡面每一點代表一個speaker 
其實不要那麼多維啦
我只要找到五十維的一個subspace 
其實每一點投影到上面來
它就在這個五十維裡面已經代表每一個speaker 之間的關係了
這樣的意思
那我現在做這件事情其實是很像的
所不同的是現在我本來每一個word 要多少十萬維
我們說這十萬個dimension 
也就是說每一個word 在這十萬篇文章裡面
分別出現的次數為基礎所得到的這個vector 
代表那個word 的特性
所以呢就word 而言呢我似乎應該我也可以想成是這樣子
我有一個十萬維的空間
這是十萬的dimension 
那這裡面的每一個點是十萬維的vector 
就是一個詞
譬如說這個是某一個w i 
這是某一個詞w j 
那麼基本上是這樣子
可是真的需要十萬維嗎不見得
我現在想辦法做一件事情
就是把這個十萬維reduce 到變成八百維
那怎麼reduce 跟這個情形是一樣的我在這裡面找一個八百維的subspace 
那也有點像是這樣子ok 
我這邊也有也找到一個一個八百維的subspace 
我這裡的每一點都投影到這八百維的上面來
然後呢這些八百維的點其實就跟原來十萬維的是一樣的
那我變成一個新的space 
這邊只有八百維
我在這邊的每一個原來的這裡的每一個點
都投到這邊來變成這個
我其實這八百維其實就是原來的這十萬維
於是我的每一個詞都只要八百維
每一個word 都只要八百維就可以描述了
這是一個什麼樣的關係呢你如果如果回過頭來看的話
就是這個關係
不過我現在需要把它擦掉了嗯
恩我把這個擦掉了
我要畫就是右右黑板那個那個powerpoint 上面的這個圖啦
就是這個圖
這個圖你現在是變成這樣子的一個然後乘上一個這個
再乘上這樣子一個對不對
這三個乘起來相當於原來的一個w 
所以這個是我們所謂的u 
這是我們這邊所謂的s 
這邊是所謂的v transpose 
這三個東西相乘會變成原來這個w 
這個是我們在這邊所說的這個singular value decomposition 的意思
這個u 乘上這個s
乘上這個v transpose
會得到我原來這個這個w喔
不是真的exactly 不過就是一個approximation 
會得到這個
這到底是什麼意思呢
我們現在可以來看
譬如說我們如果前兩個相乘你想會是什麼
這個跟這個相乘其實仍然
這兩個相乘仍然得到一個
這個乘這個對不對
這個是m 乘上r 
這個是r 乘上r 
所以乘完之後還是m 乘上r 
這個是u 乘上s 的一個matrix 
那這個u 乘上s 的matrix 呢
它的每一個dimension 是什麼東西
譬如說這裡的每一個row 
這個是第i 個row 的話
這裡面只有第八百個dimension 
這裡的每一個row你想想看其實是什麼呢 
跟這裡的每一個column 去做內積
就得到這邊的那一個row 
我們我們再講一次
就是這個這裡的譬如說這裡我第i 個row 
我第i 個row 這邊有八百個element 
它是不是這八百個乘上這八百個相加得到第一個值
這八百個乘上這八百個第二個相加得到第二個值
對不對它乘上第三個八百個相加得到第三個值等等
那我這邊有多少個有十萬個
結果我就得到這十萬個
對不對所以呢我原來的這十萬個這十萬個呢
其實就是第i 個word 
在原來這個dimen 在原來的這個十萬維空間裡面的那一個點就是那一個點
那一個點的那十萬個就是這邊的這十萬個嘛
那這十萬的每一個你可以看成是這些東西乘上這些東西加起來對不對
這些東西乘上這些東西加起來等等等等
你如果這樣看的話我們是不是可以回過頭來想
這個東西是什麼
這個東西其實是我們這邊所畫的
e one prime e two prime 等等
所以呢這裡面其實它的每一個row 
這個row 是e one prime 
這個row 是e two prime 等等等等
也就是這個的這個的eigen vector 
那如果我把這個值叫做第一個值叫做a one 
第二個值叫做a two 等等的話
那這個vect 那這個vector 其實是不是summation 的a i e i prime 
summation over i 
這個就是這個
再講一次喔
你仔細你要想一想才能夠想清楚這件事喔
就是說你可以想成是這個eigen vector e one prime 乘上這個a one 
再加上這個eigen vector e two prime 乘上這個a two 
這個e 三prime 乘上這個a 三
到這個e 八百乘上這個a 八百
這些東西是乘加起來其實就是這個嘛
這中間關係其實就是這樣子
因此才會這裡的這裡的八百個跟這八百個相乘得到第一個
就是它們的所有的第一個dimension 
這些個eigen vector 第一個dimension 分別乘上這個
得到這個的的第一個dimension 
這些eigen vector 第二個dimension 分別乘上這個
加起來得到這個第二個dimension 等等
那這樣我有十萬個嘛我十萬個就這樣得到了
這樣的關係其實就是e one 的eigen vector 
e one prime 乘上這個a one 
e two 的vector prime r 一個vector two prime 乘上a two 等等
加起來就是a i 的e i prime 
就是這個vector 
那這個的意思其實是不是相當於這八百個e i prime 
就是這八百維
所以現在這個個其實就是第一維就是e one prime 
第二維就是e two prime 
第三維是e 三prime 等等
我沒有辦法畫更多但其實這八百維是在這裡
而這裡的每一點呢
就是那些個a one 到a 八百
也就是說
我現在的a one 到a 八百是什麼
是這裡的一個row 
就是這裡的這兩個相乘的那個row 
換句話說
我原來的這個大matrix 裡面
這十萬維的vector 
代表一個word 
我現在在這個matrix 裡面我只要八百維就代表了
這八百維跟這十萬維是同樣的事情
只是
這八百維你如果分別
每一維其實代表這八百個每一個分別代表這裡的一個eigen vector 而已
ok 
所以呢我等於是
這八百個我等於是在這本來這十萬維的空間裡面
我找到八百個十萬維的vector 
它們是orthogonal 的
構成一個八百維的子空間
那這裡面八百維的子空間裡面每一點其實就是這裡面每一點對應過來的
而對應過來之後呢
我只要
這裡的每一個component 其實就是這些東西的weight 加起來
就得到原來這一點了
那這個意思跟我們原來這個意思是完全一樣的
我等於把十萬維的空間所描述的所有的
word 所有的詞之間的關係reduce 到一個八百維的空間裡面的關係
其實它們關係是完全一樣的
有一點點error 但是error 很小就是了
ok 
那麼這個情形就是我們在在這邊講的
那這這八百個其實就是那十萬維裡面的八百個vector 
我在那十萬維裡面找到這八百個
每一個都是十萬維哦
這都是十萬維的
但是我現在有八百個這十萬維的的東西構成一個八百維的空間
那你裡面的每一個原來這裡的每一點就變成這八百維的那個點
因此它們這些裡面的每一個值
就代表他們的weight 
就是這種weight 加起來
就是這個東西
那這個話就是寫在這邊的這個
你現在所謂的u 這個這是這個u 乘上這個s 
得到u 跟s 相乘得到的這個matrix 
u 跟s 相乘這個matrix 裡面的每一個row 
我叫做u i 的
加一個下下面加一個bar 
一個underline 的u i 
就是
所以這個東西就是我那邊講的那個u i 
有一個underline 的u i 
那你的這個u i 呢
就是原來這個u i 乘上s 嘛
對不對
就是這邊的這個u i 
這個這個八百維的也就是這邊的這個的
的第i 個row 
這個u i 乘上那個s 
就得到這個u i 的bar 
的underline 的u i 
這個東西呢就是一個vector 
它的dimension 由原來的十萬reduce 到現在只有八百
而這個東西呢
你可以看成什麼呢
那這八百個row vector 
那其實呢
就是這八百個row vector of v t v transpose 
就是這邊這八百個row vector 
其實也就是原來v 的八百個column 
這八百個row vector 就是這邊v 的八百個column 是一樣的
那這些東西呢也就是原來這個w transpose w 的
eigen vector 裡面最重要的八百個
構成一個orthonormal basis 
那那個basis 就是我們所謂的latent semantic space 
就是那邊的那個八百維的那個空間
就是一個潛藏的語意的空間
它的dimension 呢就是八百
而在這裡面呢
我每一個u i 的underline 的u i 呢
就是一個詞
也就是說我在原來的要十萬維的那一個詞
現在變成只有八百維就夠了
那也就是說我把這個十萬維的裡面
我找到一個八百維的把它通通都投到這八百維上面去了
那或者說你可以想像我現在有一個八百維的
那這裡的每一個維都代表了一堆東西了
那麼這些究竟是什麼東西呢
其實你是可以這樣子想的
這個我寫在下兩頁哦
哦我想想看在這裡
在這一頁裡面的這句話
每一個component 在這個reduce 的word vector 裡面
就是association of the word with the corresponding concept 
什麼意思
我沒我我現在說這裡的每一個eigen vector 代表某一個concept 
你可能很難想像為什麼這是一個concept 的one 
這個是concept two 
這個每一個代表一個concept 
代到底到底是什麼concept 呢
你可以看
這個eigen vector 就是這個eigen vector 
那如果在這個eigen vector 裡面你可以發現譬如說它這個詞
佔了零點零三
這個詞佔了零點四五
這個詞佔了零點三一
等等這些加起來之後
就是這個vector 
那你去看欸
這邊講的這個零點四五這邊是恐怖攻擊
這個零點三一這個是賓拉登
這個零點零三這個是布希
這個零點零一的那個是白宮
你發現它們加起來其實它代表就是
九二一嗯就是九一一恐怖攻擊的那件事情
它們
ok 你了解我的意思喔就是說你現在如果去看這個eigen vector 它裡面的值因為它是一個normalized unit vector 嘛
它某一些一堆相對於一堆詞都是零
因為它跟那些詞沒有關係
它跟某些詞有關係
如果這個是恐怖攻擊它有零點四五
這個是賓拉登它有零點三一
這個是白宮它有零點零三
那個是紐約那個是零點二什麼東西你加起來發現你會發現
它其實就是在講那個event 
那那個就是一個concept 
那同理呢你如果看一一三的話它有另外一堆是零
另外一堆有數字
你可能發現這個是講台北市政府
那個是講國民黨
譬如說這個有零點二一
這個講台北市政府
這個零點零五
嗯這個零點一三
這是講國民黨
那這個是零點一二
這個是講這個什麼選舉
你把它加起來發現這個就是是馬英九
那如果這樣的話那這個其實就是e two 這個就是馬英九
啊等等
所以你可以這樣想的話呢
那麼其實你只要看它裡面相對於每一個word 的weight 
你可以看得出來它其實代表
某一種concept 
那如果是這樣的話那現在這個word 是什麼
這個word 是這個concept 有零點幾這個concept 有零點幾這個concept 有零點幾加起來的結果
就是這個
於是你會發現說
現在如果這個這個word 
你會發現ok 它
馬英九有零點三五
陳水扁有零點二一
這什麼什麼都有結果發現它什麼
它其實是對美外交
啊等等
ok 
那所以這個word 其實就是對美外交
那它就是跟這些個concept 都有關係
那麼它們的關係就是這個weight 
就是這些a i 
ok 
那就是這邊講的這個意思
就是每一個component 在這個reduce 的vector 
u u j 的這個嗯underline 的u j 就是這個東西裡面的這裡的每一個a i 
這些東西其實就是它的
這個word 
這個word w i 我們講譬如說這個是這個對美外交
那這個word 的話呢它裡面的
譬如說這個跟這個陳水扁有多少的關係
跟馬英九有多少關係
跟外交部有多少關係跟等等等等等就剛好那它就是它們的
相對於每一個corresponding concept 的association 
那就是這些東西
ok 
那這樣的的話呢嗯
恩我們等於是把把這個word 的所代表的這些concept 
我們把它抽象的具現出來
發現這些東西是e one e two e 三到八百個
當然剛才舉的例子是比較具體一點或者應該講誇張一點
你真的是不見得看得出來啦喔
你如果真的去做這樣的分析之後
你要看每一個e i 代表什麼concept 
不是那麼容易看
但是有一點這樣的味道
那基本上呢你可以發現
大概每一個分別因為它們都是orthogonal 的
基本上是都是不同的東西
代表不同的concept 
那如果是這樣的話呢我上面這句話的意思
就是說我現在的word 在這個空間裡面所代表的是
只要越接近表示它們的是比較相關的
因此呢你譬如說這個如果是
一個陳水扁一個總統府
那這就會比較就會在附近
你如果是賓拉拉登跟這個阿富汗也會比較接近
那所以呢在這個地方
你在這邊也會比較接近不過這地方難看難看因為有十萬維你搞不清楚
但是這邊呢會清楚很多
相關的詞彙它們相關的詞它們的concept 接近的話
它在這些dimension 上是會接近的
因此呢如果有有類似的similar 的這個語意上的關係的話
它們在這個空間裡面的relation 應該是比較接近的
而且這裡有一個很大的好處是現在
它們只要有appear in similar type of document 就可以了
不需要exactly in 不需要in exactly same document 
什麼意思呢
就是說你如果在原來的這個w i 裡面
原來在這個w i 裡面
你如果要賓拉登跟這個阿富汗有關的話
它們必須出現在相同的文章裡面
這篇文章裡面賓拉登也出現好幾次阿富汗也出現好幾次
在這篇文章裡面也是
必須它們在同樣一篇文章裡面一再的同樣的出現
我才知道這兩個賓拉登跟阿富汗是有關的
可是我現在不是不再有十萬個了
我現在只有八百個
那每一個八百個是concept 不是那篇文章啊
我等於把很多篇文章reduce 成為一個concept 
所以呢這個時候我不再需要它們要出現在同樣的文章裡面
我只要讓它們出現在類似的文章就可以了
similar type document 
那很可能這些文章merge 成為一個
這些文章merge 成為一個
嗯你可以這樣想
所以我才會變成由十萬變成八百嘛
所以它們只會出現在類似的文章裡面
它們就會發現在那八百維的空間裡面是很接近的
雖然在這邊沒有出現在同一篇文章裡面也可以
那這是它一個非常大的優點
也就是本來我們講的某一個concept 
不是一定要哪個詞才是那個concept 
當你每次講對美外交的時候不是一定要有陳水扁也不是一定要有外交部
你凡是講到
布希你凡是講到什麼
凡是講到這個什麼其實它們都是講同一件事
那麼因此呢你不見得要有同一個詞
也不見得要在在同一篇文章裡面
那麼因此呢它現在就是這樣子
那麼我現在就是就是這個這個它們不見得一定要出現在同一篇文章裡面
只要在相類似的的文章的type 裡面我就可以把它們抓到發現它們是很接近的
那這個大致就是我們剛才講的這個這一頁的意思
因此呢我現在要代表一個word 
原來是要這邊十萬維
現在變成就是這個u u i 的bar 
就是這兩個相乘的
這個東西的u i 的bar 
我只要八百維就夠了
那麼這個意思有一點好像是說我把原來的這個十萬維
這十萬維呢是等於是discrete 
由n 個document 所define 出來的
我有十萬篇文章對不對
我有十萬篇文章所define 出來的十萬維
我現在reduce 到變成只有八百維
而這好像是變成continuous 的了
因為這裡的這八百維是這八百維
這八百維是這它有零點幾這個零點幾這些加起來
所以好像是一個好像continuous 的東西
把一些東西reduce 成為一維
把一些東西reduce 成為一維這樣子來看
那麼因此呢那這八百維的的每一個每一個dimension 是什麼
就是我這邊的eigen vector 
那也就是也就是我現在我的vector 就在這個上面表現喔
那這個觀念如果你可以想像的話
那下一頁跟這個是完全平行的
我下一頁我圖都完全一樣就是上面圖是完全co 過來的
就上面這個圖也就是下面這個圖
是完全co 過來的
那我現在是反過來來看後面這一半
我剛才是講這兩個相乘是這個
那我現在可以看另外一件事情是這兩個相乘
這兩個相乘了還是這個
還是一個這樣的東西
這個是r 乘以r 
這個是r 乘以m 
所以這兩個相乘之後還是r 乘以m 
就是這個東西
那同樣的情形我現在在這上面看的
這裡的每一個譬如說第j 個document 這個
這也是八百維
其實相對的是這邊的八百維
這是e one e two 這邊的八百維
e m 
那如果你剛才那個觀念可以了解的話現在是完全一樣對稱過來
我剛才是這兩個變成一個r 乘m 乘r 的
這寫錯了
這個是r 乘n 啦
我現在是這兩個相乘的仍然是一個r 乘n 這寫錯了
變成一個仍然是一個r 乘n 的
但是現在這裡的每一個column 其實只有八百維
就代表剛才這裡的column 
是有二萬維
我剛才的每一個column 
這邊的兩萬維
也就是這邊的兩萬個嘛
也就是說在這裡的每一維它這邊有多少個詞對不對
它每一個維代表它跟這個詞之間的關係
每一個維代表它跟一個詞之間的關係
這樣我也總共有兩萬個詞
所以有兩萬維
來代表這個document 
那我現在這個document 也不再需要兩萬維了
我只需要八百維了
為什麼只需要八百維這意思是一樣的
你可以想一想這八百這八百個
變成我的b i 
如果這個叫做b one 
這個叫做b two 的話
這個b one 其實乘上這個e i 
summation 的b i e i 
這個b one 乘上這個e one 
b two 乘上這個e two 
全部乘起來加起來就是這一個
這個詳細的數學我想你自己去去figure out 
這個這個觀念跟剛剛是相同只是反過來
所以呢這邊的這個兩萬維的這個vector 
其實你可以看成是這八百維的e i 
分別weighted by 這個八百個b i 加起來的結果
所以b i e i 就是這個
就像剛才的a i e i prime 就像這個是一樣的
那麼因此呢我這邊的也有相同的情形
就是我本來的一個document 
是要兩萬維的
這是另外一個space 
我的documents 
我這邊有一個兩萬維的空間
這裡面的每一個點代表一篇document 
代表一篇文章
那我也是一樣這兩萬維裡面我重新找一個八百維的子空間
之後我把這些所有的點都投影到這個這個八百維上面來
因此我其實reduce 成為一個八百維的一個新的只有八百維的subspace 
這裡面的每一個dimension 
其實就是e one e two e 三
當然我只能畫三個但是你只能想像有八百個
這個e one e two e 三就是剛才那些個那些個eigen vector 
那於是我把這些點呢重新點到這上面來只有八百維了
那之後呢那就是我們這邊所講的這件事情
那這裡面的所有的話都跟剛才是平行的
所以這個意思你只要剛才的了解的話其實就是反過來就一樣了
所以這個第一句話的意思
跟剛才的那第一句話是一樣的
我的每一個剛才說我每一個row 代表一個word 
本來說要八百個我現在只要本來要十萬個
現在只要八百個了
就是這個u i bar 
u i 的underline 就是u i 乘上s 
s 就是這個u i 乘上這個s 
這個乘上s 就是這個i 
那我現在也一樣就是這個v j 的bar 是什麼
就是這一條
真正講應該是右邊這個啦
就是我們真正講它是一個column 
它是一個column 
所以呢我現在的這一個
這一個就是我現在講的這一條
就是我這邊說的v j 的bar transpose 
v j bar 的transpose 
嗯這樣子寫是因為我完全follow 剛才reference 裡面第一篇的reference 它的寫法
它的寫法裡面凡是寫一個v j 這種東西都是一個row 
所以現在這個明明是一個column 
是一個column 所以它就必必須要寫一個transpose 
所以我這邊講它其實是一個column 
但是你要把它寫成就把它寫成transpose 所以就是就是這個東西
就是這個v j 的bar 這個東西v j 的bar 的transpose 的這個東西
那它是什麼是s 乘上v j 的transpose 
它是什麼
它就是這個s 乘上這裡的這一個
這裡的這一個第j 個
這個s 乘上這個就是這個嘛就這樣看對不對
這個乘上這個就是這個
所以也就是s 乘上v j 的t 
就是這裡的就變成加一個bar 
所以加一個bar 是表示這兩個相乘的結果
剛才再加一個這兩個相乘的結果是一樣的
那這個是一個這是寫成一個column 
你如果要寫成row 的話就變成這樣子了
這個是因為那篇我第一個reference 它的寫法
把這個寫成這個也是可以的那只是把它transpose 一下
所以這個的transpose 變成這個嘛
那這個transpose 變成這個嘛
把它transpose 過來的話就變成這個是寫成row 的寫法
所以我說這個是row 的寫法
這是column 的寫法就是了
但這個意思是完全一樣的
就是我現在本來是有這裡本來是有兩萬個dimension 
代表兩萬個詞的這個vector row vector 
現在變成只有八百百維了就是這個意思
那麼因此呢我這本來是有兩萬個word 所代表的這兩萬個dimension 
也就reduce 到只有八百維了
那這每一個維是什麼呢
每一個維是這邊的eigen vector 
這個eigen vector 是什麼其實你也可以看
譬如說你如果看這個eigen vector 的話呢
這個呢其實相當於這個
那你看這上面是什麼譬如說e one 是什麼是這個
那你看它是它是這個這個嗯這個就是零點三五這零點多少這零點多少
那你看它哪些是零
它代表哪些詞你把那些詞的觀念加起來其實就是代表它的那個concept 等等
那我們剛才的的詞的時候的那個dimension 呢
你應該我可能講錯一點
你這個零點零三這個其實是這個對應的是這邊的document 
所以你應該是說譬如說這個零點四五這個零點三一
這篇document 是在講九二一恐怖攻擊
這個零點三一是在講阿富汗跟達凱組織什麼什麼
你把它加起來的話它們就是所以這裡的每一個是代表這邊的一篇文章 
ok 所以呢我們剛才在詞的那裡的那八百維
在詞的那裡的那八百維每一維
是這個e i prime 
那這個e i prime 這裡面每一個component 
其實是代表哪一篇文章
你可以去看這些文章加起來是什麼意思
就是那個concept 
那我現在的這裡的這個八百維就底下這個八百維呢
變成是我是這個
那每一維是這裡的每一個component 
它代表的是它這邊的的詞
你把這些詞的意思加起來就是那個觀念
就是那個concept ok 
好那麼如果是這樣的話
那我們也同樣的嗯就是這邊所講的
就是說我現在就是把這個嗯這些eigen vector 
就做成變成normal 這是orthonormal basis 
來展開一個space
那它dimension 就是底下這個space 
那在這裡面也同樣情形我們在下一頁的這一句話
是在講這一件事情
就是說呢每一個component 在這裡面呢
就是代表association with 這個concept 
也就是我這就是就是在講這件事
這裡的每一個component b one b two 分別代表這邊的e one e two 的weight 
就是這個意思喔
所以呢每一個component 在這個裡面的
就是這個component 的就是這裡每一個e i 的association 或者它的weight 
所以就代表這個document 其實是哪些concept 
那因此呢這樣你大概可以想像就是說
我每一篇我每一篇document 
其實是裡面有好些個concept 
而每一個word 也有好些好些個concept 
那麼因此呢
這裡的每一個document 是是一堆這種東西
有好些個concept 加起來
這裡的每一個word 也是這些東西
也是好好些個concept 加起來喔這樣子
好那如果是這樣子的話
那麼我們上面這句話也有類跟剛才是這句話是對應到剛才這句話了
那麼上面這句話是對應到上面這句話
就是說我現在在這個空間裡面
如果兩篇文章講的東西很像
它們都在講紐約恐怖攻擊的話
那兩篇文章在這邊就會很接近
就會在這裡
那因為它們相對的那些dimension 會在一起
所以它們就會在這裡接近
而它們接近的時候呢
它們不需要有exactly same words 
就是說我原來在這個我原來在這個裡面
你如果要說這篇文章跟這篇文章像的話
除非它們的word 都一樣
同樣的word 它們都多
同樣的word 它們都少
這樣才我才知道它們兩個是相像的
我現在不用了
我現在是在那個八百維的空間裡面
不見得需要有完全相同的word 才知道它們像
我只要知道那八百維裡面那空間上距離近就像喔
所以呢我現在就是說它們不需要再有完全相同的word 
只要它們有有這個類似的type of word 
同一類的word 在一起的話
就表示它們是同一個了
所以呢它們只要是concept 接近的話就會在那裡就會接近
好那如果這個個concept 你可以了解這個這個想法你可以了解的話
那我們這兩頁的底下這句話你大概就可以想像了
就是說我把原來的所謂的association structure between words and words and document 
就是這個matrix 
這個matrix 所描述的就是word 跟document 之間所有的relation 
我現在呢可以可以完全保留
幾乎是完全保留
而且我可以把 noise information 拿掉
但是我的 dimension reduce 到變成一個只有 r 個 dimension 
也就是說原來的這一堆詞
這些 word 之間的關係 
reduce 到這邊來
那麼所有關係都在
原來這些 document 之間的關係 
reduce 到這邊來原來都還在
而且我還可以把 noise information 拿掉
什麼叫 noise information 拿掉
你可以想像其實就是在這個過程之中我把這些東西拿掉
我把這些東西拿掉我把這些東西拿掉
就這些東西其實是很可能造成 noise 的部分
我保留了最乾淨的部分這是 eigen value 的意思 
eigen vector 的意思
你記得 eigen vector 就是在做它不是隨便找一個八百維
它是找最有意義的八百維
你記得我們在講 p c a 的時候講過一個 case 
就是你如果這些點在這裡的話
你最後找的是這個這個軸
因為在這個軸裡面它的分得最開
而不會你不會找到這一軸
因為這軸它們比較緊
你不會找這軸這軸它們比較緊
一樣的意思
那我其實是找一個真正能夠描述它的 distribution 
最清楚那些 dimension 
我現在八百維都是這樣來的
都是找到它最能夠描述它 distribution 的那些 dimension 
所以呢我是把一堆我丟掉的是那些
所以我丟掉的這些東西基本上是比較 noisy 的
這些東西或者這些東西是比較 noisy 
我可以得到比較乾淨的
那麼於是我可以得到一個但是我的這個 association structure 幾乎是維持不變
然後呢還有還有一個有趣的地方是重要的地方是這樣
我由這個十萬維成八百維的時候
這是一個詞的空間
我這兩萬維變成這八百這是一個文件的空間
可是你發現這兩個空間其實它的每一個 dimension 是對應的
這個 e one 就是 e one prime 
每一個 dimension 就是對應的
為什麼
因為它們都是對應到同一個 eigen value ok 
也就是說這裡的 e one 的那個 concept 
其實對應到這個 eigen value 
這個 e one prime 的那個 concept 
也是這個 e one prime 其實是同一個
所以雖然說它們是兩個你真正講起來是兩個 space 
這個八百維這個也八百維
可是其實如果 e one 是描述恐怖攻擊的話
這個 e one prime 也是的
是同一個 concept 
如果 e 三是描述對美外交的話 e 三 prime 也是的
它們其實是同一件事
因此呢你也可以想像成我真的需要畫兩個嗎不用
我可以畫成一個行不行可以
所以呢在有的人的說法裡面它就說
其實我只有一個就夠了
這個 dimension 是 e one 
同是也是 e one prime 
因為是同一個 concept 
這個 dimension 是 e two 
同時也是 e two prime 
是同一個 concept 
你如果這樣看的話呢
我的詞也在這裡
我的文件也在這裡
它們通通都你可以看的是都在同一起
這樣也可以
但是你可以想的其實是這兩個啦
其實是這兩個啦只不過它們的每一個 dimension 其實是指同一件事
那畢竟這裡的每一個是一個兩萬維的代表 document 的一個 concept 
這裡每一個是十萬維的代表 word 的一個 concept 
但是其實是講同一件事
所以你也可以是想的是同一個
你如果想成是 concept 的話呢
那就是同一個了
這是另外一個說法
這是 concept one 
這是 concept two 
如果這樣的話這就是同一個了
ok 那這個就是我們這邊在做這個 s v d 的意思
那有了這個之後
我們再下來的如果這點都能夠想像的話再下來就比較容易了
譬如說我們可以拿來做什麼事
這個剛才講的這套就是所謂的剛才講的這套就是所謂的這個嗯 latent semantic analysis 
所謂的 lsa 
那這個東西可以拿來做很多用途
不僅僅是做 language model 
那麼我們可以看一下有些什麼用途
譬如說把詞分群
詞分群幹嘛
就是把相類似的代表相同意思詞 group 在一起
這個可以做很多用途
譬如說一個例子是做 language model 做 class space language model 
你記得我們之前講過的
我可以把相類似的詞變成一個 class 
然後拿那個來做 class 的 n gram 
而不要做每一個 word 的 n gram 
那這個詞分群這一個方法就是用這個
那同樣呢因為你知道你哪些詞在一起嘛
你現在每一個詞都是這八百維裡面的一個點了
那你可以在這邊做 v q 或者做什麼東西都可以了嗎對不對
你就可以把詞分群了嘛
那同樣你可以做 information retrieval 
就是說你在做搜尋的時候
譬如說你現在打進去 google 
我要找九一一恐怖攻擊
那它只會找文章裡面有九一一恐怖攻擊的事情
如果文章裡面沒有講九一一但是它講了賓拉登呢
會不會呢不見得會
可是在這裡我就會啊
因為我知道九一一跟賓拉登是在一起的它們很近嘛
所以呢因此我在做 information retrieval 
就是你在搜尋的時候我可以不完全根據字
而是你可以看它們的在這裡面的距離近不近
近的就是就是有關的嘛
那這邊就是剛才已經講過了
因為我的 word 在這裡
你只要看兩點接近就代表它們的意思是相關的
那一個可能的做法
這是一個例子不是這是 example 的 similarity 你怎麼量這個詞像不像
這個例子就是內積嘛
對不對你現在有兩個 vector 在這裡
你算它們像不像你就算內積嘛
那這個內積就是就是這個嘛
就是內積除以它們的長度
這就是 cosine theta 
就代表它們的相似度嘛對不對
這是一個例子你可以用這個
那就用這些 vector 代進來做
那這裡的每一個 vector 其實就是 u j 的 bar 
就是 u s 
這裡的每一個我每一個 vector 怎麼算就是畏用這個來算
就是由 underline 的這個 vector 
也就是 u 乘上 s 
u i 乘上 s 
所以你就是 u i 乘上 s 去算
就變成這樣子
那這就是詞的相似度
你可以詞可以做 clustering 
同樣呢我文章也可以做 clustering 
文章 clustering 是什麼呢
你可以想像有一個什麼用途譬如說 class language model 
我現在從我現在上網抓十萬篇文章
我可以用這十萬篇文章 train 一個 language model 
不過這個 train 出來 n gram 是很亂的
因為裡面什麼東西都有
它從從這個李安的電影到賓拉登到什麼全部都在一起
混在一起變成一個大的 language model 之後
中間的各種 n gram relation 是被攪混的
那因此呢我可以把它分群嘛
我如果上網找了一百萬篇文章之後把它分成一百群
每一群是比較接近的文章
再把那一群去 train language model 
那就會比較好嘛對不對
那這就是所謂的 class 的 language model 的意思
我可以把找到的一百萬篇文章先分成一百群
分別去做 language model 
同樣我也因此可以做 language model 的 adaptation 喔
這個我們底下還會再說到不過你可以想到就是
那我現在如果知道這一百個
我分成一百群做成一百個 language model 的時候
它們每一百每一個有各自的 topic 
有各自的 concept 
因此 depends on 你現在講什麼我就用哪一個
那這個就是 language model 的 adaptation 
同樣我也可以做 information retrieval 
我今天我要找什麼東西的時候
我如果把這一百萬篇文章先分成一百群
你輸入一個一個 instruction 
我先看你要找找的是哪一群
再從那一群裡去找比較好找嘛等等
所以你文章分群的話也可以很有用的
那這些都是我們的 linguist processing 
那文章分群在這邊的這邊的這個剛才已經講過了
那我的做法我就是找文章之間的關係
這也就是剛才一跟這個完全一樣的
求 cosine theta 就是做內積
然後除以它的長度
只不過我現在每一個 v 呢是這個 v 
這個 v 是什麼v 是這個 s 乘上這個 v j 
所以呢就變成這樣子
嗯那這就是文章分群
那同樣呢我現在要做搜尋
我要做 information retrieval 的話
你說我要找什麼東西的時候
我們原來像 google 現在的基本上現在是一個 lexical matching 
就是在 match 它的 word 
你說我要找九一一恐怖攻擊它就去找所有的九一一恐怖攻擊的
但是如果那篇文章裡面沒有講九一一恐怖攻擊
可是裡面講了賓拉登講了阿富汗
它不知道
那你如果有這個的話你就知道了因為你現在是 concept matching 
所以你可以去 match 它的 concept 
所以呢你的相關的文件你是可以找得到的
只要有相類似的 concept 
它們不需要有 exactly same words 
也可以找得到
那這個做法怎麼做
簡單的解釋就是說你把你的 query 當成是一個新的 document 
放進去
然後去量它的 similarity 這是一個簡單的例子
什麼意思就是說你的我現在要找我要找賓拉登恐怖攻擊什麼你就把你那個文章
把你這句這個 query 你把它輸入這個 query 
當成是一個 document 
你就去看那個 document 跟其它所有的 document 那個相似度
就放在那個空間那個那個八百維的空間裡面去算
它跟誰像那就是誰嘛
喔等等這就是做這個 information retrieval 
好那這時候有一個重要的問題我們要解決的要說明的就是所謂的 fold in 
什麼叫做 fold in 
fold in 是說我們這這整套是假設
我上網找到十萬篇文章
我有一個辭典是兩萬詞我都已經做好了之後
我整個這樣做做完了
做好之後我得到這一套
但是呢我網路上不斷有新的文章出來啊
今天每天多了一萬篇新的文章我這新的文章怎麼辦
我每次新的文章來我要重新 train 一次
那這樣不是累死了嗎
所以最好是我 train 好之後新文章可以放進來
我把新的文章塞進這個 model 把它放進來就好了
我 as long as 新的文章所描述的這些 concept 沒有新的
如果有新的 concept 的話那當然你得要重新把它弄進來了
但是如果說你描述的新的文章沒有新的 concept 
只是原來的話
你就把它塞進來就好了
怎麼塞就是這邊所謂的 fill fold in 
就是把新的文章只要沒有新的 concept 的話
就是 assuming 它們沒有新的 concept 所以這些 u 跟 s 都不變 
u 就是左邊的這個 matrix 
左邊這個 singular matrix 
v 就是右邊singular 這個左右都不變
那麼中間這個都不變
如果這樣子的話呢我新的怎麼做
我只要讓這個再多增加一維
這個叫做 d d p 
我剛才如果 d one 到 d n 是十萬篇文章的話
我新的文章進來就是 d 十萬零一篇
那個 p 就是在後面我多加一維就好了
如果這個多加一維的話呢
那這邊其實發現就是這邊多加一維
所以這就是 v p 
這邊多加一維意思就是這邊多加一維那這個不動
這個不動那就是我們這邊所講的這件事情
ok 就是說你現在如果一篇新的文章進來 
outside outside of 原來那個 training corpus t 的話
我們只要假設如果它的整個 language pattern 
跟它的 concept 都是不變的話
那麼我其實只要把新的放到這來
因為新的那些文章我馬上可以數一數它裡它裡面有哪些 word 
我已經把它裡面哪些 word 數一數我就得到一個新的 d p 在這裡
所以 d p p 大於 n 就是指在原來的十萬之外的
譬如說十萬零一
我就排在這裡
排進來之後呢那我整個可以不要動
其實原來的這個 relation 不動只是這邊再多一行
這一行是 v p 就是了
就是這邊的這一行跟這邊這行是一樣
就好像這邊這一行跟這邊這一行是一樣的嘛
所以你這邊多一行就是了
或者說這邊也多一行
這邊也多一行
那麼所以這個乘以這個多的這一行就是這一行
那這一行就是原來的這一行就這樣子
你把這行塞這邊多一行這邊多一行就好了
那麼它們的關係是什麼呢就是這個式子
這個式子其實就是原來這個式子是一樣的
原來的這個式子是說你這個整個的 w 
是 u 乘上 s 乘以 v transpose 
這 u 乘上 s 乘上 v transpose 
那這個意思你可以看成是 
u 乘上 s 乘上這裡面的每一個 column 
就是這裡的那個 column 
就是這個意思 ok 
所以你原來的 w 是它乘以它乘上它
跟它裡面的這個 column 
相當於它乘以它乘上這個 column 
是一樣的意思
那我現在只要把這個 column 換作這個 column 
這個 column 換成這個 column 就一樣了
所以就變成這個式子
所以這個 d p 呢就是 u 乘上 s 乘上 v p 的 transpose 
就是這個東西
它這邊有一個這個 t 還是一樣的
因為在在那篇 paper 的 notation 裡面
這個 v 是一個 row 
你要現在是一個 column 所以就是加一個 t 就是了
所以這個 column 其實就是這個 u 乘上 s 乘上這個
就是這個式子
那如果是這樣的話呢這個很容易求啊
那 u 跟 s 你已經知道啦
所以這個你也就知道啦
那就底下這個式子的意思 
ok 這個 u 跟 s 是我原來 train 好的
新的文章進來這個就數一數有幾個 word 就出來了
所以這個也很容易求
如果這也很容易求的話呢那這兩個都已知那這個很容易求嘛
就是你拿這個式子求一求變成這個式子
底下這個式子跟上面這個式子是完全一樣的
只是說你現在如果 u 是已知 s 是已知
 d p 是很容易求的也是已知
那你 v p 怎麼求就是這樣求
只是在解這 equation 而已
那這個也沒什麼特別其實就是你要的這個v 這個是 v p 的
這個是這個式子是 v p 的 underline 
這個 v p 的 underline 呢就是這個 v p 乘上 s 
就是這個 v p 就是這個乘上跟 s 相乘
那其實是什麼呢
你可以看其實就是這個式子重新解一解而已
我只要把它的這個 transpose 這個 transpose 
然後呢這個這個 transpose 就變成 v p s 
那因此這個 transpose 搬過來就變成 d t b 五
就變成這樣的意思
因此我就得到一個這個或者說這個
就是一個 r dimension representation of the new document 
所以一個新的文章把它塞進來這樣塞就可以了
直接帶到原來的關係塞進來就可以 work 
那這個意思其實也可以說就是把你的 d p 投影到這個跟 u 去相乘
就是投影到新的空間
你可以想像我這個 d p 這個 vector d p 是什麼
是在這裡的一個新的點
它在一個新的點進來
不過這點我其實可以對應到這邊來
看這點對應到哪裡來就是這點
就把這點對應過來就是了
那怎麼對應
其實就是在每一個上面做投影對不對
我拿它對每一個 e i 去做投影
那這件事情其實就是這件事情
你看這個 u 其實就是 u 其實就是一堆 e i 嘛
所以你去跟它去做所以你這個 d p 去跟這個去做去相乘
其實就是它分別去做內積
它跟它一個個分別去做內積它就分別在做投影
所以投出來就是那裡面那一點
所以就得到
所以它跟這八百個去做內積就是這八百個投影
就得到八百個維就是這裡的八百個或這裡的八百個
就這樣的意思
所以這樣我就 fold in 進去了
好有了以上這些我們現在底下就可以來說我怎麼來做 language model 
那你可以想像我們這邊所講的 lsa 其實不限於做 language model 
其實是一個很 general 的 concept 
可以拿來分析很多東西可以做很多用途的
那我們可以拿來做 language model adaptation 
當你說到哪裡的時候你知道你在講哪一個 concept 
所以後面的 word 你可以猜它應該是哪一個 word 
我可以用那個 concept 跟那個 topic 
其中考的
你如果看考古題就知道我上面都是期中考的形式都跟考古題一樣啊都是那個樣子的
那我上面是是有寫說這個open everything 
也就是說是open everything 你不需要去記任何東西
啊
那你可以帶任何你要帶的reference 來看
但是呢我有另外一個條件就是說所有文字要用中文寫
啊
那意思是說你如果open everything 的話
如果你可以直接從課本上抄一段
你根本不用懂它的意思你也可以抄一段來因為它是跟課本一樣
所以一定要給你滿分
所以這個不合理嘛喔
所以你必須要看懂了
用中文寫才有分數
ok 
所以我想這個這個規定就是比照去年的考古題是一樣的就是那樣子
那我們的範圍考到八點零我們講過了喔
好我們現在來看最後這一段
就是這個怎麼樣拿來做n gram 
做n gram 的基本精神是說ok 如果我這邊都已經知道的情形的話
我現在怎麼做n gram 
今天如果一個人在那邊講話
他已經講了一堆話了
他已經講我已經辨識出出來他講了這一堆話講完之後講到這裡的時候辨識下一個字
這是我這邊講的w q 
喔
這個地方有一點confuse 就是在這裡的時候這個w q 變成q 是sequence number sequence index 
因為之前的我們的w i i 都是辭典裡面的第i 個word 
i 是在辭典裡面的第i 個word 
但是現在的w q 不是辭典裡面第q 個word 
而是你現在在講話的時候講到第q 個word 
ok 
你講到第q 個word 的時候呢
你前面所講的
從這個d 的q 減一就是你前面的recognize history 
那麼這邊呢叫做d 的q 減一
那你馬上猜得到是怎麼回事了
我就已經辨識到q 減一為止的這個呢當成一篇新的document 
當成一篇新的document 塞進來嘛
我把那個當成一篇新的document 塞進來之後我就可以得到它的八百維的vector 
那我就知道那個八百維的vector 在這裡放到這裡來
那你就把這個d 的q 減一放到這邊來
那你就知道我現在在講的是這個topic 
所以從這那當然在這附近的word 會發生的機率就高了嘛
那這個其實就是對應到這邊來我們說這兩個其實是同一個空間啊
所以我就把它對應到這邊來那你就看這邊的空這哪些word 跟它相關嘛
那你應該會講跟它相關的word 
喔
基本觀念就是這樣子
所以呢我的language model 可以這樣子做
那麼我就是把w q 是第q 個word 
然然後呢q 減一就是到目前為止你所recognize 的history 
那麼因此呢
這件事情其實就是把那個d q 減一把它塞進來
把那個d q 減一這個這個equation 就是剛才這個equation 
就是剛才這個equation 就是fold in 這個equation 
所以這個意思只是把只是把這個d 的q 減一呢我重新放到這個vector 裡面來或者這個vector 裡面來
變成那八百維裡面的那一點
有了那個之後呢其實我現在就可以算
given given 它你前面講的這些話
所以下面的那個出現那個word 它機率是多少
那你可以想像就會就會在它的附近
所以你就可以根據這個來算
其中這個word 你可以算它的u q 
這個d 你可以算它的v 
那麼嗯我這裡稍微有一點點錯
這裡是v 的q 減一的underline
應該是representation of v 的d 的q 減一by v 的q 減一
這邊應該都是q 減一啦是指這個都是指前面這q 減一的到q 減一為止所辨識出來的結果嘛
所以那個是d 的q 減一
然後representation by v 的q 減一這兩個都應該是減一的啦
啊
應該都有減一
然後呢我現在就是算
那麼因此呢我就可以我的這個history 就可以用v 的q 減一來代表
而我現在下一個word 的w q 也可以用u 的q 來代來代表
那這個u 的q 也就是把我要哪一個word 我現在放在這個裡面放在這個dimension 裡面
那就是我u 的q 嘛
那這個是我d 的q 減一嘛
那這兩個其實是同一件事
因為是同一個我們講你其實可以放同一個八百維裡面來看
那你就可以在裡面看它們之間的關係
喔就這麼回事兒
那這個詳細的說法呢是可以說它是可以跟n gram 整合在一起的
為什麼說跟n gram 整合呢
因為這邊講的這個跟n gram 是互補的
為什麼
n gram 給我們local relationship 
而這裡的l s a 給我的是semantic concept 
這兩個是互補的
怎麼講
我的n gram 是告訴我說這個word 後面要接這個word 
我如果遻這是畫隻字詞的話
或者這兩個word 後面要接下一個word 
這三字詞的話
對不對
所以呢它n gram 是告訴我說locally 這些relation 
但是沒有告訴我它沒有去分析這邊到底講什麼話
那我現在這種l s a 講的是你在說什麼concept 
是一個global 的relation 
但是它沒有講local relation 
那exactly 這個後面會不會接它呢這其實是n gram 告訴我們的
這個不見得告訴我啊
ok 
所以l s a 告訴我的是它到底是在講什麼topic 所以它應該出現什麼word 
那n gram 是告訴我前面有什麼東西是後面接什麼東西所以它們兩個是互補的
一個是local 的一個是concept 
那你也可以說
這個l s a 呢比較強調是主要的content word 的關係
而n gram 是把所有的的word 包括function word 一起算
什麼叫function word 就是我們之前講的譬如說的
譬如說他的爸爸
那這個的後面都算進去
那我在算它們的n gram 的時候
我把所謂的function word 就是這東西
或者非常
就是說沒有真正的意思
不真正的代表content 的內容的東西
像非常啦這個什麼這種什麼的啦這都是屬於function word 
那麼如果講爸爸
這就是content word 
或者說李安或者說馬英九這種都是所謂的content word 
所以呢我們這邊講的這個semantics 比較強調的是主key content word 
是content word 裡面的key word 那麼應該會出現哪些東西
但是它漏掉了這些function word 
因為function word 我們一開始就把它拿掉了嘛
那樣才能夠得到一個比較清楚的東西
所以function word 我一開始就拿掉了所以呢function word 其實也是你在算n gram 當然是跟function word 一起算的
喔
所以它它們是是互補的
喔
因此呢
你可以怎麼做
我現在的這個這個這個出現下一個word 的機率given 前面的history 
這個大h 呢是所有前面的history 
我可以包含兩件事情
一個是n gram 
一個是前面的這個
這個d 的q 減一就是我已經辨識到這裡為止
到底辨識多少東西我這個可以放進這邊來看
它是什麼
對這個是d 的q 減一
但是另外呢我還可以這個是是什麼
h 的q 減一的n 呢就是到q 減一為止的n gram 
我可以到word q 減一為止的n gram 前面的n 個
我就可以算n gram 
那這兩個可以一起用
所以呢我可以given n gram 再given 這個之後來算這個機率
那這個詳細怎麼算呢我這邊就不再說下去了
那在paper 裡面有你如果有興趣的話去看那個paper 
同樣呢它們個推導我不認為一定是最理想的所以那個一定還有改進的空間
啊
那麼所以那邊我就不再講下去但是你基本上可以想像是這麼回事
所以我就可以這樣子做
那底下要講的一件事情就是說
你如果這樣子講的話變成我每每辨識一個word 
就有一個新的d q 減一出來對不對我每辨識一個word 這邊就q 加一
我辨識一個word 出來我就得到一個新的document 
對不對我這就放進來了每辨識一個word 我就變成一個新的
那這樣的話我每次都要把它重新再算再放進去不是很麻煩嗎
其實不麻煩
這只是要iterative 加進來就好
我每一次只加一個word 
每一次只加一個word 是什麼
只加那一個word 就好
因為你的那個d 的q 減一就變成一個一個row 一個column 在這裡
對不對
你如果在d 的q 減一的那個你前面辨識到d 的q 減一的時候變成一個column 在這裡
再多辨識一個word 的話
那個word 看它哪裡就後面加個一
如果下一個d q 
如果下一個word q 
它其實是這裡的某一個word 
你辨識出來
你就這邊這邊加一嘛
你就這邊加一就好了嘛
這個加一之後就整個全部都照做就行了
那這個加一的動作其實很簡單就是這邊加一
假設你下一個word 
下一個word 進來是第i 個word 的話你就在第i 個component 那邊加一個一
別的地方方加零就可以了
那這邊為什麼會有這些呢
那只是因為我們一開始算這個的時候
有這個normalization 你記得
我們一開始的時候
我有除以n j 
還要再乘以e 減epsilon i 
我有這個normalization 
所以你不是光是如果光是這個就加一就好啦
可是因為有這兩個所以你要稍微做一點調整
就行了
那就是這邊講的
你譬如說原來的那個東西不是直接加一
要除以q 
要乘以q 減一再除以q 
q 就是全部的長
就是全部的字數嘛
所以q 這個q 其實就是我們剛才那個n j 的意思就是這邊的n j 
就是n j 就是全部的字數詞數
就是n j 
那我現在呢都除了n 減一的都除了q 減一的
都除了n 減一的都除了q 減一的所以我現在要乘上q 減一除以q 因為現在變成q 了
所以就變成要乘上這個factor 
同樣呢我那個e 也不是直接加一
因為要除以q 
還要乘以e 減epsilon 
所以這就是一個normalization 的過程
所以你如果前面d 的q 減一做完之後
下一個d q 很簡單就這麼做
我iterate iteratively 加上去就可以了
那如果d 是這樣做的話
那我這邊要算那個vector 也很容易算照算
所以這就是那個式子
我照算那個就是要算那個vector 或者這個vector 也很容易算就照樣代進去
就是這樣斷就可以了
那這樣我們就得到這個嗯用l s a 來做language model 的方法
那這個就等於說是我用l s a 的分析我來判斷說我現在應該在說什麼話
然後你就
那麼因此呢我的language model 應該跟著它走
那這邊最後這句話就是講這件事情就是說呢
你一開始的時候
你的v q 會在那個移動
會在那個空間裡面移動
最後會settle down somewhere 
也就是說假設
譬如說這個布希發表這個一個演講
就是說這個什麼什麼國情諮聞
他在講某一段在講譬如說在講經濟政策的時候
你一開始它的它的頭幾句話會跳來跳去
因為頭幾句話它不見得exactly 針對這個主題
所以一開始的時候你可能在這裡會發現它會動一動
可是講到若干句話以後
你就會清楚它是在它的主題是那個
所以它就會settle down 在那個地方
ok 
所以你開始講的時候它會動一下
然後呢最後會settle down 哪裡
之後一直在講那個topic 
等到它這段講完
下一段它要講外交政策的時候呢
那又會開始動
動到另外一個地方去
然後過一陣之後會settle down 說它講的是中東
就是中東
等等
所以呢基本上是一個這樣的過程
那這個是l s a 
有了這個l s a 之後呢
我們現在來說一下就是我剛才前面給的reference 
我剛才講的絕大部分都是based on 這一篇
這算是寫得最清楚最完整
所以我想這是一個很好的reference 
那跟這個相關的這是一個嗯special issue 
裡面有很多篇文章
它講的不見得是這個
但是它講的都是language model 有關的
以及linguistic prosody 有關的
還有dialogue 
因為其實dialogue 裡面dialogue 你要跟這個系統對話你要跟user 對話你很多都是要用這個linguist processing 的觀念
所以呢它是把dialogue 跟這個放在一起
有很多篇
這其實是相當好的一個reference 
我想你現在其實是有些東西是可以看的
所以這也是一個很好的reference 
不過我們剛才沒什麼說到
你可以去裡面看很多關於language model 相關的文章
那第三篇呢是把這個再衍伸
其實同一個作者
他從二千年到二千零五年
嗯這個是二千零五年
他重寫了一篇
這個時候他把它已經把它extend 
它把它改名叫做latent semantic mapping 
那現在不再它變成一個mapping 了
所以不再限於詞跟document 
它可以把它extend 到很多不同的對應關係跟不同的應用
所以呢這篇就會比這篇多了很多東西
變成是不同的應用啊等等
那還有它有很多新的reference 出來
都在那個裡面喔
我們舉個例子來講
它一個應用是e mail 的那個垃圾信件的排除
就每一個user 我都收到到一大堆垃圾信件
但是哪些是我的垃圾信件哪些是我要看的
你其實可以把它歸類
然後你會要看的一定是哪些topic 
那那些你不要看那些topic 就是你的垃圾
啊所以根據那個就可以來做垃圾信件分類等等
也可以用這個再來做其它很多application喔 
這都在這篇裡面有講
這是三
然後因為我們這邊講一大堆matrix 
那麼你如果對matrix 不是那麼熟悉的話
很多課本關於matrix 的
那我這邊已列其中的一本
不一定是這一本這本很多啦
你可以找一本來
那底下我要說一下第五個
就是probabilistic 
也就是說這個剛才講的這些都沒有太多機率
它有統計啦
其實裡面的每一個這個element 是統計嘛
是還有entropy 還有什麼
是有統計
但基本上它整個不是靠機率來算的
它是用matrix 來算的
那後來就有人發現其實這樣不夠好
更好的應該是整套都用機率
那就好像h m m 一樣
整套都用機率有什麼好處
就是它可以容易學習嘛
你可以把h m m 那一類的所有的那些個training 的方法都拿來
然後可以讓它用各種的machine learning 方法來做
然後這個你新的data 進來可以不斷地學習喔
很多很多好的方法好的情形在有了機率都可以
所以後來就有人說我應該把機率來重新formulate 這個問題
那這個就是所謂的probabilistic latent semantic analysis 
或者說index p l s i 
那這一篇是p l s i 的原始paper 
最早出現的一篇是在a c m 的sig i r 裡面
一九九九年
倒不是最好看的一篇
那不過你可以去找就是說
嗯在這個之後會有好幾篇
裡面有寫得非常完整的會比較好看的喔
那我們來說一下這個是什麼
這個就是在我剛才的再下一頁
基本上還是一樣的事情
我有一堆文件有一堆詞
不過這裡我用的詞term 不太一樣
我這個詞現在叫做term 
所以呢這個term t j 
term 就相當於我們原來這邊的word 
我這邊的word 其實它叫做t j 叫做term 
那這邊的呢就是document 
變成d i 
不過我現在變成大寫的就是
所以符號有點不一樣不過你大概知道我還是這兩個東西
就是這個詞或者是term 
跟這個文件document 之間的關係
然後我希望在中間找一堆就是我所謂的topic 
這還是一樣是topic 
所不同的是我現在全部都變成用機率的關係
什麼機率的關係呢
你可以看到
這個就是說我看到一篇document 
它會是在講哪一個topic 
有一個機率
我假設這邊有八百個topic 的話
我看到一篇東西
我可以算說它有多少機率講這篇
零點三的機率講這個topic 
零點一的機率在講這個topic 
這些topic 它機率是零或者怎樣
所以我看到一篇文章
我可以去分析它談每一個topic 的機率
同樣的呢
我如果知道它是講某一個topic 的話
ok
會在這個topic 裡面會講到這個word 
或者這個term 的機率是多少
我可以算這個term 的機率
所以呢在某一個topic 裡面
某一個term 會用到的機率是什麼
是這個
當我有這兩個機率之後
我現在變成這個式子
也就是說
我現在如果在一篇文章裡面
要看到那個詞的機率
是一個機率
不是直接數的了
我們剛才在這裡的話
在這一篇文章裡面它出現幾次
我數一數就知道是幾嘛
這是一個deterministic value 
對不對
它文章裡面它出現幾次我數一數就好了
現在不是了
現在是一個機率
在它裡面會出現這個的機率是多少呢
應該是這樣算的
就是呢你你如果看到它
你可以分析它可能是哪一個topic 的機率
然後在那個topic 裡面它會講到這個term 的機率
然後你現在這個乘起來之後把所有的topic 加起來
這才是這個機率
那你現在要要train 這個東西
就是要讓這個東西跟真的在這裡所看到的這個數字要像
那也就是我這個機率
那這個是真正的frequency count of term in document 
我要這個東西
那我要把這個東西maximize 
這個就是一個likelihood function 這是一個likelihood function 
在這個document 裡面看到這個term 的like 的likelihood function 
然後我現在要maximize 這個likelihood function 
我用這個來train 
所以我有一大堆文件
一大堆詞我可以train 這個東西
那這整個的數學的formulation 跟這個完全不一樣了
因為它不再用什麼vect matrix 這些都沒有了
它完全用機率所以數學的formulation 完全不一樣了
它的觀念是很像的
這個觀念幾乎是相同的
啊它完全用數學來做
那這怎麼train 
又又是用e m 
那e m 我們考完期中考之後我們會講e m 
那你就會清楚它裡面的一大堆數學
那你如果有興趣的話這也是一個很好的這個我們這邊講的這些都是蠻好的這個寫報告的題材喔
所以我想這一段應該是到這裡
ya 我們十二點零講的就是這些東西
那我想這個
ok 我們今天就上到這裡好不好
ok 嗯本來今天我說是要講的是第九點零的e m 喔
不過我後來決定說我應該再多講一個十五點零的這個robustness 
那原因也是一樣就是啊這是另外一個非常重要的大領域
那啊有非常豐富的研究主題在裡面
所以呢我覺得好像應該我們先講這一個
這樣子讓各位可以早一點可以接觸這些東西
那啊你可以早一點想可能的研究的題材
那我們如果把這個十五點零講完
我們再回去講九點零我想應該是ok 
那如果這樣的話呢我們你可以這個早一點開始多想一點其它的可以做報告的題目
那當然另外一個原因是因為其實十五點零
跟我們之前講的九十一點零跟十二點零是啊啊都是屬於這個adaptation 系列的
那其實這個觀念都是相通的
所以呢也許我們是可以一路接下來講
是比較順一點
