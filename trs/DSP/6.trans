那我想這部分我們說到這裡
那這個是我們五點零就說到這裡
那等於是我們把這個h m m 的部分
講到這裡為止我們現在要進入六點零
六點零要講的是language model
那language model 最基本的原理就是這個bigram trigram 這些個n gram
這些我們都已經說過了
那這裡面其實還有很多進一步的學問要講的
那我們現在來說這些language model 
那麼那這部分的話大概這幾本書都一樣的提到大概類似的東西
所以呢我這邊寫一個or
就是說你其實大概只要選其中一個來看就夠了
你如果看這本的話你如果看這本的話
大概是這些
看這本的話大概是這些
看這本大概是這些
那這裡面應該說是講得最淺最容易看的
可能是第二個
那個講得比較完整都說到的可能是第一個
那比起來三呢就是稍微難看一點就是它講的這個中間跳的東西比較多一點就是了
大概是這樣
那我想你只要所謂or 就是你只要選裡面的一個來看就好了
那在開始講之前我們再回到我們前兩天補課的時候所說的information theory 裡面的這個entropy 的東西
因為我們底下還是要用這個來解讀這裡面講的事情
我們舉一個非常簡單的例子就是
這個這個entropy 呢是我們說它的上限是log n 下限是零
那它所描述的是這個一個變化的這個uncertainty
或者說是一個distribution 的分散的程度
或者是它的純度等等的
意思我們上次都已經說過
那麼我們是在講一個這樣的東西
如果我有一個一個information source 
出來一堆m one m two m 三一直到m j 等等
那麼m j 是指第j 個
那每一個是什麼每一個是一個random variable
它都可以是譬如說x one x two 到x 大m 對不對
這就是那每一個都有個機率
這個就是p 的x one p 的x two 等等
那麼因此呢我們可以畫一個distribution 說
它們怎樣
這個是x one 的機率x two 的機率
一直到x m 的機率
這是我們上次補課的時候所說的我們基本上在講這麼一件事情
如果說它的每一個時間t 的時候
j 的時候送出一個m j 的一個symbol
那它們都可以是這大m 的裡面的一個
那每一個都有一個機率它都有一個distribution 
那這些東西就是我們講的p of xi 的這些東西
那然後呢那我們就說呢我們現在可以根據這個
來算它的entropy
這個entropy 就是h of s
就是這個東西
那麼它的上限是log m 下限是零
什麼時候上限是log m 呢
就是如果這些機率都一樣
變成一個完全相同的uniform 
的從x one 到x m 
是完全平的時候
那這個時候它的它的這個entropy 最大
就是所謂的它的值就是log m m 就是總數
那什麼時候最小呢
最小的時候是只有一個對不對
那個x j 的機率是一
其它全部都是零
這個時候呢就變成零
所以它的這個它的這個這個entropy 呢是介於這個之間
這是我們上次所說的事情
好那我們現在以這個以這個狀況
我們以這個狀況來現在看假設是我現在的這個是一個language source
這個跑出language 出來
舉例來講假設你在用你的手機或什麼在接收某一個massage 
假設這個massage 是這樣進來
那麼如果說假設這個massage 一個字母一個字母進來
一次跳出一個字母一次跳出一個字母來
所以進來一個t 一個h 然後一個i 一個s 哦我知道這個字是this
然後等等那因為跳進來的話
你可以想像成這裡每一個字母就是一個m j 
就是一個字母
那這樣來看我就把每一個字母看成是那這裡講的random variable
這裡的每一個是一個random variable 嘛
那我就把這裡面的每一個字母看成是random variable 的話
那你想我這樣的話總共有多少個
我的entropy 有多大
那你可以想這是那就是那這就每一個字母就這些x m 
那我這個m 呢應該是二十六乘以二還要再多
為什麼二十六個字母
所以我只有二十六種
乘以二是為有大寫有小寫嘛所以乘以二
然後還有一些標點符號啊還有空白也算是一個symbol 啦等等
所以大概是比這個還要多一些
不過應該小於六十四
六十四是二的六次方
所以你如果取log m 的話呢
應該是大約是六個bits
比六個bits 還要少
所以呢你可以說如果它是一個字母一個字母跳出來的
每一個字母看成是一個random variable 的話
那那個字母帶給我的information 是多少
大概是比六個bits 少一點
這個是這句話的意思
所以你一個字母大概給我這個一個字母給我大概六個bits 少一點的information
那這個其實是可以仔細算的我們這個只是很粗的這樣子講
我用這個上限等於log m 的這個來看
那上限這個是在講上限嘛
就是log m 就是六個bits
那事實上其實你是可以算的
那你知道我們的英文字母其實每一個字母它的機率都可以算的出來
那麼有的字母是機率比較高的有的字母是機率比較低的
我把這個二十六個字母如果算成a b c d 這樣算到z 的話
你知道什麼字母是機率很高的譬如說t 
是機率很高的
那麼e 也是機率很高的
什麼是機率很低的
z 機率很低的
那麼q 機率很低的等等
你其實每個字母的機率都算得出來的
所以你可以真的可以照我們那個公式就是p log p 
這個東西不是p log p 嗎我們上次講過
就是p xi log p xi 
然後summation over i 嘛
就是這個東西嘛你可以算嘛
你其實可以算精確的算出來每一這樣子的話一個字母倒底給我多少information 
這可以算得出來的
那我們這邊沒有去算它我只是舉個例子算它的上限
這個上限是log m 
那麼這個log m 大約是六個bits of information per character 
所以呢你如果說假設說我的手機可以一個一個字母跳出來看到收到我的message 的話
每一個字母給我多少information 
大概是六個bit 
那這個應該很接近我們的直覺
因為你知道我們本來英文字母一個字母就是用這個ascii representation 就是六個bit 嘛
所以六個bits 大概給我一個字母是沒有錯的
那如果它不是一個字母一個字母跳出來是一個字一個字跳出來的話
就第一次跳出來就是一個this
後面跳出來是course 
一個一個字跳出來的話
我就把每一個字看做一個random variable 可不可以
也可以
我如果把一整個字看做一個random variable 的話
那就是那我這就變成一個一個word 了
一個一個word 的話我總共有多少個word 呢
那英文的word 數目很多了
我們假設是三萬個的話
三萬個大概是二的十五次方這個寫錯了應該是二
是二的十五次方
那你如果算成二的十五次方就表示說呢
我那也是一樣其實英文的每一個字
每一個word 都有它的機率
譬如說這個字的出現機率都可以算的嘛
就是unigram 
所以你可以算它的機率
你機率算出來你可以算那個p log p
你可以算這個entropy 算得出來的
那我現在也是只是簡單地用log m 這個上限來說的話
假設它是三萬個它的上限就是二的十五次方
那就是十五個bit 
所以你可以說喂一個如果這樣一個字一個字跳出來的話呢
這個字給我大概十五個bit information
你大概可以這樣子看
那這個是我們用information theory 裡面的entropy 的觀點
來解讀這個這個language source 譬如說英文
假設我現在這個出現的是中文的話
那還有不同的情形啊
假設你用聽的
我是一個一個音聽到
我聽到這個音聽到這個音我一個一個音聽到的話
我聽到一個音我得到多少information 呢
那我們剛才說我們音的總數大概一千三百個
這大概是二的十一次方
因此呢你聽到一個音大概是聽到十一個bit 
或者更少一點
我還是一樣我用log m 來算
假設它的上限是log m
那麼我就是用這個log m 來算
假設這樣的distribution 的話
那麼那麼譬如說你在聽你的你聽你的手機的電話
我每聽到一個音
那那個音等於是這裡面的一個random variable
那它有一千三百種嘛
所以呢我所得到的information 應該是log m 
為上限的一個information 的量
所以大約是十一個bit 或者更少
那也是一樣其實每個音有它的distribution 
有些音機率比較高有些音機率比較低的
我們隨便舉例你也知道
機率比較高的是什麼譬如說ㄕ啊
因為施時使是都有很多常用詞
常用字所以ㄕ是機率很高的
那有的機率很低的等等
那那你也可以真的去算把它們機率高低通通都算出來
你也可以得到真正精確的每一個音給我多少information
但是我們不算我們只算上限的話
你聽到一個音大概是這樣
如果如果我聽不出它的它的聲調了
我剛才是including 的tone 是一千三百個
我如果聽不出聲調的話會怎樣
我們假設一個外國人他聽不出聲調來
對他而言不管是第幾聲聽起來都一樣
那這個時候他只聽到四百多個聲音
那那個時候大概是九個bits 
那這個合理嘛因為你少了四聲
四個聲調大概是兩個bit 嘛對不對
所以你如果聽不出聲調來的人
他聽到一個字大概聽到九個bit 的information
那當然你也可以是一個字一個字跳出來當成字來看
如果一個一個字來看的話呢
那depend on 我們算是多少字
我也可以算這樣的事情那假設我們常用字的數目是八千的話
那八千是這樣子二的十三次方
所以呢你每看到一個字
每跳出一個字來看到一個字
大概是十三個bit information
那這樣只是一個general idea 你比較有數我們怎麼在算這些東西
那這底下這個只是隨便意思意思我們舉個例子了解一下這個玩一玩而已
譬如說這女孩相對於girl 這可能是一個最match 的例子
為什麼呢
這裡有兩個字
如果一個字是十三個bit 的話
那兩個字大概是二十六個bit information
那這四個英文字母呢一個英文字母如果是六個bit 的話呢
四個大概是二十四個bit
這個二十四個bit 這個二十六個bit 很接近嘛
所以也就是說你如果從英文來看從中文來看
in general 它們給我們的information 的量是接近的
就因為我們這些language 所靠的東西是大概類似的concept 所以是差不多的
那當然這裡只是一個特別的例子那剛好看起來很接近
有些是不接近的啦
這裡也是兩個字這邊有這麼多字母嘛
三個字有這麼多字母嘛所以這個就不見得接近這只是一個例子而已
好以上只是一個簡單的一些例子在說我們真正要說的是底下這個perplexity
當我們在講language model 的時候
所有的課本所有的文獻都講perplexity 
那麼我們來解釋一下perplexity 是什麼
怎麼樣用它來看language model
當我聽到一段聲音的時候
我先去判斷它的第一個字是什麼
第一個字是w one
但是w one 其實假設我有咦我已經跳掉了
w one 其實是有這個很多個w one 其實是有很多個可能啊
譬如說我有六萬個詞的話
我有譬如說英文我們假設英文我們有六萬個可能的word 
所以呢其實w one 有六萬個可能
那麼你可以想像它可以是我總共有這麼多可能
它可以是這個word 可以是那個word 我有六萬個可能
同樣的呢w two 呢第二個字呢
也有六萬個可能對不對
所以呢其實我每一個word 都有六萬個可能w 三還是有六萬個可能
那其實language model 在幹什麼它就是在告訴我說它們不是都是六萬個可能
而是你如果知道了前面
後面的可能性其實就集中了
那舉例來講呢
這個w one 的這六萬個可能呢
很可能是我們剛才所說的
這六萬個可能是我們剛才所說的嗯這樣子畫
假設這邊總共是六萬個
從x one 到x m
我總共有六萬個
這六萬個裡面有高有低嘛
這就它們unigram 有高有低
那麼有的word 機率比較高有的word 機率比較低
這裡面就有六萬個可能
當我如果知道了w one 之後我再看w two 的話呢
它還是有六萬個可能
可是這六萬個可能會怎樣
它distribution 會不一樣
它會集中在某些地方比較高
另外的地方就很小了
為什麼呢根據bigram 對不對
根據bigram 如果知道前面是w one 的話
那這個剛才w one 的地方這個因為完全沒有任何前面的字的information 
所以我只好根據原來的distribution
原來distribution 是怎樣就怎樣
那這個看它是怎樣就怎樣
所以呢是根據原來的distribution
但是當我有了第一個word 有了第一個word 之後的第二個
的第二個的時候呢
它就這個distribution 跟這個不一樣
它基本上應該集中在若干個word 上面
其它的就很低了
因為我有bigram 
那等到第三個的時候呢那就更好了
因為我有了前面兩個之後的我有trigram
那就更清楚地說ok 它是這些高這些機率高
別人的機率很低了
那這又是另外一種distribution 對不對
那麼因此呢那其實是說這個一開始的這個distribution 最散
應該說這個的entropy 最大
這個的entropy 最大
然後當我有了這有了w one 之後再看w two
因為有了bigram 
所以呢我這個entropy 會小一點
稍微集中在若干比較少的數目的
稍微集中在若干比較少的數目的word 上面
當我有了這兩個之後第三個我會更好一點等等
那你如果這樣子看的話我們language model 的目的在做什麼事情
language model 是在做是在做一種linguist constraint 
to help in word selection 
對不對
就是說你你本來是完全沒有知識的時候我六萬個word 都要選
那當我有了前面一個word 我下一個word 的話
我就集中在少數譬如說一千個word 裡面去選就好了
當我有了這兩個之後我在這邊選我搞不好就在嗯兩百個裡面去選就好了
那就是我的有越來越多的constraint 在給我
讓我來選字的時候比較容易選的
language model 你可以看成是這樣的一件事情
如果是這樣的一件事情的話呢
那一個language model 它真正的功能是什麼呢
就是倒底給我多強的constraint
這個constraint 倒底多強
那這個constraint 如果越強的話
也就是讓我這邊要選的越少的話是越好嘛
就是我們剛剛講如果這邊要選六萬個
這邊只要選一千個了
到這邊只要選兩百個了
那這樣的話呢就表示說我的constraint 
那如果這邊不是兩百個只要選五十個那不是更好對不對
所以就是說你如果越到後面的話
我如果這個constraint 越緊
我其實是越有幫助越不會錯嘛
那這個所謂的這個這個六萬個還是兩一千個還是五十個這個是什麼
這個其實就是我們之前講的entropy 對不對
entropy 就是在告訴我們這件事情
就是它倒底是uniform 的這麼多呢還是集中在這裡
那我們上上次說過你可以把這個看化成很多種嘛
你是這個完全uniform 的完全不知道呢
還是你會集中在某些地方
還是更集中在某些地方
別的就沒有了
還是完全集中在一個
那這個就是在算entropy 
所以呢那麼你你其實entropy越小的話就表示我這個constraint 越強
那我就是用這個entropy
在我就是用這個entropy 在描述這個constraint
那麼因此這個我們這邊底下所講的東西都是在講entropy 
是這樣來的
其實這些這些perplexity 這些東西都是跟entropy 有關
那其實entropy 就是在描述這個linguist constraint
那什麼意思呢
應該這樣子講
這六萬個word 裡面誰的機率高誰的機率低
我有很複雜的distribution 
而且每一個word 出來
它的bigram 都不一樣
你可以想像我現在如果前面換成另外一個字的話bigram 又不一樣了
這又是另外一個
對不對
你這邊出來了兩個字之後
後面又有另外一個trigram
那這兩個字只要不一樣這個又不一樣所以這個實在太複雜了
那我們怎麼辦
我們就用剛才的這個entropy 
我就算這個entropy 就說明它的constraint 
ok
那我算entropy 來說明constraint 意思是怎樣呢
我們舉例來講
假設這個那我的一個辦法是這樣
譬如說即使是這樣它還是即使用entropy 去算這還是很複雜嘛
那我怎麼樣把這個entropy 轉成比較簡單的觀念呢
我就是把它用二的次方
所以你看我這個式子其實就是算entropy
p x i 乘以log
這就是算entropy 嘛
那每一個xi 就是我的一個word 
這裡的每一個xi 這裡的每一個xi 就是我的一個word 
這是出來第一個word 這是出來第二個word
那麼這裡出來每一個word 的
所謂出來每一個word 就是我們剛才講的這個這個例子裡面ok
這是第一個word 
假設我這個這個一個一個word 出來
那麼每一個word 有一個機率
那個機率就是xi 
那這樣的話呢我這個我的這個entropy 就是log xi 乘上咦跳到下一頁去了
就是這個那這就是我的entropy
但是我怎麼辦呢我把那個entropy 再一個二的次方
叫做perplexity
這個二的次方是什麼意思呢
是一個size of virtual vocabulary in which all words are equally probable
什麼意思就是說
如果我這邊算出來是某一個entropy 的值
我把它二的幾次方之後變成另外的東西
變成一個譬如說一千八百七十二點六七
如果我二的把這個entropy 算出來之後
我算出來之後我二的這個次方之後得到一千八百七十二點六七的意思是
如果剛好真的有一千八百點六七個word 
它們是equally probable 的話
它的機率就是log m 
log two 的這個一千八百七十二點六七呢
我們叫做m ’好了
那它就是log two 的m ’
那你可以想像好比是存那這個的這個的constraint 跟這個的constraint 是一樣的
因為它的entropy 是一樣ok
我們講這個entropy 怎麼算
就是log m
如果我有m 個而這m 個是equally probable
機率完全相同的時候的m 的話
它的entropy 就是log m 嘛
那當然我真的算的時候不見得不見得會剛好是
你我算出來的entropy 
我現在就是把它把它二的如果現在算出這個東西來
然後給它二的這個次方不就是m 嘛對不對
所以等於就是說假設我剛好是一千八百七十二ok 是一個整數那好那就是這麼多個
但是實際上你算出來不見得是整數
是一個這個任何一個real number 
那也沒有關係你就想像成這個譬如說我算出來是一千八百七十二
你可以想像好比是好比是相當於是
因為你這個三萬個這麼複雜你很難想像它到底是什麼
我們不如都把它轉成一個轉成一個virtual vocabulary
那裡面所有的word 或所有的unit 都是equally probable 的
那假設我這樣來轉的話
那如果我算出來的那個我算出來的那個entropy 
我用二的這個次方去算得到一千八百七十二點六七
我就可以想像這是一個m ’假設我真的就是一千八百七十二點六七的話
那它們的而它們是equally probable 的話
這個entropy 跟這個entropy 是一樣的
那如果是這樣的話呢
那假設我的這個那假設這個是我的第一個字
那到了第二個字的時候呢
因為有了前面的這個我有了bigram 
結果我算出來的東西呢
這個entropy 就小了很多嘛對不對
當我這個有了bigram 之後
這個entropy 這個distribution 又更集中了嘛
變成變成更集中了
當我這邊變成更集中了也許我這邊算出來變成一個更小的
譬如說呢
我變成一個譬如說你可以想像成我現在的這個m ’呢
只有兩百五十七點一二
那也就是說假設存在一個virtual vocabulary 只有兩百五十七點一二的話呢
那它的entropy 它們機率equally probable 的話
它的entropy 就是這個的log 
那
那那等於是說我的這個bigram
已經把我從原來一千八百多個的的這個constraint 縮小到只有兩百多個了
那如果是這樣的話呢
到了trigram 的時候呢這個可能更小了
譬如說我其實entropy 更小因為它更集中了
entropy 更小的時候呢我其實只有一個譬如說三十八點一四
那這個是我的m ’
那也就是說我的entropy 變成log 的這個三十八點一四了
那你如果這樣看的話這個就這就是我更小的一個constraint ok
那這個意思是說
我們因為你對每一個不同的word 你的bigram trigram 都不一樣
給你的這個三萬個word 的這個distribution
你其實真的一直都在考慮這三萬word 的distribution 
不過三萬word 的distribution 實在是每個distribution 都不一樣
你給你每一個bigram trigram 都不一樣實在有夠複雜我們希望用一個簡單的數字來呈現它
那個簡單數字就是我用另外一個distribution 
它是uniform 的
看它總數是多少
那個總數越小就是constraint 越小
那那個總數其實就是entropy 的二的entropy 次方ok 
那這麼一來的話這個東西其實就是我們講的size of virtual vocabulary in which all words are equally probable
那麼等於說在這個case 呢
我們本來第一個word 相當於要考慮一千三百七十二個字
它會equally probable 的那樣的選擇的
它的困難度是那樣的程度
到了bigram 這困難程度變成譬如說
相當於在兩百五十七個裡面
要選一個
這兩百五十七個是equally probable 的
到了trigram 的時候呢我剩下是這麼多個裡面要選一個
等於是這樣的意思
那這些個數字就是我們在講的linguist constraint 
那我的language model 就是希望把這個把這個linguist constraint 一路把它縮小
然後我們可以看到它縮到多小
那這個東西叫做perplexity of the language
那你如果有一個language source s 
譬如說是英文
或者譬如說是中文
或者說是文言文
或者說是白話文或者說是什麼你都可以這樣子來算
你可以算它的這個東西
那這個呢其實就是我們的每一個word 的出來的entropy 
然後二的entropy 次方
相當於一個virtual vocabulary
那麼那底下我這個例子是跟剛才講的是一樣的例子啦
就是說你假設你算出來的那個是十個bits information
就相當於一千零二十四個
每一個都有相同的機率
那在那個情形之下呢
你的這個perplexity 所謂的perplexity 就是一零二四
perplexity 這個字你去查字典的話呢
是混淆度
也就是說你現在還無法選你的selection
你的constraint 就是只有這樣子
我有這麼大的混淆度
其實這個混淆度就是entropy
因為entropy 其實也就是混淆度的意思
那用另外一個字來講就是所謂的branching factor
所謂的branching factor 的意思是說就是你這邊你可以想像有多少個選擇
你要說三萬個六萬個也沒錯但是你也可以說就是一千八百七十二個選擇
對不對這你如果說這個word 有六萬個選擇的話是說
這六萬個選擇機率有大有小
所以呢倒底六萬個是都怎麼好選法你其實講不出來
那其實它的因為有大有小的關係
其實相當於共有一千八百個才要選的
這一千八百個是一樣的
那我就說它的branching factor 是一千八百
這是所謂的branching factor 的意思
那有了這個第一個word 之後我下一個word 的branching factor 就降低到兩百多了
因為我有了bigram 
我就知道其實相當於我要選的第二個字是相當於是只有兩百五十七個裡面選一個
那這兩百五十七個是一樣的機率就是了
你這樣來看我的branching factor 變成兩百五十七
那麼等到有了這兩個字之後呢我的第三個word 呢我的branching factor 剩下三十八
那就是說呢我現在的這個嗯只有三十八了等等
那麼我們如果這樣看的話呢這個就是所謂的branching factor
好有了這個之後呢
我們剛才所講的是一個language source 本身的perplexity
底下我們要講給我一個language model 
我就可以算這個language model 的perplexity
那這個language model perplexity 也就是這個language model 的的嗯它的給我的linguist constraint
或者它給我的branching factor 
那麼怎麼算呢
那麼你現在可以這樣想
其實一個language model 給我的就是每一個word 的機率
given 前面的condition
所以呢一個language model 就是給我這個下一個possible word w i 的機率
given 它的condition c i 
例如假設對某一個word sequence 是w n 的話
這是我們某一個word sequence 
我們講過後面這個應該沒問題你知道
就是我現在第一個word 的unigram
然後呢有了第一個word 之後第二個word 的bigram 
以及第三個以後呢假設我這是用trigram 來算的話
就是前兩個的第三個的trigram
那這樣來看的話不管是哪一種unigram trigram bigram 這些gram 都一樣
都是一個某一個condition 下的下一個word
在這個case 的話因為它是unigram 
所以那個condition 就是沒有condition 就是空集合
就是c i 就是空集合
那這個bigram 的話我的condition 就是前面一個word
第一個word 之後下一個是bigram
那後面的trigram 它的condition 就是前面兩個word
所以不管是unigram trigram bigram 它們等等的這些n gram
我都可以看成是一個condition c i 之後的下一個word w i
那如果是這樣的話呢
那其實用我們剛才這個圖來解釋的話呢
那這個每一個不是一個機率
就是一個distribution 就是一個這種東西
一個這種東西distribution
因為就是你現在給我這個condition 
那這個word 有六萬種對不對
給我這個condition 這個word 有六萬種
給我這個condition 這個word 有六萬種
那這個六萬種呢有一個它的distribution 
那那個distribution 不好算我就都算它的perplexity 
或者說算它的branching factor
或者說就是把它考慮成為一個另外一個virtual vocabulary 它的size 是m ’
我都把它看成這樣子
那我怎麼做呢
那given 一個language model 我必需要有一個test corpus
那如果沒有一個test corpus 我是無法算這件事
那麼我們這話怎麼講我們底下就解釋
假設我有一個test corpus d
它裡面有n 個sentence 
所以呢這是一個測試的database
那麼這個測試的database 裡面呢有總共有n 個sentence
所以呢每一個sentence 是大w 的一到n 
那大w i 呢就是裡面有一堆word
就是小w 是它裡面的word
它的長度是n i 個word
所以呢它有n 個sentence
其中第i 個sentence 就是大w i
它的長度是n i 個word
所以呢這是小w 就是它裡面n i 個word
然後呢我總共的word 的數目是大n d
總共word 的數目是大n d
如果是這樣的話那我怎麼算它的perplexity
就是底下這個式子
這式子看起來有點複雜
不過其實很簡單
那它的意思應該就是說
用我們剛才這個來講的話就是說
我有一個測試的database 
就是大d 
它裡面有第一個句子叫做大w one
第二個句子就做大w two 等等等等
那麼這個長度是n one 這個長度是n two 等等等等
那全部有多少個word 呢有大n 個word 
而不是大n d 個word 
那就是這邊所說的事情
那如果是這樣的話那我怎麼算呢
我其實就是把這裡面的每一個word 　
每一個word 都可以根據前面它的condition 不管是bigram trigram 什麼gram
根據它前面的condition 都可以算這個word 的機率嘛
就是這個東西
就是這個機率嘛
那我就這個機率把整個sentence 通通取log 全部加起來
所以每一個第一個第一個可能是要算它的unigram
第二個算它的bigram 後面算它的trigram 等等
我把這機率全部加起來
然後呢那我就得到這一個
我就得到這一個sentence 的和
就是括號裡面這個東西對不對
我就是把每一個機率這個從第一個字的unigram
加上第二個字的bigram 第三個字的trigram 後面每一個每一個都有trigram 
我都這樣去算
那麼把它全部加起來
就是這個式子
然後呢我每個句子都做這件事
全部加起來
然後呢除以全部的字數
那其實是什麼就是就是我在算所有的字的機率
等於說是我每一個字都在算它的它的unigram
它的bigram 
然後它的trigram 等等
我就把每一個通通都這樣算把每一個字出現的也就是說我的我的language model 本來就是在給我這一堆機率
只是說這堆機率顯然你沒有辦法自己算
一定要給我一篇文章給我一篇文章
之後我才可以算每一個字每一個字的機率是多少
你沒有給我一篇文章我沒辦法算嘛
所以呢你就要給我一篇文章
這篇文章就是我們所謂的test corpus
那我有了這篇文章之後我就可以算每一個到底機率是多少
那這個式子其實只是在算所有的機率的平均對不對
你把所有的機率取log 再加起來再除以所有的字數
就是在算平均嘛
所以我就是把它這全部加起來這所有的機率我每一個字那每一個機率是什麼
其實每一個機率就是我們這邊講的這麼一個東西
它其實是一個distribution 裡面的某一個嘛
那它其實是在算這種東西嘛
那麼因此呢我其實是在算平均
那這個全部加起來算平均是因為我取了log
那如果取了log 加起來算平均的話
相當於我先全部都這個取n 分之一然後加起來再取log 是一樣的嘛
那你如果取n 分之一相當於是什麼相當於是做幾何平均嘛
相當於在做幾何平均嘛
那是什麼意思呢
那其實我最後的這個perplexity 就是二的這個次方
那二的這個次方的意思就跟我們之前講的這個二的這個次方是一樣的意思
你其實是在算一個你其實是在算一個virtual vocabulary
那這個講起來其實用底下這個講起來就很容易看
就好比這樣子
我的某一句裡面
這是這個是unigram 
它算出來相當於是一零二四分之一
這個是bigram 算起來是五百一十二分之一
這是trigram 算起來是兩百五十六分之一
這是trigram 這是一百二十八分之一
這是trigram 等等等等等等等等
你把所有東西都平均起來看看到底這個language model這個language model 給我每一個word 都有一個機率
那其實這個機率就是告訴我這個一零二四或者五百一十二或者兩百五十六就是告訴我這個size m pron 這個size 
就是告訴我它的branching factor 
就是告訴我這個constraint 大還是小
就告訴我那個branching factor
那這個東西那我現在要算它的平均
怎麼算
什麼叫平均
我不是把一零二四跟五百一十二這些東西來平均不對的
我應該是把這些個機率一零二四分之一這些東西來做一個幾何平均
幾何平均之後再inverse 回來
譬如說平均出來變成三百一十二
那意思就是說in average 
我每一次要看下一個word 是有三百一十二個可能
所以呢in average 我的branching factor 是三百一十二對不對
是這個意思
所以我們這個式子寫了半天其實就是只不過是做這件事情而已
ok
那我等於是在算這個language model 給我每一次要predict 下一個字的時候
每一次要算下一個字的時候就是在看你給我多好的多強的linguist constraint
讓我這個selection 的range 多大或者是多小
那因此呢那麼如果這是一零二四分之一相當於是說它有一零二四個這個一零二四分之一相當於我這邊這邊講的一八七二一樣的意思嘛
等於說我其實branching factor 是一八七二
我一零二四裡面我要選一個
這相當於是五百一十二裡面選一個
那平均起來在這整篇文章裡面
這個整篇文章來數一次平均一下就知道我這個language model 到底給我怎麼樣的constraint 呢
那不是把一零二四跟五一二去做平均
而是把它們的分之一這個機率來做幾何平均
之後的inverse 得到三百一十二
那才是平均那這個意思等於是說in average 
我要算每一個下一個字的時候是有三百一十二個選擇
等於是在三百一十二個
這個變成三百一十二
對不對就等於說我這個m 等於三百一十二
我在三我有一個等於說in average 每一次要選下一個字是有三百一十二equally probable 的字裡面
要選一個
那這樣的話呢這些東西其實就是所謂的一個average branching factor 
所謂的average 是什麼average in the sense of geometric mean of 
就是這個一零二四的倒數
五百一十二的倒數它們的幾何平均
那麼如果這樣看的話呢上面這個其實就是在做這件事
我們這個這邊只是在做它的平均嘛
平均之後再做二的幾次方回來其實就是在做這件事是一樣的意思
所以呢這個是一個perplexity 
這是這個language model 它的混淆度
或者說也可以說就是它所提供的那個linguist constraint 
那個linguist constraint
也就是capability of language model 
它可以predict 下一個word 那麼那麼這個這個的能力嘛
那麼你在這個language model 就是在幫我predict 下一個字
那麼它的這個constraint 有多強
因此呢是越小越好對不對
越小的話表示我的constraint 越緊
那表示我的越好
那麼因此呢我們可以根據那個language model 大小
來看它的這個language model 好不好
不過呢這個language model 我們不能自己來算
一定要有一個test corpus 
所謂test corpus 就是要有一個文章嘛
一篇文章或者一萬篇文章要有這個東西之後
每一篇文章算出來都不一樣的嘛
所以一定要有一個測試的文章或者是一群文章
然後做為test corpus
這個時候你的language model 就可以去算
你就可以知道它的能力有多強
所以它是function of 這個p 是這個language model 
就是這個p 
我們就用這個我們就用這個p 的這個代表一個language model
那這個language model 就是這個p 
然後這個d 就是我的這個test corpus
它是一個function of 這兩個東西
它是一個function of 這兩個東西
那這樣子的話呢我就可以算出這個language model 好壞
那這個也就是我們講的perplexity
也就是它的這個average 的branching factor
那也就是我的這個也可以說就是一個跟我們這邊意思是一樣的就是size of 一個virtual vocabulary 
就是in average 如果我得到三百一十二的話呢
就是in average 我每一個字都相當於好像有一個三百一十二個word 的一個vocabulary 在那裡
讓我來算
等於是這樣的意思
ok 好我們停在這裡休息十分鐘
好有了剛才的那個perplexity 的定義之後
你了解它的意思我們先來看一個比較簡單的例子
這個例子很簡單這樣你比較有點感覺它們在講什麼
這邊所做的事情是我們把網路上的這個新聞網站去download 一堆新聞
就是我們中文的新聞網站台灣的新聞網站download 一堆新聞
然後這些新聞裡面它的網站它自動分類了
這政治新聞啊什麼等等財經新聞體育新聞等等等等社會新聞這樣
總共各download 量這麼多
然後總數是這麼多
那這個呢我們拿它來train
當成這個training corpus 然後去train language model
所謂train language model 就是去算那個n gram 
去算unigram bigram trigram 
當你language 算好之後
我就可以來算
那我另外還要找一堆testing corpus
因為我要算我們剛剛講要算這個perplexity 一定要有個文章才能算嘛
我另外要再找一堆testing 的
去算這個testing 的我沒有寫在這裡就是了
那另外算找一堆testing 的這個嗯corpus 來算它的language model
那我們剛才講的這件事情其實也可以這樣子看就是說
你顯然是要用兩個database 
一個是train 它一個是test 它
所以譬如說我這個是一個這個training training corpus 
我用這個來train 那堆language model
所以呢我就會我用這個language model 的training 的演算法
去算它的n gram 
所以這邊得到的就是n gram 
就是我們剛才所寫的w i 的機率
這個東西
這就是我的n gram 
我有了n gram 之後呢
我要有另一個testing corpus 
d 
就是我們講的testing corpus
p 跟d 嘛
就是我們前面這一頁所說的p 跟d 
算出這個東西出來這是perplexity evaluation
ok
所以我顯然需要兩個兩個database 
這個是拿來train 
train 出來這些東西之後我用另外一個去測它
那因為光是這個不能算嘛我一定要有個東西去測它所以拿這個來來測它然後算出這個來
那我剛才講的這個例子也是這樣
那麼我們這邊是
分別為每一類的新聞做一個language model
我有政治新聞的language model我有體育新聞的language model 等等等等
那你看到這個總共這麼五十八點一的裡面
政治新聞占了十九點六台灣的新聞裡面政治新聞比例之高啊
這個台灣人之熱衷政治可以想像
那那我現在假設是
這裡的每一類我都做了個language model 之後我都在每一類我也都找個testing corpus 去去測的話
算出來的perplexity 在這裡
這是第一類第二類這是右邊這一
我現在講的是domain specific 
就是每一個domain 做一個language model 
所以我政治新聞做一個language model
財經新聞做一個language model 等等等等的話呢
我然後我就分別用政治新聞去測
我有另外政治新聞的testing corpus
財經新聞有財經新聞的testing corpus 分別去測的話
得到的perplexity 是右邊這條灰色的
那你可以看到呢譬如說在這裡最低的是什麼我們說越低越好嘛
因為它代表的是我的我的這個constraint 
對不對
我的branching factor 是越低越好嘛
最低的是五號
五號是體育新聞
它體育新聞其實只用了最小的training data 它只有這麼小的量
就train 出一個來
它就只有三百多
那這什麼意思呢你可以想像其實體育新聞是一種最簡單的語言
因為它講的其實就是一堆體育的事件
球賽啊
這隊多打一球那隊多贏一球然後它輸了一球所以它多了兩分然後它贏了
等等
喔
它大敗它什麼東西就這樣子
所以它的內容其實它的辭彙也很單純
它的語言結構也很單純就是這些事情
那這就是體育新聞所以呢它雖然有很小的training corpus 它得到的perplexity 也是最低的
但是你注意看一下
我們的政治新聞perplexity 也很低
我們的政治新聞裡面其實是內容如果我們去看的話是千變萬化
裡面的這些政治人物說的話是多的不得了很有學問這樣那樣
但是如果我們去分析他們說的語言的話
它的辭彙也就是那些辭彙
調過來調過去
然後它的句型也就是那些句型而已
所以它講的話其實跟打一場球是一樣的
只是在
只是在打另外一種球好像是不同的球就是了
那就是我們政治新聞裡面的
這個說穿了就是口水了口水就是這樣
那你說什麼是perplexity 最高的
四號
四號你看這麼高比人家高好多好多
四是什麼呢
這個其實是文教新聞
就是這個文教也就是包括教育文化啦你可以想像有教改
有大學校長的問題有這個什麼這個哪一個這個嗯哪一個博物館怎樣啦有一個什麼這個歌劇來表演啦又是什麼
什麼樣的都在裡面
所以文教是一個最複雜的環境
它的辭彙也最豐富它的語言也最豐富所以它的perplexity 是最高的
我想這是一個簡單的例子大概你應該這樣你比較了解我們講的perplexity 是什麼
那左邊這個藍色的線是什麼呢
左邊藍色的線是說我現在做一個general domain 的
就是我現在把所有的東西
我不分領域我當成一個然後train 一個language model 的話
然後再分別去去那個language model 就等於是全部平均在一起啦
全部平均在一起之後我再拿那個去算剛才的那些個測試的database 
我剛才有政治新聞有財經新聞我那些我再去測的話呢
那你會發現它們的perplexity 都比較高
這藍色這個都比較高
換句話說它的我所得到的機率的這個我所得到的機率的這個branching factor 都比較大
那意思也就是說我的language model 比較差
對不對
perplexity 越高就是越差嘛
那換句話說
為什麼比較差就是因為其實每一個domain 它有它自己的語言結構
它有它自己的辭彙跟語言結構
所以財經新聞跟體育新聞跟政治新聞跟社會新聞它們內容顯然是不一樣的
所以它們之間的n gram 的關係是不同的
你如果做個general 的domain 的language model 把它們全部平均在一起當然也可以
但是呢也就因此你的constraint 是比較差嘛
你如果針對每個domain 自己去算的話都會比較小
因為它的domain 是比較精緻
它的那個變化比較少變化所以比較perplexity 比較小
這就是這邊講的domain specific 這個domain specific 的language model 呢
我用domain specific 的corpus 去train 的話
那麼它都會有一個比較好的
都會比這個general domain 的language model 來的好
而這裡面呢體育新聞是最低的perplexity
那它是語言結構最單純的
那這個例子大概有一點感覺我們再回過頭來剛才上一頁
還有一樣東西沒有講
就是這個cross entropy
那麼
這個倒沒什麼特別這個只是在你去讀這些書的時候
你如果讀前面我們講的這些這些書裡面不管哪一本它都說
這個language model 的perplexity 是一種cross entropy 
它們都說它是一個cross entropy 
這個perplexity 為什麼是一個cross entropy 呢這點不太容易了解
所以我們這邊解這一頁其實是在解釋說perplexity 為什麼是一種cross entropy 
那麼我們在禮拜天補課的時候曾經說過這個
所謂的cross entropy 的定義是一個這樣的東西
也就是我有兩個distribution 一個是p 一個是q 
然後我p log p 除以減掉p log q 的這個東西
通常是大於零的
這個東西叫做cross entropy 
它是p 跟q 兩個distribution的一個
它是p 跟q 兩個distribution的一個distance measure 
或者叫做k l distance
這是我們上次所說的
那麼在那個情況之下呢我們也說過這個p log p 比p log q 大
我們可以把它寫出來一個是p log p 一個是p log q
那這兩個東西寫起來就變成這樣的一個式子
這個式子就是所謂的jason inequality 
那這個也是我們上次所說過的
那這個也是我們上次所說過的
那我現在要講的是說
它們課本上說這個perplexity 是一種它們說perplexity 是一種cross entropy 
這句話所講的cross entropy 不是這個cross entropy 
是哪一個是這一個
喔
也就是說
這個什麼叫做cross entropy 其實不同的人它的取的名字講的東西不太一樣
所以你不要被confuse 了
那麼很多人講的cross entropy 是講這一個
這是我們週末補課的時候講的是這個
但是呢也有很多人把這個叫做cross entropy
就是p 跟q 之間的p 跟q 之間的差異
p 跟q 之間的差異
那麼
這個叫做那我我這邊為了區別起見我把它寫成x 的
那麼這個叫做那我我這邊為了區別起見我把它寫成x 的p 跟q
這個x 也是cross 的意思嘛
那那麼有的人是把這個叫做cross entropy
那它們課本上所謂的perplexity 是一種cross entropy
是指是一個這種東西
那我們底下要解釋的是為什麼這個東西叫做
那我們底下要解釋的是為什麼這個東西叫做為什麼這個東西就是我們講的perplexity
那麼這個東西是什麼就是這我們禮拜六補課講的
就是說
就是說它是一種entropy
當p of x 是被當成q 了弄錯了的話
它整個的entropy 會變大對不對
我本來是算p log p 的
但是假設這個p 不是正確的p 是變成q 的話
它會變大
那那個變大的那個差異叫做cross entropy
那這是它們的這個定義
那底下我要講的事情是說
我們剛才講的這個perplexity
我們剛才講的這個perplexity剛才講的這個perplexity 其實就是一個這個東西
所以它們說它是一個cross entropy
為什麼是這個東西呢
因為它其實就是一堆
因為它其實就是一堆log 的平均值
喔
那這個式子其實就是我們上一堂課講的
你看我們上一堂課講的這個perplexity
這樣算的式子是這樣寫的
這是什麼就是一堆p
這是什麼就是一堆p一堆probability 然後這個取了log 之後全部加起來平均嘛
對不對
一堆probability 取了log 之後全部加起來然後全部平均嘛就是這個式子嘛
那跟我們現在寫的這個式子是完全一樣的
一堆probability 取了log 之後全部加起來然後平均嘛
對不對
喔
所以這個式子跟剛才是完全一樣所以我們那個perplexity 可以說就是這個
那其中的這個q 就是我們所說的這個language model 的這些n gram 的機率
就是我們這邊所講的q
那我其實這些perplexity 的這個東西就是這些q 的全部加起來平均
那如果是這樣的話
那麼我們要說呢這個其實可以看成是p log q
也就是一個cross entropy
為什麼呢
根據所謂的law of large numbers 
這個名詞你一定是熟悉的
你在小的時候就偷這個
機率還是統計什麼就學過所謂的law of large numbers
什麼是law of large numbers 呢
我們說假設有一個random variable 
你做了很多的experiment 去測它是多少
你得到一堆值
譬如說我們說它的值
跟它的次數
假設說是投一個擲一個骰子或者什麼東西
我的值是a one 的時候
我總共得到幾次呢總共得到n one 次
值是a two 的時候我得到
n two 次
等等值是ai 的時候我得到n i 次等等等等
我總共做了
n 次實驗
我做這麼多次實驗加起來有這麼多個n 次
假設是擲一個骰子這就是一二三四五六
各有多少次當然也可以是其它的各種random variable 的實驗
那麼這個時候到底平均值是多少呢
那麼我們的算法可以是這樣算就是summation 的ai n i
然後再除以n 
對不對
這就是平均
我就是把ok 得這麼多的這麼多次
那麼我就其實就是把全部乘起來
然後全部加起來
那總共有大n 次我就除以n 這就得到平均值
那這個平均值你可以有另外一個寫法
就是把它寫成
這個summation 的
嗯ai 乘上n i 除以n 
那這個其實就是機率pi 
也就是說
發生a one 有幾次是n one 除以n 的這樣的機率對不對
發生a two 是n two 除以n 等等
所以這個就是這個pi 就是發生
值是ai 的機率
我就每一個值乘上它發生的機率乘起來不就是平均嘛
我就可以算成這樣
那所謂的law of large numbers 是說呢你做了夠多的實驗之後
那麼這個limit
n 趨近於無限大的時候
這個東西會等於它
當你實驗做的不夠多的時候這個這個機率不準嘛
實驗做的不夠多這個這個不準你不見得是
等於但是當你實驗做到夠多次的實候n 趨近於無限大的時候你就可以用這個來算
這個叫law of large numbers
那這個應該很容易沒有問題你如果這個了解的話那我們這邊講的事情是一樣的事情
那我們說左邊的這個式子其實就是在
就是我們剛才算的perplexity
就是在把所有的機率加起來
那相當於就是
在我這個情形就變成是
a one 加a one 加a one 
總共加幾次加n one 次
再加上a two 加上a two
總共加了嗯不是小
n one 次總共加了n two 次
等等等等
這個a one 加了這個
a one 加了n one 次其實就是
就是a one n one
這邊都寫錯了這都是小寫
小寫這樣比較不會弄錯這是
這樣子
那麼你如果說這個
對不對你現在如果a one 總共
發生了n one 次你就全部給它加起來就是a one 乘上n one
然後a two 總共發生n two 次等等
就是這裡面的一項就是這裡面的一項
等等等等全部加起來的話你其實可以分別
你如果知道
你如果知道a one 發生的機率是多少乘上它的機率就好了嘛
就是這個意思
那你如果從這來看的話呢這就是我們那些language model 的機率
就是相當於這邊的
這些個東西
這些個a one 
就是因為我就是要算這些機率的平均嘛我的perplexity 就算這些機率的平均嘛
那我怎麼算我就是把它們全部加起來平均嘛就是左邊那個式子
就是把它們全部加起來平均嘛
但是你可以看成是用這個東西乘上它出現的a one 乘上它出現的機率
這樣也可以啊
如果你知道它的機率的話
所以呢你看我這個等號左邊寫的就是說
averaging by all sample 你把全部通通都加起來
你把它們全部通通加起來來做平均
就是averaging by all samples 
那右邊呢是averaging 如果它的機率是知道的話
如果你求的出來這個機率你知道的話
對不對我現在的對應到那上面的式子
這個ai 就是我的q of x 
我現在在求那個q of x
就是這些機率所以q of x k 其實就是
就是我們前面在算的那些perplexity 的機率
那我要算這些東西的平均
那我們的算法就把它全部加起來除以n 嘛
但是你如果知道它每一個的機率是多少的話
譬如說你知道它的ai 的機率是pi 的話
你其實這樣算就可以啦
當你寫成這個式子的時候
那麼於是呢就是當它的值是某一個值的時候它的機率是多少你把它寫成這樣子的話
那這個東西其實就是上面這個式子
它就是一個cross entropy
那你如果這樣看的話呢這個q of x 
就是我們的這個
而這個p of x 是什麼
就是真正的機率
所謂所謂的真正的機率是
這裡面算出來的機率
那麼
我現在的這個這個
n gram
是用這個算的
問題就在這裡因為這兩個不一樣
我必須用一個training data 去train 出這些n gram 來
這個n gram 所得到的這些機率跟你現在測試這篇文章裡面的本來就不見得一樣
那這兩個不一樣其實就是
我們這邊講的p 跟q 是不大一樣的就是這兩個p 跟q 不大一樣
所得到的cross entropy 就是這個p 跟q 的
它不大一樣
那麼換句話說
我要求的我這邊在算的這個perplexity
是在算這裡面的n gram 的機率
從這裡面來算的
那麼我得到的就是這個東西
但是我平均怎麼平均法
是在這上面做平均
對不對
我是在這上面做平均是在這上面做平均所以呢
我是在用這個做平均如果我知道它的每一個case 是機率是多少的話
如果每一個pi 是知道機率是多少我可以把它寫成這個式子
那麼在那個情形的話呢我的這個仍然是我的q of x 
而這個東西呢就是我的這邊所寫的p of x
那我如果知道說
有有百分之多少是這個值有百分之多少是這個值
我就根本用這個加就可以加出來嘛
那那這個東西p of x 機率是什麼
其實是在這個testing 的這個d 裡面的機率
而不是那個
那這兩個會不同
所以呢true probability 這個bar 加一個bar 這個true probability 是指這個
是在這裡面的true probability
跟我真正的那個是有差的
那麼因為這個的關係所以我就得到這個perplexity
所以這個perplexity 可以寫成這個式子
這個式子可以看成這個式子
那它就是一個cross entropy 
那什麼是cross entropy 呢也就是我的真正在這個測試環境裡面
在這個測試環境裡面的這個機率
其實跟你的training 裡面的不大一樣
那中間的差異所構成的
恩
所以我們說它是一個cross entropy 
當你true statistic 在測試裡面是interpolate estimate as 這個東西
by the language model 
ok
就是說你真正的
你應該是測試環境裡面的那個統計特性你現在用那個來做
恩
因此你可以說它是一個cross entropy
ok
那麼當然越大越不好
這個越大越不好也就是我們剛才所說的越小越好是一樣的
在這邊來講就是越大就是它們差異越大
這個對不對我們說過這個這個東西本來就是兩個distribution 的的difference 
所以越大表示它們差的越多就表示你估計的越不對
恩
這個是這個我們解釋一下是為什麼它叫做cross entropy 的意思
好
有了這個之後再底下我們要說的是一個實際的問題就是我們真正在做language model 的時候其實不是這麼簡單來算n gram
n gram 我們這個老早就講過了
那個n gram 講起來很簡單
就是算這種東西嘛對不對
就是算這種東西嘛
這bigram 就這樣算嘛
講起來非常簡單如果真的那個簡單就沒什麼學問了
事實上的n gram 不是那麼容易算的
為什麼
因為有data sparseness 的問題
什麼是data sparseness 的問題就是有很多event 就是不會發生
也就是unseen event 
又來了
那麼因為我們這邊所有的從h m m 到language model 到n gram 
都是用統計的觀點來做的事情
那統計永遠是盡信統計不如無統計
那麼統計你不能那麼相信它因為統計永遠有一堆unseen event 
在你的統計資料裡面沒有看到的東西不表示它不會發生
只是它沒有
那就像我們之前講的tri phone 我有一堆unseen tri phone
我得把它train 出來
看不到也得train 出來
一樣那這裡我的n gram 也有一堆unseen 的n gram 我要怎麼處理
同樣的問題
那麼這個例子我們前面已經說過了就是
假設有一句話叫做jason immediately stand up
這是多麼普通的一句話
但是很可能你算它的n gram 機率就是零
為什麼呢
你有很多jason
你的training data 裡面很多jason 它後面就是不接immediately
你有很多immediately 它前面就是不接jason 
所以呢
這個jason 後面接immediately 的機率就是零
只要這個bigram 是零的話它就是零了
那你這個句子永遠不會辨識正確
一定就錯掉了
可是沒有道理啊這麼普通的句子怎麼可能是零呢
顯然就是一個unseen event
沒有出現在database 裡面但是我們必須讓它有值
不能讓它沒有值
所以怎麼辦
trying to assign some non zero 的probability to all events
即使它們沒有出現
所有的event 你都要給它一個值
恩
即使沒有出現都要給它一個值
那是怎樣
就我們剛才的說法來講
我們用每一個event 有一個機率這樣來看的話
會有一堆是沒有的
會有一堆沒有怎麼辦所有沒有的你都要給它一個值
或者有大有小或者怎樣你都要給它一個值
當你給它一些值之後顯然要把別的值弄小一點
因為加起來才是一啊對不對
所以你那些大的要變小一點
然後呢讓這些沒有的都讓它有一點點
或者有大有小都要讓它有
那這個過程叫做smoothing
恩
那就是所謂的language model smoothing 
那你看就知道這個意思是smoothing 就是把高的弄低一點
然後塞到這些低的地方去嘛
所以是一個smoothing 的過程
這是我們所謂的language model smoothing 
那怎麼做這件事情呢
一個最簡單的想法是所謂add one smoothing
這是最簡單的想法不過這個方法很不好
你做出來效果很差就是了
講起來是很直覺
就是我把所有的event 次數都加一
我所有的次數都加一就是once more occurs once more than actual does 
我所有次數都加一所以所有零次都給成一
一都變成二二都變成三三都變成四
全部都加一的話這樣都有了
那麼舉例來講
要train 這個bigram 的時候這個bigram 我們說過本來就是這樣子嘛
對不對
就是我這兩個詞連在一起的count 除以前面那個全部的count 
這就是我的bigram
那我全部都加一的結果呢就是
那你可以看得出來我的分母所謂的所有的這個前面那個word 的count 就是它後面接的任何的一個word 的count 嘛
那我這樣通通都加一的結果呢就是分子的這個count 加了一
分母就加了我的詞的總數
因為後面接的每一個都有一次都要加一嘛
恩
所以就加就變這樣
這就是所謂的add one smoothing 
它有一定的效果它讓我沒有零的了
全部都有是沒有錯
但是這樣做出來的效果並不好
所以後來沒有人真的用這個方法
不過這個通常是這個最容易最直覺想像的所以呢我們就說一下
因此呢我的bigram 就變成分子加一
然後分母呢就是加一個total number of 不同的word
這樣我就把所有的後面會接的通通都算進去都加了一就是了
那比較有效的smoothing 通常是底下這兩個觀念的衍伸
真正的做法不見得是這樣但是呢大概用到這兩個觀念
一個是所謂的back off
一個是所謂的interpolation 
那麼什麼是back off 呢
這個back off 的這個觀念很簡單
這個式子寫成這樣是有夠頭大
其實很簡單我們不要看這個式子的話
用簡單的符號來寫的話呢
應該是可以寫成這樣的意思
p n 的bar 是等於嗯p n 如果p n 大於零
如果a 的p n 減一如果p n 等於零
喔
其實只是這樣的意思
那我這裡所謂p n 就是n gram 的那個機率
像你仔細看這個東西搞了半天其實它就是一個n gram 嘛
就是given 前given 從這個這個w i 的機率
given i 減一i 減二一直到i 減n 加一這不就是n gram 嗎
所以就是我這邊寫的p n 就是n gram 的意思
那我要求的n gram 呢如果那個n gram 存在的話大於零就是那個count
這個是從i 減n 加一一直到i 的那個word sequence 兜在一起的那個count 
如果真的是大於零的話這個就是就是這個n gram 是存在的嘛
那我就用就用原來這個n gram
如果它不存在呢如果count 是零的話沒有的話怎麼辦呢
我就用n 減一gram
所謂的lower order 就是n 減一gram 
所以你看這個變成n 減一gram 所以是從從這個word i 的機率given 從i 減n 加二到i 減一
也就是n 減一gram
我就把n 減一gram 拿來然後呢乘上一個某一個factor a
這個的意思是什麼呢
是只要是n 減一gram 大的話n gram 應該也會大
n 減一gram 小的話n 也會小
為什麼呢我們舉底下一個例子這個例子是課本上的一個例子
你知道這個thou 跟you 是同義字
這個是古字這個是現代字
你如果去念莎士比亞的作品什麼羅密歐茱麗葉他們講的話的you 都是thou 
喔
所以thou 就是you 
但是呢這個是古字這個是現在字所以現在來看的話呢
這個機率是很大
這個機率是很小
如果這個機率比這個機率大很多的話
那你在任何一個字後面看到它的機率也是一樣的嘛
譬如說
你see 後面看到它跟see 後面看到它
顯然也是這個大這個小嘛
就是這個意思ok 
你如果這一個字機率比它大的話
那麼它在那麼它在別的後面看它也是機率比較大嘛
那因此呢
我們剛才的這個是unigram
比較大的unigram 
它如果後面要接什麼的話它的前面有什麼東西的話呢它的bigram 也是比較大的嘛
那bigram 比較大trigram 也是比較大的嘛
trigram 比較大的話four gram 也是比較大的話
所以呢只要n gram n 減一gram 比較大n gram 大概也是比較大
n 減一gram 比較小n gram 大概也是比較小
那反過來這個n 越小這個gram 越容易算得出來越會存在
對不對
因為n 越小的話你只是在算一個越小的單位的機率越會出現越容易算到
n 越大越算不到嘛
所以呢如果一個那麼大的一個count 不存在的話
我就退一步算n 減一gram 的機率
然後呢基本上你可以假設
這個n gram 跟n 減一gram 應該是有一個比例關係
當然問題是這個a 怎麼求你得有辦法求這個a 就是了
恩
你如果有辦法求這個a 的話呢
你就可以用n 減一gram 來估計n gram
那這個a 我們這邊寫就是說它是一個function of 這些東西
那depends on 你這些東西是什麼要怎麼算這個a
喔
那我們後面還會說到一些怎麼算這a 的情形不過基本上就是這樣
所以呢就變成說是所以我這邊畫一個bar
就是你真正經過這個smoothing 之後
經過這個smoothing 之後的這個n gram 就是畫一個bar 的
那沒有經過smoothing 直接照我們原來公式去算的就是沒有畫bar 的
所以呢你的smoothing 的功能只是在如果那個count 不存在的時候
就用n 減一gram 來做
然後乘上一個factor
那個factor 你要算就是了
那這個方法叫做back off
也就是退一步
退到一個n 減一gram 去算
退到lower order
就是這樣的意思
那跟這個很像的就是interpolation 意思其實是很像的
那這個式子也是這樣看起來很複雜
其實你如果用這樣來寫比較簡單的話呢
就是寫成寫成這樣
就是我的n gram 是什麼呢
就是b 乘上
嗯
就是b 乘上n gram 加上一減b 乘上n 減一gram
我看對不對
yeah 沒有錯
也就是說呢怎麼講呢就是即使即使你的n gram 有non zero counts
即使你有non zero counts 你也不要太相信那個n gram 
你不如用n 減一gram 去跟它interpolate 一下
阿
也就是說我們剛才講你n 減一gram 永遠比n gram可靠
為什麼你的數的數目比較多嘛
你你
你n 減一gram 出現出現的機率一定比n gram 要來的大
所以數的數目比較多之後一定比較reliable
所以你永遠去跟n 減一gram 去做一個平均做一個interpolate
這樣得到的n gram 會比直接算這個的n gram 要來的可靠一點
恩
那問題是這個b 是多少
那b again 是一個function of 這堆東西
就你要想辦法估計這個b 之後
拿n 減一gram 來跟它來跟它做一次這個interpolation
那也是一樣我這邊加了bar 的表示是我smooth 之後的
我是用smooth 之後的n 減一gram
跟我新得到的n gram 去做一個interpolation 
得到我smooth 之後的n gram 
等等
恩
那這樣子呢即使我有count 是non zero 我也這樣做
這樣會比較好
這是所謂的interpolation
這是基本觀念是這樣
那詳細的做法那就有底下的這些做法了
恩
那麼你如果去看前面給你那些reference 課本裡面
它都會說好幾種重要的smoothing 的方法
因為事實上language model 都需要做smoothing 就是因為我們剛才講的這個問題
就這些unseen event
那這堆問題事實上都蠻多的
那麼你永遠都要做這些事情
那
嗯
有好幾種方法
那麼我們沒有辦法說哪種比較好
因為可能在不同的狀況
有的時候這個比較好有的時候那個比較好
那麼不見得一定是誰比較好
所以每一種都存在都可都值得用
喔
那我們沒那麼多時間講每一種所以我們就舉一個例子
這個例子就是所謂的good turing smoothing 
那麼good turing 的基本觀念
good turing 顧名思義你知道這是兩個人的名字
一個人叫做good 一個人叫做turing
恩
不是不是說他很好
那麼
那麼
他是用這個good turing 的一個方法來做這件事情
阿
那它的基本精神是怎樣呢
就是說properly decreasing 的frequency for observe event allocate some pro frequency to unseen event 
這講起來其實就是我們剛剛講的話了
就是你有一堆unseen event 怎麼辦你一定要把一些機率給它嘛
你就要把你就要把你看得到的event 一些frequency count 降低
降低去丟給它
喔
那怎麼做這件事情呢
我們用底下的一頁的簡單的例子來講
這是它的一個假說
倒不見得一定說是最正確的方法
只是它用這個假設來這樣子做
恩
那這個這個這個例子在這裡
這個例子是假設你去釣魚
假設你去釣魚
那麼我希望根據我釣到的魚來判斷海裡的所有的魚它們的distribution
那當然就是有一堆魚你沒釣到嘛
那就是unseen event
那你怎麼估計那些你沒釣到的魚
恩
它說呢假設我去釣魚總共鈞到六種魚
我這一號二號三號四號五號六號
就是我總共鈞到六種魚
鈞到幾條呢
一號魚顯然是很貪吃所以一來就吃到它馬上就釣起來
所以我總共釣到十條
二號魚鈞到三條三號魚釣到兩條
四號五號六號各釣到一條總共釣到十八條
於是呢我總共看到這我總共得到這十八條魚
我根據這十八條魚裡面譬如說一號有十條之多
然後二號三號等等等等
那裡面我有四五六是各一條
這樣總共有十八條
我要根據這十八條魚來判斷海裡總共有多少魚
做個統計的話
顯然是有問題
因為還有一大堆魚沒有看到
假設海裡有一千種魚的話
我只看到六種
還有九百九十四種呢都是unseen
恩
那因此怎麼辦呢我要在這十八裡面把某一些魚算成是unseen 的
然後呢我再來
然後我把每一個魚的其它的每一種魚的機率都降低
阿
它等於是這樣的一種這樣的一種想法
ok 好我們在這裡停十分鐘
恩
那我們這個
休息十分鐘的時候請助教進來set up 我們的那個習題
這樣各位可以可以可以開始去看我們要做的習題
ok
OK 好 
哦 我們現在先請助教來講一下我們的第一題的習題哦
那 第一習題第一題的習題就是我剛才講過就是我們把這個整個 HMM 從 從這個 哦 training 到最後測試我們都給各位做一次
那這個 因為我們進度太慢了
本來這個應該是在期中考以前做完 哦
那 那因為我們進度太慢所以現在才剛剛把它講完而已
各位可能也需要一點時間來把它讀清楚然後再來做等等
那因為下週又放假了實在很頭大
那麼我們好容易才把進度補上來又放假了
但是要再補課嗎 嘶
我想大家都補怕了我也補怕了
所以 也許我們下週先放假再說 哈
那麼先放假再說 但是我又怕進度耽誤太多 所以我們呢 我們就先給各位這個習題 哦
那麼什麼時候交 我們待會再再再討論什麼時候交
不過我們至少先給各位習題 這樣子要趕快念的人可以先趕快做這樣子 哦
那麼關於我們補課的時候這錄影帶的問題 有人因為某些原因不能沒有來上
想要借錄影帶這個問題呢 我們會再研究清楚怎麼做
然後在網路上 在網站上公告怎麼進行好不好 哦
就是說你補課的時候因故不能來 需要想要這個 看這個錄影的話我們用什麼方式來做到
那麼因為基本上這個是我們學校的進修推廣部來幫我們做這件事情
那麼他們擁有這個智財權等等的問題我們不能隨便 copy 給人家等等 有這個問題在
所以我們要想一個比較有效的辦法來做這件事情
哦 所以我們有什麼辦法做的的做的時候會在網站上公告
那基本上各位去上網注意看我們想辦法做到這件事這樣 OK 
那現在還不知道我們再研究一下這樣子
好那現在先請助教來講這個題目好不好
嗯大家好我們第一個作業就是有關於 HTK 的
然後至於這個 toolkit 可以到這個網頁的這個地方去下載有關它的東西
然後嗯這網頁是你要必須要 log  in 才可以給你下載的
所以你可能需要先註冊一個帳號
然後註冊完以後第三點是請大家稍微注意一下的
因為嗯我們這一 這一個作業裡面我們提供了同學一些批次檔可以直接使用
但是因為嗯就是 HTK 最新的版本的時候有些東西跟我們原先的批次檔是不符合的所以執行上可能會有錯誤
所以希望同學要記得是要下載舊的 這個是前一個版本的
那大家稍微注意一下是先進入 download 的頁面下以後會找到一個叫 browse  HTK  software  archive 
然後下面會有一個連結叫 HTK  software 在這裡面會有所有的 HTK 的 version 
然後大家就大家就點點選這一個下載
然後這片投影片我們已經放到那個網站上了
然後如果說你是在工作站上執行 HTK 的話
然後你就是把程式碼自行 compile 
然後嗯 如果是你是在 windows 上的話我們在這一份作業裡面我們也有直接附執行檔
所以你可以直接在 windows 上也可以跑就對了
就不用再去下載那些 tool 
嗯然後我們的資料一樣是公告在網站上就在平常的地方下載
那這個稍微注意一下不要隨便改變目錄結構
因為那個批次檔都是已經寫好的
所以如果你隨便改變可能會導致 error 
好然後接下來是大概講解一下嗯執行的流程
然後反正就是那個我們在在那個 BAT 這個資料夾下面有寫著每一個批次檔
然後嗯嗯 小妞可以開一個批次檔的範例
應該已經打開了
對所以是大概是這樣子的啦
所以大概 大家可以看一下這個東西然後 HTK 的網站上面也有 HTK  book 
所以大家可以 check 一下看每個指令是在做什麼的
對然後這個我們的第一步是抽取聲音的特徵參數然後抽完之後
下一張
抽完之後我們就要算 mean 跟 variance 下一張
對然後接下來這個地方就是因為因為接下來的批次檔會在這個位置去讀檔案所以請大家要複製一份上面都寫的其實還蠻清楚的
然後嗯你可以用程式複製或是直接把它手動 copy 過來都可以
對然後接下來就是再重新估算重新估算它的 mean 跟 variance 
然後這樣的嗯剛才的流程就已經完成了是只有一個 mixture 的 model 
那我們還可以繼續反覆反覆這兩個動作
就是前兩頁的那兩個動作就可以一區估算比較多的 mixture 這樣通常會有比較好的效果
就是像這邊講的
然後我們的範例就是我們的批次檔裡面我們只寫 有寫到四個 mixture 
啊如果大家要更多我們就是就是自行跑
然後嗯看結果會有適當的加分
然後最後存會把 model 檔存在這個地方
然後我們跑完 training 之後我們也要跑 testing 所以 testing 的話就是拿來跑 viterbi 然後批次檔是執行這個
然後最後跑完以後我們還可以跟正確答案比較這也是寫好了所以大家就可以知道你跑完你所 train 的 model 在 testing  data 裡面正確率是多少
然後我們這個這個範例的正確率其實沒有特別的高
對 然後作業要求很簡單就是至少把所有的範例程式跑一遍就是把批次檔有寫給你的東西你把它跑一遍
然後如果你有改進正確率就是結果會 improve 的話那我們會適當加分
然後希望大家把那個就是這個 homework  one 的這個來信主旨寫成這樣子不要寄成很奇怪的名字這樣可能不小心就刪掉這樣
就是 DSP  homework  one 後 後面是你的學號
然後寄給嗯我或者是另外一位助教都可以
就是 data 一樣是在網頁上就直接下載就全部都包在一起了連這個說明文件都在一起
OK 　好我補充一下哦
補充一下我想剛才講的 HTK 就是我早上一開始講過就是這個那套這個 HMM  toolkit 
那這個照剛才助教講的就是你怎麼樣去去註冊登記然後就可以下載
你要下載正確的版本才不才才不會不 match 
那然後這個我們會給你這個這個測試語料跟訓練語料
那是一堆所謂的連續平衡句在後面 嗯
那這些個語料基本上來講都是有智慧財產的不能隨便外流的 阿
所以呢也就是說你知道我我們這些東西都是這樣
你如果沒有 data 什麼都不能做
那這個這個有天大本領沒有 data 就是不能做所以呢所有事情是 rely  on  data  data 是最重要的
所以這個在很多時候 data 都是最重要的財產
那麼我們的這些 data 都是跟人家有簽過約的都是不得外流的
那不過我們當時簽的約都有一個但書就是教育目的給少數人學習練習是可以的
阿 所以呢我們都拿來給各位當習題用
但是呢就是這個拜託各位 make  sure 就是不要流出去只是你用而已
就是你 download 下來你用
用完之後不要給別人哦你就是就是只是放在你那裡就是我沒有叫你還給我
但是就是
但是你不要給別人 阿 你給別人就不對了 阿
然後因為這樣的關係我們我們基本上是這個只在網站上於三天七十二小時
你要在三天之內 download 完畢
哦 你什麼時候做沒有關係但是你三天之內沒有 download 的話我們就收起來了
然後就 哦 那基本上你也不應該再轉給別人所以你如果沒有 download 你就沒有了就是了
我們從什麼時候開始
現在放好沒有
嗯現在已經放好了
現在已經放好了
所以 就是從從今天開始 
所以那就從今天中午十二點開始嘛好不好
所以就是今天中午十二點
今天是禮拜二所以到禮拜五中午十二點為止好不好
三天之內七十二小時之內 download 完畢
過了時間我們就就就收掉了就是了 哦
哦 然後哦我們這邊講的就是這些 這些步驟我想你你像這個助教剛才都已經講過了
那你一開始先要 train 這個 HMM 
那 train 的時候這裡所謂的一個 mixture 就是一個 Gaussian 
所以你一開始先做第一個 Gaussian 
之後你可以把它 split 變成兩個變成四個等等
那它這邊說這個這邊所寫的只有做到四個 Gaussian 哦
範例只有做到四個
當然你要再多跑你可以變成八個十六個都可以做
但是並不表示越多就一定越好因為 data 不夠多 哦
我們給你的 data 其實是很少的
所以呢你如果 train 太多的 Gaussian 之後一定 data 不夠多那 Gaussian 會都不好了
所以呢會有一個有一個上限就上不去了這個你自己可以做實驗就知道
那麼這個我們給你的這個叫做 我看看哦我們叫做連續平衡句
那麼所謂的連續平衡句是這個這個 balance 的 sentence  
balance  sentence 意思是說我在 minimum  number  of  sentence 裡面要 cover 所有的音
哦 就是說我其實就是用最少的數目的的句子
minimum  number  of  sentence 我讓所有的音都出現
那其實這個很難我們剛才我們之前講過就知道
你要有那麼多 phone 你要每一個音都出現其實不是那麼容易
一定是你如果隨便弄一堆句子的話一定是那堆高頻的音出現很多
一些低頻的音沒有出現 哦
那麼因此呢都要經過特別選的
所以像這個 balance  sentence 怎麼選呢也是用 computer 選的啦 哦
就是說我們用一個很大的語料庫
然後這個 有一個程式去選這些
我儘可能選那些這個一句裡面有最多不同的音的句子
然後我一定優先選那些低頻的音 哦
越是低頻的音先優先選出來
然後這個那它同時就會把高頻的音帶進來
之後看還缺什麼音就把那些缺的音再把他選進來等等
這也是一個程式做出來
所以呢我們那些 你如果去看那些連續平衡句的話
它那個句子也是特別的句子
那麼那樣的話在最少的句子裡面把所有音都都有
所以你才可以 train 
所以這個你去 其實 train 出來是我們國語所有的音所有的 syllable 的
我們現在 train 的單位應該是 initial  final 
就是聲母韻母
哦 所以你就會把所有的聲母韻母都 train 的 哦
到但是呢其實我們的連續平衡句的 database 是非常多的
應該是有一百個男生一百個女生
然後每一個人都念了多少遍 哦
但是現在總共只給你三百句什麼的
是那裡面很小的一個 subset 
所以也就是說 train 出來一定不好 哦
那麼因此你到時候做測試的時候你測試出來正確率一定是低的啦 哦
你如果正確率低不要覺得是差
是因為我們給你的 data data 很少的關係 哦
所以呢並不是以那個正確率高低來看 哦
那這個總共這個裡面有三百句是訓練語料
有五十句是測試語料
那你用那三百句 train 好之後用那五十句去測 哦
那然後再 等等 你可以算正確率等等
你可以算你要做幾個 Gaussian 等等 哦
那 所以要寄到哪
寄給助教寄 email  address 有寫嗎
寫在網站上 哦
所以 OK 你就 就照網站上講就行了
那麼
哦 我想剛才應該裡面有提到說有些地方你如果要做得比較詳細可以加分
之外呢我想你如果仔細看那個 HTK 的 HTK 的網站可以 download 整套的 manual 
有詳細的裡面所有東西的說明 哦
你如果真的有興趣你可以仔細地去 去了解
把裡面做的事情了解清楚
然後這個在交的時候 交的比較完整有更清楚說明我想那些都是可以加分的
那我剛才提到就是說整個的過程
是非常複雜的一堆
那我們裡面有一些我們講得比較清楚
譬如說哪個怎麼做哪個怎麼做
但是有的很多
其實我們沒有說那麼清楚
那麼因此呢看你 你要自己用心到什麼程度
那你其實用就就就照助教這幾個 step 這樣跑你也可以跑得出來
那你要多下一點功夫多去看也可以就可以看更多 哦
那 那個 那本 那 那個詳細的 manual 你如果詳細看的話可以更了解 哦
那都可以加分
所以我想這個是看你要下多少功夫而已
所以我們就給你大概這個讓這個 哦
下禮拜如果我們又又放假的話那我們至少就是你如果要要趕快多多念一下的話你就可以先來做這些事情
好 那我們要來討論一下的是 什麼時候要交
今天是三月二十八
下週是四月四號吧好像
十一號十八號二十五號 哦
那現在下週變成 下週變成放假
所以呢
哦 如果兩週以後就是十一號
但是好像十八號開始是期中考是吧
好像這一週是期中考週 哦
所以呢我們一個合理的時間如果是兩週的話
是在期中考開始之前交
但是如果這個有點太趕的話因為其實我們才剛剛教完
這個 TRI PHONE 什麼什麼東西
那你如果太趕的話我們晚一點再交也可以
不過又碰到期中考好像晚也沒有太大意義
所以我們決定一下看要什麼時候交
各位覺得
那我們的
這個裡面是 這個習題裡面是包括 training 跟 testing 
你 testing 其實就是跑 viterbi 我們前面講過的 viterbi 
所以其實我們都有說過
但是我們其實都只有講裡面的一小塊
並沒有真的整個 link 起來過 起來過
那這個題目應該是讓你把所有東西都 link 起來就是了
我想一種可能是在十一號交
那是兩週以後
一種可能是再晚一點
乾脆十八號二十五號
譬如說
不過 好像也沒有太太好因為這時候是期中考
有沒有意見
哈 我們的期中考
我會傾向於晚一點
哦 那麼因為我在五月中要出國
我希望在那一週考期中考
這樣子的話
阿 五月中 阿
那 如果是那樣的話就等於補考了 等於補課了啦 哦
否則的話又變成又又必須至少還要再補課啦
阿 那如果說我五月中的出國的那一次
來考期中考的話我們期中考就會比校例規定期中考大概晚好幾週 大概晚個三四週吧
那這個 那一方面因為我們的 期中考範圍本來就比較 比一半多一點所以應該是要稍微晚一點
那乾脆如果再晚個一兩週就變成我出國時間這樣的話呢就變成少補一次課
哦 那所以是那樣的話我們的期中考其實還還還有一段時間 哦
所以看我們要不要定哪 哪一個時候
早一點就是十一號
十一號好不好 
不好
十八
二十五
十八好了啦
十八這樣有三週應該差不多了啦好不好 
OK 好那這樣我們就定了就是十一月十八號嗯
四月十八號
四月十八號 那麼就是有三週的時間
那麼交的方式就是直接 mail 助教你去上網看就有了這樣子好不好
 OK 那然後就是我們剛才講補課的時候的錄影帶怎麼處理
那你也上網去看我們有決定怎麼辦的時候 
在網路上告訴你這樣子好不好 哦
OK 好 那這樣子我們今天就上到這裡 嗯
好 我們上週停了一次課
很久不見
那我們兩週前正在講六點零language model 
然後我們說到smoothing
那麼我們說所謂smoothing 的問題
是指這個data sparse ness 
也就是說有一堆unseen event 
就跟就跟在五點零我們講的unseen 的triphone 一樣
主要就是有一堆unseen event
你不能完全根據data 來看
這個例子我們說過很多次就是像這個例子
jason immediately stand up 
這個名字非常普通
但它很可能在你的大量的training data 裡面它後面就是不接這個字
這個字也很普通
在你大量的training data 裡面搞不好前面就是不接這個字
那這樣的話呢這個bigram 就機率就是零
那麼這個呢就是機率就是零
所以這樣的一句話它就是出不來
那麼因此呢你就不能完全用train 的來得到你的bigram
那因為就是有一大堆unseen event
那這種情形 嗯 也就是說我們盡信統計不如無統計
你一定要想辦法讓所有的unseen data 不能機率是零  
那麼unseen data 機率不能讓它是零的情形就是說你現在雖然有的data 機率很高
有的是零
那你顯然要讓所有的零我們都讓它有一點點data
所有的零都要讓它有一點data
有一點機率
如果零都有一點機率的話就表示那些不是零的機率要降低一點這樣整個機率才是一嘛  
不是零的降低一點然後把所有的零的都把它讓它多一點
這樣的一個想法就是所謂的smoothing
也就是aside some non zero probability to 所有的events
即使never occur in the training data 你也不能假設它一定完全沒有
所以我們都要讓它多讓它多放一點進去
這是這個sparse ness 的觀念  
那最原始的想法就是add one
所有的event 次數都加一
所以零都變成一
一都變成二
不過這個方法呢並不好
那我們說比較好的想法來自一個是back off
一個是interpolation
所謂的back off 的觀念就是說你用n 減一的gram
就是說你用這個next lower order
你退一步退到next order 也就是說你如果n gram
你如果n gram 找不到的話我就用n 減一gram
像這個地方其實這個就是n gram  
那n gram 找不到的話我就用n 減一gram
這是n 減二所以n i 減n 加二嘛所以就是n 減一gram  
那理由就是n 減一gram gram 永遠都比n gram 要reliable 
你曉得這個this is a 
a 這個前面要在is 之後的a  
這個是bigram 
那麼你如果這個找不到的話至少a 一定比較多嘛對不對
is a 一定比較少a 一定比較多嘛
所以呢這個unigram 這個這個一定比bigram 容易找到  
那你如果要找this is a 的話
那這個找不到的話呢is a 八成是有的
也就是n 減一gram 永遠比n gram 要機會要來得多嘛
所以呢你這個如果n gram 找不到的話就用n 減一gram 
來替代 
做某一種scaling
那這個是這個back off 的意思
就退一步退到n 減一gram 去
然後呢interpolation 是說你想辦法用一個比較reliable 的來跟它interpolate 
那在這邊的例子呢就是你就乾脆就用n 減一gram來interpolate
所以呢你就用一個n 減一gram
你得到的n gram 也許不可靠你就用一個n 減一gram 來跟它interpolate 那這樣比較可靠一點
那這一類的想法 
那這兩個只是基本的精神back off 跟interpolation  
那真正做的時候呢就是嗯要有一些做法  
那麼你如果去看我們前面給你的reference 的話
這些書上課本上都會說到不只一種
你不管是這一本還是這一本還是這個大概都會說到不只一種這個smoothing 的方法
那其實我們今天看到所常用而有效的smoothing 方法不是一種
是好幾種
那麼我們沒有辦法說哪一種比較好
因為嗯有的有的狀況這種比較好有的狀況那種比較好
換句話說因為這非常depend on 你是用哪一堆data 去train 
然後你的training data 缺哪些東西
那麼然後你是在什麼情況之下都不一樣所以呢在這個狀況這個方法比較好
在那個狀況它還是比較好
所以呢事實上是存在好幾種方法都不錯
然後你真的要比的話呢有的時候它比較好有的時候它比較好這樣子
那我們不會有那麼多時間講那麼多種所以我們就講一個例子
就是good turing
那good turing 的基本精神呢
就是我們這邊講的就是說呢我想辦法decreasing relative frequency
對於observe events
然後呢把這些機率呢allocate 到unseen event 去
其實就是我們這邊講的這些本來是unseen 的我都給它一些機率
這這些都要給它機率的話就表示說我這些看到的機率要降低一點
凡是observe 的data 機率要降低一點然後呢給這些unseen 的機率
那它的這個這個這上面就是剛才講的
我把這個這個observe 的event 的機率降低一點
然後呢把這些降降掉的機率呢allocate 給unseen event 
那這底下所說就是它的formulation 
假設說我的event 總共有大k 種
那麼小k 等等代表某一個event 
然後呢那種event 的發生的次數
叫做n 的k
那就以剛才的例子而言
一號魚就是k 等於一嘛
二號魚就是k 等於二
然後n 呢等於k 就是它看到幾次嘛
所以以剛才的例子而言
k 等於一就是一號魚就看到看到十次就是釣到十條嘛等等
那麼n 等於六就是六號魚一條等等
好那那我現在就以n of k 代表是k number k 的event 
看到的次數
那你總共看
那你這個這個那當然這個啊你裡面有一堆是unseen 的
那些unseen 的event 的n of k 都是零嘛
unseen event 的n of k 等於零
但你現在把所有的event 通通加起來就是你的總共的observation 數目是大n 
就是我總共看了這麼多次
ok 
那它現在重新排一次
它說我們重新這樣的看
我重新這樣子看
就是event 出現的次數
跟這種次數的不同的event 
就是出現r 譬如說出現零次的
叫做n 零
出現一次的
叫做n one 
出現二次的叫做n two 
出現r 次的叫做n r 
什麼意思
所謂出現零次的
是說你那些unseen event 那些unseen event 
到底有幾幾種unseen event 
就是n 零
以我們剛才的例子
就是九百九十四
你有九百九十四種unseen 都是你沒看過的
所以unseen event 就是有n 零
種那你只看到一次的那種event 
叫做n one 種
那以剛才那個例子而言
我只看到一次就是三條魚嘛
就是就是三種
就是三種
三種event 
不同event 的總數
不同event 總數所以剛才看到只只看到一條的那種呢
有三次
所以這不同event 的有有三種魚啊
所以n one 
那麼看到有兩次的有幾種
n two 
看到r 次的有n r 種
我們重新不用剛才這個n of k 而現在用n r 來代表
所以呢這邊講就是這個意思
n r 是number of 不同的event
occur r 次哦
就是你現在發生你看到r 次的
不同的event 有n r 種
那麼different event 的
它的次它的這個出現的次數是r 次
所以出現r 次的有n r 種
好如果是這樣的話呢我的總共的event 數目剛才是n k 
summation over k 我現在n r 
要乘上r 之後summation over r 
也是一樣的
對不對
就是說零次乘以n 零
沒就全部沒有看到次數
一次乘以n one 兩次乘以n two 嘛
那全部加起來像剛才的話加起來的就是十八嘛
十八條就是這樣來的
就是一乘三啊什麼什麼什麼加起來
那就是十八
那這就是r 乘以n r
好那麼good turing 的基本的原則就是底下所說的這些
那這些是什麼呢其實就是我們剛才講的
我們說
你等於是把只看到一次的那十八分之三
當成是看到沒看過的機率
你就從現在所看到的十八條魚裡面
有三條是第一次出現的
你就可以假設十八分之三是會看到沒有看過的魚的機率
所以呢一次的那種就當成是unseen event 
那十八分之三是什麼就是這個
一乘以n one 
你把這個一乘以n one 的總次數歸給它
因為這邊是零次
對不對
這個乘以它就是就是它次數就是就每一種的次數都在這裡嘛所以這是零
這個一零乘以n 零一乘以n one 二乘以n two r 乘以n r 
這通通加起來就是總次數所以這每一個乘起來都是它的次數
就是這個r 乘以n r 的這個次數
那麼你就把這個一乘以n n one 的這個呢就歸給它
算是沒有看過的
那麼這麼一來我這是可以了我unseen event 可以這樣分之後
那一次的沒有了怎麼辦呢
那我就把兩次的這些event 
歸給它
那兩次的有了歸給他之後那兩次的沒有了怎麼辦呢那我就把三次的歸給它
就這樣子
那所以呢r 次的這些東西呢就歸給r 減一次
那r 次又沒有了怎麼辦呢
我用r 加一次
它有n 的r 加一
我把它歸給r 次
ok 
所以這個想法呢跟有一點像是都減一的味道
這個發生一次的
這個東西呢給零次
發生兩次的呢給一次
發生三次給兩次
發生r 加一次的給r 次
好我們剛才有一個說是全部給它加一
這個有一點像減一不完全是啦
不過有一點這樣的味道
那這個想法呢就是這個所謂的good turing 
那麼嗯寫在底下這句話裡面
就是所有發生的次數
那麼你的這個
原來是第r 次乘上r 乘上n r 就是
發生r 次的總共的發生r 次的所有的event 總共的次數是r 乘以n r 
我現在變成是r 加一乘上n 的r 加一了
ok 
原來發生r 次的總共的event 應該是r 乘上n r 
現在變成r 加一乘上n r 加一
所以呢
就是就是這一句話所說的意思 
當你變成這樣子之後呢
那麼你unseen event 到底被分到多少呢
就是n one 
n one 次一乘以n one 的分給n 零個event 
於是呢我每一個event 被分到的機的次數是多少是
n one 次除以n 零
嘛那以剛才那個例子就是三條魚分給九百九十四種
每一種魚是九百九十四分之三的機率
的的次數
所以呢你的每這個unseen event 
所以呢你的每這個unseen event 每一個分到的次數是n one 除以n 零
那如果是這樣的話呢
那我總共有n 零個event 的嘛
所以這邊總共被分到多少呢
就是n 零乘上這個就是n one 
就是這個n one 次分給n 零了
那這就是底下最後這一行所說的
我所有的account 給unseen 的呢就是我每一個unseen 的被分到的次數是n one 除以n 零
然後我總共有n 零個所以就是我總共是n one 次分給unseen event 
好那如果是這樣的話
當然我現在全部都可以重算一次
它重算一次的話呢
我現在這個
譬如說我r 次的原來是r 次
總共有n r 種
現在這個次數送給它了
之後我用了r 加一的n 的r 加一的這些次數給它了嘛
所以呢我重新算一算它變成幾次呢
那它就它就被當成是r 的star
我重新算它是r 的star 
那麼這個二呢也有二的star 
一也有一的star
零也有零的star 
就是我重新用這個方法來算之後
它的每一個event 到底有幾次呢
以剛才為為為例的話呢
這個零的star 就是n one 除以n 零
也就是我現在unseen event 的每一個unseen event 發生的次數
是n one 次除以n 零嘛
那那個呢叫做新的零的次數就叫做零的star
同樣呢我也可以有一的star
二的star
那r 的star 是什麼呢
就是r 加一乘上n 的r 加一除以n r 嘛
也就是我把r 加一乘上n 的r 加一
這是這邊的總共的次數
每一個發生r 加一次
然後有n 的r 加一種
所以這麼多次之後呢
歸給上面這個了
那麼上面這個其實只有n r 種啊所以呢就要它它乘它除以它嘛
所以呢就要它它乘它除以它嘛
ok 
所以這就是r 的
ok 所以這就是r 的star 的意思
r 加一乘上n 的r 加一就是我這邊的n 的r 加一種每一種發生r 加一次
這麼多的次數我現在分給它了
分給n r 之後
那麼現在它的次數呢我們叫做r star 
那就它乘它除以它
所以呢那就是good turing 的基本精神就是這麼做
那當然如果這麼做的話呢
那麼我r 的star 就不是零嘛所以unseen event 都有一個次數
就是這個次數
那如果是這樣的話
那我的總次數仍然沒有改變
他用這個推的意思就是說總次數沒有改變嘛
那你看就知道沒有改變嘛它它只是換一換位置而已總次數沒有改變
那他這個式子這樣寫的方法意思是說呢我現在r 這個r 次的變成r star 次了
r star 次仍然是有n r 種
對不對r star 次仍然是有n r 種
然後我summation over 所有的r 的話呢
那麼現在r star 呢我換成這個式子
r star 換成上面這個式子乘以n r 的話呢
那其實相當於這樣子
那這樣加起來還是n 嘛
所以呢我的總次數沒有改變
這就是good turing 的的estimate 的基本的精神就是這樣子
那這個方法基本上是不能算是完全對但是是一種做法來解決
因為你真的不知道unseen event 到底是多少
和用這個方法來做
那這個方法其實是有一些基本的問題的
那有它有兩個最明顯的問題存在
第一個問題是說它把所有的unseen event 的機率看成一樣
那你你可以想像我海底的沒有看到的魚有九百九十四種
難道它們都機率都一樣嗎
這裡面還是有機率高跟機率低的嘛
那顯然不是這樣平分的嘛
那他現在是用平分的
這個是不合理的地方
那你照說這個應該不是平分
這是第一個問題
然後第二個問題是ok 它分給它它分給它它分給它這都講得通
那最最大的那個怎麼辦
假設說原來是出現最多的是r 跟n r 
出現最多的那個event 是出現大r 次
它有n r 種的話
現在呢這堆呢都分給r 減一去了
所以出現最多的這一群其實應該是機率最高的那一群沒有了對不對
這是機率最高的那一群是最重要的那一群現在沒有了因為都分給r 減一去了
所以最高的沒有了
所以這是它的兩個明顯的問題需要解決的
那你如果去看我們說過good turing 這是兩個人的名字
他們是統計學家他們其實做的是生物統計
你如果去看他們原始good turing 的paper 的話
他是在他們都不是做language model 他們是在做細菌的統計
那麼在統計細菌的時候
有千千萬萬的細菌所以這個不是問題
沒有沒有這樣的問題因為細菌千千萬萬
所以你這個加是summation 是加到無限大去
這個加到無限大去所以沒這個問題
可是在我們這裡這個問題是存在的
那因此呢怎麼解決這個時問題呢
ok 所以我們剛才所說的就是用這個方法來做
所以我們剛才這裡講你如果把十八分之三歸歸給看不見的九百九十四種魚的話
那麼你的原來六號魚應該是十八分之一
現在就變成十八分之一的star 
那一的star 就是我們剛才講的那件事情
就是你把二兩次的那個歸給一次這樣來算
那一的star 之後你這樣這樣我分的是十八分之一star 
這樣算就會變成二十七之一
它的機率就會降低了就是這個例子在說明
那麼我們講這個good turing 有它的問題所以怎麼辦呢
那問題是怎麼樣做這件事
那good turing 這個是兩個統計學家他們當年在解決統計問題統計問題有一堆這種問題
所以呢他當時想了一個方法就是所謂good turing 的smoothing  
那這個我們也許用下一頁的這個例子來解釋是最清楚的
我們上次上課的時候就停在這裡  
假設說你現在出海去釣魚
那總共呢釣到十八條魚
這十八條魚總共有十總共有六種
n 這個一號魚二號魚三號魚四號魚五號魚六號魚總共有六種
那麼一號魚呢特別笨特別貪吃
所以呢海面裡很多一釣總共釣起來十條是一號魚
二號魚呢有三條三號魚有兩條
四五六各有一條等等
這樣呢你總共釣到十八條魚
那你想根據你釣到十八條魚來估計海裡面的所有的魚的distribution 是怎樣的
那這個事實上就是一個非常不太可能的事情
那麼你海裡面有千千萬萬的魚你怎麼憑什麼用你釣到十八條來統計呢
那其實就是我observe 的event 就只有這麼多
然後它只有這六種
我們假設說我海裡總共有一千種魚
海裡面有一千種魚
但是我們現在呢總共只釣到了十八十八條
這十八條裡面呢是這樣distribution 
那我憑這個十八條我我怎麼判斷這個整個所有的魚的distribution 呢
那他說你第一件事情你就要知道我現在只看到六種魚
所以我九百另外還有剩下九百九十四種
我是沒有看到的是unseen event 
我必需把這十八裡面的一些機率分給它
剩下的機率再分給這十六條啊這這這六種
那你到底要分多少機率給這個九百九十四呢
他有一個非常簡單的假說
這個假說不見得很合理但是就是它的做法
那麼有一點道理但不是很對
但是呢他就是這麼做的
那他這個做法就是說呢
這十八條魚裡面現在有三條只釣到一條了有這有這三種
四號五號六號這三種魚只看到只釣到一條
既然只釣只釣到一條的話呢就相當於是說
你釣起來之前你是沒有看過的
也就是說這四號五號六號這三種魚
你釣起來的那個時候你是沒有看過的
所以把它看成是你沒有看過的魚
如果這樣的話呢你沒有看過的魚總共是十八分之三
ok 你就是把只出現一次的當成是unseen 
也就是說你你可以想成這個他的想法等於說你這個你現在這十八次裡面有三次釣到的是沒看過的新魚
所以你就可以假設是說你凡是那種這這個十八分之三也就是你會釣到沒看過的新魚的機率
那如果這樣子來想的話呢
這個說法是有一點道理
不見得完全對但是呢他就是這樣做
所以呢我這個十八分之三呢就是沒有看過的新魚
所以呢你那些沒有看過的九百九十四種魚呢
你就把它除以十八分之三除以九百九十四
那所以呢那九百九十四種魚呢就給它這個機率
那你現在的再看到的這十條
這這六這六種魚呢
你你就把十八分之十五重新按照比例分給這六種魚
ok 
這就是good turing 的最基本的原理
那麼因此呢我就變成說是把這十八分之三分給九百九十四種
那麼每一種呢沒看過的魚每一種有這樣的機率
那看過的這六種魚呢
來平來分這個十八分之十五
這十八分之十五按照它們的比例重新分一次
於是呢譬如說
六號魚分到的本來應該是十八分之一
它現在變成二十七之一了
就少了一點
那這個他他有一個算法
那麼這樣的話呢我就是讓它這個己經看到呢就是分這十八分之十五
所以呢就會少了一點
他就變成二十七分之一了
等等
這是所謂的good turing 的觀念
那說得更清楚一點我們就回到前一頁的這個
那它的這個這個這上面就是剛才講的
我把這個這個observe 的event 的機率降低一點
然後呢把這些降降掉的機率呢allocate 給unseen event 
那這底下所說就是它的formulation 
假設說我的event 總共有大k 種
那麼小k 等等代表某一個event 
然後呢那種event 的發生的次數
叫做n 的k
那就以剛才的例子而言
一號魚就是k 等於一嘛
二號魚就是k 等於二
然後n 呢等於k 就是它看到幾次嘛
所以以剛才的例子而言
k 等於一就是一號魚就看到看到十次就是釣到十條嘛等等
那麼n 等於六就是六號魚一條等等
好那那我現在就以n of k 代表是k number k 的event 
看到的次數
那你總共看
那你這個這個那當然這個啊你裡面有一堆是unseen 的
那些unseen 的event 的n of k 都是零嘛
unseen event 的n of k 等於零
但你現在把所有的event 通通加起來就是你的總共的observation 數目是大n 
就是我總共看了這麼多次
ok 
那它現在重新排一次
它說我們重新這樣的看
我重新這樣子看
就是event 出現的次數
跟這種次數的不同的event 
就是出現r 譬如說出現零次的
叫做n 零
出現一次的
叫做n one 
出現二次的叫做n two 
出現r 次的叫做n r 
什麼意思
所謂出現零次的
是說你那些unseen event 那些unseen event 
到底有幾幾種unseen event 
就是n 零
以我們剛才的例子
就是九百九十四
你有九百九十四種unseen 都是你沒看過的
所以unseen event 就是有n 零
種那你只看到一次的那種event 
叫做n one 種
那以剛才那個例子而言
我只看到一次就是三條魚嘛
就是就是三種
就是三種
三種event 
不同event 的總數
不同event 總數所以剛才看到只只看到一條的那種呢
有三次
所以這不同event 的有有三種魚啊
所以n one 
那麼看到有兩次的有幾種
n two 
看到r 次的有n r 種
我們重新不用剛才這個n of k 而現在用n r 來代表
所以呢這邊講就是這個意思
n r 是number of 不同的event
occur r 次哦
就是你現在發生你看到r 次的
不同的event 有n r 種
那麼different event 的
它的次它的這個出現的次數是r 次
所以出現r 次的有n r 種
好如果是這樣的話呢我的總共的event 數目剛才是n k 
summation over k 我現在n r 
要乘上r 之後summation over r 
也是一樣的
對不對
就是說零次乘以n 零
沒就全部沒有看到次數
一次乘以n one 兩次乘以n two 嘛
那全部加起來像剛才的話加起來的就是十八嘛
十八條就是這樣來的
就是一乘三啊什麼什麼什麼加起來
那就是十八
那這就是r 乘以n r
好那麼good turing 的基本的原則就是底下所說的這些
那這些是什麼呢其實就是我們剛才講的
我們說
你等於是把只看到一次的那十八分之三
當成是看到沒看過的機率
你就從現在所看到的十八條魚裡面
有三條是第一次出現的
你就可以假設十八分之三是會看到沒有看過的魚的機率
所以呢一次的那種就當成是unseen event 
那十八分之三是什麼就是這個
一乘以n one 
你把這個一乘以n one 的總次數歸給它
因為這邊是零次
對不對
這個乘以它就是就是它次數就是就每一種的次數都在這裡嘛所以這是零
這個一零乘以n 零一乘以n one 二乘以n two r 乘以n r 
這通通加起來就是總次數所以這每一個乘起來都是它的次數
就是這個r 乘以n r 的這個次數
那麼你就把這個一乘以n n one 的這個呢就歸給它
算是沒有看過的
那麼這麼一來我這是可以了我unseen event 可以這樣分之後
那一次的沒有了怎麼辦呢
那我就把兩次的這些event 
歸給它
那兩次的有了歸給他之後那兩次的沒有了怎麼辦呢那我就把三次的歸給它
就這樣子
那所以呢r 次的這些東西呢就歸給r 減一次
那r 次又沒有了怎麼辦呢
我用r 加一次
它有n 的r 加一
我把它歸給r 次
ok 
所以這個想法呢跟有一點像是都減一的味道
這個發生一次的
這個東西呢給零次
發生兩次的呢給一次
發生三次給兩次
發生r 加一次的給r 次
好我們剛才有一個說是全部給它加一
這個有一點像減一不完全是啦
不過有一點這樣的味道
那這個想法呢就是這個所謂的good turing 
那麼嗯寫在底下這句話裡面
就是所有發生的次數
那麼你的這個
原來是第r 次乘上r 乘上n r 就是
發生r 次的總共的發生r 次的所有的event 總共的次數是r 乘以n r 
我現在變成是r 加一乘上n 的r 加一了
ok 
原來發生r 次的總共的event 應該是r 乘上n r 
現在變成r 加一乘上n r 加一
所以呢
就是就是這一句話所說的意思 
當你變成這樣子之後呢
那麼你unseen event 到底被分到多少呢
就是n one 
n one 次一乘以n one 的分給n 零個event 
於是呢我每一個event 被分到的機的次數是多少是
n one 次除以n 零
嘛那以剛才那個例子就是三條魚分給九百九十四種
每一種魚是九百九十四分之三的機率
的的次數
所以呢你的每這個unseen event 
所以呢你的每這個unseen event 每一個分到的次數是n one 除以n 零
那如果是這樣的話呢
那我總共有n 零個event 的嘛
所以這邊總共被分到多少呢
就是n 零乘上這個就是n one 
就是這個n one 次分給n 零了
那這就是底下最後這一行所說的
我所有的account 給unseen 的呢就是我每一個unseen 的被分到的次數是n one 除以n 零
然後我總共有n 零個所以就是我總共是n one 次分給unseen event 
好那如果是這樣的話
當然我現在全部都可以重算一次
它重算一次的話呢
我現在這個
譬如說我r 次的原來是r 次
總共有n r 種
現在這個次數送給它了
之後我用了r 加一的n 的r 加一的這些次數給它了嘛
所以呢我重新算一算它變成幾次呢
那它就它就被當成是r 的star
我重新算它是r 的star 
那麼這個二呢也有二的star 
一也有一的star
零也有零的star 
就是我重新用這個方法來算之後
它的每一個event 到底有幾次呢
以剛才為為為例的話呢
這個零的star 就是n one 除以n 零
也就是我現在unseen event 的每一個unseen event 發生的次數
是n one 次除以n 零嘛
那那個呢叫做新的零的次數就叫做零的star
同樣呢我也可以有一的star
二的star
那r 的star 是什麼呢
就是r 加一乘上n 的r 加一除以n r 嘛
也就是我把r 加一乘上n 的r 加一
這是這邊的總共的次數
每一個發生r 加一次
然後有n 的r 加一種
所以這麼多次之後呢
歸給上面這個了
那麼上面這個其實只有n r 種啊所以呢就要它它乘它除以它嘛
所以呢就要它它乘它除以它嘛
ok 
所以這就是r 的
ok 所以這就是r 的star 的意思
r 加一乘上n 的r 加一就是我這邊的n 的r 加一種每一種發生r 加一次
這麼多的次數我現在分給它了
分給n r 之後
那麼現在它的次數呢我們叫做r star 
那就它乘它除以它
所以呢那就是good turing 的基本精神就是這麼做
那當然如果這麼做的話呢
那麼我r 的star 就不是零嘛所以unseen event 都有一個次數
就是這個次數
那如果是這樣的話
那我的總次數仍然沒有改變
他用這個推的意思就是說總次數沒有改變嘛
那你看就知道沒有改變嘛它它只是換一換位置而已總次數沒有改變
那他這個式子這樣寫的方法意思是說呢我現在r 這個r 次的變成r star 次了
r star 次仍然是有n r 種
對不對r star 次仍然是有n r 種
然後我summation over 所有的r 的話呢
那麼現在r star 呢我換成這個式子
r star 換成上面這個式子乘以n r 的話呢
那其實相當於這樣子
那這樣加起來還是n 嘛
所以呢我的總次數沒有改變
這就是good turing 的的estimate 的基本的精神就是這樣子
那這個方法基本上是不能算是完全對但是是一種做法來解決
因為你真的不知道unseen event 到底是多少
和用這個方法來做
那這個方法其實是有一些基本的問題的
那有它有兩個最明顯的問題存在
第一個問題是說它把所有的unseen event 的機率看成一樣
那你你可以想像我海底的沒有看到的魚有九百九十四種
難道它們都機率都一樣嗎
這裡面還是有機率高跟機率低的嘛
那顯然不是這樣平分的嘛
那他現在是用平分的
這個是不合理的地方
那你照說這個應該不是平分
這是第一個問題
然後第二個問題是ok 它分給它它分給它它分給它這都講得通
那最最大的那個怎麼辦
假設說原來是出現最多的是r 跟n r 
出現最多的那個event 是出現大r 次
它有n r 種的話
現在呢這堆呢都分給r 減一去了
所以出現最多的這一群其實應該是機率最高的那一群沒有了對不對
這是機率最高的那一群是最重要的那一群現在沒有了因為都分給r 減一去了
所以最高的沒有了
所以這是它的兩個明顯的問題需要解決的
那你如果去看我們說過good turing 這是兩個人的名字
他們是統計學家他們其實做的是生物統計
你如果去看他們原始good turing 的paper 的話
他是在他們都不是做language model 他們是在做細菌的統計
那麼在統計細菌的時候
有千千萬萬的細菌所以這個不是問題
沒有沒有這樣的問題因為細菌千千萬萬
所以你這個加是summation 是加到無限大去
這個加到無限大去所以沒這個問題
可是在我們這裡這個問題是存在的
那因此呢怎麼解決這個時問題呢
ok 所以我們剛才所說的就是用這個方法來做
所以我們剛才這裡講你如果把十八分之三歸歸給看不見的九百九十四種魚的話
那麼你的原來六號魚應該是十八分之一
現在就變成十八分之一的star 
那一的star 就是我們剛才講的那件事情
就是你把二兩次的那個歸給一次這樣來算
那一的star 之後你這樣這樣我分的是十八分之一star 
這樣算就會變成二十七之一
它的機率就會降低了就是這個例子在說明
那麼我們講這個good turing 有它的問題所以怎麼辦呢
後來真正由good turing 所發展的用得最多的是所謂的katz 
那我們剛才講其實嗯有效用而用得多的方法不是只有katz 還有別的啦
那我們沒有那麼多時間講那麼多種
所以我們就以他為例
那katz 的基本精神就是這樣子做
但是呢他解決我們剛才講的兩大問題
第一個問題是
零零的event 並不是平分的
它們應該要有高低
就這九百九十四種魚還是有高低的嘛
不是平分的
第二個問題就是我機率最大的沒有了怎麼行呢這個不通嘛
所以呢解決這兩個問題的話呢就變成所謂的katz 
那我們來說一下katz 是怎樣的
katz 的基本的想法跟剛才的是非常像的
它就是從剛才的good turing 來的但是呢它稍微有一點不一樣
我們還是一樣一個是次數一個是不同的event 數
那麼如果沒有的是n 零種
一次的是n one 種
兩次的是n two 種等等等等
那它的基本原則是什麼呢
第一個你如果次數夠多的話是reliable 的就不要動它
我們舉例來講
那如果說是這個嗯r 零次好了
我們以這個r 零為一個上限
r 零次數是n r 零種
嗯不是r 零次的那種不同的event 有n 的r 零那麼多個
我就以這個為為限
在這個以上的我就不動它了
這是r 零加一一直到大r 
這個是n 的r 零加一一直到n 的大r 
這些我都不動了因為越上面的話是越重要的
它的統計越準嘛
統計準我就不要動它
所以呢large counts are reliable 所以unchanged
我這以下的我全部都不動
要動的呢就是動這裡的
從一到零到r 零為止
我只動這個比較少的這個少的本來本來反正本來就不準
本來就不準
因為次次次數太少反正本來就不可靠我來動這裡
這是第一個原則
第二個原則呢就是small small counts 嗯就把它discount 
那麼換句話說呢
我這個出現一次的我把它算成這個我打一個折扣叫做dr 
那麼dr 呢就是這邊的discount ratio 
打一個打打一個折扣
discount ratio for event with r times 
就是我像這個一次的這些東西呢現在不是一次
算什麼零點九次還是零點七次
我打一個折扣
那麼這個打一個折扣這個這個折扣的原則是根據剛才的
也就是說根據剛才的good turing 的話呢
這個應該變成一的star 嘛
所以呢這個應該是d one 這個是d two dr 零
啊我們這邊可以再加一個r 次的話
是dr 
ok 
這樣子
也就是說而我dr呢是根據剛才的r 的star 除以r 
我們剛才是應該出現的r 次把它變成r star 次對不對
那這個就是我在這邊的good turing 的原則
就是一次變成一star r 次變成r star 次
那這個呢等於是等於是這個做一個改變
那我的這個這個基本上就是根據這個r 變成r star 這個原則
來做這個discount ratio 
所以呢我們每一個都給它打一個折扣
打了折扣之後呢
那麼我真正就會少了多少次呢
那就是那就是變成是這個譬如說是r 是打了dr 折扣的話這個原來次數是n r 
那我真正的是多少呢是n r 乘上一減掉
所以呢是n r 乘上
於是我就會變成這個r 次乘上n r 
乘上一減掉dr 
這些東西全部加起來的就是給了n 零對不對
譬如說我我這邊是譬譬如說我我這個一次變成現在只一次的打了一個折扣變成了零點九次的話
那我其實可以分多少給上面呢
是一乘以這個一減掉零點九
就是零點一
然後乘上n 的這些東西可以歸給它對不對
那麼兩次的乘以一減掉d 二
這個打的折扣剩下的呢
再乘以這麼多個event 
這些東西可以歸給它
所以呢r 次乘以一減dr 乘以n r 
這些東西可以歸給它
這樣子那所有的這些東西加起來就是我通通歸給它的
是多少呢
我歸給它的要等於n one 
所以總共呢是n one 次
那這一點又是跟剛才是一樣的
這一點是剛跟剛才一樣我們剛才講的就是我總共把多少個次數歸給unseen 的呢
就是n one 次對不對
那把這個把n one 次歸給unseen 是的道理就是我們剛才講的
你總共只看到一次的
out off 所有的機率
好像剛才的十八分之三
就好像是unseen event 的機率
這樣來看的話我就把n one 歸給它吧
那我現在的做法呢就是
我只動從零到r 零為止我只動這些
那每一個呢打一個折扣
這打折扣的原則也跟good turing 是一樣的
就是這個原則
打折扣用這個原則
然後呢打折扣之後剩扣出來的那些東西呢就是一減掉dr 乘上這個嘛對不對
那麼就就一次的而言就是一次乘上一減掉dr 
這個如果打打零點九的折扣那零點一的再乘上n one 的那些呢
給了它
這個如果打零點七的折扣的話呢那剩上的零點三的呢乘以它乘以它呢分給它等等
所以所有的這些東西加起來呢
就是我的要分給unseen 的就是n one 
ok 
所以呢那麼這邊講的這個這裡的原則其實都是剛才good turing 的基本的精神
就是我總共有n one 個event 
分給unseen 
n one 個event 分給unseen 
那這n one 怎麼來
就是在一到r 零裡面的每一個分別去打折扣之後出來的
而這個折扣的原則是這個原則ok 
好那這樣的話呢在這情形之下你其實是可以算得出來
因為我現在總共折扣的原則是這個比例
這個是你可以算得出來的
然後呢我的總數從一加到r 零的時候
就是r 等於一加到r 零的這個總數
也是確定的也是n one 
所以given 這兩個條件given 這個總數是r one 
以及每一個discount 的ratio 的原則是這個原則
given 這兩個條件你其實可以算得出來所有的東西
那就這些給了unseen 
給了unseen 之後呢我們剛剛講
你這個unseen 的n 零你不應該是平分
因為unseen 裡面還是有高有低呀
那怎麼辦
我的distribution of count among unseen event 呢
是根據n 減一gram 
就是next lower order 就是back off 
就是我們剛剛講的back off 的原則
那你並不是全部平分給這n 零個
而是說那你看n 減一gram 怎樣
n 減一gram 高的就高低的就低
給他一個比例
那這樣的話呢就解決了我們剛才講的兩個問題
我們剛才講它的第一個問題就是n 減一就是你你那個unseen 並不是平分嘛
那它現在說我就是用n 減一gram 來分
所以這是解決第一個問題
第二個問題我們說機率最高的變成沒有了不通嘛
那它不會
就是r r 零以上的我都不動
所以這邊都不動
我只動中間的
那這樣這剛才講的兩大問題它都解決了
那這個呢就是所謂的katz 
那我們可以舉一個例子來講一個這個的精神就是我們舉bigram 為例
那這個bigram 是怎樣的呢
就是我要算這個i 減一後面看到i 的機率
那你看到這個第一條式子就是說
超過r 零的就照原來的
就是這邊講的我超過r 零的這些呢
就照原來嘛不動嘛
就照原來
所以呢前面這個式子就是我們本來所講的求n gram 的式子
這個bigram 本來就是這樣了就是這兩個出現的次數除以那那一個出現的次數嘛
嗯所以這兩個就是原來的bigram 的式子
那然後呢如果是在r 零到一之間的話
一到r 零之間的話是什麼呢
就是我原來的次數乘上這個discount ratio 
我本來應該算是一次的我現在變成是零點九次
所以呢我的原來的bigram 計算呢
要乘以零點九嘛
這個本來要出現兩次的現在變成要打一個折扣零點七的話
那我原來的bigram 就要乘上零點七嘛
所以就是乘上這個discount ratio 
所以你看第二個呢在r 零到一之間嗯
一到r 零之間
就是第二個式子
那這後面這個這一堆就是原來的bigram 
現在乘上這個discount ratio 
ok 
然後第三個式子就是說這個unseen event 怎麼辦
unseen event 我己經把n one 分給它了
那麼我這個時候用什麼來算
用它的n 減一gram 就是unit gram 
所以呢我現在這個bigram 呢就是以unit gram 來計算
那麼我以unit gram 的高的就高低的就低
根據unit gram 來分這個分這些次數
那只是說呢這個分的比例常數你得要算一算就是了
所以這個東西的算法就是使得你的total count 等於原來你所看到的count
count 不改變
那這這幾個原則有的話你就可以算出這些東西來
那這就是用這個katz 來算bigram 的算法
啊那基本講起來的精神就是這樣
那詳細比較複雜課本裡面有啊
那我想是不管是這一本還是這一本都有
啊你看這本裡面也有講這本裡面也有講
大概都有說到
所以呢詳細怎麼算的那它會說得清楚
那我們就不多講
那麼你們大概了解這就是我們所講的smoothing 
那課本裡面還會講不只這一種還會有別種
那這個嗯你自己看就好了我想基本精神都很像
我們在第二個習題裡會給你去train language model 
那裡面你就會去也是一樣
有一個有一個現階段全世界的語音做語音的group 所用的軟體
叫做是這個sri 的language model 
那那個language model 的的工具
它裡面就有所有的smoothing 的方法
那麼這些我們這邊講這些方法都在裡面
那麼你可以用那些來做
你就會知道做出來的結果會怎樣哦等等
那我們就不多講下去
你就自己看就行了
底下我們來講下一個topic 
就是說language model 另外的一個常常常使用的很有效的方法就是所謂的class based
什麼叫做class based 呢
意思是說其實你真的一定要算這麼複雜的這種n gram 嗎
好像不盡然
因為明明很多個word 是屬於同一個class 
所謂屬屬於同一個class 就是指它們在語意上或者在文法上其實是很像很像的
舉例來講譬如說john saw a dog 
如果john 可以saw a dog 的話當然mary 也可以saw a dog 
然後其他所有人的名字都一樣可以saw a dog 
那這些人有區別嗎好像沒有區別
如果要你如果要算這個john 後面要接這個saw 的這個bigram 的話呢
只要你其中一個人算得出來
換誰都一樣嘛
就這樣一個想法
那麼如果這樣來想的話你其實是可以把很多東西
凡是屬於它們很像的
either 是語意上很像
or 是文法上很像都變成一個class 
舉例來講你可以john saw a dog 當然也可以可以saw a cat 嘛
那麼cat 跟這個dog 好像沒有什麼區別嘛都一樣嘛
那麼因此呢好像可以放在一起哦
那你如果這樣子來想的話呢
那麼如果這些人可以幹嘛的話
那he 跟she 也可以嘛
如果he 跟she 可以的話my father 當然也可以my sister 當然也可以嘛
那麼可以是father 當然也可以是sister 啦對不對等等
那如果這樣來看的話當然我也可以drove a car 
如果可以drove a car 的話當然也可以drove a bus 嘛對不對等等
所以這麼一來的話你可以把凡是相同的一群
變成一個所謂的class 
如果變成一個class 的話呢
那我的n gram 就可以變成class based 的n gram 
這所謂class based n gram 的意思
你現在就不需要去算這裡每一個word 這個word 後面這個這兩個word 後面接這個word 機率
你可以算這個是屬於那個word 屬於哪個class 
如果你每一個word 都屬於一個class 的話呢你現在可以算這個class 的n gram 
就是你如果前面看到這兩個class 的話
後面會接哪一個class 
這個想法非常的單純啊很很容易想
但是呢你這個這個class 完了之後
這個這兩個class 之後接這個class 機率算出來之後呢
你應該還要算一下在這個class 裡面真的會看到這個word 的機率
換句話說這一堆人名
還有很常出現的跟很不常出現的
你如果john 是一個很常出現的話
換一個很不常出現的人名的話
他的這個可以不一樣嘛對不對
嗯那麼你你所以你可以再多一個這個
就是你看這個人名之後
在那個人名你看到這個人名之後
那人名裡面會看到他的機率是大還是小
你可以算可以算進去
嘛如果是這樣的話呢
我這樣子的class 的trigram 
再乘上看到這個class 之後會看到這個人名的機率
這樣子這個東西呢就可以取代我原來的這個機率
那麼這麼一來的話
我的其實這個這就是所謂的class based language model 
那class based language model 基本上它有smoothing 的效果
為什麼有smoothing 效果
那也是一樣把原來的unseen 都放在這裡嘛
你本來譬如說有一堆很很少人見的人名那些人名我根本不看到的
我都給他歸在這裡
那他也就有機率了嘛對不對
那麼我我我特別去算這些人名裡誰的機率高誰的機率低
那機率再低還是有機率嘛
所以這個就做到相當程度的smoothing 的效果
那如果這個可以做到smoothing 效果的話呢
那麼它跟我們剛才講的用lower order 啊lower order 就是我們講的back off 
就是我們之前在這裡講的back off 
就是用lower order 
那它跟這個方法常常是互補的
也就是說你有的時候可以用back off 到lower order 的方式來算你的n gram 
那有的時候其實你如果是可以是class 的話呢
我就用class 算也可以嘛
所以它跟lower order 這個是常常是一個互補的方法
常常是一個互補的方法
那麼也因為這樣子所以我class 的數目也可以大為降低
所謂class 數目大為降低的意思是說
你現在n gram 不要那麼多了因為class 很少可以少很多嘛
那麼以這個最常用以我們常習慣的中文為例
我們說中文的常用詞六萬個
我們中中文常用詞是六萬的話
你n trigram 有多少個機率
是六萬的三次方
你的trigram 有這麼多
然後你就要算這麼多個機率出來然後存起來
然後每一次去找這這麼多個
那如果我用這個方式的話中文的六萬個詞可以分成幾個class 呢
這個我們過去做的經驗呢大概在七百到一千五百之間就夠了
你你的分的class 大概分成七百個到一千五百個
大概夠了
以一千五百個為例你的trigram 變成這麼少嘛
這個比這個少很多啊
嗯那這就是指我的那我做起來什麼都方便了
這就是這個parameter 就大為減少
那在這個情形之下呢
可能最大的問題是那些word 放在一起變成一個class class
那這就是所謂的這個word clustering 的問題
你如何把什麼叫做語意上文法上相近這是什麼意思
哪些word 真的可以變成一個class 
這變成一個真的困難的地方在這裡
那這裡面舉了講了兩個例子
那這個是limit domain 的例子
這個是假設我要做的事情是一個limit domain 的話
其實很容易
基本上就用人來做啦
舉例來講譬如說這是一個買飛機票的一個dialogue 系統
你就專門給人家打電話進去買飛機票
那那個不是一個旅行社的一個一個agent 而是一台電腦
那你打進去問說tell me all fights of united from taipei to los angeles 
那你其實你可以想像所有的航空公司的名字變成一個class 
所有的城市名字變成一個class 嘛
你這個用人做就好了根本不要去
那我我講的就是這些東西嘛
所以你只要把所有的城市名變成一個class 
然後sunday monday tuesday 變成一個class 等等等等的話
你總共就這麼多個class 嘛
那這個時候呢你就直接用人手做這些class 之後
你你只要train 這些language model 
就是這個這個class 在這些後面的機率
這個class 這這個class 接在from 接在to 後面的機率等等等等這樣就好了嘛
那這樣而且有一個好處就是new item 不用去train 
直接加進去
今天有個新的航空公司長榮
那你就加進去就好了跟本就不用去train 嗯
你有一個新的航點飛到里斯本
ok 就加進去就好了
啊根本不要train 
那麼你大不了就算一下里斯本在所有的city name 裡面它的機率是多少
最多多算一個這個機率
那這個機率可能是以航班的比例來算
或者以旅客的比例來算你可以加一個機率在這裡
那這樣的話就是這個機率那這樣就好了嘛
所以你基本用人手來做而且呢你都可以不用data 
新的新的item 直接加進去就好了
這是limit domain 的情形
但是如果不是limit domain 的話呢而是general domain 的話那比較難了
你可以想我們我們這麼多個word 
英文的常用詞word 的數目大概跟我們中文的六萬差不多同樣的order 
你這麼多word 到底是誰跟誰是同一組
你不能全都用眼睛看嘛哦用人來看是可以看啦
但是你完全用用人又分不出來所以這個時候呢general domain怎麼做
那就要有一些所謂的這個word clustering 的方法
基本上是用這個這個這個這個啊data 以data driven 為主啦
你就是要用大量的data 去算
底下我們舉兩個例子
這個其實這種就是在general domain 裡面的這個general domain 裡裡面的這種這個大量的詞到底怎麼樣去做這個word clustering 是一個很有趣的問題
那麼在大概九零年代八零年代幾零年代是有非常多的研究在做這件事情
那麼確實他們做了不少非常好的language model 就是這樣class 的
那我們底下舉兩個例子這兩個例子應該是相當具有代表性的啊
那我不準備說詳細
都有一個reference 給你看
那麼我們大概說一下
那麼詳細的話我們你看這個reference 
那這兩個可以算是這個九零年代初期八零年代末期相當具有代表性的兩個經典作品
你現在看起來這個reference 很早啦
這個是十四年前的
這個是再早十十七年前的所以這個都很很古老的
不過這些都是經典作品
那麼我們今天其實那些在用的那種clustering class based 相當好用
所以今天還有很多的很多的系統真的就是用這種class based language model 
那他們所用的class 常常還是用這種方法在train 在train 出來的
或者這種方法衍申出來的更精緻的方法就是了
所以這些仍然是經典的是非常好用的經典哦
所以是很值得參考的所以我們還是把它列在這裡
那第一個例子是說
你就是把所有的word 一開始把所有的word 都當成一個 cluster
然後呢你每一次去找誰跟誰最像
把它黏起來
也就是說假設我有六萬個word 
一開始每一個word 都自己是一個cluster 
所以呢我有六萬個cluster 
然後呢我每一個iteration 的時候我去找誰跟誰最像
它們應該可以黏起來變成一個class 
那麼譬如說呢也許發現它跟它很像
可以黏成一個cluster 
那什麼叫做像呢
那基本原則就是minimize over all perplexity 
我原來用這個方式train 出來的language model perplexity 是多少
我現在如果把它們兩個黏成一個class 
連成一個class 的話
我可以重新再做這樣子的language model 它的perplexity 是多少
那我們說過language model 我們是希望perplexity 要降要小嘛
所以看誰跟誰黏起來讓我的overall perplexity 降的最多的
那你如果這個iteration 發現是它跟它黏起來的話ok 
我就把它跟它黏起來變成一個class 
於是我現在的class 數目少一個了
那再下一次我可能發現是它跟它連起來
那麼它跟它連起來的時候呢我的perplexity 降的最多那我就把它跟它連起來
那再下一次我可能發現其實是這個時候呢再把它連起來
是降得最多的
以此類推你這樣的你可以想像這個是一個計算量非常大的一個一個程式
但是它是有效的啊
也就是說因為你你假設你有六六萬個詞
你兩兩去看它跟它黏起來是會變成多少
它跟它黏起來這個光是兩兩都算一次你就算就算不少嘛
那當然這裡面是有一些技巧的
你怎麼樣讓這個需要計算量降到最低這是是有學問的
如果有興趣就看這篇paper 啊
這個是後來所有的講到用這個class based language model 的時候幾乎都是site 這一篇
這是最重要的一篇reference 
那你要用一些方法來讓這個計算量不至於太大
但即使那樣這仍然是一個很大的程式
我要用一個很大的training data 
然後還要另外一個data 來算perplexity 
然後呢我就是讓它不斷的跑
那計算量非常大然後最後我可以得到一個相當不錯的一一組class 
你這個這個iteration 到什麼時候停止呢你有一個條件嘛
就是如果你的這個perplexity 降不下去了
你的perplexity 降不下去了你就停在那裡
你這個時候你就得到你的一組
那這是第一個example 所用的方法
第二個example 呢完全不一樣了
它是用我們上次說過的cart 
也就是decision tree 
你記得我們說過我把一個一大堆東西放在這裡之後呢
想辦法用一個question 把它分成兩個
這個原則是甚麼降低perplexity
然後呢再把它拆出來用一個question 再把它拆出來
這個decision tree 我們在五點零所說的
它用這個東西來做
所以這是所謂的tree based 
那你現在要分的是什麼東西分的東西不一樣了我現在要分的東西是history
那你想每一個你可以看譬如說我現在講一一個word sequence 從w one 到w n
那它等於是每一個given 前面從w one 到i 減一之後
看到第i 個word 的機率i 一路乘下來的對不對
就是我從w one 開始看到w i 減一之後
那麼你看前面i 減一個
然後呢會看到第i 個的機率
然後我i 從一到n 
這個就是這個的機率
那這裡面的一到i 減一就是所謂的history 
就是w i 的history 
就是h i 
就是一到i 減一
你如果這樣來看的話我現在就可以把我的我有一個training data 
我把這個training 這個語料庫裡面的所有的
所有的各種各樣的history 統統拿來
然後呢我嗯基本上應該是這樣講的我為每一個word 
每一個word 都有一大堆它的history 放在這裡
然後去去把它拆出來等等
那麼我怎麼拆這個history 也是用一堆question 
不過這個question 是什麼
是這些history 的question 
舉例來講這個history 的question 會是什麼呢
不像我們之前所講的是我們之前在第五章講的說它左邊是子音還是母音
右邊是母音還是子音發右邊那個母音的時候嘴嘴巴是怎麼的甚麼甚麼那個那個的時候是為了要分做tri gram 
所以看左邊是什麼右邊是什麼
左邊是母音嘴巴怎樣什麼什麼的
那現在呢那因為我現在是在算language model 
是在是根據這個history 來的
所以我的question 變成我的history 裡面的question 
譬如說history 裡面這個history 裡面有沒有哪一個字
裡面有沒有動詞
有沒有名詞
有沒有的
有沒有什麼什麼字take 
你你可以你你可以想到所有的這一類的question 
就都是指我的history 裡面有沒有什麼字有沒有哪一種詞類
然後它有沒有什麼什麼東西哦等等
那用這個方式有一大堆的question set 
那一樣我也可以用它來我一樣算entropy 
那根據entropy 降低來確定說我應該用哪一個question set 把它拆開來
於是最後我的變成一大的堆的history 
那麼變成很像的一堆history 
那每一history 很像那些東西呢變成一個class 等等
那這個方法的基本的精神你可以想像跟我們之前講的那個一樣
就是包括了文法跟統計兩種knowledge 
所也就是一個是這個grammar 這個grammatical driven 
用文法的因為你現在講它的history 裡有沒有那一個動詞
有沒有那一個名詞這個是在講文法嘛
你一方面是講文法一方面是在算算這個entropy 是在講這個data driven 
所以你其實是這個文法跟data driven 這兩種通通都都算了
而且呢同時它包括了local 跟long distance relationship 
這話的意思是說我們一般的n gram 都只有local 的relationship 
什麼意思
這個n gram 永遠只算到前譬如說前面的n 減一個
跟後面的這一個
我n gram 算不到更遠的
所以n gram 永遠只是一個local relationship 
而沒有long distance relationship 
可是在真正的句子裡面long distance relationship 是永遠存在
而且重要的
我們舉一個例子來講
在不論中文跟英文任何一種語言都一樣
我們在講話的時候的語言顯然不是只靠local relation 
而是有long distance 
譬如說我們最簡最簡單的一個例子
洗了一個很舒服的澡
這個洗跟澡這是很有趣的
因為澡本身不是一個東西
你說吃了一個很吃了一個味道很很味道很甜的蘋果的話
那你你也是一樣吃跟蘋果
中間夾了很多很多東西
那你其實是應該是吃跟蘋果之間的有這個bigram 的關係
那麼你那個味道很好什麼東西
你你你這些東西呢插在中間
把吃跟蘋果中間會拉得很遠
那洗澡就更複雜了因為澡不是一個東西
那麼是一個詞是洗跟澡連起來的一個詞但是這個詞根本就切開了
那它們的關係是被拉到這麼遠
那你可以想像我們真正在講話的時候我們的很多很多句子裡面
真正你的bigram 應該train 的出來的bigram 關係可能是很遠的
在英文而言譬如說the boy walking on the street 
decides to 幹嘛幹嘛幹嘛
那同樣的是the boy decides 
那中間這些東西呢其實是跟decides 會在the street 後面嗎
好像沒什麼道理
它其實是在這後面的
這種就是所謂的long distance relationship 
那你不管那一種語言我們在講話的時候都會有一大堆這種的long distance relationship 
這種東西都是n gram 不會做的
n gram 做不出來的
那麼因此呢你其實是這是一種方法來做它因為如果你放在history 裡面的話
history 其實裡面就包括了long distance 都在history 裡面了
所以呢這是它的好處
那這個詳細的這個去看這個reference 
我們這邊不多講
那我把這個reference 列在中間的這種情形是指說這個reference 是給你參考
然後做為這個其實是可以做期末報告題目的你如果有興趣的話
這種東西可以拿來做期末報告哦
那跟我寫在前面一個chapter 前面的那些reference 是不一樣
的如果我寫在chapter 的那裡的reference 的話
這種reference 的意思是說是你回去真的要唸的
啊期中考會考的
就是說你當天準備期中考的時候這種東西是要看的
但是呢你如果是寫在這中間的這種
是說考試不會考的
但是是很好的references 
你如果考慮期末報告的話這種東西是可以做的
是這樣的意思
ok 好那所以這兩個我們不多說我們大概說到這裡
那以上大概我們把language model 裡面一些最重要的問題
從它的perplexity 到smoothing 這些東西class 我們大概都說到了
那啊底下我們要稍為講一些中文的一些狀況
那我們在這休息十分
ok
我們底下稍微講一點中文的狀況
那第一個呢就是class based language model 中文也一樣可以做這種事
我們中文也一樣可以把很多個詞兜在一起變成一個class 
那可以怎麼兜法呢其實你的data driven 方法是不見得是剛才的那兩種
其實你馬上可以想出很多種來
剛才那兩個是最具代表性的就是了
最簡單的方法你可以想怎麼做
假設我有我有n 個詞
我有n 個word 
我可以建一個matrix 
這邊也是n 個word 
以bigram 的精神來講的話呢
你可以數這個word 後面會接什麼word 
譬如說後面接這個word 後面出現五十一次
接那個word 十二次接這邊都其他都是零次
這個二十六次等等等等等等
那麼這個word 呢又又他的一一堆
那這個word 呢在這邊出現了有二十六次在這邊有出現十一次
這邊有出現十一次
等等等等
那你會發現這個word 跟這個word 是不是很像
它們後面都會接這個word 
都會接這個word 
都會接這個word 
那它跟它就很像了
等等
所以呢你只要做一個這樣子譬如說n 個word 跟n 個word 之間的table 
這其實就是它們的bigram 這其實不是bigram 
那你如果這樣來看的話呢
這其實這等於是它們的它們的feature vector 
這個word 後面會接這些東西這是它的feature vector 
這是它後面的feature vector 
你如果這樣看成這樣的話呢這就是這些vector 你就可以做vq 嘛
我就可以去用vq 的演算法去做clustering 
可以分成幾群那幾群大概後面接起來就很像
他們就可以等於是根據bigram 精神就可以把它們分群了
同理呢這樣子也可以啊
這表示這個word 前面會接什麼word 
對不對
這個word 前面會接什麼word 
如果它們像的話它們就會像嘛我就可以用這個來做
哦等等
那像這類從前我們都做過
這類都是可行的方法
你都會有效的把一些很像的詞兜在一起
啊
這這些都是簡單的做法
那當然這樣子的這一類的data driven 的data driven 的方法
它有它的弱最大的一個問題就是低頻詞很難做
因為你這個都是要靠頻率高的嘛
sil
如果是它本身真的是一個低頻的詞
很少見的罕用詞的話
low frequency word 不太容易做
那有什麼辦法呢
這是一個辦法就是用人工來加入
這裡這裡講的這個方法是我們在九零年代的時候曾經做過
效果非常好的一個word class 
是這樣做的
那基本上是怎麼辦呢我們分成三個stage 來做這件事情
這三個stage 裡面第一個stage 是其實是用人工
那這是根據什麼呢
是根據所謂的pos feature 什麼是pos 呢pos 就是所謂的part of speech 
也就是詞類
這是在語言學上的所謂的part of speech 
就是我們說的詞類
你可以根據它的
分成詞類
譬如說動詞名詞介系詞形容詞
那你可以分到很細
那我們當時所採用的是根據中研院的一個group 它們做的
他們是由語言學家跟這些人在裡面一起做
他們把中文的詞分成一百九十八個詞類
為什麼會有這麼多
它分得很細
譬如說名詞它可以分成幾十種名詞啊
這個有生命的跟沒有生命的有生命裡面是人的跟不是人的
是人的裡面有是大人還是小孩的
啊是什麼
有生命的裡面有這個是不是人的是動物的還是植物的還是什麼
你可以這樣的分得很細
同樣呢這個
不是沒有生命的東西你也可以分成譬如說是建築物還是什麼東西
什麼你都可以這樣的分
所以它的名詞可以分將近一百種好像
分得很細很細
這是根據他的語意可以分得很細
同樣動詞也分了好幾十種
就是有動作有狀態的是人的動作還是生物的動作
還是什麼啊各種各樣
及物的不及物的有幾個動詞有幾個受詞你都可以這樣分
所以它可以分成幾所以這樣總共可以分成一百九十八類
那我們就根據它的一百九十九十八類呢它有一個詞典
每一個詞都說這個詞可以當成哪幾類
譬如說我們隨便舉一個例子譬如說組織
這是一個什麼
它可以當成名詞
這是一個organization 
可以當成動詞to organize 
對不對
可以當成它會有好幾種詞類
那詞類a 詞類b 詞類c 詞類d 詞類e 這樣構成
這個就代表這個詞的它的一種feature 
那麼在中研院他們當時是因為他們每一個詞他都他有一個詞典
他們已經分好用語言學家他們去分好了
那所以我們就用就用這個當成第一第一個stage 的分法
就是我每一個我讓每一個每一個詞belong to 一個class 
而那個class 呢是由一組詞類來組成的
譬如說假設這個詞是有這幾種詞類的話
那我就是就是這幾種詞類決定某一個class 
那必須要同樣也有同樣也就是這幾種詞類的詞變成一個class 
ok 
所以呢這個就是講每一個word belong to 一個class 那個class 是由每一組詞類決定的
那這個詞類裡面包括這個syntactic 跟semantic 我們剛才講的語這個語意這個是語法啦
語法是說它及物的不及物的它後面要接什麼幾個受詞什麼東西這是講語法
這是講語意這個是有狀態的有動作的有什麼的
啊有生命的什麼這是這是semantic 
你根據這些來分的
這樣子之後我把所有的詞先分分出第第一個class 
到這個階段之後呢
所以這個階段是用人的知識來做的
那之後呢第二個階段是data driven 
我現在根據它後面會接什麼前面會接什麼根據它的data driven 再分第二次
分到第二次之後己經分到很細了之後呢
第三次再merge 一次
因為很可能雖然在這邊的時候是根據詞類把它分開來了
但是最後其實它們其實是可以merge 在一起的我可以再把它merge 起來
這邊有一個簡單的例子就是譬如說汽車巴士火車飛機這些都是屬於交通工具
所以最後會變成一個class 
相對於這個class 的呢這一堆動詞是屬於狀態的動詞
坐呀搭呀乘呀這屬於狀態的動詞
那這個呢是屬於去開它的去駕駛的這是屬於有動作的動詞
所以這兩種動詞我一開始講詞類的話
就分開來了
這個跟這個一開始我就就分到不同的地方去
然後最後呢它們譬如說這個呢會到這裡來那那這個呢會到這來
可是其實我最後發現它們後面都是接同樣的東西
我最後可以再merge 起來
等等
那這就是這個三個階段的做法
那這樣子得到一個非常好的一組word class 的class 的n gram 
那就是我剛才說到中文的詞大概分到七百到一千五百就夠了
那就是這個
這樣大概這中間參數可以調你要調到七百到一千五百之間都不錯啊
那
這個是當時我們的一篇碩士論文你如果要查還是可以查得到
那
在這個情形最大的一個好處是什麼呢為什麼要用人來做
就是漢用詞低頻詞
因為低頻詞是你如果完全用data driven 的方式它因為它的頻率太低你做不做不好
它會分到亂七八糟的地方去
那怎麼辦呢我一開始因為根據它的人的知識分到那裡去之後
你怎樣後面data 很少它也這個八九不離十差不遠啊
人的知識分到那裡去之後
你怎樣後面data 很少它也這個八九不離十差不遠啊
所以所有的低頻詞漢用詞大概都會分到一個合理的地方去
然後那當然這個方法會變成both data driven 跟human knowledge 
driven 你一方面是用了人的知識的作為第一階段的分群
之後後面再用data driven 的話你這兩者加起來這效果是不錯的
那這是一個簡單的例子
那後來這個方法有再進一步的版本就是我們把每一個詞的意思拆開來
因為我們很多詞是有好多意思
譬如說就這句話而言你知道這是一個單字詞
這個單字詞在這裡的意思跟這裡的意思是不一樣的
那麼因此到底哪一個詞歸哪裡
那麼在剛才的方法裡面的話ok 
這個會這個哦這個是一個非常有非常豐富的詞類的詞
它可以是這個意思可以是這種詞類可以是這個詞類可以是這個詞類
有很多種
那它必須要有另外一個詞跟它完全一樣有那麼多的才跟它變成同一個class 
這樣太複雜了
所以一個辦法是說
你就讓一個詞你如果它有不同的詞類的話呢就給它more than one class 
那你乾脆就是就是這兩種通常是一樣的是
那這兩種是通常是在一起的
這些詞類是同樣的這些詞類都是同樣的詞會有的那我就乾脆那同時是它是它的話我乾脆把
這個詞在這裡也放在這裡讓它出現在兩邊
啊這是第二種方法
就是我的詞呢我之前的剛才那個法是每個詞只放在一個一個class 裡面
我現在可以讓它我現在可以讓它放在兩個以上的class 裡面所以同一個word 可以放在兩個以上的class 裡面
然後就那樣去做
可是這個時候做你的language model 要很難就很難train 了
為什麼
你要知道它是屬於那一個詞類
我們剛才是剛才是凡是有這個詞我就是就是照樣去train 了根本不管因為那個詞就是那個詞類
現在不是了
因為這個詞有的時候是這些詞類有的時候是這些詞類
那你怎樣train 它的language model 
你要train n gram 的時候
對不對
這個後面這後面這兩個後面接這個
那你要你你現在變成要知道
你要train 你要知道說它是什麼詞類
它是什麼詞類你就知道這兩個詞後面會接什麼詞類
於是你的training data 裡面要知道每一個word 是什麼詞類才行
那就是所謂的tag corpus 
這個tag 這個tag 這個字的意思是把這個詞類掛上去
那啊語言學界他們很早就做這件事
就是你要有一個自動掛詞類的方的程式
你在一篇文章裡面每一個詞屬於什麼詞類自動把它掛上去
那麼你要用這種掛好詞類的的文章去train 
那他就會知道他什麼詞類
所以你就知道什麼詞類後面什麼詞類會接什麼詞類
那這樣子的話呢可以做
不過這個是工程比較浩大
因為你你的這個要把詞類掛進去才能train 
但是這樣出來的language model 會好嘛
因為它把哪個詞類後面會接什麼詞類都己經算進去了
那這樣子的這個嗯
那我們說一下你怎樣掛詞類
掛詞類的辦法其實也是一樣
你第一個呢就是
所所謂的pos 的tagging 就是掛詞類
你就是為句子裡面的每一個詞
你就是為句子裡面的每一個詞都把它的詞類標上去那麼你標詞類的方法呢基本上就是有一個詞典
都把它的詞類標上去
那麼你標詞類的方法呢基本上就是有一個詞典
每一個詞會有那些詞類你都有了
有一個詞類的n gram 
就是說你有一個詞類的n gram 
什麼詞類跟什麼詞類後面會接什麼詞類
這是詞類的n gram 
那麼通常因為詞類的總數有限
你的n gram n 可以比較大
我的word 有有六萬個所以我的trigram 沒辦法太大
可是如果我的詞類才兩百個嘛
我可以變成four gram five gram 
那麼它們通常詞類的n gram 是做到
至少做five gram 
因為你總共只有只有幾十個到上百個
所以不難做
你做到five gram 的話呢
你就有這個詞類的之間的關係
全部都有那就是所謂的pos n gram 
你如果有這個pos n gram 的話
那麼這pos n gram 怎麼來是要用人先用人去標一次嘛
譬如說你拿一個database 
是用人標好的詞類
你就可以train 詞類的n gram 
然後呢你現在就可以做這個自動標詞類的工作
那你可以想像是這樣子
就是你現在有個詞典
這個詞
它有這個詞類這個詞類這個詞類
這個詞它有這個詞類這個詞類
這個詞它有這個詞類這個詞類這個詞類
你有一個這樣的詞典
這個詞典就是我這邊所謂的lexicon of words with possible pos 
就是我都有這個詞典
我有這個詞典之後呢
我今天任何一個句子出來
這個詞可以是這三種詞類
這個是可以是這兩種詞類
這個是可以這三種詞類等等
那你可以想像我是一個這樣子的net work 
那到底是哪一個呢
我用我的n gram 來算
我現在有的詞類的n gram 
你就知道這個後面應該是
如果是它的話它後面接這個接這個
的機率是多少對不對
那如果是它的話呢它後面接這個
後面接這個後面接這個機率是多少
你這個都可以算嘛
你就就可以根據這個詞類的n gram 來算出來
這個是自動掛詞類的方去
就是根據這兩樣東西
就是一個是有標詞類的詞典
一個是這個啊詞類的n gram 
你根據這個就可以把自動標詞類
你自動標詞類標好之後你就有一個很大的database 
來可以做自動標詞類的的的標好詞類的語料庫
你就可以train 這種
好以上大概這個這個也是另外一篇碩士論文你如果有興趣的話可以查得到的
那我們要講一下就是說你
其實這個class based 跟word based language model 其實是可以整合的
那什麼意思呢
你基本上來講
這個word 以word 來做的n gram 
跟以class 來做的n gram 哪一種比較好
其實是如果是真的是word 的你train 得好的話word 比較好啊
因為它比較精緻
那class base 是比較粗的
因為它把一大堆class 搞在一起嘛對不對
你可以想像
class 是比較粗的language model 
因為你把一堆東西搞在一起
你想這個比較細緻還是這個比較細緻
當然是這個比較細緻啦
你如果這是哪一個word 跟哪一個word 之後會碰見
你如果這個train 得好的話
這個鐵定比這個好啊
這個是把一大堆word 跟一大堆word 跟一大堆word 放在一起了
所以這個是比較粗的
只是說這個的data 可能不夠你可能train 不好對不對
是data 不夠你train 不好的問題
所以這個其實這個class base 是有點像smoothing 的效果
train 不好的話不如用這個嘛
可是你如果train 得好的話這個顯然比這個好嘛
這是我們必須了解這一點
所以呢class 的不一定表示說一點會比word 好
應該是說word base 的是more precise 
如果是常用詞
你的頻率夠高的話
你count 的數目對的話
其實是比較好的
只是說當你data 不夠的時候
你可能需要用class 
所以呢比較好的辦法應該是把這兩者整合
你back off 到class 
如果count 不夠的話
就是說你其實是可以做就是只要count 的數目夠高
我就用word 的的的n gram 
數目不夠高我才用class 
那這是一個比較最好的辦法
數目不夠高就用class 
數目高我就用word 
然後呢只要它的夠高夠高頻的word 就單獨一個class 
夠高頻的那有很多很特你你知道在中文裡面有很多很特別的字
舉例來講把
這是一個非常有意思的一個字
在中文裡面
把什麼幹什麼這個由
那那那這是一非常高頻的詞它你你你要它跟誰在一起呢
不如它自己一個人在一起比較好嗯
那凡是這種很高頻的這個
那當然最高頻的是這個字嘛
那
這個可以用來做各種各樣的目的
啊
這個很漂亮的
坐在那裡的同學啊
那這個的意思都不一樣啊
譬如說坐在那裡的同學這個的是who 
是一個介是一個這個是這個who sitting there 
這是這是這個這個的是那個who 的意思哦等等
所以呢你可以想像就是如果說他是高頻到一個程度的話
就當成一個單單獨一個word 當成自己當成一個class 來做等等哦
這個是你可以事實上是可以把word base 跟這個class base 可以混在一起
好那再來我們再講一個問題就是在中文裡面有一個很大的問題就是
中文有字有詞啦
其實我們最早年開始做中文的時候並沒有了解到這一點
因為在英文裡面看到的文獻都是講words 
對不對你的n gram 都是講words 
都是w i 這個i 減二i 減一
都是在看這個東西
那這是什麼這是words 嘛
word 是什麼呢我們從小學的word 就是字嘛
所以我們在八零年代剛開始做的時候一直算的n gram 就是算字的n gram 
就是字的n gram 
電後面腦電腦後面接科腦科後面接技這樣子
一直算n 的字字的n gram 可以算
效果也有一定效果也不錯
但是後來才曉得
其實在words 在中文而言不是字是什麼是詞
那麼換句話說呢你其實應該是算這個n gram 是什麼是詞的關係
而應該不是字的關係
那麼以這邊為例的話你可以想像譬如說不後面接改
這實在這個這個bigram 沒有什麼道理
可是進步後面接改變這個bigram 就很有道理
ok 
你這個做後面接方沒有什麼道理
這些字的bigram 字的trigram 都可以算的都有用的
可是你真的看的話就知道
字之間的relationship 是弱很多
因為不見得很有道理
可是詞中間的relationship 就很清楚
sil
那麼因此呢
really 這個n gram 應該是用詞來做會比用字來做要來得好
但是你如果要用詞來做有一個最大的問題哪裡是詞
sil
雖然我們從小就學造詞造句所以呢我們很知道什麼是詞
可以如果你仔細想一想的話呢我們其實中文的詞是一個not well defined 的東西
舉例來講我這邊舉的例子就是這個意思
就是說電腦是一個詞
科技是一個詞
的也是一個詞
但是科技的當然也是一個詞
電腦科技也是一個詞
電腦科技的也是一個詞
改變是一個詞
了是一個詞
改變了當然也是一個詞
工作是一個詞方式是一個詞工作方式當然也是一個詞
換句話說你給我一個句子
到底怎麼樣來切成哪幾個字是連起來是一個詞的
這是我們所謂的斷詞word segmentation 
sil
斷詞
斷詞本身就不是一個有unique solution 的事情
它的solution 不是unique 的
那麼有不同的斷法
你兩個人來斷就斷得不一樣
sil
那麼因此呢到底什麼是一個詞不能確定
如果是這樣的話你的詞典其實不知道應該放哪些
所以在英文裡面是很容易
英文你只要是有一個有一個空白這就是一個word 
有一個空白那就是一個word 
那麼因此這些word 就該放在詞典裡面很簡單
可是中文不是
中文因為沒有空白
所以你不知道到底應該放哪
一個那當然有人想一些rule 
sil
他說怎麼樣才可以
譬如說的單獨是一個word 
不要跟科技的合在一起
等等
sil
科技的我一定要把它拆成科技跟的
但是呢那你電腦跟科技到底能不能合成一個詞呢這就是這個很多問題
所以呢你並不存在一個真正的lexicon 是大家都公認的
所以呢你的那個詞典到底應該怎麼做你我們一直在講的那個lexicon 本身就是一個問題
因為到底你們該什麼什麼知道
為什麼會這樣的基本原因是因為我們的每一個字這是叫做character 
我們每一個字都是有意思的
不像英文的character 是這個叫做character 
這個character 有沒有意思呢多半不見得
除了少數例外
sil
除了少數譬如說a 這個character 是有意思的
對不對
sil
你英文的話character 沒有意思所以呢word 才有意思所以word 很清楚
中文是因為每一個字自己有意思
所以它自己也有它的rule 
那麼你可以想像中文的構詞是多麼有趣的一個問題
譬如說改變
其實改跟變各有它的意思
兜起來仍然是相當接近的兩個意思
啊
相當接近的一個意思
思想
思跟想都是有意思的字單字
不過它們合起來是一個意思
啊一個有關的
那其實在中文裡面的詞的意思跟它的component character 字的意思之間是有很有趣的關係
像這種是兩種
那還有相反的對不對
這個有有相似的有相反的譬如說歡喜
這兩個是意思相似的構成一個新的詞意思是跟它們都相像的
有相反的譬如說緩急對不對
那緩急到底是緩還是急
啊
這個這個是相反的構成的
那這個也有是類似而
譬如說大學
不是一個很大的學
但是它顯然是跟大有關係也跟學有關係
這是一個新的詞
那也有的詞是跟它的component word 沒有關係的
譬如說和尚
和尚跟和跟尚都沒有關係但是和尚是另外一個詞
對不對
光棍
光棍光有一個意思棍有一個意思這光棍到底跟光跟棍有沒有關係
好像有一點啊
這個所以你會有很複雜的東西所以你如果這樣子來看的話你就知道就是在一個句子裡面的
所以我剛才講就是說我們一開始做就是做字的n gram 
唉它有work 的
字的gram n gram 也work 因為它字本來有意思嘛
但是字的n gram 比起詞的n gram 來哪一個好呢
詞的n gram 好
為什麼因為你可以想像是應該進步接改變嘛
工作接方式嘛
不是做接方嘛
對不對
工接作的話
譬如說字的n gram 為什麼會會很會不錯是因為這種詞的話它有非常強的n gram 譬如說工作
這個bigram 非常強因為這本來是一個詞
方式這個bigram 非常強嘛
所以雖然作後面接方好像不太通
但結果你還是work 
sil
那麼那麼從這樣來看的話呢用
那你還有一點就是這邊講的就是我們中文的這個構詞非常的open 
啊
是幾乎是unlimited 的詞不斷的出現
我們講過各種新詞多得不得了
這個電加腦變成電腦
啊
只有大概只有中文這麼聰明把把computer 叫做電腦
那麼人家只把computer 想成是一個會compute 的machine 
只有
中文是很早就知道computer 不只是會compute 
它就跟腦一樣啊
早在這個這個詞出現的多早
在三十年前就己經叫這個名字了
當時的電腦只會compute 
但是為什麼會知道它是腦
這個這個名詞很了不起的
那麼
我們中文是很容易把長詞自動縮短成為短詞
台灣大學就叫台大
有人定義過嗎沒有
國家科學委員會叫做國科會有人定義過嗎沒有
啊
我我們自動就可以把它因為這是因為每一個字都有意思的關係
所以你就可以自動把它選擇最重要的字
意思最代表這個詞的字就把它拿來就兜起來了
你譬如說中統
有人定義過什麼是中統嗎沒有
可是你講中統就知道是什麼
sil
因為它己經把因為那個字本身有意思
它就把這些字有有它代表意思的最重要的那兩個字兜起來這就變成一個新詞
這是中文的特性
人可以隨便把人的名字拆開來
把他這個拆開來之後把它的title 塞到中間進去
啊
然後
這個除了短詞加起來變成長詞之外長哦這個是長詞可以縮短成為短詞
短詞可以加起來變成長詞
然後可以短詞加起來長詞之後再縮短譬如說北二高
是北部第二高速公路
很多北部第二然後高速公路
兜起來之後我再把它縮短就變成北二高
所以你很多這種你結構都不需要去define 而自動就會出現因為每個字都是有意思的
所以中文的詞其實是unlimited 
你要做一個詞典是很難的
那麼因此呢我們就會發生一大堆的所謂的這個oov 的問題
在語音辨識裡面任何一國的語言都有所謂的oov 的問題
這是所謂的o out of vocabulary 的問題
什麼是oov 
就是我們在一開始畫的那張地圖裡面需要一個詞典
sil
sil
我們在一點零裡面的那張最基本的圖
這張圖
在這張圖裡面的
就是要有一個lexicon 
但是呢你永遠lexicon 永遠裝沒有辦法裝滿所有的詞
你一定會漏掉一些詞
嗯任何語言都一樣
那麼你如果那個詞不在你的詞典裡面你永遠不會辨識對
對不對
只要那個詞不在你的lexicon 裡面你就不會對了
而如果這樣子的話你可能前後都會錯因為你有n gram 
所以你如果那個詞不在詞典裡面的話
那個詞鐵定不會對
這個word 如果不在lexicon 裡面這個word 鐵定不會對
那它不對的時候可能前後都會錯
因為是你靠n gram 做的
你就會錯
那這個問題就是所謂的oov 的問題就是out of vocabulary 
就是你任何語言你都沒辦法知道exactly 多少詞才是那些詞是你是會用的因為你永遠會說到一些沒有的詞
那這就是所謂的oov problem 
ok 那我們回到剛才的
這是這個我這個這是所謂的out out of vocabulary problem 
只是說呢我們通常這個的算法是所說的oov rate
就是你一般的你的一個一篇文章裡面
你你會out of vocabulary 的詞的比例是多少
在英文啊什麼oov rate 是低的
也是會有oov 但是比較低
我們隨便舉個例來講譬如說你會說ibm 
看你那個辭典有沒有把它放進去
如果沒有沒有的話這就是oov 
等等
那最常出現的oov 就是這種
專有名詞嘛專有名詞很可能不在裡面
那麼但是你凡是碰到專有名詞不在裡面你那邊就會錯掉啊
那麼在中文而言oov rate 特別高
中文的這個oov rate 會特別高就是因為我們不斷的產生各種各樣的新詞
那麼因此呢
我們整個的整個的這個recognition 的方法是word based 
在因為是在西方的語言英文為基礎的西方語言所發展出來的技術就我們剛才看的那張圖
一點零的那張圖
那張圖是以可以說是是word based approach 
因為你的lexicon 就是用詞來做的
然後你的language model 就是n gram 
什麼是n gram 
是詞來做的n gram 
所以你都是以詞為基礎來做的但是你現在如果詞搞不清楚的話
你整個都比較會有問題
這是中文裡面最常碰到的一個狀況
那麼這個時候呢
嗯
那還有一個問題就是你要會斷詞
你並不是隨便上網拿一大堆database 下來我就可以train language model 了
那你那樣能夠train 的就是字的language model 
字的n gram 你還可以train 
詞的n gram 你還不能train 你得要先會斷詞
那你的斷詞就是你要有一個程式去做斷詞的程式
那斷詞程式怎麼做
其實是不難
但是要做到好也是不容易的
你可以猜這個斷詞程式怎麼做
就是我有我有一個詞典
然後根據詞典做matching 
這是最基本的原則
我有一個詞典告訴我這些詞兩字詞三字詞單字詞
我然後來做matching 
我看到第一個詞的時候呢先去看所有的單字詞裡面有沒有它
如果它是一個單字詞的話那它可以是一個單字詞
可是呢我走到第二個詞去看的時候呢它是不是一個雙字詞
唉如果它有雙字詞的話呢長詞優先
如果這兩個是一個雙字詞的話呢我先考慮這可能是一個雙字詞
那就不要考慮它是單字詞了
那我再找到三字詞發現唉發現唉這個也是一個三字詞那三字詞又優先了
那我就考慮這個
這還有沒有四字詞沒有了
我從這裡再開始重新再來斷
發現了唉這邊這有一個雙字詞
再發現這邊有等等啊
你這第一個原則就是去做matching 
然後長詞優先
然後你可以從從左邊切左邊match 過來
你可以從右邊match 過來
你可以兩邊來走
然後另外一個就是你可以用詞的n gram 
sil
那到底是
你有的時候這樣子斷也可以這樣子斷也可以
那誰比較對
用n gram 來算
等等等等
這類的方法都可以做
這是所謂的這個斷詞的問題
那當然有的時候有ambiguity 當然有ambiguity 
譬如說不知道理由
是知道跟理由各是一個詞呢
還是道理這個不知道理跟由
那憑什麼n gram 是一種solution 告訴我說知道理由
而不是道理跟由
啊等等
那那這這些就是那那這個就是所謂斷詞的問題那你第一個呢你要有一個自動斷詞的程式
這樣子我上網去抓一大堆training data 來之後
我斷了詞才能夠train 我的language model 如果是以詞來做的話
等等
那這是中文的一堆問題
在這個問題之下我們就可以講了
就是說那我可以用字來train n gram 
我也可以用詞來train n gram 
到底那個比較好
它們之間的關係是什麼
這邊是講用字所train 的n gram 
這個上面是講用詞所train 的n gram 
那基本上來講呢
這個
用字來train 的最大的好處就是你就避免了一個斷詞的問題了
詞的結構你也不管了啦
就是用把字去數字就好了嘛所以呢它這個這是最大的好處你沒有斷詞的問題
也沒有oov 的問題沒有oov 的問題
你反正就去把每一個字都都當成是一個單字
它就自動都有所有的
這都是一個一個word 就對了
你這樣就去算就是了
這是最大的好處
但是它有最大的壞處就是它顯然是比較weak 
因為沒有word level information 
就像我們剛才講的
你就只能把它看成是
不後面會接改
雖然沒什麼道理但是你就把它看成是這樣子
哦
這個腦後面接科你還以為是那個醫學院的腦科
等等
那麼做後面接方那不曉得為什麼不過反正有
你就是就是因為沒有word level 的沒有詞的level 的知識你只有字level 那你就用字level 去算
是可以但是你缺少一點東西
那所以它會比較weak 
那還有一個很大的問題就是說
你可能需要higher order 到相當程度才能夠代表
為什麼因為譬如說你你這個trigram 
是這兩個word 後面接這個word 
但是這是三字詞這是兩字詞這是三字詞
所以你如果要handle 這個relation 的話你要幾個一二三四五六七八要八gram 
你如果用字來做的話要八gram 才能夠做到這個relation 
啊
你如果只是bigram trigram 是不太夠的
你哦你這個relation 是用字來講是應該要用八gram 
那八gram 當然很難做了
所以你需要higher order n gram 
才能夠做到像詞這樣子的關係這trigram 
可是你要這麼n 如果這麼大的話是不容易做的
所以這個是用字來做的缺點
那用詞來做的話當然好處就是說詞就是真正的building block of sentence 
我們造個句子其實是用詞造的嘛啊
就像我們剛才講的
這個句子顯然你不是用一個字一個字去拼的你是用一個詞一個詞去拼的
不管你心目中的詞的哪一個
那你是用詞造出來的
所以呢你你你造個句字的時候事實上是是真正是用word 用詞來做所以是是比較好的
而且你可以加更多的information 
就以我們之前的所謂加更多的information 就以我們之前的例子來講
你就是可以把像它的詞類啦
什麼嗯
這些詞類的知識可以放進來這就是用詞做的好處用字就沒有了嘛
然後呢但是呢關鍵是說你有沒有一個好的詞典
你如果這個lexicon 如果不存在就沒有辦法了
那但是呢我們不太容易有一個很好的詞典
因為你不管做得多好的詞典總會漏掉一大堆詞
我們的這個oov rate 是非常高的
那麼你你不管你我的詞我們剛才講六萬詞你即使變成十六萬詞還是很多詞都沒有
二十六萬詞還是很多詞都沒有
你這是不太容易做
那當然還有一個問題就是你怎麼做斷詞
那麼我們剛才講一堆做斷詞的方法不表示這個就就很work 
因為斷詞永遠有有一堆錯誤嘛
所以
那通常我們的做法就是說
你斷詞不可能期待百分之一百正確
但是就是用一個軟體來做斷詞的好處就是讓它consistent 
如果它會把誰斷開來就它每一次都斷開就對了
譬如說科技的它一定會把的斷開來的話
你就的就會斷開來
你不能說有的時候的跟前面連在一起有的時候不連那這個就麻煩了
你就是要做consistent 的斷
不管它斷得對不對要consistent 
用一個程式來做有一個好處就是它會做得consistent 的話呢有它還有一定的效果就是了
所以會有嚴重的oov 的問題
sil
那到底怎麼做比較好那麼
有一段時間大家都不知道怎麼做比較好
不過後來我們想了一個非常好的辦法turns out 
這是一個簡單而有效的solution 
那麼這個solution 用了十多年前我們就已經知道
那麼一直到今天仍然是一個非常好的solution 
甚麼solution 呢就是我們講的你用詞來做
但是你把所有的字都當成單字詞也放進去
什麼意思
我有一個詞
六萬詞典六萬個
ok 
我就把這六萬個詞的詞典
那我還有一大堆的oov 怎麼辦
我就把所有的字
我有八千個常用字當成單字詞放進去
於是我就有六萬八千個
我這裡面所有的詞都有
常所有的常用詞都有
所有的單字詞都有
那這樣之後呢
我somehow 可以handle oov 的問題
為什麼
譬如說只要是一個常用的oov 
我可以解決
為什麼呢我們舉例來講譬如說呂秀蓮
這是一個oov 假設我沒有把她放在詞典裡面的話她是一個oov 
但是呢我現在假設我從網路裡面download 一大堆data 來開始要train language model 的時候
我就用這個詞典來斷詞
我用這個詞典來斷詞的時候呢這個詞典裡面沒有呂秀蓮
但是這三個字都是單字詞
所以我就會把它斷成三個單字詞
這三個單字詞
如果這個這三個單字詞連在一起的這個呂秀蓮這個這個詞在這個網路上面出現得夠多次的話
它的bigram trigram 
train 得夠強的話
它們自己就有就強了嘛
所以到時候我現在聽到一個呂
一個秀
一個蓮的時候
唉怎麼辦
我現在沒有一個雙字詞是呂秀沒有一個秀蓮的話嗯
那結果我就可能會把他變成這個是呂這是秀這是蓮
因為它們的trigram 
它們bigram trigram 夠強結果出來就是呂秀蓮
這個在在多少年以前的當時這個做出來的時候人家很不相信
你說這個字沒有在詞典裡面
可是為什麼我說這三個這三個音的時候它會知道呢
不可能嘛
是可能的就是這樣做的啊
這是這個
只是說這個只能只只限於就是在database 裡面頻率夠高的高頻詞的話如果它夠高頻
即使它是oov 也會對
啊那就是因為這些單字詞是都變成一個單這些字都變成單所有的字都變成單字詞放在詞典裡面
所以我斷詞的時候就把它們當成一堆非常高頻的trigram 跟bigram 的單字詞
所以呢我就辨識都會正確
但是呢這個限於高頻你如果頻率不夠高的話
這個n gram 不夠強是出不來的就是了
這是講這句話講的意思就是就是這一招
那當然你也可以想說我是可以把這個我是可以把這個嗯同時做詞的做字的跟做class 的
真都都全部可以整合在一起也是做得到的啦啊
那會比較好了不不過有夠複雜就是了
sil
ok 那再來呢這還有另外一招就是說
如果說詞這麼難弄的話
我們也有一個辦法
是根本就不要去管什麼是詞了
我們就把凡是經常會出現在一起的pattern 當成一個詞
嘛這個我們稱之為second pattern 
什麼是second pattern 呢就是幾個字經常出現在一起
不管它是不是詞我也不管詞的bound 詞的boundary 在哪裡我都不管
只要經常出現在一起的
我就當它是一個pattern 
我就把這個pattern 放在lexicon 裡面
我這個lexicon 不要想成是詞典
就是把這堆pattern 放在一起這些東西就叫做lexicon 
什麼叫做經常出現在一起的字把它兜在就把它放在lexicon 裡面呢
我們底下有一些例子你一看就了解了
就是經常出現一起的pattern 
我們直接從網路新聞裡面把它抽出來
那它其實不是在詞典裡面的詞的
就兩字的pattern 而言你看這種兩字你就知道
啊
有些是oov 像這個oov 啦
這個只是一個oov 而已
但是你因為在那那段時間它新聞出現很多的話
它其實是可以當你你可以抓得出來
那多數是兩個單字詞
但是呢這兩個單字詞談常常會連在一起
那你就乾脆的連在一起了啊
那三字的pattern 呢常常是一個兩字詞跟一個單字詞
它們經常連在一起就會變成一個三字的pattern 
當然oov 也是會這樣子可以抽得出來
那雙四字的話呢常常是兩個四字的pattern 常常是兩個雙字詞
譬如說兩個雙字詞
那它們這兩個雙字詞經常連在一起就乾脆變成一個四字詞算了嘛
sil
那這個觀念就是說我不不去管什麼是詞
我只是管我用我的database 去train 
說哪些經常黏在一起的就給它黏在一起
黏在一起的時候呢就變成一個所謂的segment pattern 
然後呢我就把這些pattern 存起來當成是lexicon 
那這個好處呢你馬上想得到就是我就不管這個詞的到底哪裡是詞
而我可以完全就是你只要給我夠多的database 
我就上網去抓我要的東西之後我就自己抽
那我就建一個我的詞典就對了
我根本不管那些那
怎麼抽這個pattern 這也有很多方法
不過我們用的最成最成功的簡單的方法就是我們之前講的那種
就是minimize perplexity 
就是剛才講的跟剛才講的這個很像啊
這個
跟這個reference 講得很像就用它的方法
那所不同的是我現在是要連起來的
我現在是要連起來的怎樣呢就是說
剛才那個是說你要你任意兩兩的詞你都要去算它們能不能連
我現在不用
我現在只是相連的才要看它們能不能連嘛
所以譬如說我的網路上抓到的文章是這樣子的話
你一開始
每一個字自己是一個pattern 
然後第二步開始哪兩個字可以連成一個pattern 呢
你去看這個能不能
這個能不能
對不對
這個能不能這個你你你這相鄰兩個去看它們能不能連起來
看這個能不能
那你怎麼看呢也是一樣
就是這個啊minimizing perplexity 
就是我現在如果它們連成一個詞的話
連成一個pattern 的話
我的perplexity 有沒有降低
降低多少
它們兩個連起來的話呢我的perplexity 有沒有有沒有降低降低多少
我就用這個來算
那當然你不必去考慮說
這
個跟這個會不會連
它們現在不在一起你就不用管了嘛
所以跟剛才那個不一樣的地方是在這裡
我現在是有一個語料在這裡所以我就我不必管它跟它能不能黏這種問題不用考慮
那你只要看它跟旁邊能不能黏
如果它跟它己經黏起來的話
那下一步是這個東西跟左邊能不能黏
對不對
如果這個已經黏起來的話呢那再來要考慮的是它左邊要不要黏
右邊能不能黏
就這樣子
那你用這個方法來你都每一次都算
我的perplexity 是有增加還是減少
增加多少
用這個方法你可以得到一個minimum perplexity 的一個新的lexicon 
那它不是我們平常所習慣的詞
但是裡面有很多很多我們的詞都自動抓得出來
那那譬如說專有名詞自動都會抓得出來
然後很多這種pattern 都自動可以抓出來你就得到一個新的詞典
你那個詞典就是我們所謂的所謂的segment pattern 的lexicon 
那這個效果也不錯
那麼底下這一頁是在舉一個例子就是說
有了這個跟你沒有是有何不同
這是一篇新聞
那麼譬如說開車
這是一個pattern 
但是其實應該開是一個單字詞車是一個單字詞
你這句話要辨識對的話
你如果那你必須是要有夠強的bigram trigram 
才能夠把這個開跟這個車跟這個抓得出來
可是如果你現在開車是一個pattern 的話呢你只要有一個開的音有一個車的音的話
你自動就會把這個詞這個這個這個pattern 抓到
然後呢你那你就看它跟些的bigram 夠強trigram 夠強它就出來了嘛
所以基本上你如果是他們構成一個pattern 的話
是比沒有pattern 會辨識正確的機會是提高
那我們如果隨便拿網路上的新聞來比的話
大概有百分之二十八的是詞典裡面沒有的pattern 
啊就是這種這種有百分之二十八可以抓出來是詞典裡面沒有的pattern 
那這個是有助於辨識的
sil
那這個是用這個second pattern 來做的情形就是了
ok 好關於中文的我們說到這裡
