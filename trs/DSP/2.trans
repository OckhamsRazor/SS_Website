我們今天是二點零喀
那麼二點零要講的事情其實是
呃我們一點零裡面講過一張圖阿
我已經他們有個 mouse 所以我剛才已經
如果記得的話我們上週有一張圖在講這個大字彙的語音辨識
那張圖呃裡面有好多塊東西
那我們其實今天是把那張圖裡面的幾塊重要東西簡單的說一下他的基本的原理
那呃第一個要講的是 hidden  markov  model
那麼之後我們底下要講的是
這個怎麼樣前面求 feature
然後我們會講後面的 language  model 等等
那麼我們主要講這三塊
那麼這三塊是我們在一點零上週有過那麼一張圖裡面的三個重要的部份
那這三塊我們今天講的也只是個非常簡單的簡介
這個讓各位有一點初步的 feeling 那個是在幹什麼的
那個那麼等到後面我們還有陸續好幾周都要繼續在講這三塊
比如說我們後面的四點零五點零還是在講這個 hidden  markov  model
然後六點零會講 language  model
七點零會講那個 feature
那都是在這裡
然後八點零在講怎麼作 search
所以我們一直到八點零講完都還在講這堆東西
那今天這裡只是一些最基本的觀念
我們把一些基本東西先說一次這樣你比較容易有概念他們是什麼
那第一個我們先說得是 hidden  markov  model
那這個呢應該可以算是今天所有的作語音的研究裡最主要的最常用的 model
也是最成功的 model
雖然他不是唯一的
那也有人用其他的
不過這個到目前為止仍然是最成功的用的最普遍的
所以呢我們來說一下
那麼 hidden  markov  model 是什麼呢
你基本上可以想像成是一個我們把每一個想要辨識的聲音都建一個 model
舉例來講呢假設我們用最簡單的例子來講
我現在要辨識零到九的十個聲音
那我就為每一個聲音建一個 model
這個 model 就長得像這個樣子
那麼於是我就會零有一個 model 一有一個 model 二有一個 model 一直到到九
總共呢就有十個 model
那麼每一個 model 是怎麼樣的呢
有這麼多個 state  n 個 state
那麼舉例來說假設零
這是一個為零所建的 model 的話
那他的第一個 state 第二個 state 分別代表我發那個零的時候的聲音訊號變化的狀態
舉例來講呢
那這第一個 state
很可能是我在發這個零的時候那個了
那個了我這個還沒有發出來的那個時候那個了那個時候那個就是可能是第一個 model
第一個 state 所描述的
第二個 state 可能是了發出來了
不過那個了可能後面帶著了一
那個了一的時候可能是第二個 state 等等
你到後面第三第四個 state 可能是一
零裡面那個了一一的音
然後等等等等到最後一個音可能是
最後一個 state 可能是零的那個登登的音等等
那這樣的話那我這個聲音一路慢慢跳過去呢
我就描述了這個零這個聲音的情形
基本上是有時間順序的
從一二三這樣一路下去的話
我描述那一個聲音的情形
那除了這樣子之外呢
那麼我們也可以通常把語音的訊號看成一系列的 vector
那這一點呢我們在上週可能有畫過
我有一點忘了
不過 anyway 我們重來一次
假設說我的聲音是假設說我的零是這樣子的
那麼如果是這樣的話呢
我的取第一個 frame 譬如說是裡面的兩百五十六點
或者是五百一十二點
那你知道我裡面每一個都是一堆 sample嘛齁
都是一堆 sample嘛
那等等假設說我的第一個 frame 是兩百五十六點
我把他呢經過某一個演算法之後
把他變成一系列的參數
這堆參數就是我們所謂的這個 feature  parameters
那麼他就構成一個 vector 我們叫做 o  one
那就是這邊的這個 o  one
就是我的第一個 feature  vector
那麼這種東西呢我們稱之為 feature  vector
因為這裡面的每一個呢就是我們所稱的 feature  parameter
也就是說他是描述這堆聲音裡面的一些特徵
那麼待會我把這個 window 向右 shift 過來之後
我可以產生我可以算出第二個來
他也是這堆那麼這個呢我們叫做 o  two
待會兒我再 shift 一個過來我就算出一個來
那這個呢就是 o 三等等等等
那麼換句話說我們上週提過
這個訊號這裡面千變萬化
你每一個聲音永遠不一樣
但是呢這個通常你在訊號裡看不出什麼東西出來
但是你如果把他求想辦法從裡面求出一些特別的特徵參數來描述這個聲音就會比較清楚
這就是我們所謂 feature  parameter 跟 feature  vector
那我們把這些 parameter 兜成一個 vector 就是所謂的 feature  vector
那麼於是呢現在我可以把一系列的訊號變更成一系列的 vector
那這些 factor 我稱為 o  one  o  two 等等等等
我叫做 observation  sequence
因為這是真正我可以 observe 的我可以看得到的我收的到的我量的到的這是我的 observation  sequence
那麼每一個呢我叫的 o  one  o  two 呢我就通稱為 o  t
這個小 t 呢就等於我的 time  index
不過這個 t 是整數
這個 t 是整數就是一二三四這樣就是第 t 個 factor 的意思
那麼那麼每一個 t 的 ot 裡面呢
就是這些東西呢我總共有幾個參數呢
X 一 X 二到 X 大 D
這個大 D 是我們這個 feature 的 dimension
或者說就是我的參數的總數
那我們後面就會說我們這個大 D 其實滿大的
我們常用的大 D 大約是三十九的 order喔
不是一定要三十九啦大概是這樣的數字
三十多個到五十多個差不多這樣的數字才夠
你大概會有那麼多個
那麼所以這個大 D 是一個滿大的數字
這樣這些 x 就是我們所謂的 feature  parameter
那這樣的 o  t 這些 o 呢就是我的 feature  vector
就構成我們所謂的 observation  sequence
那如果你是這樣看的話呢
那麼現在我假設這個是零好了
這是零的 model
這是零的聲音
那如果是這樣的話呢
那我事實上是會有
舉例來講也許前面這前面這三個是屬於第一個 state
我們剛剛說的第一個 state 可能是在描述那個了那個還沒有出來的聲音
如果是那樣的話也許是前面三個 state 前面三個的 vector
到第四五六七的時候很可能是代表第二個 state
這個時候是了一出來了
那麼如果是這樣來看的話你可以把這個 observation  sequence拆成 一段一段一段
那這一段我們可以想像他好比是描述這個 state 這一段好比是這個 state 這一段好比是這個 state 等等
那為了要區別這件事我們就 define 第二個 sequence 叫做 state  sequence
這些 q  one  q  two 就是分別對應到 o  one  o  two
那他是什麼呢他就是一到 n 的 state  number
那換句話說如果我們說一二三這三個是屬於 state  one 的話
表示說我就讓 q  one  q  two  q 三都等於一
q  one  q  two  q 三都等於一的話表示說他是第一個 state
如果 q 四這個四到七是在第二個 state 的話呢
我就是 q 四 q 五 q 六 q 七等於二就表示他們是在第二個 state 等等
因此呢我這個 q 的這個 sequence 其實只是一一一二二二二三三等等等等
因為他們都是一到 n 的整數
分別 indicate 我的第幾個 observation 是掉在第幾個 state 裡面的意思
所以呢這是我的 q 的這個 state  sequence
那這個時候有一個重要的東西在這裡
就是我們有這個 state 會跳的這個 a 一二 a 二二這些東西
那這些東西呢我們稱之為 state  transition  PROBABILITY
那這裡的 a  i  j 的意思是說我在 t 減一的時候是在 state  i 那
t 的時候是在 j 那裡的跳的機率
ok 換句話說呢就是我在前一個瞬間
這個 t 就是這個 t 嘛就是這個 t 嘛就是說我的前一瞬間
前一個 t 還在 state  i 但是下一個 t 會到 state  j 的機率叫做 a  i  j
所以呢 a 一一就是跳回原來的 a 一二就是從一跳到二的 a 一三就是從一跳到三的等等
這是 a 二二 a 二三 a 二四等等
喀喀那麼這個究竟是什麼意思我們舉個簡單的例子來看你就可以想像他是在說什麼
譬如說呢現在如果這個是這個 state 這是下一個 state
我們假設說這裡只有兩個
如果這個是零點一而這個是零點九的話
那表示什麼意思呢絕大多數的狀況他仍然會下一個仍然是原來的
只有零點百分之十的機會他下一個是跳到下一個去了
對不對所以呢你可以想像假設這是一這是二
q  one 呢可能是等於一 q  two 可能還是等於一 q 三可能還是一
因為我一開始是這個的話呢有百分之九十的機會下一個仍然會是他
對不對然後呢還是有百分之九十的機會下來還是他
還是有百分之九十的機會下來還是他所以呢他可能一直會這樣在這邊好多個
你可以想像可能到了八九個以後他可能會跳到下一個來
因為我十分之一的機會會到下一個去嘛
所以大概八九個才會到這邊來對不對
因此呢這個時候你可以想像我的 state  one 可能有好多個
要夠多個之後他才會跳過來
那反過來呢我現在如果這個是零點一這個是零點九的話
表示什麼意思呢
表示 q  one 是一的時候 q  q  two 很可能就跳到二了對不對因為我有百分之九十的機會會過來嗎
所以 q  two 很可能就匯二了是不是
那麼我會繼續留在這裡的機會很小呀只有十分之一的機會呀
所以他就不斷的往這邊一進來就會跳出去嘛對不對
那麼因此呢這些個機率就告訴我說
這個訊號會停留在這個 state 裡面長短
以我們剛才的例子如果是這個是零點一這個是零點九的話
在這邊就會比較長要夠長之後他才會有機率跳過來嘛對不對
那麼如果現在現在是零點九這個是零點一的話就表示這裡面很短一下子就可以跳出來嘛 ok
那麼因此呢我們事實上是在描述一個現象
因為在在我們的聲音裡面我們永遠不知道我們隨便講一個講一個字的時候我們永遠不知道我們到底會在哪一個 state 到底會多長
這是一個 random 的情形
我們舉個簡單的例子譬如說零有的人可以說是零有的人說是這個零
那麼你的每一個這裡面每一個 state 到底有多長是不一定的是可長可短的
我們舉更複雜的例子譬如說一個英文字 San  FRANCISCO
San  FRANCISCO 這裡有好多好多的音
你是 San  FRANCISCO 還是 San  FRANCISCO 還是怎樣你的每一個音是可長可短的
換句話說在我們講話的時候的任何一個你講的任何一個
如果用一個 model 來描述他的話到底哪一段到底哪一個要多長齁
我們剛才講譬如說這是屬於第一個 state 這屬於第二個 state 這屬於第三個第四個第五個第六個 state 的時候
到底每一個 state 的時間是多長多短實在是很難說的他是非常 random 的
同一個人講兩次的話他的長短都不一樣的
所以我們只能用一個這樣的 model 來描述他的一個時間上的長短伸縮的 random 的特性
那麼我們剛才講過這個這個意思就是
這個大小的話就可以看得出來他在這裡會長還是會短
那同理呢我有的時候可以跳像這個地方我可以從 a  one 會跳到 s
從 state  one 會直接跳到三的意思是有些音我其實可能會根本就沒有念就滑過去了
那麼一個簡單例子譬如說零
你念零的時後裡面到底有沒有那個一的音不一定有的時候那個一根本沒有發他就是跳掉了啊
那麼那麼因此呢你有的時候是可以跳的
所以呢那這些呢就是我們這一堆 a  i  j 這些個 state  transition  PROBABILITY 的角色的功能
那 in  general 這應該是 define  for 所有的 i 跟 j
因為任何一個 state 可以跳到另一個 state 去
所以呢他是一個 a  i  j 的 matrix
那這個 matrix  in  general 應該是 n  by  n 的
我的總共有 n  by  n 個 element
但是在大多數狀況我們都把他簡化到只有這樣子就是他跳回自己跟下一個跟下兩個
再多我們就不弄了因為太複雜了
那你可以想像如果是只有這三個的話我的這個 matrix 會簡化到只有每一行只有三個嘛
a 一一 a 一二 a 一三只有這三個別的都是零
然後呢這是 a 二一 a 二二
呃沒有二一應該是呃 a 二二 a 二三 a 二四然後呢是零等等
所以呢我基本上只有這一排讓他不是零其他的呢都是零
那如果是這樣的話我們的 model 稍微簡化一點
我們通常會作這樣的假設以免太過複雜
那事實上你也可以更 general 的情形應該是讓他每一個 a  i  j 都可以存在
如果每一個 a  i  j 都可以存在的話他甚至可以從後面跳到前面來
對不對你四可以跳到二來那這樣的話太複雜了我們通常簡化到只讓他只讓他那樣就好了
好那這是講這些 state 的情形
那再來呢你如果是知道這些聲音 o 一 o 二 o 三是在這個 state 一裡面呢
他會長怎樣還是不一定的
即使這個是發了的那個音的話
我都知道是那個音了並不表示他的 o 會長的怎樣他會長任何一個樣子
這個是語音特別難的地方就是即使是同樣的聲音同樣的人發兩次訊號都是絕對不一樣的
我們舉例來講同樣的一個人一個人發啊你把你發啊發三次
你用你用這個這個電腦收進去把 wave  form 畫出來看看那三個啊鐵定不一樣
絕對不會三個一樣的阿
你就是同樣的人發同樣的音三次鐵定是不一樣的阿
那麼同音詞你算出來的這個東西鐵定是不一樣的
所以即使你知道他們都是這個 state 並不表示他們會長怎樣他會長任何一個樣子
所以怎麼辦我們只能用一個機率的 distribution 來描述他這就是 b  one 的 o 的意思
就是說你如果是在 state  one 的話這個 o 會長怎樣呢我們不知道
我們只知道他會有一個某一種 distribution 這是 b  one
你如果說這些東西是在 state  two 裡面的話呢他會長怎樣我們不知道
我們只知道他會有一種 distribution 那這個音跟這個音有何不同呢
這兩個 distribution 不一樣就是了
但是我並不能夠知道說這個 o 一定是怎樣跟這個 o 一定怎樣這個我們很難講
只是我只能說他們是這個 distribution 他們是這個 distribution 而這兩個不一樣
ok 那這些 b  one  b  two  b 三就是當我這些 o 在 state  one  two  three 的時候他們的 distribution
那我們叫做 b  j 的 o 那我們叫做 observation 的 probability
那也就是這個 b  j 的 o 你看這就知道了
這個 j 就是這個 state 一到 n 嘛這個 j 就是這個 state 的 index
然後他有一個 distribution
那麼這個會是怎樣呢
你可以想像他的複雜性就是他
配這任何一個 distribution 去找任何一個樣子
那麼我們簡直無法去描述了
那我們的辦法就是說我們無法描述我只知道他是這個 distribution 我就說他是 gaussian
我就說他是 gaussian
那你說怎麼可能是 gaussian 呢因為我可以是任何長相呀
沒有錯你可以是任何長相因為他可以是任何長相
我們不見得能夠說他是一個 gaussian
這是一個 gaussian 對不對
如果是一個 gaussian 他的長相就是這個樣子啦那不見得是這樣啦
那怎麼辦呢我們就說他是一把 gaussian 齁
所以呢我們這邊是是一堆 gaussian 的組合
換句話說呢我如果是亂七八糟一堆東西的話呢我永遠可以說他是一堆 gaussian
譬如說這是一個 gaussian 這是一個 gaussian 這是一個 gaussian 這是一個 gaussian
我永遠可以說他是一堆 gaussian 的組合
當我的 gaussian 夠多的時候他大概可以描述他的 distribution
ok 因此呢我們從這個觀念來講不論他長得多麼複雜
如果給我一把夠多數目的 Gaussian 的話那麼他大概可以描述他這個樣子
那就是我們這邊的意思
所以呢是譬如說我不是一個 Gaussian
而是大 M 個大 M 這個大 M 是這個小 k 從一到 M 小 k 就是 Gaussian 的 index
所以呢總共有 k 個 Gaussian 總共有 k 個 Gaussian
然後呢把他們分別 waited  by 這個 c 的喔然後呢加起來才是我這個 distribution
這樣的話呢我就能描述一個任何一個 distribution 的長相我都可以用這個方式來描述了
那麼因此呢我的 b  j  k 的第一個 index  j 是這個 j 就是這個 j 就是我的 state  index 是指我第 j 個 state 的 distribution
那第二個 index  k 呢就代表他是第 k 個 Gaussian 我總共有大 M 個
那麼通常呢這裡的每一個 Gaussian 我們通常叫另一個名字叫做 mixture
所以第 k 個的 mixture 其實就是第 k 個 gaussian 的意思
我就是把 k 個 Gaussian 混在一起啦所以變成 k 個所以呢總共有這是第 k 個 Gaussian  mixture 第 k 個 mixture
然後第 j 個 state
然後呢如果是這樣的話我必須這個這個 weight 這個 c 的 j  k 是 b 的 j  k 的 weight
這些 weight 加起來要等於一
換句話說這個 distribution 我積分還是要等於一嘛
我本來一個 Gaussian 積分是一
我這邊如果是十個 Gaussian 積分不是等於是十了嗎
所以顯然不行我一定要我一定要全部積分仍然是一呀
所以顯然譬如說這個 Gaussian 的積分是零點一這個 Gaussian 我乘上 weight 是零點一
這個乘上 weight 是零點零五那個是零點二等等這樣加起來我積分才會是一嘛
所以呢我就必須要有所有的 weight 要 summation 等於一的這個條件
那這樣的話呢我的這個積分起來才會仍然是一嘛
好那麼我們這樣畫好像很簡單其實不對因為我這個只是一個 one  D 的 DISTRIBUTION
我現在不是我現在這個 o 是什麼 o 是一個很多 D 我們說三十九維的 vector
那這個的 Gaussian 是怎樣呢
這個有點複雜那麼我們用一個呃簡單的式子來寫也不簡單
那我現在有一把我這邊有一把 gaussian 大 M 個那這一把 gaussian 呢就東一個西一個那麼散成一團
那麼他們加在一起的話那這個呢那這就是我的譬如說 b  one 的 o
就是一個長成這樣的東西 ok
所以呢假設我的 b  one 的 o
我的在 state  one 的話是一個這樣子的東西
那在 state  two 的時候他就是不一樣就對了
state  two 的時候我可能是另外一堆譬如說這個 gaussian 是長這樣的這個 gaussian 長這樣的
那你可以想像這是一堆 gaussian 這是一個 gaussian 這是一個 gaussian
那麼可能還有這也是一個這也是一個啊等等
那這個呢就是 b  two  o
那換句話說呢
我們剛才講當你是在 state  one 跟 state  two 的時候我 really 並不知道這堆 o 跟這堆 o 會長怎樣
在這個 state 裡面這個 o 是任何一個可以長任何樣子
我只知道他有一個 distribution
那那個 distribution 是這樣的
同樣呢我如果知道這個
這堆東西在第二個 state 裡面的話呢那他是怎樣我仍然不知道他是可以任何一個長相
不過呢我知道他的 distribution 是這樣的
那現在第一個 state 跟第二個 state 有何不同呢
只是說這兩個 distribution 不一樣而已
ok 因此呢是有一個問題譬如說某一個 vector  o 四
到底是在一裡面還是二裡面我們其實不知道
因為我只有 observe 到這裡而已這是我的 observation
我只有 observe 到這個他並沒有告訴我誰在哪一個 state 裡面
那譬如說 o 四真的是在第二個裡面不在第一個裡面嗎我們不知道
我們只知道他的機率不一樣
o 四是某一個 vector
譬如說這裡的時候是掉在這個位置
在這裡的話是掉在這個位置
嗯你會發現如果是掉在這個位置的話呢他接近這個 gaussian 的 mean
所以他機率是滿大的
這個的話呢是在他的邊緣他機率是很小的
我只知道這個區別而已 ok
所以呢對這裡面任何一個 observation  o 是一個 vector 而言
他掉到任何一個地方去
他在哪一個 state 裡面其實是我們是不知道的
那麼我只知道我可以算機率那麼他在這裡的位置在這裡
他比較接近這個橢圓的 mean 所以他是機率是比較大的
我如果掉在這裡的話他只在某一個橢圓的邊緣它機率是比較小的我只知道這樣子而已
ok 那我就是用這堆機率來描述每個 state 不一樣
所以呢那你就可以想像我的真實狀況變成一群這樣子的 state
譬如說我是這是第一個 state 這是第二個 state 這是第三個 state 等等
這是什麼音這是什麼音這樣一路滑過來就是某一個聲音
那所不同的就是說他們的第一個 state 呢有他的一個 distribution
那麼第二個 state 呢有他的 distribution
是有不同的
第三個 state 也有他的 distribution
他們就是不一樣就是了
那這個就是所謂 b  one 的 o 這是所謂 b  two 的 o 這是 b 三的 o
ok 那這堆不同的 o 呢 b  one  b  b  j 的 o 呢就描述了這個 state 長他們會怎樣
ok 那就是我們底下這段所說的事情
那麼於是呢我現在就把所有這些 b  j 的 o 我用一個大 B 來代表
也就是說呢這些東西呢
我叫做大 B 叫做大 B
那我用大 B 也就是這一堆 distribution 來描述每一個 state 的聲音會長的怎樣
那然後呢大 A 是什麼呢大 A 是我們的剛才的 state  transition  PROBABILITY
這些 a  i  j 我們說他是一個 matrix 嘛
那這些 A  I  J 構成一個大 A 呢就代表我所有的 state  transition  PROBABILITY 所構成的集合就大 A
那除了這個大 A 大 B 之外我還有第三個參數叫 pi
pi 是什麼呢 pi 是 initial  PROBABILITY 就是 q  one 等於 i 的機率
q  one 是什麼 q  one 是第一個 observation 我的第一個 vector 會掉在哪一個 state 裡面
ok 這個 q  one 等於一就表示我的第一個 state
第一個 observation  o  one 掉在 state  one 裡面
q  one 等於二呢表示我一開始是從二開始的
q  one 等於三呢表示是從三開始的
所以呢那麼這個 q  one 等於 i 呢叫做 pi  i
換句話說就是我的整個的 observation 會從哪一個 state 開始跳
理論上我不見得需要從第一個 state 開始跳我可以從第三個開始跳我可以從第四個開始跳我可以從任何地方開始跳
看你從哪一個開始跳的機率是多少
所以呢現在 pi  one 就是我從第一個開始跳的機率 pi  two 就是我從第二個開始跳的機率等等
那這些個 pi  i 構成的集合呢叫做一個 pi
ok 於是呢我這三組參數加起來就構成我所說的一個 hidden  markov  model
那麼這個 A 跟這個 B 跟這個 pi
就構成我的一個我的 hidden  markov  model 我們叫做 lambda
這都是我們的簡寫因為這東西太多了
一個 hidden  markov  model 這樣的一個 model 有這麼一大把的參數
喔有一大把參數那麼因此呢我們就簡寫一個是大 A 一個是大 B 一個是 pi
那這個 pi 裡面呢
我們可以簡單說一下這個 pi 基本上應該
任何一個 pi  one  pi  two 都可以是不同的機率
對不對他們加起來是一嘛
你可以想像我可以是譬如說
我有零點五的機率在第一個 state 開始從第一個 state 開始跳
零點一的機率在第二個 state 開始跳
零點一零點一譬如說這樣子後面是零
對不對我有一半的機率從第一個開始跳另外有零點一的機率從第二個或第三個等等等等
這可以這樣子的這是我的 pi
所以這是 pi  one 這是 pi  two 等等
不過這樣有點太複雜了
就如我們剛才講我的 transition  PROBABILITY 這個 a  i  j 我後來簡化了
這裡我們通常也簡化
我們絕大多數可以就讓他從第一個開始跳不要那麼複雜
因此很多時候我們都簡化成為就是 pi  one 等於一後面全部都是零
我就一律讓他從第一個開始跳
這樣比較簡單
所以我們後面做的時候常常是把他作這個簡化
雖然 in  general 他們可以是任何一個樣子
那這個式子其實是你從前學機率的時候都學過的 Gaussian  distribution
那麼最上面的式子是一維的 Gaussian 那是你所熟悉的
底下呢變成 n 維 n 維的 Gaussian
你唸過的不過也許你忘了我們很快的複習一下
一維的 Gaussian 是你所熟悉的假設我這個是 x 他的 distribution 是一個這樣子的話
那麼這個是他的 mean 叫做小 m
那麼這邊他有一個他的 variance  sigma 是他的肥度看他有多肥
因此呢我的一維的 GAUSSIAN 寫起來就是這個樣子
那這個寫法呢這個大 X 表示說是我那個 random  VARIABLE  X 的符號
大 X 那個 random  variable 的符號就是這個 X
小 x 是他的這個真正的值
而這個值呢可以從負無限大到正無限大
不過他有一個他的他的 distribution 的 mean 是小 m
所以我得到一個這樣的關係我想這個式子是你所熟悉的
這是一個人的 variable 的時候
當我有一把 random  variable 的時候像那樣我們那邊有三十九個嘛喔我有這麼多
當我有一打一把 random  variable 的時候他們之間的關係我是可以寫成這樣一個這樣子的 vector
就變成一個 random  vector  random  variable 所構成的 vector 大 X
這個大 X
那麼我們真的寫起來應該是這個樣子 X  one  X  two 到 X 的 n
我這邊如果是有大我如果有小 n 個 random  variable 的話呢我總共有
我總共有小 n 個 random  variable 把他寫起來寫成一個 random  vector 所構成 random  variable 所構成的 vector
那麼這就是我們這邊所寫的 x
我這邊的小 t 是代表他的 transpose
那麼我的符號這裡有一點 confuse
因為剛才前一頁的 transpose 是大 T
抱歉有一點不清楚
我這邊是用大 T 代表 transpose
下一頁變成小 t 了你你你清楚就好啊這個就這個 transpose
當我寫成這樣之後我也可以寫成這整個 vector 的 distribution
那麼就像我寫成上面那一個一樣那我寫成底下這一個
因此呢我這個變成是我把這個一個 random  variable 的 x 寫成是整個 random  vector 大 X 就是這個東西的 distribution 就變成這樣
這個式子看起來有點複雜不過其實他長得跟上面那個是完全一樣只是這是 SCALAR  form 這是 vector  form
所以呢這裡每一個每一個 x 就變成他的值
那這裡面的每一個都可以有一個值
所以這個值呢變成這個 random  variable 值是這個 x  one
相當於這個 random  variable 的值這個一樣的意思嘛齁
那麼因此呢這個這個 x  one 的值是 x  one
這個 x  two 的值是 x  two
這個的值是 x  n 這些小的 x 是代表他的值
那麼因此呢我這個呢叫做小 x 他也是一個 vector
然後呢那就是我這邊寫的小 x 的 vector 的意思
然後呢那他的每一個 random  variable 他也有他的 mean
那這個 mean 呢就是 mean 的 x  one  mean 的 x  two 這一堆就是他的 mean
那寫起來呢就是我的這個 mean 的 x  one 這是這是指這個的 mean
對這個而言他有一個這個 mean 嘛對不對
對 x  two 也有一個這個東西他就是就是也是他的 mean 就是 mean 的 x  two
等等等等一直到 mean 的 x  n
那他構成了一個就是我們這邊所寫的這個 mean 的 vector
那這個也就是我這邊寫的這個式子我們真正講起來應該是這樣
那麼因此你會看到這上面的這個 x 減 m 的平方的這個地方本來是 x 減 m 的平方在這裡嘛
我現在就變成 x 的 vector 減這兩個 vector 相減就是就是他減他嘛他減他嘛
那也就是他減他他減他那就是每一個對每一個 random  variable 而言就是在算這個 x 減 m 是一樣的嘛
只不過我現在是怎麼樣呢是
你可以先如果我們先不看中間這個的話那這個只是他他跟他他的 transpose 是不是再乘上他因此呢他減他跟他相減的 transpose 變成一個橫的 vector
然後呢另外一個呢是縱的 vector
那這兩個相乘是什麼呢這兩個 vector 相乘就是 square 相加嘛
他跟他平方就是平方相加嘛對不對
所以呢這個就是所有的 mean 所有的這一項
對每一個 variable 而言相乘相加就是這個東西
那然後這裡要除以 sigma 平方怎麼辦
我現在不是 sigma 平方了我的 sigma 會變成一大堆 sigma  i  j 就是所謂的 covariance  matrix
換句話說呢我還有一個 matrix
這個 matrix 裡面的任何一個 element 叫做 sigma  i  j
是第 i 個跟第 j 個
那他是什麼呢他是這個
第 i 個第 i 個 random  variable 減掉他的 mean
以及第 j 個 random  variable 減掉他的 mean 的平均 expected  value
那你可以看到當 i 等於 j 的時候就是在對角線上的這些的話
就是我們原來的每一個那裡的 variance 對不對
我本來如果是一個的話如果是一個 random  variable 的話
他的這個 variance 是什麼就是這個 sigma 平方這個 variance 就是他跟他自己嘛
就是就是這兩個就是就是 x 減掉他的 mean 的平方的 expectation 嘛
對不對所以呢當 i 等於 j 的時候也就是這個 matrix 的對角線上其實就是分別每一個他自己自己的的那個 variance
或是那個 gaussian 的肥度
但是我現在除了在對角線以外我的任何一個位置都有
那那個是什麼呢是第 i 個第 j 個之間的關係
所以呢我就會有的 i 個跟第 j 個之間的關係那構成一個大的 matrix
那這個 matrix 呢我們叫做這個 sigma 的話
那麼於是呢我在原來在 skeleton 這裡是除以就除這個 square 除以這個 variance 平方就好了
我現在沒這麼複沒這麼簡單我現在是變成怎樣要乘上他的 inverse
所以這個除以就變成他的 inverse 就變成一個這樣的式子
所以你如果仔細看的話哦還有那這個是甚麼呢我們這邊是除以他的 square  root 嘛
這個 variance 平方再開 square  root 這個東西在這裡呢變成這個這是什麼這是那個 covariance 的 determinant
ok 就是這個 matrix 的 determinant 就是這個東西在這裡
那除了這個之外呢其實他是長的很像的你如果仔細看的話這裡的每一項分別對應到這邊的每一個 vector
只不過這邊是 scalar 這邊變成 vector 了
然後呢這邊變成 matrix 了
那原來這邊的 scalar 呢變成 determinant
然後這邊本來是 two  pi 的 square  root 現在變成他有 n 個了就這樣不同而已
那麼這些你如果覺得不熟悉的話那麼會建議你回去翻翻開你讀機率的時候的課本他一定會仔細的講這一段的
那麼我們也許舉一個例子來說會比較容易說
當我只有一個 DIMENSION 的時候這個是 x 我們剛才說這個是一個 gaussian
這是我的 mean 這是我的肥度就是我的 variance
這是你所熟悉的
如果說這樣一個 n  dimension 一看起來有一點太複雜你不太容易想像他是什麼的話
那麼最簡單的辦法通常如果太多了不容易想像我們最簡單的方法就是把他簡化變成只有兩個
如果 n 等於二的話只有兩個會怎樣那你可以想像他是這樣這是 x  one 這是 x  two
那麼在 x  one 的時候他有這樣的一個 distribution 假設他的 mean 在這裡
x  one 的話我們把他的 mean 畫在原點比較容易畫當然 mean 不一定要是原點
那麼這是 x  one 那麼然後呢這裡是 x  two 是另外一個 gaussian
ok 那麼於是呢這個是 x  two
那麼這個是 x  one
這樣你大概可以想樣他是這是這是是 two  D 的對不對
那麼在這個時候呢我我如果這樣的話看起來他們兩個肥度是一樣的
也就是說在 x  one 而言我在這上面看到的是他的 x  one 的肥度 sigma  one  one
那我也在這裡可以看到另外一個
是他跟他的呃是他跟他的
這個是 sigma  two  two 是指白色的那個的那個的肥度是 sigma  two  two  ok
那麼我們姑且現在看起來他們兩個是一樣的
你可以想像我的那個 sigma 是什麼呢
是 sigma  sigma  one  one 跟 sigma  two  two 是一樣的這兩個值相同我們不讓他不同我們讓他一樣
那麼如果是這樣的話
喔這個應該是有平方的
這樣子的話那然後呢我讓他的這個 ma 那這個時候的這個 matrix 很簡單這個 matrix 只有二乘二
這只有兩個嘛只有二乘二然後呢我讓他對角線以外這個都是零
這表示什麼意思呢表示他們這個相鄰的這個 i 跟 j 一跟二呢是 independent 他們 independent 所以呢這兩個是零這個時候就是長得像這樣
那就變成一個這樣的東西那這個我想這你比較容易想像這個就變成這樣子
等於說是兩個 gaussian 在 x 跟 x 各是高各是同樣長得完全一樣連肥度都完全一樣的兩個 gaussian
那麼就變成一個這樣的 distribution
那這樣的 distribution 也許我們簡單畫起來把它畫成這樣可不可以也可以
這是 x  one 這是 x  two 的時候呢他就是一個這個我畫等高線又是球形的對不對
呃你可以想像是一個這樣子的
那麼中間比較高外面比較
ok 那如果我畫這樣的話等於是說從上面來看那他就是一個這樣的東西
這是這個情形
好那我現在如果改變一下我讓他的這個變成是這兩個值不一樣了
譬如是 one  one 是比較小的 two  two 是比較大的會怎樣
那你知道呀我的意思就是說我的 one  one 的這個呢這個會變得很瘦
但是呢那個 two 的那個呢會變得比較肥對不對如果這個小這個大的話
那那個時候會是怎樣的意思呢
那個時候在這裡比較難畫在這裡就很清楚了那這裡的時候就變成這個是 x  one 這個是 x  two
我 x  one 變得很瘦 x  two 變得很肥的話就變成一個橢圓形
變成這樣子了
或者說呢就是我的這個 distribution 是長的
對不對那麼這個時候呢還就是這個是這邊比較瘦這邊比較寬
但基本上他們仍然是 independent 變成這樣子
好如果這樣子講你可以瞭解的話我現在再下一步我讓這兩個不是零
我現在第在第三個狀況呢我現在讓這兩個除了一一跟二二
這個小這個大之外我現在讓這個一二跟這個二一不是零
那是什麼意思當一二跟二一不是零的時候表示說他們之間是有 CORRELATION 的
他們之間的關係是存在的他不是 independent 了
那那個時候是什麼意思呢
那個時候如果我們要再畫其實就把這個圖把他歪過來那麼你就會變成一個像這樣的圖
這是 x  one 這是 x  two 他變成一個變成一個這樣子的了
這是什麼意思呢這個就是說我 x  one 跟 x  two 是有 CORRELATION 嘛
那麼我變成一個這樣子的橢圓
這個時候的意思是說譬如說我如果 x  one 的值在這裡的話我的 distribution 是這樣子的
可是我如果 x  one 在這裡的話我的 distribution 是在這裡的是不一樣的
我的 x  two 的 distribution 是 depend  on  x  one 的值會不同的
呃這是什麼意思呢
在這裡的話不會在這裡的話我在這個地方得到的也是這個 distribution 在這個地方得到的也是這個 distribution
是一樣的那麼你這邊看起來不太一樣只是因為我的要乘上這邊的關係
那這個是一樣的那我們打個比喻來講你就容易瞭解是什麼意思了
假設說這是這些 x 都是這個學生考試的成績列在教育教務處的成績股裡面的話
如果 x  one 是體育的成績 x  two 是微積分的成績這兩個基本上是沒有關係的
如果他體育成績很高很會打球不表示他的微積分會特別好或者特別不好
所以呢結果不管體育成績是八十分還是體育成績是七十七十分還是還是體育成績是九十分
他的微積分的成績永遠是一個這樣 distribution
但是呢如果說我的 x  one 是物理的成績 x  two 是微積分的成績的話
這兩個就有 CORRELATION 了
你如果把物理都考九十分的人都拿出來作個 distribution 的話
他的微積分的成績大概也是比較高的就是這個 distribution
反過來呢如果物理只有考二十分的人你都拿出來作個 distribution 的話呢
他的微積分的成績也是個 distribution 不過呢他在這裡
ok 那這兩個就是有 CORRELATION  ok
那麼因此呢你如果看這個物理的成績跟微積分的成績他們是有 CORRELATION 的所以呢他們的 gaussian 呢就是一個 y 的 gaussian
那因為這個高低跟這個高低是有關係的
那麼你如果是體育成績跟微積分成績呢就是沒有關係的所以他們就是就是一個正的
那麼這個關係他們之間的關係呢就表現在這裡那麼這個
就是他們的所謂的 CORRELATION
就這兩個之間的東西就是那個東西他們各自的關係啊
你如果喔沒有這麼清楚的話回去再去翻你的機率課本這就是他的 CORRELATION  coefficient
那他是在算他的這些東西
那當這兩個東西等於零的時候就表示他們沒有關係就好像體育跟微積分沒有關係一樣
於是呢這個時候他就變成長的像這樣子那這變成一個正的
好那如果這兩個 dimension 的狀況這樣子我們舉這些例子你大概瞭解這兩個 dimension 狀況是這個意思的話
我們再延伸為 n 個 dimension 是一樣的意思
那我們剛才說我們現在不是只有兩個 dimension
我們有幾個 dimension 我們剛才講我在這裡其實我的 o 是有很多 dimension
我們剛才講這個大 D 其實是三十九我有三十九維
三十九維不是那麼容易可以畫的那麼我們無法畫
不過呢你可以想像我們講的是一個三十九維的狀況所以呢我就變成是
三十九維我沒辦法畫我們只能畫出一個三維的來
但是但是呢假設這是三十九維的話呢我就是等於我有一堆 gaussian
這一堆 gaussian 的 distribution 加起來描述某一個 state 裡面的長相
那你可以想像譬如說這好比這是一個 gaussian 這是一個 gaussian 這是一個 gaussian
這一個個的 GAUSSIAN 就像我剛才畫在這邊的
如果一維的時候
我如果一維的話我說任何一個長相我沒關係我都可以把它畫成一堆 Gaussian 對不對
這是一個一個一個我把他看成是一堆就是了
一維我可以看成這樣
那 n 維怎麼辦呢也是一樣哪 n 維就看成一堆 gaussian 相加
不過每一個 gaussian 我我們剛才說呢你可以想像他是一個橢圓形的東西一個 n  dimension 裡面的一個橢圓
那麼他可以是歪來歪去的嘛因為他們之間可能互相有關所以他可能是歪來歪去一個橢圓
那如果這樣的話我的每一個個在 n 維空間裡面每一個 gaussian 是一個橢圓
好當我有了這些之後這是一個簡單的描述講 H  M  M 是什麼
那麼這裡面最大的特徵應該有兩個
第一個就是他的 state 是跳來跳去的
這個這些都是 random 的
我們並沒有規定他幾個 observation 之後會跳到哪一個 state 去沒有規定
他只是有一個機率會跳而已這是一個機率
他只是有一個機率會跳而已他沒有一定要怎麼跳法
所以這個是 random 的
那麼因此呢你假設這個這是一個零的 model 的話
零裡面到底那一個 s  tate 會怎麼跳我們其實是不知道的因為每一個人講的零都不一樣
同一個人講兩次你可以講零可以講零也可以講零都不一樣
所以呢我們說這個是一個 random 的
第二個 random 是說呢
我 given 在哪一個 state 裡面在這一個 state 裡面他的 observation 仍然不是固定的仍然是一個 distribution
這就是我們底下所說的雙重的
雙重的 double  layer  stochastic  process 他有雙重的這個這個隨機的特性
第一層就是我的 state 本身是 random  transition
為什麼 for  time  wrapping
這個 time  wrapping 我們剛才講過就是
我的發一個聲音的時候我每一個每一個音段的長短是很 random 的
就是說我們剛才講過你假設這個是 san  FRANCISCO
裡面的 s 到底有多長然後 fran 每一個音到底多長是不一定的
你如果把每一個對應到某一個 state 去的話
他的這個每一個每一個 event 每一個 acoustic  event 長短都是可伸可縮可長可短的
那因此我在時間軸上都每一個 event 都可以伸縮的
那這個特性就是所謂的這個這兩個字就是 time 的 wrapping 這兩個字的意思
那我就用那個 state  transition  PROBABILITY 來描述這現象
因此我的 state 本身是所謂的 hidden  state
什麼叫 hidden  state 你在想意思就是說我其實只 observe 到這個
我並不真的知道他在誰在那個 state 裡面我只是假設他在他這裡他在他這裡
但是其實譬如說 o 四 o 五一定要在這裡嗎
他不能在這裡嗎當然也可以我們剛才說只是機率不一樣而已
你可能把他放在這邊來的時候發現這個機率比較小
把他放到這邊機率比較大就這樣子而已
那麼我其實永遠不知道到底誰在哪個 state 裡面
所以這個 state 本身是隱藏起來的我其實沒有看到
那就是我們所謂 hidden  state 的意思這是第一層的 random
第二層的 random 是說呢即使你知道他是哪一個 state 的話
他真的會長怎麼樣還是不知道那就是我們剛才講的這件事
就是我們剛才講的
即使你知道他是在這裡面這個 o 一 o 二 o 三會長怎麼樣仍然不知道
我只知道他的一個 distribution
所以呢我我即使知道這個 state 我也只知道他長那樣
至於長那樣到底是怎樣會是哪一個我是不知道的
所以呢就是這邊講的即使是
知道哪一個 state 我其實仍然有一個 random 的 output 我是不知道
所以是雙重的 stochastic
那麼第一層是說我在這裡是 random 的第二層是說即使在那裡我這邊仍然是 random 的
好那麼在這個情形之下
那麼喔我們真正會怎麼作我們舉例來講
如果我要辨識零到九的十個聲音
我就是為零建一個 model 一建一個 model 每一個都建一個
怎麼建我要有夠多的 training  data 說零這個是零這個是零
我有夠多譬如說有十個人各唸十個零就有一百個零
把那一百個零拿來計算
然後可以求出這裡面所有的參數那那就是零的 model 的參數
有很多參數包括你的每一個 state 裡面的這個 A  I  J 的機率是多少
那這裡面 b 最多了因為我的每一個 gaussian 有他的 mean 有他的 covariance 對不對
我們舉例來講這個我每一個 gaussian
每一個 gaussian 我要算他的 mean
假設這一個 gaussian 的話他的 mean 在這裡
還有他的 covariance 對不對
covariance 可以看成是他的在每一個 dimension 他的肥度就相當於這個這種東西但是我現在有很多很多個
假設我現在是三十九維的話這個 n 是三十九
我的 mean 就有三十九個 mean
covariance 有多少個有三十九乘以三十九個
喔所以這裡非常多個參數這是一個 gaussian 然後我可以有一把 gaussian
所以呢這個參數非常多
我可以把一大堆的零拿來來 train 出零的零的所有的參數那就是這個零的 model 等等
那麼於是呢我怎麼作辨識
我如果有一大堆我可以為零建一個 model
我們現在一個一個 ma 一個 model 我們就用一個 LAMBDA 代表
零的 model 一的 model 二的 model  k 的 model
總共到九的 model
那這時候進來一個新的聲音某一個未知的聲音進來譬如說我們用就是像這種東西我們用一個大 O 來代表
或者是甚麼呃我這邊是用 ok 用大 O 來代表
假設我進了一個新的聲音大 O 的話他就是一堆 observation  vector
那我就算這個東西
算我這個大 O
如果 given 他是零的話機率是多少
如果他是 k 的話機率是多少
然後看誰的機率最大他就是誰
舉例來講我零有零的 model 一有一的 model 二有二的 model 八有八的 model 九有九的 model
今天我如果進來這個聲音是八的話
八的聲音放到零的 model 的話這個機率會很低我一樣可以放進去的
一樣可以放進去因為你永遠可以把前面的若干個 vector 放在第一個 state 裡面
這個仍然放在第二個 state 裡面這當然可以
問題是如果這個聲音是八那個 model 是零的話你放進去機率都不太對嘛
你就會都會放在一些一些 gaussian 的邊緣他的機率都很小
反過來如果那個 model 那個聲音是八
你放在八的 model 裡面的話呢那他們就會都對上於是我的機率就會比較大
於是你的就會分別就會掉在 gaussian 比較靠近中間的地方機率就會大
因此呢我現在一個未知聲音進來我就把他放在每一個 model 裡面
去算這個機率機率最大的那個就是我的答案齁
這是用 hidden  markov  model 來作辨識一個最簡單的解釋
因為這樣子的關係所以我們這邊就會有
一系列的三個 basic  problem 要解的
那麼這個詳細的解法我們事實上就會在下週以及下下週
的兩次上課裡面我們會講這些 problem 怎麼解
那這些 problem 不容易解因為你要把之前之前所有的數學通通用進來
齁那麼然後呢不過我們到時候就會發現其實也還好因為其實我們並不真的用數學解他
而是用 computer 去解他
我們到時候這三個 problem 都是變成一個 iteration 的程式
經過好幾個 iteration 之後答案就出來了啊
不過這個 iteration 的過程是用這些機率來算的就是了
那第一個 problem 就是我們剛才講的所謂的 evaluation  problem 就是在算這個機率
給我一個八的聲音那放在八的 model 裡面應該是機率最高的
放在五還是三的 model 裡面應該是機率是很低的
我用這個來作 recognition 齁所以這是第一個 problem
那這個怎麼算就是我們下週會講的怎麼算這個東西
第二個 decoding  problem 是說假設你這個聲音是八而且我這個 model 也是八的話
那到底我的 state  sequence 是什麼
到底哪幾個 vector 放在第一個 state 裡面哪幾個放在第二個裡面才是最合理的一個 state  sequence
ok 就是我們剛才講的這個問題那麼其實你永遠不知道他在哪裡他在哪裡
那麼因此呢你就只能夠這個這個找出一個比較好的說這些是他這些是他那就是我的 sequence
所以呢第二個 problem 就是說
你如果知道他是八而且這個是八的話那麼到底哪些放在哪個 state 裡面這是 decode  problem
第三個是 learning  problem 就是我怎麼 train 這個 model
假設說這個 model 是八
我也知道這個聲音是八了
這是一個新的聲音我知道他是八那我想把這個新的聲音的八 train 到這個裡面去讓這個學到他的聲音
所以呢我要想辦法作這件事情就是讓調這個調這個 lambda 讓我的這個新的八放進去之後機率能夠調到最大
也就是說讓我這個這個 lambda 裡面的所有參數學了這個新的聲音之後
他的參數會被調一調之後使的我這個新的聲音放進去之後會機率會變大
那這個呢就是所謂的 learning  problem 就我這個 model 要不斷的學習
根據新的聲音進來我要不斷的學習齁
喔那這是我們講的三個 basic  problem
ok 好我們先停在這邊休息十分鐘
OK 呃有一件事情要說一下我們需要討論補課的時間
不過我想我們等一下在十二點下課的時候我們再來討論呃
呃也就是說我們上週放掉一週我們整個進度 delay
那麼我發現下四月初還有一次所謂的溫書假還要放一次那呃
我其實如果我禮拜一知道我我如果第一週知道上週會停課的話我就會希望上週最好是可以是原時間原時原地補課啊
因為否則我們我們現在是是進度 delay 的很厲害然後呃本學期我還會出國兩次
所以會有兩週要停課所以我們的進度是有嚴重的問題
那呃所以我們需要找這個補課的時間不過我想我們在呃下十二點下課之後再來討論
我們先來講這邊的就是說我們說剛才有三個 basic  problem
這個詳細的我們從下週會講這兩個再下週會講這一個
那基本上就是要用剛才我們的這一堆數學的 formulation 來解這些 problem
所以呢你這些數學符號他的意思要弄清楚否則下週下下週你就會聽不進去了
那麼那我們到時候就會知道其實這些這些 problem 的 solution 都是 ITERATION 的程式就寫程式就可以了倒不需要寫的這麼詳細嗯
那麼我們再補充一件事情就是有有很多課本用這個比喻我覺得也不錯他就是說你可以想像一個狀況假設有三個桶子
三個或者五個桶都可以啦假設我有三個桶一桶二桶三
裡邊有一堆不同顏色的球
譬如說有這個紅的綠的黃的
譬如說這些有一些紅球有一些黃球有一些綠球阿白球好了等等
那麼你這三個桶子裡面的各自的紅綠白的數數目比例不一樣
因此你 random 裡面拿一個球的話拿紅色或者綠色或者黃色的白的球的話這三個桶的機率是不同的
那你今天如果有一個有一個幕把這全部擋起來
一個人躲在幕後他可以 random 選任何一個桶抽任何一個球然後告訴你說我的是紅的
然後待會兒呢之後他又可以再 random 再選另一個桶再抽另外一個球他說我這個是黃的
那麼於是呢你就會得到他聽到他說紅的黃的黃的綠的綠的紅的紅的黃的
但是呢你不知道他到底是從哪一個桶抽哪一個球出來
那這個比喻就是我們這邊講的這這個呃這 state 是完全一樣的那每一個桶就是一個 state
那所謂 hidden  state 就是說你其實是抽球的那個人躲在幕後你並不知道他是從哪一個桶裡面抽的球
然後呢他這次是抽這個桶下次是可以抽另外一個桶的你也不知道所以這就 state  one 相當於這邊的這三個 state
然後在每一個 state 裡面到底紅的綠的機率是多少你是不知道的那麼他們就是不一樣就對了
那就相當於我們這邊所說的他這每一個 state
這好像是三個桶他的每一個桶裡面的每一個就是不一樣就是了
那麼你並不知道他是從哪一個出來的今天你得到某一個得到某一個 O  T 的時候
他可以是在這裡也可以是在這裡也可以是在這裡只是機率不一樣而已就像你得到一個紅球他可以是從這裡出來從這裡出來從這裡出來是一樣的只是機率不同而已
那麼如果這個人可以 random 的隨便抓任何個桶來抽的話就相當於我們這邊講的這個有這個 random  state 的 transaction
ok 那然後呢即使他知道哪一個桶之後他仍然不知只知道是哪一個桶的話你仍然不知道會抽出哪一個桶來哪哪一種顏色球來就相當於我們說的這個 B  B 這個東西呢也是一個機率
那如果這樣子講的話你比較容易想像
這所謂的 Hidden  Markov  Models 的意思
什麼叫 hidden 就是他躲在幕後
他在幕後去做這些事情
你只知道紅
紅紅黃白白這樣你只你只知道這些事情
這是我的 observation  sequence  O  t
那我我 observe 到的是這個但是我並不知道這個幕後是怎樣的
在我們的整個 model 裡面你其實只只知道這個
你並不知道他到底是從哪個 state 在哪兒跳的你是看不到的對所以這些都是 hidden 的那這就是這個 H  M  M 的意思
好這一段講完我們再要講第二件事情就是怎麼樣找這些個 feature
那麼怎麼樣算這個 feature 我們之前大概說了一下就是我可以把
我用一個在移動的 window 那麼不斷取一段
譬如說兩百五十六兩百五十六點來算一個 feature
譬如說這個是 signal
我拿這一段譬如說兩百五十六點
我算出一個 feature 來
就是一系列的 feature  vector
我們說三十九個我叫做 O  one
那麼待會兒我再 shift 多少得到第二個我得到一個叫做 O  two 等等
那麼我們現在來講的是這怎麼做
怎麼樣 given 這兩百五十六點或五百一十二點我怎麼算這個東西
那這這就是我們所謂的 Front  end  Signal  Processing 就是最前端的做這件事情
然後呢就是我們所謂的 Feature  Extraction 抽這個 feature
這裡面要做的第一件事情就是所謂的 Pre  emphasis
喔要講這個東西呢我們就要說一下 time 跟 frequency 之間的關係
那我假設多數人都知道都了解 time 跟 frequency 之間的關係
但是也許有少數人並不見得 exactly 清楚所以我做一個非常簡單的解釋
我們知道在時間軸上
我們很習慣的在時間軸上來看一個 signal
譬如說這個是這樣子的
如果這個是 X  T 也好然後或者我在上面取 sample 就得到 X  N 也好
這個是 X  N
這是我們所謂的在時間軸上來呈現一個 signal 就是 time  domain 的
representation
那事實上還有另外一種 representation 方法呢是在 frequency 的軸上
譬如說這個是 omega 這是 frequency
所以我就有 frequency  domain 的 representation
那在這上面的話呢我任何的一個點
就像這上面任何的一個點代表某一個時間一樣
這裡面任何一個點所代表的是一個 frequency 譬如說 omega  one 這是一個 frequency
omega  two 代表另外一個 frequency
那他是什麼意思呢 omega  one 所代表的是某一個
譬如說 e 的 J  omega one T 的一個東西
omega  two 的話所代表的是另外一個 e 的 J  omega  two  T 的這個東西
那麼這些東西是什麼東西呢
我們簡單的解釋就是在這上面也會有一個
就像這上面會有一個一樣這上面也會有一個
那麼我如果把這個叫做 X 的 omega  one
就像這邊是譬如說這個是 T 的話這邊我有 X 的 T 是完全一樣的
這個是 T  one 的話這邊有 X  T  one 嘛
這邊如果 omega  one 我也有 X 的 omega  one
那這個東西呢我可以寫成譬如說 E 的呃我可以寫成譬如說他通常是一個複數是一個 complex  number A  one  E 的 j  phy  one
就是這個東西
那麼這是什麼玩意兒
那麼他有一個複數他是一個 complex  number 所以有一個 amplitude 有一個 phase
你如果不容易想像他是什麼東西的話呢就把他跟他兜在一起他其實就是這個東西的 coefficient
你就會得到譬如說 A  one  E 的 J  phy  one
就是這個東西乘上 E 的 J  omega  one  T
那麼這個東西
就是這個 frequency
就是這個這個 omega  one 代表的是這個 frequency E 的 J  omega  one  T 的這個東西
那他的大小呢是他的 coefficient 呢是這個 A  one  E 的 J  phy  one
你因此得到這個東西
這是什麼東西呢一個最簡單的想法就是我取他的 real  part
我如果取他的 real  part 的話就得到 A  one  cosine 的 omega  one  T 加上 phy  one
噢你這樣這樣你就了解了他只不過是一個 cosine
這個 cosine 的大小 A  one 就是剛才這邊的這個 A  one
他的 phace  phy  one 就是這邊的 phy  one
而他的 frequency  omega  one 就是這邊的 omega  one
ok 那同理我這邊也會有另外一個譬如說是
x 的 omega  two 在 omega  two 上面有 x 的 omega  two
他呢是 A  two 他也是一個 complex  number 是這個東西
那他其實跟這個兜在一起的話呢跟剛才一樣他所代表的呢是另外一個就是 A  two  cosine 的 omega  two  T 加上 phy  two 的一個這樣的 cosine
那這個 cosine 的大小是在這裡他的 phase 是在這裡他的 frequency 是在這裡等等
那麼於是呢我們真正可以做的事情是把它變成一個
complex 的 representation
我這邊的任何一點
任何一個 omega 三上面也都有一個
omega 四上面也有一個
他分別其實是都像這個一樣都是代表某一個 cosine
那麼於是呢我在這上面你可以想像成是一大把 cosine
這是一個這是一大把 cosine
呃這是一個 cosine
這是一個 cosine
這是一個 cosine 等等
那每一個 cosine 的大小
到底他的振幅有多大這個大小呢就由這個 A  one  A  two 決定了
那麼這個 phy  one  phy  two 是什麼呢是他這個 cosine 的相對於時間 T 零的位置
對不對你這個這個 cosine 後面這個的東西代表零在哪裡嘛
那如果你這個值不同的話這是這個 cosine 是前後移動
這個前後移動的位置是這個 phase
然後它到底震動的有多快就是這個 frequency
所以這三樣東西就決定了這這 cosine 裡面的三個參數這個大小它相對於零的位置以及它震動的多快
那麼我可以把這個 signal 拆解成為這一大堆的 cosine
那麼我這裡的每一點都代表一個 cosine 你把它加起來就是這一個
ok 因此我有兩種 representation
第一種是 time  domain  representation 我直接在 time 上來看每一個時間上它是有多少
我也可以是一個 frequency  domain  representation
那麼它是等於是說我在每一個 frequency 代表某一個 cosine 它的它的這個大小它的這個位置在哪裡
那麼這些這些 cosine 加起來應該會等於那一個
那這兩種 representation 之間的關係呢那就是我們所知道的所謂的 fourier  transform
那麼也包括你如果修別的課你學到所謂的 fast  fourier  transform  F  F  T
那我如果用程式來算通常是算這個東西那我有個快速演算法可以把這個算成這個這個算成這個呃那之間的關係就是這個
好我們簡單解釋這個 time 跟 frequency 之間的關係是這樣那我這邊馬上就要用到這裡
我們說我做第一件事情是所謂的 pre  emphasis
這事情是幹麻的其實只是一個這樣的過程而已
就是假設我原來的聲音這個叫做 X  pron 的 N 的話
我做的第一件事情就是把它做一個這樣子的運算
把 X  pron 的 N 變成 X 的 N
什麼運算就是這邊的這一個式子
很簡單的只是他減掉前一個然後乘上一個 A
通常這一個 A 是一個非常接近於一比一小一點點的一個值譬如說零點九六或者零點九七這樣子的一個數字
我這樣做之後得到了這個 A 那這個過程叫做 pre  emphasis
那麼熟悉 Z 的人各位之中如果有人對於 Z  transform 很熟的話就知道這個關係就可以寫成這個 Z 的關係
如果你不熟的話不知道也沒關係
那也就是說這個呢我把它寫成 h  of  z
這個 h  of  z 呢就是
我這邊寫的一減掉 A 這個負一
那其實這個 h  of  z 的這樣的意思其實就是底下這個式子的意思就是我做了這麼一個這個運算
那如果你並不了解這個為什麼是這樣的話沒什麼關係我們這只是一個符號而已
那麼這樣是幹什麼的
那你如果熟悉 Z  TRANSFORM 的人就了解你可以分析出來說它其實就是把高頻拉高的意思
那什麼意思呢就是我們剛才說我的我的 signal
我 always 可以作經過這個 fourier  TRANSFORM 把它轉到 frequency  domain 來看
看這個 frequency  domain 的 representation 來看每一個 frequency 的那個 cosine 長怎樣的話
那麼我來做的時候
我拿這個做剛才的那個 fourier  transform 我基本上會得到
像這樣子的
我的聲音基本上是越高頻的它就越低
越到了高頻它就會一直掉下來它會一直掉下來
因此呢越高頻的聲音基本上是越微弱我比較不容易分析出來那些東西
因為這樣子的關係所以呢這個 pre  emphasis 一個很簡單的目的是把它重新拉高
把這個地方呢把它拉上去
讓他們這些高頻的部分都往上拉
越是高頻掉的越多我就越拉的越多讓它基本上比較平一點
這樣高頻的聲音也夠
夠強到我可以分析的出來 ok 這個是 pre  emphasis 最基本的意思是這樣子解釋
ya 不是
很多很多很多聲音是很高頻但是很重要的譬如說嘶
這是非常高頻的像雜訊一樣你講的也沒錯但是我們聲音裡面沙的 sh 撒的 s 都是非常高頻的
ok 是很像你講的雜訊但是都是我們聲音很重要的部分 ok
好那麼所以這個是講這個 pre  emphasis 的部分
之後呢再下一件事要說的就是 End  point  Detection
也就是說我們其實我們的聲音裡面
充滿了沒有說話的部分
當我講一句話的時候從某一個某一段時間開始講
前面沒有講
那麼你不要以為一定是這樣的乾乾淨淨的其實沒有講的部分呢它也一樣有東西這就是什麼呢就是剛才你講的雜訊
那麼通常
雜訊是 everywhere 都存在的它永遠有的當你有有聲音的時候其實雜訊是加在這上面
但是當你沒有聲音的時候它也是有的
因此就 computer 而言就你的那個 speech  processor 部分而言它怎麼知道哪裡是聲音哪裡是雜訊呢
我們需要有辦法知道 ok 從這裡以後是有聲音之前都是雜訊那我盡可能不要把這些東西拿來算
你可以想像如果我不知道這一步我沒有做這一步的話
我會把一堆雜訊都拿來當成是某一個聲音我去辨識它是什麼聲音我可以辨識出來一堆東西不過那一堆東西是錯的那這樣會非常複雜所以我需要做這件事情把它切開
切出來哪裡是真正聲音的 ENDPOINT
或者說做這個這個我們叫做 SPEECH 這個我們叫做 silence  silence 不是真的 silence 是 noisy 的 silence
所以呢我要做這個 speech 跟 silence 的 discrimination 所以我需要做這件事情
那這個問題其實是很難的
那麼但是用處是非常多的就以你今天
打電話而言那你知道你跟你的跟你的朋友用手機通電話的時候
你們兩個在講話其實是等於是
用了兩個線等於是用了兩條線
當你在說話的時候你的朋友在聽
當你說完的時候他開始講話
他講的時候你你是聽的
除非你們兩個在吵架
那麼否則的話呢
他講完你再講
所以其實你說話的時間只有這些
他說話的時間只有這些
基本上來講一個人說話的時間不到一半不到百分之五十
那這些部分是什麼這些部分都是 silence 或者說都是 noisy
那你如果有本領把它切出來的話那這些我都不要送
我只傳這個我只傳這個這樣我才節省我的 bits  per  second
我如果這些東西都當成是語音在那邊傳送的話我用了很多 bits 傳的都是 noise
那這些這個所以這個是用途在很多地方都有用的東西
問題是怎麼做這件事
因為他根本跟我們的聲音是看起來是很像的這就是所謂的 Endpoint  Detection
那我這邊講的是一個最基本簡單的方法就是算這個 short  time  energy
那麼所謂 short  short  time  energy 你看這個式子就了解了
我只是把每一個 sample 平方
然後讓他加到某一個 window 裡面
這個 window 最常用的 window 就是
不是最常用最簡單的一個 window 就是長方形的
也就是說
也就是說我現在是一個
零到 L 減一是一別的地方是零
對不對這樣子這個是 window 的 N
阿我這邊寫 M 也可以這個是 M window 的 M
這個是一個最容易想像的最簡單的長方形的 window  window 是 M
如果是這樣的話呢你看這意思不外乎只是說
我把我的我只算一個 window 裡面的 square 加起來嘛
那那在這個式子而言你可以想像我是把那 window 放在 N 的位置上
就得到時間 N 的這個 short  time  energy
譬如說在這裡而言我的這個橫軸是 M
然後呢假設假設這一點是 N
我就在這邊放了一個 window
我就把這裡面的每一個 sample 的平方加起來
只加這個 window 而已嘛這外面都是零嘛對不對於是我就得到一個點得到某一個點叫做 E  N
這個就是我的 E  N 對不對
我我是把我這個 window 放在這個這個零的位置放在 N 的位置
我就得到一個這個 window 然後我把這裡面的每一個點分別把它平方
平方加起來我就得到這個 E  N
然後我現在把這個 window 不斷的移動我這個 E  N 就變成一個一個數值
那你可以想像我這個 E  N 大約是這樣子
當我 window 移到這兒來的時候開始有這個訊號比較大的話他就會比較上來
E  N 就會慢慢的上來
那這就是我的 E  N
這 short  time  energy  as  a  function  of  time
那我這樣的 window 不斷移動我的 E  N 不斷大起來我可以定某一個 threshold
譬如說呢當我超過這裡的時候這是我的 threshold
我就假設從這裡開始是語音
這是一個非常簡單的方法
那麼他的基本的精神是假設說
畢竟你講話的聲音應該比 noise 大一些
所以呢你你如果講的聲音也跟 noise 一樣大的話那當然就 noise 跟聲音混在一起你就不知道了嘛
所以你講講的聲音 suppose 你的語音要比 noise 要大一些所以我就不斷的求這個 short  time  energy
然後我這個 E  N 不斷的 shift
當我超過某個 threshold 的時候我就認為是開始講話了
這是一個最簡單的方法即使是這個方法呢我仍然需要 depends  on 這個 noise 有多大
你可以想像我在教室裡面我如果開了冷氣或者什麼或者是
背景雜訊比較厲害的話這個比較大的話
我的 threshold 要不一樣
我的 threshold 可能是隨時要調的
那這個時候的情形很可能是
你在還沒開始講話一開機還沒開始說話的時候先收一次
當時的 noise 狀況算一個當時的 E  N 然後用那個來 define 一個 threshold
於是呢我的 threshold 超過那個我就算是我有在說話等等
所以這就是我這邊所謂的 adaptive  threshold 你就是要隨時
在一開機還沒說話之前隨時收當時的雜訊算當時的 background  noise 的 E  N 是多少然後定那個東西等等
那即使是這樣做那個方法顯然不夠好
那麼那麼你可以想像出來我在很吵雜的環境
我在街上走的時候我打的手機
這個街上的雜訊跟我講的話幾乎是一樣大聲的話我怎麼辦等等顯然不是這麼容易
所以這只是一個非常簡單的 example 說明這一類的方法那真的做法顯然要比這個複雜的多
那這個我們在有個專有名詞叫做 V  A  D 就是 VOICE  activity  detection
這是所謂的 V  A  D 那麼也就是說
我其實隨時要去自動 detect 什麼時候是 VOICE 的
active 譬如說只有這一段這一段是 active 那這一中間這一段呢是沒有 active 我要能夠抓的出來
那一直到今天這仍然是一個非常重要的研究的課題
那麼一大堆 paper 在在做這個因為你到底怎麼做的好
尤其在一個非常吵的環境在路上在街上在地鐵裡面在這個
雞尾酒會裡面在餐廳裡面那麼各有不同的狀況你如何做這件事情這是所謂的 Endpoint  detection
那麼當我做到這點之後 ok 那再來就比較好了
我再來我就可以在這裡面從這裡面來做我取一個 window 我再來算我的那些參數
那這裡我們順便說一下利用這個機會我們就講一下這個要
剛才這個的 short  time  energy 其實是一個例子在說明做 window 的過程
那我的這個這個式子是一個 general  form 其實就是底下這個式子的一個 general  form 底下這個它的 special  case
你看我我我如果把這裡的話呢我把這個 T 的是某一種 operator
等於說我對這個 signal 操作某一個 operator 之後乘上一個 window 然後加起來得到我的某一個參數
這是一個 general  form 這是一個 special  case
當我這個 operator 只是一個平方的時候我所得到 Q  N 就是 short  time  energy
但是呢我這個可以是更複雜的 operation 我就得到更多的東西
就像我們後面所講的我現在要
取一個 window 之後經過一堆演演算得到它的 feature  vector 的話也是一樣的意思
我也是這個求一個 window 之後在 window 上面做一堆 operation 之後得到我我所要的參數是一樣的
那這裡面我們最簡單容易了解的 window 是長方形就是我們剛講的這一個
只不過長方形的 window 其實不好
我們最常用的不是長方形而是這一個所謂的 hamming
這個 hamming 是怎樣的呢 hamming 不是這樣而是一個 cosine 的形狀
它是在這個裡面是是這樣
那麼從零到 L 減一其他都是零
或者說跟長方形比起來他就是
他的兩邊很小中間很大是一個 cosine
你如果把它這兩邊都擦掉的話
就變成 hamming hamming 是一個這樣子的東西這個數學式子寫成這樣
那麼為什麼要變成這樣這這個關係比較複雜要解釋 hamming 的什麼要用 hamming 的原因我們會在七點零的時候會再仔仔細說 hamming
跟 Rectangular 之間的關係
我我們這邊可以做一個非常簡單的解釋
這個解釋比較容易想像
沒有太多學理
那麼假設說我的因為我的語音很可能是這樣子我們舉一個例子哼
我們後面會看到語音常常長這樣那麼
如果是這樣的話
我用 Rectangular 來求的時候呢我這個 window 可能算到這裡
在這裡面算的某某一個譬如說 short  time  energy
待會兒當我的 window 搬到這來的時候
會怎樣
我一下少掉一個非常高的 peak 的很多東西但是多的是一些很小的值
因此從這裡到這裡會變化非常大
然後我如果再過來一點的話呢
我又丟掉了一堆這邊很大的東西多了一堆是很小的東西
因此我這樣子的話我得到的那個值會會 fluctuate 非常厲害
不像我這邊畫的這麼 smooth
而很可能是非常不 smooth 的
會變成這樣子非常不 smooth
為什麼非常不 smooth 就是我們剛才講的原因因為我其實
我這裡邊變化非常大很可能我向前移動一下的時候丟掉的是的值很大增加的值很小
或者你在移動過來的話我增加的值很大丟掉的值很小所以我這個變化非常多
那如何如何那如果它這個 fluctuate 非常厲害的話
我其實不容易判斷到底那個是什麼就很難講了我其實比較希望他 smooth 嘛
那它如何那它 smooth 呢一個簡單的辦法就是我不要讓它從頭到尾一樣 weight
而我把它兩邊 weight 降低我主要算中間的
那這樣的話是不是就會比較好呢當然會好很多對不對因為我這兩邊我都我都都拿掉了嘛
所以呢當我搬過來的時候我我丟掉的東西本來就很少
我增加的東西也就很少嘛我主要是以中間這堆東西為主
那中間這堆變化比較少嘛就比較 smooth 嘛
我們可以用這樣子簡單的理由來說
那詳細其實它是有學理的關係的不過那個我們留到七點零再講
我們現在先姑且用這個方法來解釋那大致的意思就是我們比較喜歡用的其實是 win 其實是像 hamming 這樣子的 window 而不是一個長方形的 window
好那有了這個之後再來是怎麼做
再來就變成說
我們剛才講我第一步先做了這個 pre  emphasis
把這個高頻的部分拉高了以後
那第二步我現在把這個呃呃哪裡是 noise 切掉了我知道這堆是聲音了
第三步呢我現在就可以取取一個 window 做一個 hamming  window 我就一段就出來了之後我這一段怎麼算東西呢
我我這一段這個我這個 hamming 加在這裡嘛對不對
我的這個我這個 hamming 加在加在這裡譬如說我從這裡加一個 hamming 過來
我得到這一些點假設這個 hamming 是 L 是兩百五十六或者五百一十二我就得到五百一十二點這個時候我可以拿來怎麼做呢
那麼第一件事情所謂的 discrete  fourier  transform 這就是我們剛才說的 fourier  transform
也就是我把 time  domain 轉到 frequency 上的 representation 去
於是我得到一堆 frequency 上面的 representation 像這樣像這樣嗯是在 frequency 上面的
那為什麼要這樣子做
那也是一個很簡單的原因是因為我們人的耳朵是聽 frequency 的
我們剛才說 ok 我們得到一個像這樣的東西
這是 frequency
那麼根據嗯心理學聽研研究人的認知醫學研究人的聽覺的人的研究那麼人的耳朵聽的是 frequency
我們本來就是聽 frequency 的所以呢嗯我們就要學人怎麼做的人是這樣做的
所以呢舉例來講
人的聽覺這個據他們的研究結果是
一組一組的聽覺神經分別在管一堆一堆的 frequency
譬如說這堆 frequency 是某一組聽覺神經在管的
下一堆是另外一組聽覺神經在管的
再過來是另外一組聽覺神經在管的
他們每一組在管裡面一堆
而這一堆呢彼此是 over  lap 的
那麼換句話說今天如果有兩個 frequency 都在這個裡面的話我們聽是聽不出來他們有區別
可是如果一個在這裡一個在這裡的話呢我們會聽出來兩個不同的聲音
ok 那麼一個在這裡一個在這裡我聽出兩個不同的聲音如果兩個都在這裡我是聽不出來他們有什麼區別的嗯有這樣的關係
然後呢在這些 over  lap 的地方呢是這一組也聽到這一組也聽到的
那麼如何去去做一個這樣子的事情呢
那麼早年的 engineer 想了很多辦法之後他們想的辦法就是所謂的這邊講的這個 filter  bank
它的意思就等於是說 ok 既然是這樣的話
你可以想像其實
我們是 ok 這一堆聽覺神經這一組它聽的這一堆 frequency 之後呢它有一個訊息
送到大腦去
那這一堆的呢它也聽到了之後有一個訊息
送到大腦裡面的聽覺的部分等等這有另外一個
既然如此我們也來做這一件事
那譬如說呢
這一堆呢你可以想像也許就是從這裡到這裡吧
我想辦法把這一些個 frequency
這一些個 frequency 的 signal 我把它加乘起來變成某一個訊號
拿來用
那這一堆呢我也加乘起來
譬如說這個我我加乘起來當成當成某一個訊號來用等等等等
那這個怎麼做呢就是你就取這當你做了做了這個
當你做了這個 discrete  fourier  transform 的時候你就會得到像這樣子的東西嘛
你就可以把這堆 frequency 加起來把這堆 frequency 加起來等等
那這個的這個的功能就是所謂的 filter  bank
就是一個一個的 filter
那它的做法呢就變成像這樣
就是我我想辦法取這一堆 frequency
那麼分別乘上這些值對不對
我取這分別乘上這些值把它加起來
可是如果是這樣的話呢這個 engineer 覺得很困擾
因為這樣子的話那這個呢要加這個對不對
那難道說這些 frequency 聽兩次呀
那我們真的聽兩次呀在這邊聽到在這邊聽到呀
難道他們聽兩次嗎這好像有點怪怪的
那麼後來他們就想了一個辦法
這個辦法其實沒有什麼特別道理這只是一個一個一個非常 ENGINEERING 的想法
它就說那這樣子吧我們把它變成三角形
這個呢把它變成另外一個三角形
那麼也就是說呢我現在的這個這個呢
這是所謂的 filter 就是我只取這一段的 frequency
但是呢我這個是乘它這個是乘它這個是乘它因此呢在三角形的中間的地方我 weight 最大
兩邊我就 weight 比較小為什麼兩邊 weight 比較小呢因為這邊會有另外一組會聽到它嘛
那麼因此在中間這些 frequency 呢兩邊都聽到
它應該是兩邊聽到的比較弱好像比較合理
其實這一點並沒有真正的科學根據只是他們是很 ARBITRARY 這樣畫一個三角形
那麼因此呢我等於說是在這個 frequency 這一堆 frequency 裡面我讓中間 weight 最大
距離中間越來越遠的話呢 weight 越來越小所於他們這邊分別乘上這邊 weight 加起來才是那個 signal
那這樣之後我這個才是一個 signal 送出去拿去計算
同樣這是另外一個 signal 拿去計算
那用這個方式的話呢表示說我在兩邊的 frequency 被 weight 比較小
我離越靠近邊緣 weight 越小
那這樣的話呢我這兩個三角形這樣加起來好像比較合理嗯
表示說這樣中間雖然被聽被聽兩次了
可是呢左邊也聽的比較小聲一點右邊也聽的比較小聲一點用用這樣來看
那如果是這樣的話呢我們就得到一系列的
這每一個就是所謂的 filter
就是我只取一個
我只取一個這個
這一這一堆 frequency 裡面的訊號一個 filter 的意思
因次我就得到這樣的一個一個的三角形的 filter
那這樣子呢就是所謂的三角形的 filter  bank
那這個就是我們這邊講的 triangular  shape  in  frequency 然後互相 overlapped 意思嗯
ok 我的每一個 filter 是三角形的然後讓它互相 overlap
其實這個三角形是沒有什麼太大道理喔我們後來知道其實它不見得應該是這樣
不過呢這是一個簡單的做法那就是非常 engineering 的想法就把它變成這樣子
然後呢它是 uniformly  spaced  below 一個 kilo  H  Z
在這個一個 kilo  H  Z 之下
這個也是根據人的聽覺來的
一個一個 kilo  H  Z 這些低頻是我們最多的聲音的特徵在這裡
所以呢我們要把它做的最細密嗯所以呢是 uniformly  DISTRIBUTION 在這個地方
可是呢到了一個 kilo  H  Z 以上的話我就不再是 uniform 而是以 log 的 scale
所謂的 log  scale 是說
畫大一點就會變成這樣
我在一個 kilo  H  Z 以下是一個 uniform 的
1 kilo H Z
在這上面的話我會變成 log 的越來越大越來越大嗯
我會我會以 log 的 scale 它越變越大越變越大到了高頻的時候我的
那事實上這個也是跟這個聽覺神經有關的
也就是說你越到高頻的時候你那一組聽覺神經是聽的越多 frequency 一起聽的
嗯那麼那麼因此呢我們就會變成一個 log  scale 的關係在高頻就變成 log  scale
那這一點其實你也可以解讀成為另外一個現象就是我們人的聽覺
本來聽 frequency 的高低就是聽它的 log 的
那一個最直接的
事實就是你如果學音樂你就知道
do  re  mi  FA  so  la  si
這是 do 這是 re
mi 這中間距離是完全一樣的
你可以知道這個是這是 do 這是 re 這是換一個調而已這是一樣的
那 mi 跟 FAR 是半音中間是這距離是它的一半
然後呢 so  la  si 跟下一個 do
這兩個是半音就是 mi 跟 F  A 之間跟 si 跟 do 之間是半音它的距離是其他的一半其他距離是相同的
那麼到這裡的時候
那這個是什麼 scale 這個就是 log 的 frequency
你如果是以你如果是以 它的 frequency 的 log 來看的話他們剛好就是這個等距的關係
那麼我們耳朵聽起來它就是一個這樣的關係
那其實這就是在 log  scale 上面嗯
所以其實我們的聽覺對於 frequency 感覺本來就是在我們所講的那個
喔 cosine 的那個 frequency 取了 log 之後就是我們聽到的
那你知道從這裡到這裡是變成多提高一倍嘛
這個到這邊提高一倍
然後再高八度就是提高一倍嘛差八度就是差一半的意思嘛
嗯那這個本來就是一個這樣子的這個
所以呢我們人的聽覺本來就是一個 log
那麼因此呢在高頻的時候你用 log 的 frequency 來做這樣的三角形是非常合理的
那在低頻的時候是因為我們的聲音非常大部分的區別在這裡
喔最主要的這個特徵都在這裡所以這地方我們做比較細膩一點是做 uniform 的
這個是這樣我們就得到一組這個所謂的 filter  bank
filter  bank 就是一系列的 filter
所謂的 Mel  scale 的意思 Mel  scale 的意思就是
這個詳細有定義不過我們簡單的講就是在高頻都是以 log 方式增加上去
低頻是 uniform 的
好這個就是我們做的這個 log  scale 的呃 mel  SCALE 的這個 filter  bank
再下來呢我們就每一個裡面得到一個就好像
這個聽覺神經得到一組訊號送到大腦去一樣我們就得到一組訊號
這個呢我們把它取絕對值取 energy 取 log 平方嗯
這個沒什麼特別
都有原因的我們後面會再講我們大概是這樣子你大概了解就是等於是我取的一個個的訊號然後拿來去做後面的分析
至於後面為什麼做這個 inverse  discrete  fourier  transform
這比較複雜我們留到後面再說在七點零我們會詳細的說
那這個東西我們也詳細的後面後面再說
那這樣所得到的東西呢呃我們取一個名字叫做 mel  Frequency  Cepstral  Coefficient
那這個名字怎麼來的我們後面也在後面再說好了嗯
那基本上我們取它叫做 M  F  C  C 這是我們後面所習慣的名字
那麼呃詳細的有一堆理由有一堆我們在七點零會詳細的說這個 Front  end  Signal  Front  end 我們會講這些東西
那基本上這樣做之後我們得到的是十三個參數叫做 M  F  C  C
這樣是十三個
欸那我們剛才說有三十九維呀怎麼只有有三十九怎麼只有十三個還有二十六個就是所謂的 first 跟 second  order 的 difference
也就是他們的微分
那麼換句話說呢
我如果照剛才那樣子做的話
我這樣子得到一個
我現在是十三維
十三個這樣子得到一個也是十三個
這樣子得到一個也是十三個
之後呢我再來呢就是做他們的 first  difference
這個 first  difference 最簡單的做法是兩兩相減
雖然你可以做更複雜更複雜我們也是後面會講
它減它得到它它減它得到它對不對它減它得到它它減它得到它等等我就可以得到底下的十三維
就是所謂的 first  difference  ok
它減它得到它它減它得到它它減它得到它它減它得到它等等的話呢其實這十三個
那底下的十三個等於是它的微分的意思
那同理我還可以它減它得到它我還可以再底下第兩次微分
它減它得到它它減它得到它那麼它減它得到它它減它得到它等等我又可以底下
這樣子因此我就有原始的我用剛才的方法得到的是原始的 M  F  C  C
就是 Mel  Frequency  Cepstral  Coefficients 這個名字我們後面都會解釋我們現在姑且就偷懶一點就大概了解是這麼回事兒就好我們不要花太多時間來說它我們在七點零的時候會仔細的講嗯
那麼這邊所得到 M  F  C  C 是十三個在這裡
然後我就有十三個是它的一次微分 first  difference
然後再有十三個它的兩次微分就是 first  difference 的 first  difference 就是 second  difference
那就是這邊講的 first 跟 second  order 的 difference
你可以想像他們就像是微分一樣的意思
那事實上呢還可以做的更精細我們在七點零會說呢
你不一定是要它減它你還可以做更精細的做法不過我想這些都留到後面嗯我們後面再講
同樣呢在這邊還有講到譬如說 Pitch
這些音高呀什麼這些東西我們留到後面再說
那總之我們到這裡的我們大概可以看到
基本上這些是這樣出來的
那詳細情形後面再說那這段就是我們講的 Feature  Extraction
那有了 Feature  Extraction 之後呢我們再來就可以進入後面的這個 Language  Model
那麼我們在這裡休息十分鐘
第三部份就是 language  model
我們在第一週的時候曾經說過一下
也就是說並不是我們所想的那麼容易
而是很多聲音都很像
然後很多聲音都會搞不清楚
我們舉例來講
這個假設這是時間你這個一個位置的聲音進來的時候
我們可以把裡面的很多的基本的單位音都變成一個 Hidden  Markov  Model
記得我們那時候說譬如說
this  is 你可以想成是這個這是一個基本單位音
這是一個這三個拼成這個
然後呢再來恩這個可能是這個
this  is 等等
那也就是把每一個基本的單位音都做成我們剛才講的 Hidden  Markov  Model
於是呢你可以判斷說那裡有一個這個音那裡有一個這個音這裡有一個這個音
然後呢這些音呢那那於是呢這些個東西呢拼起來呢是這個字這些個音拼起來呢是這個字等等
但事實上不是這麼簡單因為這個音也跟這個音很像這個音也跟這個音很像
於是呢你這三個可能都很像而這個呢這個跟這個音也很像這個跟這個音也很像
譬如說那麼你都可以拼來拼去變成很多不同的字出來
那我們舉例來講譬如說這個我們說一句話譬如說 the  computer  is  listening
基本上你講的是這句話
但是呢你很可能因為變成這裡面是很多小的單位音在裡面拼嘛
你可能會拼成譬如說這個是有一個 they 這裡有一個是 come 這邊有一個音是 tutor 這邊有一個這個這個 is 這裡有一個 list 這裡有一個字是 sunny
有很多這種字啊
當你有辨識出一堆這種基本單位音的時候因為都有很多 confuse 的音嘛
所以可以拼出很多奇奇怪怪的字出來
變成說they  come  tutor  is  list  sunny 這也是一句話
那憑什麼不是這句話而是這句話呢
那我們就希望知道到底哪些個字連起來哪些個 word 連起來比較像一句話
那這個方法就是我們算他的機率
我希望我算出來機率是這個 the  computer  is  listening 的這個機率跟另外一個 probability  of  they  come  tutor  is  list  sunny
那我要這個機率要比他大很多才行
因此我到時候就知道呢這個應該不對吧應該是這個吧
那這個就是 language  model 的功能
換句話說光靠前面的 hidden  markov  model 我知道它是這些基本的音
我把它拼出字來可能拼出來完全不對的字
所以呢我要靠這些這些這些字串起來到底通不通來來來算他到底合理不合理然後得到一個比較合理的答案那這就是 language  model 在做的事
所以我們就是要算假設我的一個 word  sequence 一個 sequence  of  words  w  one  w  two 都是一個 word 總共有 r 個 word 構成一個 r 的 sequence 叫做大 W 的話
我就是要算這個大 W 的機率
這個機率就好比是這個這個句子的機率或者這個 word  sequence 的機率
我要他算機率比它大
那怎麼算呢
那基本上你很容易想像這個算法
其實我後面上面的那個數學式子其實就是我們最基本的機率的算法
這個是 word  one  word  two  word 三 word 四 word 五等等等到 word  r
我先算他的機率
有了他的機率之後呢
在 w  one 之後會接 w  two 的機率
然後 w  one  w  two 之後會接 w 三的機率
然後呢一二三後面會接四的機率
一二三四後面會接五的機率以此類推一直到最後會接最後一個機率
這把他這樣一路乘起來那就是上面的這個式子的意思 ok
所以你是先把第一個 w  one 的機率算出來
先把它它的機率算出來
然後呢 given 它之後有下面的一個機率
那 given 這兩個之後有下面第三個機率
given 這三個機率有第四個等等
因此呢就是 given 一到 i 減一之後到第 i 個的機率
然後呢我 i 從二開始一直算到 r
所以後面這一堆乘起來就是我剛才講的這一些乘起來
基本上要算這個東西那這是一個標準的算法問題
只是這個無法算
為什麼無法算
因為我們的 word 很多我們以英文為例
英文日常用語我們每天日常用語的英文大約三萬是免不了的
少一點我們說兩萬五多的話要六萬才夠
depend  on 你要 cover 多少東西
我們以三萬為例
假設你有三萬個 word 的話第一個 word 有三萬種
第二個 word 也有三萬種
這都有三萬種
所以我 r 個 word  sequence 有三萬的 r 次方種
那我隨便舉裡面的一個
總共有 i 個嘛所以就是有三萬的 i 次方種對不對
因為這個有三萬種
有這麼多你要把每一個機率都求的出來會求死人的
而且也沒辦法做
所以這個是一個這是一個很直接的答案
但是不好做那我們怎麼辦
我們就做一個簡單的假設假設說你出現一個 word
只跟前面的 n 減一個 word 有關這是一啊只跟前面的 n 減一個 word 有關
也就是說我不要算那麼多啦
我不要每一次都是從一到 i 減一之後
given 從一開始到 i 減一再去看下一個 i 的機率
這個太多了
我每一次只看前面的 n 減一個因此呢就會變成怎樣呢
我們舉例來講呢我這個時候呢我要看的這個五的機率不是一二三四 given 一二三四之後五的機率
而是 given 譬如說前面的 n 減一個之後五的機率
那六的時候怎麼辦呢我要看六的機率的話呢也是前面的 n 減一個然後看這個的機率
也就是我每次都只看 n 減一個在 n 減一
再前面的我就不看了
好這是基本假設是這樣子的
這個是一個 assumption 沒有理由說他一定是這樣
那我們也知道從我們日常對語言的了解我們也知道不是這樣
那這只是為了所以這個 assumption 其實是不通的
但是為了讓它可以數學可以做
那這是一個這個 approximation 不是真的
那麼呢因此呢我每一次只用前面的 n 減一個來看下一個
因此你看我這邊的不同就是我把這個機率從 i 一二到 i 減一之後看到下一個 i 的機率
我簡化成為不是全部這麼多而是只有從 i 減 n 加一 i 減 n 加二到 i 減一
也就是我只算這是 i 的話這個是 i 減一 i 減二到一直到 i 減 n 加一
到 i 減一為止的這 n 減一個我只看前面的 n 減一個就好了
所以這個我這個機率就用這個機率來替代
那這個等號其實是不成立的
我們只是一個假設或是 approximation
如果這樣假設的話我只算前面的 n 減一個 word
那於是呢其實我每一次其實就是多少個
這邊的 n 減一個再加下一個就是 n 個嘛
所以我總共就是算這樣子嘛
對不對總共就是 n 個這 n 個 word 之間這 n 個 word 會連在一起的
given 前面這 n 減一個下面會有 n 的機率
那其實總共有多少種呢
就是三萬的 n 次方種
那這個數字至少在我的 control 之內我 n 可以小一點嘛
n 可以是比較小的數
那這種東西呢我們就叫做 n  gram  language  model
這個 gram 這個字當初他們用的時候是
grammer 的簡寫grammer 就是文法
所謂的文法的意思解讀成為就是前面什麼字後面要接什麼字
這樣叫做文法所以這個叫做 grammer
所以它叫做 n  grammer 就是我這個 n 個 word 之間的關係叫做 n  gram
那麼如果是這樣的話呢n 等於二的時候呢叫做 bi  gram 就是 given 前面一個 word 會出現下面一個 word 的機率這叫做 bi  gram
n 等於三的叫做 tri  gram 就是 given 前面兩個 n 等於四叫做 four  gram  given 前面三個等等
那那當然也可以有 uniform 當 n 等於一的時候就只算一個 word 的機率
所以我們就有這些這所謂的 n  gram
那通常我們在做的時候絕大多數簡單一點的情形我們做到 tri  gram
就是這三個的那麼比較複雜一點的就做到 four  gram 就是做四個的
你可以想像 bi  gram 是兩兩相連的機率有了這個會出現這個的機率
tri  gram 是三三相連有了前面這兩個會出現下一個的機率
那 four  gram 呢就是有前面四個等等我們大多數做到 tri  gram 複雜一點做到 four  gram 再多大概不做了因為再多太多了
那麼於是這些機率怎麼來我們用一個 training  text  code  database
也就是說你去上你最簡單的去上網嘛他們今天最常用的辦法是上網
你上網可以抓上千千萬萬個網頁每個網頁上都有一大堆文字就用那些文字去算
就可以算前面有哪幾個 word 會出現下一個的機率就可以算的出來
那麼我們以這個 tri  gram 為例那麼這樣的一個機這樣的一個 word  sequence 的機率怎麼算就是底下這個算法
那這個算法其實很容易看就是這樣
比如說這是 w  one  w  two  word 三 word 四 word 五 word 六一直到 word  r
你第一個機率是他單獨出現的機率第二個呢是有了他之後出現他的機率
從第三個開始呢就是三三相連了就是這個有了他之後出現
有了一二之後出現三然後呢有了二三之後出現四
有了三四之後出現五有了四五之後出現六等等
我每次都只看兩個不是這個都全部從頭看這裡全部從頭看我現在不是了我只看前面的兩個的下一個
這就是用 tri  gram 來算的方法就變成這樣所以呢這個是第一個這個 w  one  word  one 的機率
然後我這個呢是 word  one 之後會出現 word  two 的機率
之後呢我就是有出現 i 減一 i 減二之後的 i 的機率
所以就是兩兩兩個出現第三個等等這樣出來的
那這樣子我就用這個方法算就可以算到一個 word  FREQUENCY 的機率
也就是我這邊講的是 they  come  tutor  is  list  sunny 的機率呢還是 the  computer  is  listening 的機率等等
好那這個是這個我們用 n  gram 來做 language  model 最簡單的解釋是這樣子
那怎麼來算這些東西呢我們在下一頁有說那基本上這個算法很簡單就是這幾個式子
那這幾個式子其實是很容易解釋的很容易想像的
我們剛才說你如果上網去抓一大堆網頁出來有幾億個文字
幾億個字的構成的大的這個 database 你很容易算這些東西舉例來講
this 這個字出現了這麼多次 this  is 連在一起的時候
這是 this 出現這是 this  is 出現的機率呢剩下這樣多次
如果這樣的話那就告訴我說這個我會看到 is 前面會出現前面有 this
前面有 this 後面會看到 is 的機率是什麼就是它分之它嘛
就是有這麼多分之對不對這是一個很直覺的答案
就是說我我 this 裡面有多少五百萬個但是呢五百萬個 this 裡面只有五千個後面接了 is
對不對所以五百萬分之五千呢就是 this 後面會出現 is 的機率
那我這邊講這個簡單的例子就是這個式子這個 bi  gram 怎麼算的就是這麼算的
那我這裡跟剛才不同的我剛才的上一頁的下標是表示第幾個 word 在 word  sequence 裡面第幾個 word
我現在是上標只是說不同的 word 所以呢是某一個 word 會在某一個 word 後面的機率
就是這個 word 全部出現就像就像這個 this 有多少次有五百萬次
但是 this  is 連在一起有幾次有五千次那麼這麼一除所以這個就是這個 bi  gram 的算法
那這裡的這個 n 括號就是 number  of  counts
在你的 database 裡面有多少個 counts 就是這個東西
然後所以你這個除這個就是我們這邊講這個意思
那如果說是這一點 bi  gram 沒有問題的話 tri  gram 一樣的意思
就如果 this  is 有五千次可是 this  is  a 這個時候剩下五十次了的話
那你是不是就可以知道我的 probability  of 這個看到一個 a  given 前面有 this  is 的機率
就是這個五千分之五十對不對就是這樣的意思嘛
就是說你當我有 this  is 總共有五千個
但是裡面只有五十個後面還接了 a
所以你看到 this  is 後面會有 a 的機率就是五十個分之五五十除以五千對不對那這一個式子就是我們這邊的 tri  gram 的計算方式 ok
那這個就是這個的意思那這個是 bi  gram  tri  gram 等等 FOUR  GRAM 用類似的方法你就可以知道是怎麼做的了
那麼 uni  gram  uni  gram 最簡單了沒有什麼特別就是你單獨那一個 word 出現的機率
那譬如說我現在如果 this 出現五百萬次
可是我總共的總共的 database 是有十的二十次方個的話
那個 database 是有十的二十次方個的話那我就是十的二十次方分之五百萬
那這個就是 this 這個字的 uni  gram 它就是上面這個的意思
那就是這個你那個字出現五百萬次做為分子
那這裡面我現在是這樣寫的話其實是意思就是我的 data 總共有多少 word 的意思
那你看我這個寫法其實一樣就是把所有的 word 的次數全部加起來
你這是 w  j 是某一個 word 然後他有多少次那我把所有的 word 全部加起來
那就是其實我的 database 總共有多少 word 那麼這麼一除呢就是我的 uni  gram 就是我那個 word 的機率因此以此類推我的所有的 gram  n  gram 都可以這樣算出來
不過呢事實上是沒那麼容易因為其實會有很多這個 real  events
你是不能這樣算的就是說你我們這樣講起來都很合理但是呢我們必須了解這個盡信統計不如無統計
那麼這些統計常常是不對的那麼我們舉一個很簡單的例子來說為什麼這樣譬如說這個
Mary  immediately  cry 這是一個很普通的句子啊 Mary  immediately  cry
但是如果你照 database 去找的話呢 database 裡面有非常多個 Mary 它後面接很多很多東西它就是沒有接這個字
你有很多個 immediately 它裡面就是沒有接 Mary
因此呢你這個後面接這個的 bi  gram 就是零你算出來的這個字這個句子的機率就是零
那你就出不來你這句話就永遠不會對因為你的機率就是零但是其實不是零啊
那只是說這兩個字剛好在你的 database 他們沒有連起來過啊
所以呢你不能完全相信這個 n  gram 這樣講就會對它是有問題的
那麼這種 Mary 後面會接 immediately 這種情形呢就是我們所謂的 real  events
它就是沒有出現
那麼你怎麼辦我們要有解決的辦法
這個這個詳細我們在這裡我們在後面會講在六點零我們會仔細說這一段就會說怎麼做這些事情還有很多的所以呢我們這邊的
之前講的 language  model 之前講的那些 Hidden  Markov  Model
在四點零五點零我們還會仔細說怎麼做然後 language  model 我們會在六點零說怎麼做
然後我們上一堂課講的那些這個 front  end  signal  processing 在七點零會說怎麼做
所以我們這個大概先有一個初步的了解有這麼回事兒就好了
我這只是讓你有初步的了解

ok 好我們現在來來做這個 demo 好了
呃我們今天 demo 幾樣這個東西
那麼一個前面幾個是我們早年所做的一些東西阿大概都在九零年代的時候做過的
那麼現在看起來是像 toy 一樣只是一個玩具阿
那麼你可以感覺說但是你大概感覺我們在講是些什麼東西這門課會學的是些什麼東西
那麼阿最後會給你看的是真實的我們呃現在在做的東西
我們做的第一個 demo 是 dictation
也就是我剛才講在九零年代的做語音的人就想的就是我要用 p  c 嘛所以我就是用語音輸入嘛那最好我就是不要不用鍵盤打了我是就用嘴巴講就好了
這語音輸入所以呢那時候我們稱之為聽寫就是 dictation
那我們現在所 demo 這個是我們在一九九六年做的喔
剛好是十年以前了阿
那麼在那個年代是認為這個很重要
不過我們九六年以後就沒有再進一步去改進它
因為我們知道慢慢離開 p  c 時代它不重要了
那麼所以你今天看十年前的東西十年之內沒有改進它所以現在看是只是一個玩具而已阿
這個是在 category 它的它的這個麥克風的輸入的狀況
今天天氣非常好
早上我到學校去
遇見了我的老師
老師和我說
我的期中考
考的非常好
國立台灣大學
電機資訊學院
電機工程學系
有一門課
叫做數位語音處理
ya  ok 這個是九六年的技術了不過你可以了了解這個看起來大概是這樣阿 ok 哈哈哈哈哈哈哈
ok 好這是第一個 demo
那麼第二個 demo 是語音合成
這也這個是我們九八年做的阿
呃語音合成也就是說你給給你任何一段文字那你要把它的聲音轉成聲音念出來
那我們這個用的是這個 corpus  based
也就是說我的呃找一位播音員他念了十多小時的文字的文字跟聲音之後
分析所有的文字跟聲音之間的對應關係
然後你今天跟我任何一段文字我就配合那個關係把它的聲音做出來你就聽到它的聲音
那這個的效果呃事實也是一樣就說非常 depends  on 你的那個你的那個那個 database 要夠好
像人家現在做比這個好很多是因為他有幾百小時的聲音就把各種聲音的變化都在裡面了所以呢聽起來會更好聽
那現在我們那個時候因為在學校做我們不太可能有那麼多的 data
所以我們只有十十個小時左右所以聽起來就比較沒有那麼好聽
好我們來聽一下
ok 好這是第二個 demo 是這個語音合成
底下給各位看第三個 demo 是對話系統
這個是一個非常簡單的對話系統我我們是在一九九九年做的阿
這個對話系統是這個呃一零四查號台
但是呢一零四查號台是不容易做因為你要查千千萬萬個 user
所以呢我們只查這個呃金融機構
全台北市金融機構是的電話號碼總共兩千多個
就查那兩千多個
那麼
嗯我要找土地銀行
請問您是不是查土銀
是的
土銀本行請播二三六一三零二零
嗯我聽不清楚耶可以再說一次嗎
謝謝使用再見
老師要 demo 嗎
ok 我來試試看阿
開始
再來一次吧
查號台您好請問您要查那個金融機構呢
嗯我想查台灣銀行
台灣銀行本行請播二三一四六六九九
阿我聽不太清楚耶你能不能重講一次
台灣銀行本行請播二三一四六六九九
阿有沒有其他的分行還是分公司還是那一個部門的電話呢
請問您是查本企南企還是淡企
好吧那我就查南企好了
請問您是不是查南企
對的
南企只有信託部請撥二五零四三一零八
你是說二五零三一四零八嗎
請播二五零四三一零八
好那我能不能再找一下另外一個中國信託
請播二五零四三一零八
好吧謝謝
ok 好這是這個呃對話系統阿
好底下我們剛才講的這這都是屬於九零年代的比較早期的
那我們等於是就做一個 example 你大概了解我們在做這些東西
那底下看的一個 demo 是我們現在在做的或者是說過去幾年之內在做的東西
這個可以跳過去了這一頁前面這幾頁其實我們剛才都說了
ok 好
這一頁好了對那
阿我們在做怎樣的事情這是一個現在在做的一個研究的例子就是說
阿假設你現在上網看假設網站上有所有的所有的新聞
問題是說
那麼多的東西阿這個
如果你今天上網會發現那麼方便是因為所有的文件都是文字
你一眼看到它段落分明標題清楚
所以一目了然你馬上知道那個是你要那個是你不要
你就可以選擇你就可以 delete 掉你不要的東西
可是如果是都是變成多媒體的話其實是很難看的
因為你它沒有標題它沒有分段嘛
然後一大堆每一堆你都它都是影音訊息
你要從頭聽到尾是很累很累的事情
所以呢我們就是需要針對它的語音裡面所說的內涵概念主題來加以自動的了解跟重組
然後呢我們給它做自動的切割
譬如說它是一小時的新聞我們把它自動切成一則一則的
根據它的內涵來切割
然後每一小段有它的中心主題就每一則新聞有它的主題
根據它的內容來主題來分類
然後呢抽取每一小段裡面的專有名詞未知詞
然後呢我們來這個判斷它裡面的人事時地物
然後呢來做自動的摘要以及標題的設定
於是呢你的 user 就可以根據它的標題來選擇他要的東西
然後可以自己自己這個聽
呃先聽摘要看要不要聽確定要聽再聽全文阿等等
那我們底下來 ok 往下去
這一樣啦再下一頁
那所以呢我們這邊講其實是這樣子喔就是說我的聲音裡面是有很多專有名詞
是是真正描述這個的核心就是一些 key  term 專有名詞可能都只有聲音
那我要抓得到
那根據這個這是在 term  level 進入這 concept  level 根據它 concept 來分分成小段
抽它的人事時地物進入 summary  level 做它的 summary 做它的 title
有了標題之後呢我其實進入了 topic  level 分析它的主題阿等等
所以我們總共有這麼多東西分成這個
term  level  concept  level  summary  level 跟 topic  level 來做這些事
好下下底下
那在等一下在分主題的時候你會看到我們是這樣做的
我們現在做的這個例子是新聞阿
那麼電視新聞那電視新聞的話呢我們就是把所有的新聞那麼同樣的一則類似的主題的變成一群類似主題變成一群
然後把它呈現在一個 two  d 的平面上
如果這群的內容很多我們可以展開到下一群
所以這樣我得到一個 two  d 的這個 two  d 的一個 tree  structure 或者是一個多層的 map
那這樣的話呢就比較容易瀏覽
好底下
我們來舉一個例子先看一下一則新聞聽起來是怎樣的
霹靂遊俠男主角李麥克大衛霍索夫由於長年酗酒住進了勒戒所
大衛霍索夫的發言人茱蒂凱斯今天證實曾經主演霹靂遊俠海灘遊龍等熱門影集的大衛霍索夫已經住進了知名的貝帝福特中心希望改善自己長年酗酒的問題
上週八卦媒體國家尋問報報出了大衛霍索夫差點死亡的消息使得這名男星的健康問題成了媒體關注的焦點
與李麥克搭配的伙計霹靂車是當年霹靂遊俠影集的最大賣點
大衛霍索夫最近正積極為新版的霹靂遊俠電視影集催生希望再創演藝事業第二春
ok 這是一則新聞四十秒
那麼可是如果你不喜歡這則新聞就會覺得這實在有夠長有夠 boring
你如果給我聽的話我只要三句就夠了
好底下我們就給你一個三句的 summary
八零年代紅極一時的電視影集霹靂遊俠男主角李麥克大衛霍索夫由於長年酗酒住進了勒戒所
大衛霍索夫最近正積極為新版的霹靂遊俠電視影集催生
這是三句阿你可以看到它是從這邊前面抽了兩句後面一句
那這三句所加起來它的 concept
大致是跟原來的 concept 比較最接近的三句
那麼這樣的話呢就是可以節省時間至少你一聽就知道你要不要聽了
但是如果不喜歡這個人說唉呀你叫我聽三句也是 boring
其實你如果給我一個標題的話我一眼看到我就知道那個我不要我把它 delete 就好了
好我們就給它一個標題阿
這是自動產生的標題
那麼你可以想像要做自動的這個這個是自動產生的 summary 的話我們除了要辨識正確還要掌握它裡面的 concept 才能夠兜出一個最接近的 concept
那要產生這個標題呢是更不容易的事你要注意到這邊的這些都是專有名詞是詞典裡面所沒有的
這些音為什麼會兜成這個字這是不容易做的這些都專有名詞
然後我整個句子是相當通順的那這都很難做到
那阿但是當然有了標題之後好處是說我不喜歡我根本不要看我根本就 delete 掉呃
好這個是簡單的一個例子說明我們怎麼做好
底下我們來看
呃底下我們把剛才的這個標題跟呃摘要做在一個 database 上面這個 database 呢是有是在兩千零二年到兩千零三年錄的
那麼總共是一百二十小時七千則新聞
那麼都做同樣的事情
然後呢我們現在把它這個自動分類阿這個自動分類成為像國外政治阿國內財經阿等等等等分成這麼多類
那麼每一類有那些新聞呢我們可以看一下我們點一個譬如說國外政治
那每這裡面這麼多它都這是當時錄的時間
然後呢它們的這都是我們自動產生的標題這是阿自動產生標題
你如果用眼睛看一看大概可以發現它們大致都是國外的政治新聞
然後裡面大概都有一些專有名詞那大致去讀起來大致是都通順的阿
那我們稍微瀏覽一下就知道很多很多阿 ok 好我們回去
我們來看其中的一則譬如說我們每一則是它你如果按它的那個標題就可以聽全文按它的 summary 就可以聽它的 summary 我們按一個例子
譬如說這個這則半島電視台
九一一恐怖攻擊兩週年卡達半島電視台今天播出的蓋達組織首腦賓拉登同時半島電視台也播出了薩瓦里的錄音帶
ya 這個是這則的 summary 那如果你要聽全文就可以再按就是了我們就不聽了
那這樣的話你可以看到雖然是很多但是那那個 list 很長很長阿要去你要去找一則你要聽的也很難找阿
那麼因此怎麼辦呢那麼我們就做另外一個 map
那這就是國際新聞的 map
那麼我們每每一個群就是用它的 keyword 這讓裡面的 keyword 來做為 label
你大概了解它是幹嘛的
譬如說就這群而言你可以看的出來就像檢查人員聯合國安理會喔毀滅性的武器什麼等等
那你可以知道這就是這就聯合國安理會的那個武器檢查的 topic
那像這個是什麼呢你一看出來就是自殺炸彈阿巴勒斯坦阿這個約旦河以色列阿這個就是中東的等等等等
那我們如果這這是很大的一群我們覺得太多我們可以點到下一層去
那就是剛才的剛才的那一塊我現在分成這麼多變成九個
那你可以看到譬如譬如說這個的話呢這就是巴格達嘛有一個伊拉克阿自殺炸彈什麼巴士受傷什麼什麼
那這邊是什麼呢是中東和平嘛美國的鮑威爾約旦埃及的和平路線阿什麼什麼什麼
那這邊呢就是阿拉法特阿什麼什麼什麼等等
好那你現在如果要知道這裡面是些什麼新聞呢我可以再按進去
那麼這個時候其實是這一塊這個時候已經是我們那個 tree 的底最底層的 leaf  leaf  node 了它裡面包含這些新聞
那你看這些新聞的標題大概都是屬於這個以色列阿拉法特他們之間的這些事情呃
那我們也要在這裡聽也可以我們可以舉個例子我假設聽這裡面這一則我們聽一個 summary
以巴緊張情勢升高
巴勒斯坦激進分子在以色列境內發動了一連串的自殺炸彈攻擊事件之後
以色列間對巴勒斯坦領袖阿拉法特的總部發動了危險攻勢
這是兩千零二年九月二十號什麼的新聞阿
ok 好
那麼這個是這個我們稱為是這個 top  down  browsing 也就是說我現在是從這個 top 一直往下走然後一路看下去看它整個的這個來來瀏覽整個的新聞
那其實我不一定要 top  down 我我也可以 bottom  up
如果說是 user 心裡想說我要聽什麼新聞的話
好我們這底下來做這個 bottom  up 的這個搜尋
那麼我們現在可以輸入一個 user 想要說的話好
請幫我找以色列與阿拉法特
ok 這邊是找出來的一堆新聞同樣的呢我們也都有它自動產生的標題在這裡
所以呢你可以看到它是什麼新聞大概都有以色列阿有阿拉法特啦或者是跟中東這個以阿糾紛有關的都在這裡
那假設說 user 看到阿我要的是這一則阿拉法特反對以色列什麼什麼包圍條件什麼東西
我要的這則其它的其實是我不想要的
那這樣的話我就他就不需要這個從頭去去一則去他他就直接從這個 link 到剛才那個主題新聞
那就回到了剛才這個 map
那我們剛才這就這就其實就我們剛才聽到的那一則新聞
那很清楚的它你仔細看看這個內容就是完全都是跟它非常接近的相關的主題那也就是剛才裡面的這一塊
那麼所以呢如果真的要聽這個我就可以從這裡一路走去聽相關的
如果不夠的話我就上一層那那上一層的話呢剛才是在這裡
剛才是一塊那現在你可以聽這個旁邊跟這個相關的所以這邊有約旦河啦這邊中東和平等等可以聽這些
如果還不夠的話可以再往上走一層
再往上走一層那就是我們剛才看到全部的國際新聞阿等等
那這樣的話呢我現在這個我們稱之為 bottom  up 的 browsing 就是你從這個 tree 的底部往上去走所看到的東西阿
ok 好阿我們的 demo 就到這裡為止阿
阿喔對喔還有一個我忘掉了 ya 我們還有一個要 demo 的
這個是另外一個就是說呢你你了解就是說我們這個阿你今天如果從 google 去找一個什麼東西它馬上給你一大堆網頁的時候它是雖然有很多但是因為是文字的所以你很容易一眼就知道說喔那些是我要那我就選我要的就好
可你如果是這種東西的話你我沒辦法選吶我得一個個去聽阿
即使是一個去聽聽那個聽那個那個摘要也很累阿
所以有沒有更好的方法呢那我們也有一個那就是你可以我們自動把當場選出來的新聞就做一個 tree  structure
這樣你就可以看得更清楚阿好我們來做一個
請幫我找美國白宮
好這個是輸入這個要找美國白宮
那美國白宮相關的那這就是所有裡面有美國白宮的白宮做了很多奇奇怪怪的事情阿所以有很多很多阿
那麼你在這裡也許沒看到白宮是因為它的主題可能是別的嘛譬如說布希呼籲聯合國什麼什麼它裡面就有白宮啦阿等等等等
所以這個非常雜嘛很難看嘛那我們同時就為它做一個 tree
在這個 tree 裡面我們就看到大概譬如說第一層有幾個 node 一個是美國一個是華府一個是白宮一個是法國
啊那麼這裡面呢我們也可以要再美國底下還有布希華航台灣伊拉克什麼
那我們來看一個譬如說伊拉克好了
伊拉克會是什麼呢
喔在美國白宮裡面的伊拉克底下喔它又會有這個跟跟這個中東阿跟德國阿什麼聯合國有關等等等等
那那麼這個時候呢我們就可以看到說 ok 你如果要找的是是這個伊拉克的話我們其實這個可以這個找的到你要的是什麼
那如果真的要這個的話呢我們也可以輸入這個伊拉克做為第二個 keyword 於是我就可以再進一步的找就可以增加再再查尋一次
那現在查的呢就變成是美國白宮再加伊拉克
那我現在我們再來看一看這邊講的都是跟伊拉克有關阿的白宮等等的阿
那這邊呢這是新的一個 tree 就變成這個有有美國跟白宮然後裡面這些東西阿
ok 好我想這個是一個呃我們最最新在做的一個研究大概是這樣好我想我們的 demo 就到這裡為止
好底下我們來說 given 這些東西之後我們怎麼把它兜起來
我們先有一個初步的了解我們要做的事情是什麼假設有人說了一句話
這句話是這個 word  sequence  w  one  w  two 到 w  r
有人說這句話是這個 sequence 而我聽到的聲音是一個這個 feature  vector  sequence 就是他說的話呢我們就這個我抱歉我符號有一點前後不 consist 這就是我們前面講的那個 o 啦這就是我們之前講的 o
o  one  o  two 到 o 的大 T
我構成一個叫做大 O 這在我今天的第一堂課講是用這個符號講就是他就是每一個每一個 feature  vector
這是一個一個的 vector 那得到一系列的 vector 我叫做這個大 O
那我現在在這一頁裡面叫做 x 它是一樣的意思這個是我的聲音
所以當我聽到這堆聲音的時候我希望得到這個 word  sequence 的答案
那這個究竟是什麼呢你可以想像成就是這樣的一個 problem
就是我想要估計這個機率就 given 的這些聲音對不對這個 x 就是我的聲音嘛
given 這堆聲音之後我要找所有可能的 word  sequence  w
看看哪一個 word  sequence 給我的這個機率是最大的機率最大的那一個就是我的答案 ok
這是一個非常直覺的一個式子雖然它有非常嚴謹的理論基礎叫做 MAP  principle 這個我待會兒再解釋
我們先看我們不要看這個 MAP  principle 我們光看這個其實是一個非常直覺的答案
就是 given 我 observe 到這一堆聲音 x 我希望找所有可能的 w  word  sequence
我都可以算這個機率 given 這個聲音之後它是這個 word  sequence 的機率是多少
given 這個聲音的時候它是它是那個 word  sequence 的機率是多少我都可以算
然後我在所有可能的 word  sequence 找一個機率最大的那一個
機率最大這個最大那一個就是我要的答案 ok
所以這樣看起來這是一個非常直覺的一個式子
其實不需要什麼 MAP 來解釋也是可以的但是問題是這個機率怎麼求這個機率很難求
不過沒有關係我們要求這個機率常用的辦法就是這個式子
那這個式子其實就是我們你在機率學過的 Bayes  theorem 就是你要求 A  given  B 的機率的話如果這個不會求你可以倒過來
先求 B  given  A 然後乘上 A 除以 B 這樣子所得到的就是這個
那這個沒什麼特別應該很容易想像
那麼因此呢當我這個機率不會求的時候我可以倒過來先求我就把它倒過來就變成了是這個如果 given 這個聲這個 word  sequence 的話這個聲音的機率是多少
乘上這個 word  sequence 的機率除以這個聲音的機率變成這樣那這個只是這樣的倒過來而已
於是我現在要 maximize 這個東西就會變成 maximize 這個東西
那你再仔細看看這個式子裡面呢其實這個機率可以不要看
為什麼不要看因為我是 given 這個聲音我要找所有可能的 w 看哪一個 w 給我機率最大
對不對所以這個聲音是 given 的這已經 given 了
問題是不同的 w 給我的機率不一樣大嘛
所以我是要在不同的 w 裡面去找一個最大的
不是要看這一個這個是 given 這就是只有那一個
所以我這個可以不看當我這個可以不看的時候我就剩下這兩項
於是我就變成要求這個機率跟這個機率相乘最大
那一個 w 就是我們要的 w  ok
那麼這個你再仔細一看這個這個是什麼這個就是我們剛才所講的 language  model
我們剛才的 language  model 不是就是在算這件事嗎
就是一個 word  sequence 我就去算它的機率
所以呢我們剛才的 language  model 就給我這個機率了
那這個機率是什麼這個機率其實就是 Hidden  Markov  Model 啊
為什麼呢 Hidden  Markov  Model 有一個好處我們剛才沒有提到的
就是他很容易把小的 model 串起來變成大的 model
假設說假設說假設說這個是 s 然後呢這個是 ah 我就把它串起來這就是撒
我如果這邊後面再來一個我後面再有一個是 n 再把它串起來這就是三
ok 所以 Hidden  Markov  Model 很好的好處就是你雖然原來這是一個小 model 這是一個小 model 這是一個小 model 你只要把它裝上連結讓它可以跳過來那麼它就可以連起來
所以我可以把很多小的基本音串成 word 再把 word 串成句子都可以
因此呢我的這一個 word  sequence 也是一個可以變成一個這個 word  sequence 的 Hidden  Markov  Model
於是 given 那個 model 來算這個機率的話這就是我們剛才所講的 basic  problem  one 我現在符號有點不一致
你如果回過頭去看我們剛才的 Hidden  Markov  Model 的最後一張的的這個三個 basic  problem 的這個東西
不就是剛才那個嗎 given 某一個 model 我去算看到那個聲音的機率對不對
given 某一個 model 算看這個我們下週就會說怎麼算這個東西
那這個不就是我這邊講的這個是一樣的是一樣的 ok
所以這個是可以用 Hidden  Markov  Model 算的這是可以用 language  model 算的所以這根本就是可以算出來的
因此我是可以求這個 maximum 的 ok
那我這裡是完全用是個數學式子來看就是這個意思應該不難了解
就是我在 given 這個聲音的時候我要知道到底是哪一個 word  sequence 的話那我就看哪一個 word  sequence 給我最大的機率
那這個機率我把它倒過來那這個東西我不要看然後就看這兩個就是這兩個相乘
那在這個情形之下呢那就回到這張圖
這張圖其實就是在我們的第一週一點零裡面的那張圖的核心的那塊就是這樣子的
聲音進來我有 acoustic  model 就是在幫我算 Hidden  Markov  Model 分數
我有 language  model 就是在幫我算這個的分數
然後呢辭典告訴我說哪一些音兜起來變成哪一個詞就這些東西加起來我就在這邊找一個最大就是這件事
那麼因此呢這裡就是 acoustic  model 就是所有剛才講的譬如說s 有 s 的 model  ah 有 ah 的 model  n 有 n 的 model
這些東西呢就是所謂的他們都是以 Hidden  Markov  Model 方式存在這裡面就是所謂的 acoustic  model
那麼這些東西就是所謂的 basic  voice  units
每一個 unit 都有它的 Hidden  Markov  Model 這個字是什麼意思我們後面再講
以後後面會說到現在先不說了
那麼呢辭典告訴我說哪一些可以拼起來
告訴我說 ok 這個三其實是這個三它就是用這三個音拼起來的
那這就是一個辭典辭典裡面的事情就是一個 database 裡面所有可能的 words
然後呢每一個 word 告訴我是哪些個 basic  voice  unit 拼起來的這就是我的辭典
然後 language  model 就剛才講的我看看哪些 word 跟 word 會連起來等等
它就變成這個 ok
那這張圖的這個圖其實就是對應到我們之前講的這個在一點零裡面的那張圖
這個滑鼠非常不好用我不會用這個滑鼠
ok 好不在這裡是在哪裡
ok 應該是在這裡喔
yeah 這張圖喔
我們在一點零的這張圖裡面講的這塊嘛就是我剛才的剛才最後那張的下半就是這張圖嘛
那其實我們說了半天就是在說這張圖
那我的那些 H M M 那些 Hidden  Markov  Model 呢就是這些所有基本的音這些所有基本音存在這個裡面那我來判斷裡面到底是哪些基本的音
然後辭典告訴我說哪些個音湊起來會是什麼字等等
然後 language  model 告訴我說他們應該怎麼連起來我有 bi  gram  tri  gram 然後他們可以算它的機率
那這樣子的話呢那還有 front  end 呢就是我們剛才講怎麼樣算那個 M F C C 的參數
我進來怎麼算等等
然後剩下就在這一塊要怎麼做所以呢我們之前講的是這些事情
那麼我剛才還有一件事情沒有說的就是我想剩下的時間說一下
剛才講的那個 MAP 的 principle 稍微複雜一點
其實我們用很簡單的例子來看你就知道它是什麼意思
我們說如果有一個 random  variable 叫做 w 它是一個 random  variable
它可能有三個值只有三個值可能是 w  one  w  two 或者是 w 三
他描述今天的天氣 w  one 呢說今天是 sunny  w  two 說今天是 RAINY w 三說今天呢既不是 sunny 也不是 RAINY 那就是 cloudy
那我有一個 observation  x 它是譬如說有一堆參數是我可以觀測的天氣的參數譬如說氣溫的變化或者是說早上比起晚上來這個濕度的變化等等等等
這些東西是我的參我的 observation
那我的 problem 就是 given  observation  today 然後我去 estimate 天氣 tomorrow
我現在 given 今天所 observe 到的東西我要估計明天到底是晴天陰天還是下雨
那怎麼做這件事情呢我第一個我可以根據譬如說我現在是三月
我現在是三月我可以算出三月裡面晴天的機率是多少零點四五
雨天的機率是多少零點二零陰天的機率是多少就是零點三零
譬如說這樣子這個是我預先就知道的
可是你說我今天問我明天的機率是說少我也可以從這裡來算明天到底是什麼呢
看起來是這個機率最大我就說明天是晴天
我會對但是我也有可能錯對不對
我如果純粹從這個來看的話這個是我只是根據過去三月份晴陰雨的機率是這樣算的
那比較好的辦法呢其實我應該是算 observation
那怎麼來呢我就應該算這個東西
就是我如果 observe 到今天 observe 到這個之後明天會晴天的機率是多少
那麼我今天 observe  observe 到這個之後我明天是雨天的機率是多少
我今天 observe  observe 到這個之後明天是陰天的機率是多少
我其實算的如果是這個的話然後看誰最大
看誰最大那如果這樣看的話這個比較像嘛
對不對那這個時候我就把今天的 observation 算進去了
given 這個之後看明天的機率
可是這個怎麼算呢這有點不好算於是呢我怎麼辦呢
我可以把它倒過來這就是剛才講的是完全一樣的就這邊把它倒過來
於是呢我就可以變成我要算的是這個嘛
w  i  given  x  i 等於一二三嘛我就那邊的就是這個東西
那這個怎麼算呢我可以把它倒過來就是然後乘上 w  i 再除以 x
然後這個就這個一樣就把我剛才的那個那個那些機率其實就是這個機率
然後把它變成這樣倒過來這樣倒過來之後因為我是要在這三個裡面看哪個 w  i 最大
所以呢我就這個 x 是沒什麼關係的我不要看就看這個就好了
於是我就是算這個機率跟這個機率兩個相乘那這個機率呢不難算
為什麼呢
我只要算譬如說三月裡面是晴天的時候它前一天的 x  one  x  two  x 三等等
他們有一個 distribution
三月裡面如果是雨天的話它前一天的 x  one  x  n 也有一個 DISTRIBUTION
三月裡面如果是陰天的話它前它的前一天的也有一個 distribution
那我這個不難求啊這個不難求啊我只要去統計就好了
如果晴天的話前一天是怎樣雨天的話前一天是怎樣陰天的話這都可以求
那麼因此呢我現在現在 observe 到這個 x 的時候我可以把這個 x 放在這裡得到一個機率
放在這裡得到一個機率放在這裡得到一個機率
那就是這一項
然後呢這個是什麼呢這就是我們剛才講的我三月我也統計過晴天是零點四五雨天是零點二一這我也有啊
我把這個也弄進去當我這兩個都有的時候這兩個乘出來看誰大就表示誰這個大
那就表示誰這個大那就表示我 given 這個 observation 到底明天是什麼
那我這邊講的問題就是剛才的最後那一張抱歉現在要再轉回去很累
不過你去看我剛才的那一張的話的上面的所有的符號跟這裡是完全一樣的
我不跳過去了
就是我剛才的那裡的 word  sequence 就是這個 w
就是這個 w 那麼我這邊是只有三種可能但實際上我們 recognition 的時候這個有千萬種
我有千萬種 word  sequence 我要找一個所以這個對應到的就是這個
同樣呢我的聲音呢
我我我現在是只有是是這個 observation 相當我的聲音就是我的那個 x
等於 x  one  x  two 的那個就是這個 x
那於是呢我最後算出來的那個我們剛才說的是我算的這個嘛
就是這個 given  w 乘上這個用 language  model 算其實就是這個東西
那 language  model 算的這些東西呢就是所其實就是我們這邊的晴天陰天雨天各是機率是多少的意思
而這個用 Hidden  Markov  Model 算的呢就是這個東西
所以呢我這邊等於是用一個簡單的例子來來對應到我們剛才的那個 complicated  problem
那這裡面我們可以講幾個名字這種機率
這些個機率叫做 prior  probability
而這樣的機率呢叫做 A  posterior  probability
那麼這些字的意思 prior 的意思是事前的
這個呢是事後的
那麼所謂事前跟事後是你有所謂事後就是你 observe 到 x 之後你 observe 到 x 之後我的機率
事前就是我還沒有 observe 到 x 我完全根據過去的統計得到的機率
那麼因為這樣的關係呢我們剛才你看到我們上面有我現在不跳過去了
我那邊有一個所謂的 MAP  principle
什麼是 MAP 呢 MAP 就是 maximum  a  posterior 的 principle
就是我要 maximize 你看那個 MAP 的他所要 maximize 的就是這個東西
就是 given  x 要找出那個 w 的機率最大的那一個
那就是在 maximum  maximize 這個 A  posterior  possibility 所以呢叫做 MAP 就是 maximum  a  posterior  possibility
而在這裡面我們所用的這一個那也有一個名字
叫做這個呢或者是這個呢這是所謂的 likely  function
所謂的 likelihood  function 是指這個
就是在這個這個天氣之下或在這個情形之下我會 observe 到這個的機率的observe 到這個 likelihood 它的可能性這是 likely  function
那當然這個就是我們剛才講的 prior  probability
因此呢所謂的 language  model 所得到的分數是 prior  probability
也就是說你根本還沒有聽到聲音你就可以算的
這個是我根本沒有聽到聲音就可以算的是 prior  probability
而這個用 H M M 算的呢這個是 likely  function
是我如果聽到這個聲音的話呢它跟這個 word 之間的關係呢
跟這個 word  sequence 的關係呢是這個機率等等
那麼那麼呢這個就就你現在如果對應到我剛才的那張圖的話
我現在就跳不回去我就不跳回去了不過你你對到那個圖就知道我們這邊講的所有的東西那有的時候呢
我有一個簡單的辦法就是假設這個都是三分之一
晴陰雨各是假設我現在不知道這個我沒有 prior  probability 的話
假設我沒有 prior  probability 的話我可以假設他們都是三分之一
當他們都是三分之一個時候呢我要 maximize 這個東西的時候呢我可以不看它因為它都一樣
就剩下這個了我只要看這個就好了
那這就是所謂的 maximum  likelihood
你你如果不看這個假設我這個都是一樣的我不看了
我只看這個的話這個是 likely  function
所以這個時候我就叫做 maximum  likelihood 的 principle
因此呢在我們這裡我們剛才那一張講的我們這裡講的其實不是 make  a  likelihood
因為我們把 language  model 這個 prior  probability 算進去了
我即使沒聽到聲音我也知道這些 word  sequence 該有多少機率
在我是事前就可以算的這個機率我把這個算進去了
所以我得到的是 maximum 的 A  posterior 的 principle 就是所謂的 MAP
ok 那這些例子是比較簡單的你比較容易想到那對應到我們上面那張圖的東西
好我想我們上到這裡我倒是需要討論一下補課問題
歐我們的我想在我們這樣的一個 class 裡面要補課唯一的可能是 weekday 的晚上
or 是週末因此呢我們可能要算一下可能的補課時間
在 weekday 的晚上而言我可以補課的時間是星期二三跟五
如果要到 weekday 的話呢就是禮拜六了 ok
所以呢就是二的晚上三的晚上五的晚上
或者是六的六我們也許可以不要晚上了白天我想可能是這個時間
我們現在也許先不說是哪個禮拜
我們先看看有沒有哪一個時間是現在大家都 ok 的
我們很快看一下好不好禮拜二晚上有人不行的請舉手
有很多人不行禮拜三晚上有人不行的請舉手
禮拜四晚上阿禮拜五晚上有人不行的請舉手
三個晚上似乎都有人不行禮拜六下午不行的請舉手
禮拜六早上不行的請舉手我們似乎沒有 solution
如果沒有 solution 就比較麻煩一點
今天是三月七號下週十四下週二十一下週二十八然後是四月三號
四月四號十一號十八號按照校曆規定四月四號是溫書假是放假的
那當然有一個辦法我們就是原時原地補課如果我們沒有別的時間的話
那另外呢是這一週我出國三月二十一我出國
我比較希望在我出國前先補一次的話這樣我們進度比較不會落太遠
如果這個時候都不能補的話這又 delay 一次然後再下來問題很多
所以我會比較 prefer 這裡面我們先找一個可以的時間的話我們在這裡面補一次
這樣會比較好一點
otherwise 我們這個進度是非常糟糕
我們要不要再來一次這個如果你 really 不行才舉手好不好
因為這樣我們剛才已經顯然沒有 solution 了嘛
really 不行才舉手如果你是其實我可以調的
那盡量不要舉手這樣我們可以盡量看看可不可以找一個時間出來好不好
星期二晚上 really 不行的請舉手
我們現在先不說哪一個禮拜
我們先找出時間來好不好
星期二晚上是比較多人不行星期三晚上 really 不行的
比較少人一點我們看看幾個一二三四五六六位
星期五晚上 really 不行的一二三四五六也是六位
星期六下午 really 不行的只有一位
所以好像比較可行一點是不是有沒有辦法
星期六下午好像比較可以啦是不是現在只有一位星期六早上呢
星期六早上不行的一二兩位三位現在這裡可行性比較大嘛是不是
我們在星期六星期六下午這樣看起來是比較有沒有辦法可不可以
ok 如果我們這樣好了我們如果是星期六下午的話我們再來看
今天是三月七號所以是本週六是十一號下週六是十八號
不過下週六十八號我已經要上飛機了
所以如果要在我走以前補的禮拜六的話變成要在本週六囉
本週六行不行不行
那就變成必須要在 ok
那這樣好不好我們現在第一個第一個我們現在先 reserve 星期六的下午的時間
那麼現在是哪一個星期六呢我們再來 check 基本上本週六大家是不行的對不對
我們就就不排除本週六
那我要來 check 這個跟這個
好像我已經出發了
不知道這天我回來了沒有就是了
好像我還沒回來就到這兒來了
這個的話就變成可能要到這一天了這樣進度會會慢很多就是了
那當然另外一個狀況就是我們能不能在這一天原時原地補課
我想這個也許大家回去想一想好了
我想也許今天這樣看起來的話我們基本上是一個可能是禮拜六下午啦
可能會在這個時候或者這個時候看看啦
這個不行就到這兒來了啦就這樣子嘛
那一個就是這一天原時原地補課啦這是另外一種可能啦好不好
我們回去回去看一看時間但我想補課是少不了非補不可就是了
因為我除了這個出國我還要再出國一次
所以我們一定要補課就是了
ok 好今天上到這

我們上週二點零還剩最後一張沒有講
這一張其實就是在
我們在說二點零的時候呢
我們下課的時候是講到這個地方
我們就是說m a p的principle
也就是說given 一個word sequence given一段聲音進來 given一段聲音進來
那麼我們要想辦法找一個word sequence可以對應到他的也就是這個機率最大這就是m a p的principle
那麼我們的辦法是把這個這個這個sylable matrix 變成兩個機率在乘
那麼一個是independent 這就很簡單
我們上週下課的時候在說這件事情
那麼我們可以把它看成是一個很簡單的example 就是今天天氣
那麼我們說這樣一來的話我現在就有這個用hidden markov model來算acoustic model
然後用language model來算你可以把他們所有的mu 都
這件事情這件事情我們如果用一個比較簡單的例子來講就是我剛才的那張
那麼在那張圖裡面的我們是以國語的聲音為例
假設說我現在輸入一段聲音
問題是這段聲音我們不知道你有幾個字也不知道哪裡是你到底是哪一個syllable boundery 一個語音的boundery哪裡我也不知道
所以我只能用一堆acoustic model一些基本的音譬如說ㄅㄆㄇㄈ或者是ㄚㄨ一ㄟㄨ去兜它 given你會得到這樣的情形
舉例來講這個第第一個字第一個音我不曉得是從這裡到這裡呢還是到這裡還是到這裡
一個很簡單的例子譬如說如果我的第一個字是金
但是也很可能是雞
我第一個字是向不曉得是西跟樣呢還是向這個我都不知道嘛
那麼因此呢你就會發生說像這樣的情形我的第一個音可能是從這裡到這裡也可能是到這裡也可能是到這裡
如果是到這裡的話那第二個音從這裡開始如果是但是也可能到這裡也可能到這裡也可能到這裡
如果是第一個音到這裡的話那第二個音也可能從這兒開始等等
對每一個音而言又不一定你譬如說你說他是ㄐ好了他跟ㄑ很像他跟ㄒ很像他跟ㄓ很像那麼你你不能確定他到底是哪一個
所以呢即使是這一段而言我也不知道他究竟是哪一個我有好幾個 candidate
如果是這一段呢我也有好幾個 candidate 等等那這樣子呢我雖然有了 HMM 這基本的音的 model 我都有
可是我得到的是這樣的一個很複雜的一個我們稱之為 Syllable  Lattice
也就是說我的 syllable 可以是我這些都是 candidate 他可能是這個可能是這個可能是這個可能是這個可能是這個可能是這個等等每一個裡面有很多種可能的 candidate
這樣的一個 Syllable  Lattice 其實不太容易往下怎麼走因此我們第二步就是要一個詞典
這個辭典告訴我說雖然有這麼多音都可以但是呢他們可以湊成一個詞的沒那麼多
舉例來講呢也許如果這是一個音這個就這個音而言的話他裡面的某一個音的 candidate 跟下一個音的某一個 candidate 確實可以兜成兩個譬如說這是今這是天今天是一個詞
那同理呢如果這個詞真的是到這裡的話呢那這底下我又有兩個音可以兜成另外一個雙字詞等等
可是當然這不是唯一的可能我也許呢這裡也可以兜我如果這裡有一個音的話呢這個跟另外這個呢也可以兜成另外一個雙字詞
如果這是第一個雙字詞的話呢那從這裡開始也有另一個音可以兜下個雙字詞等等
那麼於是呢我這邊就得到一堆可能的詞由詞典兜起來這個呢我們稱之為 Word  Graph
那有了 Word  Graph 之後顯然我們還沒有答案我的答案要從哪兒來要從 language  model 來
於是我就可以算如果是這個詞接這個詞等等呢我可以算他的 n  gram 如果是這個詞接這個詞我也可以算他的 n  gram 等等
那到時候看他的 n  gram 到底誰大等等
那這樣的話我們就會得到這個 n  gram 的 language  model 分數跟 acoustic 分數然後通通兜在一起就是我們剛剛那個式子從那裡面來判斷哪個才是答案
那這裡我們講的這個這個整個的 search 的過程等於是這樣子的
那這裡所謂的這個 one  path 的意思是說你這裡我們後面在八點零還會詳細說這個
不過基本上在這個 case 的話可以假設最理想的狀況就是從頭一路走下來
當我這個聲音一路進來的時候我一路同時用 acoustic  model 去比對於是得到這些 Syllable  La 這個 Syllable  Lattice 一路建起來
建到這個地方的時候呢我這個 Word  Graph 就開始一路建起來
那麼等到這一路建下去呢我這邊就會 language  model 就會出來然後就會選這些詞等等
那麼等到這一句講完的時候我答案就出來
如果這樣我只要一步就可以走完這是最理想的 one  path  search
那當然如果這樣的話那需要的計算量是非常大而你中間這個程式寫起來是非常 challenging 的
那麼但是這個是做的到的你一路走下來一路就最後全部出來那這是所謂的 one  path  search
那這個大概解釋我們上週最後在講的事情
那麼在底下的這一張是在不是在上面的這一張是我們歐不對那我還有一張是在這個後的最後一張是在講這個這二點零的 reference
