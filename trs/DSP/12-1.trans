ok 以上是我們上週結束前下課前講的十一點零
底下今天我們主要的工作是要進入十二點零
十二點零的主軸是這個language model 的處理
那麼這裡面一個重要的方法是所謂l s a 
就是latent semantic analysis 
那我們來說這個東西
那其實這個在幹嘛其實還是一樣在做我們這邊所講的adaptation 
只是說我現在從acoustic model 的adaptation 調變成language model 的adaptation 
那麼什麼是adaptation 
就是要調它
調到你所它所適合的某一個條件
那我們我們之前在講speaker adaptation 的時候是說
我這個個hidden markov model 裡面的每一個gao 每一個state 裡面的這些mean 
每一個state 裡面的的這些mean 我都要調
因為如果是從針對某一個speaker 的話
針對某一個speaker 的話它的這些東西都會不一樣
不同的speaker variance會不一樣的
所以我最好要調這些東西
調到針對每一個人
這是我們上週所說的十一點零所說的speaker adaptation 
那我們今天在這十二點零所說的其實是language model 的 
那麼language model 有什麼好調的呢
因為你說今天後面接天氣
這個好像不管誰說都一樣嘛
這個language model 是在講這件事情
譬如說這是bi gram 
那麼今天天氣後面接好這是tri gram 
你如果這樣看這個是誰說都一樣所以language model 的的adaptation 不是為了調不同的人
而是調什麼呢調不同的domain 
不同的topic 
也就是說我們之前說過
在講language model 的時候我們說過
你如果是在談氣象的跟談政治的
用的詞彙是不一樣的
談體育的談財經的
用的詞彙是不一樣的
因此它的句型也是不一樣的
因此你如果今天當我在談不同的topic 
或者談不同的concept 
你如果講的是不同的concept 或者不同的topic 的話
我用的詞彙會不一樣
那麼它們的n gram 是不一樣的
那麼我們們在講六點零的時候曾經說過
那麼你如果是財經新聞或者體育新聞或者是這個嗯政治新聞
它們的n gram 是不一樣的
那我們現在講的是這件事
所以我language model 也是需要調的
但是不是在調不同的speaker 
是在調不同的topic 
如果我知道你是在講哪個topic 的話我應該要調到那個topic 去
然後使用那個topic 的language model 
這是所謂的language model adaptation 
那麼language model adaptation 有很多種的方法
正如我們十一點零說adapt這個speaker adaptation 也有非常多的方法一樣
這個也是一個很重要的主題有非常多的方法
那我們這邊所講的只是是其中之一
那麼倒不見得說這是最有效喔不見得
但是它是最可以算少數最general 
可以apply 到很多地方去
它等於是一種基本的的觀念
然後從它可以衍伸出很多東西出來
然後它也可以apply 到很多種不同的的應用上去
所以它有它的重要性
所以我們來說這這一個
並不表示language model 它就一定要用它
它有很多別的方法
那麼嗯我們所謂的linguist processing 也不限於是指language model adaptation 
你知道所謂的linguist processing 就是我們講的的在linguist 那一層所做的任何處理
包括詞字句的各種分析處理
都是我們所謂的linguist processing 
那麼舉一個例子
我們在六點零說過詞分群
因為詞可以分群之後做class space 的language model 
那怎麼分群呢
它的的分群本身就是一種linguist processing 
那麼所以linguist processing 只是只要在linguist 層次
那麼做的任何事情都是我們所謂的linguist processing 
那麼這個l s a 呢也是其中的一種方法就是了
那麼什麼是這個latent semantic analysis 呢
這個latent 這個字的意思我們已經從前看過了
就是潛藏的
那semantic 是什麼意思semantic 是它的語意
也就是指你裡面真正exactly 的意思是什麼
那麼因此你其實是在講一句話的時候我們希望知道它真正潛藏在裡面的意思
那些意思其實就是我們所說的是什麼topic 
跟什麼concept 
所以其實是要再分析是什麼concept 跟什麼topic 啊
這是我們這段要說的事情
那這段我這段大部分講的東西是based on 第一篇
那麼我底下主要以這個為基礎
那它算算是寫得最完整清楚的是這篇
那麼後面是些什麼我後面會再解釋
那麼這個的想法是怎樣呢我們也許用一個簡單的說法來講
那麼你今天如果說在一句話裡面看到布希
你大概會猜說它大概是講跟美國政府有關的東西
你如果看到一個另外一個詞是白宮
大概也是跟這些事情有關的
那因此你不管是布希或者是白宮可能是在講類似的東西
那你看到另外一個詞譬如說李安
那又是另外一件事它可能是跟電影或者是跟奧斯卡有關的等等
那麼你如果看到另外一件事情譬如說這個九二一
哦這是一件地震
這是又是另外一件事情等等
所以某一些詞彙可能告訴我那裡面講的concept 
或者說是嗯那個topic 
但是光是這些詞彙其實不容易讓我們了解憑什麼來分析這個
那在這個l s a 這裡
它想的辦法是說我另外找一堆文章
就是所謂的document 
那這邊呢我們可以說是word 
光是一堆words 
其實我們可以猜一堆word 是在告訴我某一些個concept 
只是我不太容易光用這些word 來分析
但是有一個很重要的東西存在就是document 
什麼document 
我上網去抓就可以抓千千萬萬篇文章下來
每一篇文章有它自己的concept 跟topic 
假設說我這個叫d one 這個叫d two 
我有我總共有大n 篇文章的話
那我有這麼多篇文章
那麼我就可以分析這些文章跟這些詞之間的關係
雖然每一篇文章我用人去看是可以說ok 這篇是在講什麼topic 
這篇在講什麼topic 
可是我我我如果不是人去看的話很難講它是什麼
正如這個詞一樣
我用人去看可以知道它是講什麼它是講什麼
可是如果沒有人去看的話我憑什麼分析呢
那它在l s a 這裡它想的辦法就是
我靠文章跟詞這兩件事情的相互關係來分析它們之間的關中間的topic 
怎麼講呢
譬如說如果是這這裡有幾篇文章都是在講美國政府什麼的話
它們可能都有白宮都有布希
那反過來呢這裡裡有幾篇文章是在講電影啊文化
搞不好他們都有李安
等等
所以呢你可以從這邊來看說誰有哪些詞誰沒有哪些詞
你也可以從這邊來看說這些詞在哪些文章裡面
你就靠這中間的關係來想辦法把它區分出它有哪些個concept 
因此呢我想辦法在中間找出一堆東西來
這些東西就是我們所謂的concept 
或者說是topic 
那它可能譬如說電影是其中一個
如果是電影的話
那就可能這邊就是李安
這邊就是有李安的文章
那如果是美國政府
那可能是另外一個觀念一個concept 
那它很可能這邊就有布希有白宮
那這邊就有這些文章章等等
那因此呢它等於說是我我在computer 我可以直接抓到data 是一堆詞跟一堆文章
那我可以算哪些詞在哪些文章裡面
哪些文章裡面有哪些詞
用這個關係去分析
抓出中間到底是哪些東西
那些東西就是topic 
那有了這個topic 我就知道今天如果這個人講話講了一堆話
應該是這個topic 的話
我就會猜它後面講的的詞還是電影有關的詞
那這個時候電影有關的詞的分數就可以跳高出來
如果那個人講的那堆事情我發現它是在講這些所以是在講這個的話
它再來應該會跳出都是跟美國政府有關的這些詞彙等等
那這個就是我們這邊所講的用l s a 來做language model adaptation 基本的觀念
也就是我們這裡要說的事情
那因為這樣的關係所以它現在做法就是
我要用用一堆word 跟一堆document 來建構中間的關係希望把這中間找出來
好有了這個背景的了解那我們現在來看這件事
它就是建一個所謂的word document matrix 
那是什麼呢
就是我這個matrix 每一個row 就是一個word 
就是這邊的每一個詞
然後每一個然後每一個docu 每一個column 呢就是一個document 
就是這邊所有的文章
那就變成一個matrix
所謂的word document matrix 
那講清楚一點的話就是我有一個辭典
這個辭典裡面有所有的詞
w one w two w i 到w 的大m 
其中大m 就是我的詞的總數
w i 就是第i 這個辭典裡面第i 個word 
那舉例來講譬如說大m 等於兩萬
假設我考慮一個兩萬詞的
這邊有有兩萬個詞所以這個matrix 是這個row 的數目是兩萬
然後呢我另外去上網抓了一大堆的文件
就所謂的document d one d two d j d n 
d j 是第j 個document 
那總共多少呢有大n 個
這大n 個呢就是譬如說說n 譬如說是十萬我抓了十萬篇出來
那就構成一個matrix 
那這個matrix 的裡面的每一個element w i j 是什麼呢
w i j 有一堆複雜的東西是這樣寫的
不過最核心的部分就是這個c i j 
c i j 是什麼
就是number of times w occurs in d j 
也就是說就這個word 而言就這個word而言它在這篇文章裡面出現幾次
我就可以數一下
那它在每一篇文章裡面出現都可以數一下
所以呢它在這裡出現幾次在這裡沒有出現這裡是零
這個word 是零這個是五十這是二這是三這等等
這樣我就可以把它全部排出來
所以我每一個word 出現在每一篇文章裡的次數給他排出來變成一個row 
換句話說你也可以看一個column 是什麼一個column 就是這篇document 裡面哪一個word 出現幾次
它不出他沒有出現就是零次
有就是一次兩次五次十次
這樣呢每一個這就是每一個document 
那你如果這樣子看的話基本上我這個c i j 就是最基本的一個這邊的word 對不對
這就是w one w two 到w m 
那麼這些個word 跟這些個d one d two 到d n 這些個最基本的關係
就是這個c i j 
就這個c i j 
只是說你如果光看這個c i j 
嗯其實它已經有相當有意義
因為你可以猜得出來如果這個word 跟這個word 譬如說一個是布希一個是白宮的話
它們這兩個row 可能很像
如果布希布希會出現在哪些文章裡面的時候
白宮可能同時會出現
那麼因此呢這個row 跟這個row 會很像就表示這兩個東西是蠻像的
反過來如果這個row 是布希那個row 是李安的話
那搞不好它們兩個row 完全不同
那麼它有的地方它沒有對不對
它是零的地方它有一堆數字
它是零的地方它有一堆數字
如果一個是布希一個是李安的話可能沒有什麼交集
等等
所以它們哪一個row 像不像
你其實在這裡已經看得出來了
那同理呢兩個column 的話也是這樣子
每一個column 代表一篇文章
如果這篇文章是在講奧斯卡
這篇文章是在講九一一恐怖攻擊
那顯然它們幾乎很少交集
除了有一篇有一部電影在演九一一恐怖攻擊之外它們幾乎沒有交集
那反過來呢如果這篇是在講恐怖攻擊那篇是在講在這個攻打伊拉克那搞不好這邊很有關係了喔等等
所以呢你這也是一樣你可以用它們出現的這個詞的頻率的的位置
就知道說誰跟誰比較像喔
所以呢這邊就是說每一個row 它是一個n dimension 的feature vector 代表每一個word 
每一個row 等於是一個那個word 的n dimension 的feature vector 
那這個只是說它每一個dimension 是什麼每一個dimension 就是相對於說它所有的文章
每一個dimension 是那相對於那篇文章的出現的次數
每一個column 呢反過來是每一個document 的feature vector 
等於在描述那篇document 它的特性
所以是它的column 的feature vector 
那麼然後呢它也一樣它是用每一個word 來做每一個dimension 的關係
可是如果你光這樣做的話其實是不夠的
那麼我們說除了c i j 之外呢我們還要做一堆這些東西
這些東西其實就是在做normalization 
我們希望把這這裡面光是這樣數的話其實有一些問題
所以我們要再做一些normalization 
第一個normalization 就是除以n j 
n j 是什麼
n j 是total number of words present in d j 
看這篇文章裡面有多少詞多少個word 
舉例來講假設說這個詞是陳水扁
他在這篇文章裡面出現兩次
在這篇文章裡面出現二十次
欸在這裡只有兩次在這邊只有有二十次那是不是表示一定這篇文章跟陳水扁的關係比較少
這篇文章跟陳水扁的關係比較大呢
不一定
要看這兩篇長短如何
對不對如果這篇文章總共才三十個詞
裡面有兩個是陳水扁
這篇文章很長有三萬個詞
裡面陳水扁才出現二十次的話
那誰跟陳水扁關係比較大
恐怕是這篇而不是那一篇
因為這篇可能很很短這篇可能很長啊
所以呢我們應該要對它的長度做一次normalization 
那就是這邊所做的事情ok 
所以斯 n j 是total number of words 
在那個文章裡面
所以我要除一除
那我這回才比較像了
所以我這回等於是說這個這個陳水扁在三萬個詞裡面出現佔百分之多少零點零三次的比例
這個在這三十三十個word 裡面出現百分之多少
那這個時候就比較有意義了
所以這個是這個除以n j 的意義
那麼前面還有這個一減epsilon i 是什麼東西呢這比較複雜一點
epsilon i 是這個式子
它是有t i 
t i 是什麼是c i j summation over j 
換句話說我是把剛才的這個c i j 對所有的j 加起來
橫的加起來
也就是說是等於是說total number of word present 
不是應該是total number of times 這個word i occurs in t 
t 是整個的document set 
也就是說呢我這個word 譬如說陳水扁
它在這邊出現幾次在這邊出現它在整個的十萬篇文章裡面出現了五千次
ok 那個五千就是t i 
就是我總共出現五千次
這五千次裡面呢
那麼我現在來看它的這個我用c i j 除以t i 呢
就代表說在這五千次裡面它在這篇文章占多少
這是什麼意思呢
那你如果仔細想一想
這個其實就是在算entropy 
那麼如果說這是那個matrix 
這是某一個word i 
這是某一個document d j 
這邊是它的次數是c i j 
所以呢我現在的這個ti 呢是把所有的c i j 全部加起來over j 
就是我這邊所有的數目全部加起來
如果這個是陳水扁的話
那麼它在所有的文章裡面總共出現五千次
在這裡出現二十次
那就是二十除以五千
在這邊出現五次就是五除以五千
等等
那它分別代表說
我這個word 在全部的word 裡面佔百分之多少
它有多少然後這邊是零零零等等
那等於它等於是某一個機率p i p j 的意思
等於是某某一種機率
如果這是這個c i j 除以t i 是某一種機率的話
那你看這個式子c i 這個機率乘以log 再乘以它這個就是我們講的entropy 
也就是summation 的p i log p i 
這不就是entropy 嘛
那這個entropy 是什麼意思呢
你可以想像某一個詞譬如說陳水扁
它會在我們如果這是d one d j 到d n 的話
它的這個機率會是怎麼分佈的
它可能會在很多文章裡面會出現一些
很多地方沒有出現
這個可能是跟選舉有關的
這個可能是跟外交有關的
這個可能是跟民進黨有關的
那這個可能是跟民進黨有關的這個可能是跟修憲有關的
但是還有一堆譬如說是什麼呢但是它就沒有了
那我如果是李安的話會是怎樣呢
它可能都沒有
只有某一些有
其它都沒有了
因為就是這些跟電影有關的才有它
否則就沒有了
如果是李安的話
那反過來我換另外一種詞譬如說非常
或者是今今天
如果換成這種詞的話會怎樣呢
很可能全部都有
那這裡面的這這個最極端的可能就是這個詞
的
如果是這個詞的話呢那全部都一樣
幾乎是完全相同的
全部都有
那因此呢這個p log p 的這個entro 這個代表什麼呢
其實就是它的分佈的情形
你可以看得出來這個其實是什麼這就是entropy 的差別
哪個entropy 最大
這個entropy 最大
然後呢這個entropy 比較小
這個entropy 最小
所以呢像entropy 最大的
就是這種非常的
這種東西其實不告訴我它是在講哪一個topic 
從從我們要分析它是講哪一個topic 的觀點來講
你如果碰到一個今天
其實沒有告訴我任何topic 
那麼因此呢這種東西我應該儘量不要算才對
那麼反過來我碰到一個李安
它非常清楚的告訴我它的topic 跟這個有關
那麼因此碰到這個的時候呢這個就很重要了
所以呢我可以用entropy 來判斷說他告訴我是哪一個topic 的重要的程度
那就是這個pi log pi 
也就是我們這邊的這個
c i j 除以除以ti 其實就是這個pi 嘛
那這個entropy 就告訴我這件事情
只不過呢我現在這個entropy 本身它的range 可以很大可以很小啊
怎麼辦
我就做一個normalization 
那你知道我現在總共的word 數目是大n 
所以呢如果這個機率完全相同的時候
是它的entropy 的上限
就就是log n 
這就這就是我entropy 的的極大值
所以呢我就除以log n 
當我除以log n 之後呢就會變成這個entropy 是介於一跟零之間了
ok 
所以前面除以log n 只是normalize 一次讓它變成介於零跟一之間
所以呢它叫做normalize entropy of 某一個word 在整個的corpus 裡面
在整個的document set 裡面
他顯示我的topic 的鑑別力
我們說如果是李安
這個鑑別力是非常明顯的表示它的topic 是電影
那麼如果這個word 是今天或者是的
它很明顯的沒有什麼鑑別力
它沒有告訴我任何topic 的訊息
喔這就是所謂index in power 
那麼因此如果你這樣做你就知道什麼時候這個epsilon i 會變成零
就是如果它只有一篇文章出現
假設某一件事情只有一篇文章有
其它的完全都沒有的話
那這個是entropy 最小最小的時候就是零
那麼這個這個如果存在的話可能是表示某件事情
譬如說這個某有一個科學上有一個新的發現
發現一個什麼什麼外太空有一個什麼星
只有一篇文章其他都還都還沒有任合人都還有在講那件事
那這個時候只有一篇文章有它
那如果你講的那個什麼什麼星座的話
那個顯然就exactly 就是指那件事了
所以它的鑑別力應是最大的
這個時候說epsilon 等於零的時候反而是鑑別力最大的
所以你你要用一來減
反過來呢什麼時候是epsilon 等於一呢就是真的就是的這個字
像的這個字的話呢就是每一篇都一樣有
那這個時候呢
我的就是它是等於這個t i 除以n 嘛
對每個都一樣
這個時候呢我的我的entropy 就是log n 
所以你一normalize 就是一
而這個東西一減一就變成零
那這些就是沒有鑑別力的像的這種東西
ok 所以呢我現在乘上這個一減epsilon i 是這樣的意思
那麼因此呢我們可以說是
我們雖然這裡的每一個element 是以c i j 為基礎
不過我們做了兩個normalization 
一個除以n j 呢等於是對於這個軸上面我們先做一次normalization 
然後呢這個一減epsilon i 呢
可以算是在這個軸上做一個normalization 
當我這兩個都做過之後
那這回它是比較清楚地描述
這個之間的關係我們底下要用這個來做了
那這個怎麼做呢在l s a 裡面
它的做法是拿來做一堆matrix 的運算
那麼這些matrix 運算是什麼呢我們來解釋一下
那麼這邊的想法其實非常接近我們上週說的eigen voice 裡面的p c a 
是很像的
所不同的地方我現在這個matrix 不是正方形的
你注意到我這個matrix 這邊是w one 這邊是w m 
這個m 是詞的總數
那縱軸是d one 到d n 
這個n 呢是文章的總數
沒有理由它們會一樣啊
因此它是一個長方形的matrix 
那麼我們上週講的那些個p c a 是一個正方形的matrix 你可以求eigen vector eigen value 
長方形的不能做了
那怎麼辦
我們可以這樣子做
就是把w 乘以w 的transpose 
這兩個一乘的話
就會變成是一個正方形
如果這個是w 
那這個是w 的transpose 
那這個是大m 乘以n 
這個是大n 乘以m 
那我乘出來就會變成一個什麼呢
變成一個正方形的m 乘以m 
當我變成一個正方形以後這就是這邊講的w w transpose 
變成一個正方形以後我又可以做這就是eigen vector 的分析
於是我就可以做eigen vector 的分析
就可以變成三個matrix 就跟上週一樣了
我變成三個matrix 相乘
這三個分別是什麼呢
我拆成三個之後
第一個是我的eigen vector 排起來的
e one e two 到e m 就是m 個eigen vector 
因為我現在是m 乘m 的matrix 嘛
所以我有m 大m 個eigen vector 
所以第一個row 就是我第一個eigen vector 
第二個row 就是我第二個eigen vector 等等等等
我可以排到第m 個eigen vector 
那這這m 個eigen vector 構成一個m 乘n 的matrix 
就是這個大u 的bar 
就是我這邊寫的這個東西
就是這個matrix 
那右邊這個也一樣你把它橫的排起來
這個就是e one 第一個eigen vector 
e two 就是它變成row 了
然後呢我有大m 個
把它這樣排
那這個就是大u 的transpose 
就是這一個
所以呢我這邊就是我的大u 
這就是我的大u 的transpose 
那中間是什麼呢中間就是eigen value 所構成的
那我們說呢它只剩下對角線有值
其它都是零
那對角線上的每一個值就是所謂的eigen value 
這個eigen value 我們現在故意把它寫成s i 的平方
所以譬如說第一個呢就是s one 的平方
第二個是s two 的平方等等等等
那為什麼寫成平方其實我們後面會有原因
不過我們現在先這樣寫
換句話說
你要把它的什麼是s one 
是它相對於第一個eigen vector 那個eigen value 的square root 
它的square root 叫做s one 
所以它的平方是它的eigen value 等等
我把每一個eigen value 都寫成他的寫成一個平方
然後我也按照大小順序排列排下來
這跟我們之前講的意思是完全一樣的
