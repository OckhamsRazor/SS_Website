那我現在到底是用什麼東西來判斷呢
那其實這個這個這想的非常好的辦法就是所謂的 phonetic  knowledge 
什麼是 phonetic  knowledge 就是去找語言學家
根據語言學家來說哪些東西會影響我的前後
哪些呢我們舉底下這個例子你比較了解
這是一個這是一個我們從前做中文的做國語的例子啦哦
就是假設我是要做ㄅ的第一個 state 
這是講ㄅ的第一個 state
啊 我這個例子不太好不一致
就是這是ㄅ的第一個 state 
這晡後面接ㄨ的後面接ㄨ的那個ㄅ的第一個 state 
這是一個 Hidden  Markov  model 的這第一個 state 
我把所有的這個ㄅ的 data 拿來我要用它來做一棵 tree 
這樣子那這個時候我用什麼樣的 criterion 
這都是從語言語言學裡面學來的
因為既然是這樣的話呢最主要的問題是這是ㄅ的第一個 state 
最影響它的是它到底左邊接什麼
左邊接什麼才是一個關鍵
那左邊的接什麼是會什麼呢
會影響的是語言學上的發音的道理
所以它左邊是一個母音嗎
還是子音
如果左邊是母音的話
那麼母音又可以分成這什麼什麼這是語音學上的分法
這是根據語音學裡面 你那個母音發音的時候 它的 發音的位置或者共振的位置或者什麼
它是在後面一點還是在低一點還是在遠一點這口這什麼什麼等等哦
那這些東西 really 就會影響
那你可以想得到就是說我的我我如果說是這個ㄅ的最左邊的這個這個ㄅ的第一個 state 的話會影響它其實最主要是左邊的
那麼它的左邊會是一哪怎麼樣的一種音
是母音還是子音是哪一種母音哪一種子音
然後那種母音那種子音它的口型會怎樣
它發聲會怎樣
那些 really 影響到這裡
所以呢它就去找所有的這種語言學上的這些區別
用這個來做這些個 question 
所以譬如說在這裡的話呢你先問什麼呢
它左邊是不是母音
如果左邊是母音的話就歸到這裡來
左邊不是母音的話呢那就是 silence 
那因為在在國語而言
這個ㄅ什麼東西前面不會有不是母音的嘛
那如果不是母音的話它它就是就是沒有就是 silence 
這就會變成這樣子
然後如如果左邊是母音的話那再來看三十說左邊是不是一個 low  vow 
如果是的話大概是屬於這幾個
如果不是的話是屬於這些
那這樣子一路再分下去
那麼看它左邊是什麼右邊什麼再看它是是屬於這個還是這個
於是分到最後就會發現
譬如說這兩個可以共用同樣一套
因為它們走到最後它的 distribution 是像的
那這兩個可以共用一套
因為走到最後它們是像的
那這個呢它不跟別人搞在一起它就自己一個 嗯
那它這個自己一個它跟別人那個等等
那這幾個都會都會很像就跟它們搞在一起哦等等
差不多是這樣的意思
那麼因此呢這樣你比較容易想像
那這樣子總共大概這個原來是最早是它們做英文
大概用了兩三百個 question 哦
就是說完全就是根據根據這個語言學家語音學的知識去分
左邊會是什麼右邊會是什麼
左邊有沒有什麼右邊有沒有什麼等等
這個語言學大概有兩三百個 question 
然後你就從這個一路走下來
那麼根據到底要從哪一個開始問起
就是看你問的是哪一個 question 我的 entropy 降得最多我就用那個 question 然後這樣一路走下來
那在這些 question 裡面其實都是比較簡單的
你如果仔細看的話這些 question 就是 yes 還是 no 
嗯是 yes 還是 no 所以是比較簡單
嗯是 yes 還是 no 所以呢只是說根據語言學的知識它它是不是什麼東西
嗯你如果是前面那個式子其實還更難
譬如說十二歲為什麼是十二歲
那其實你也可以是十三十四十五十六都可以嘛
十九八七六都可以嘛
所以你還要再選那個 threshold 是多少
像這裡你也是要選 threshold 是多少
嗯 那在這個例子反而不需要
因為在我們真正做的時候其實不需要
因為其實都是在根據就是不是 threshold 
只是在看說
你是不是哪一種具有哪一種左邊右邊是不是有哪一種特性的子音或者母音等等之類的東西
那這樣總共大概有兩三百個 question 
然後就用這個來做這個選擇
於是你可以想到這件事情其實是嗯它是 both  data  driven 跟 linguist  knowledge  driven 
它靠兩樣東西同時在操作
第一個是語言學的知識
因為你可以想像會影響它的 TRI PHONE 
左邊右邊會影響這個東西的其實是在它的左邊跟右邊是接在哪一種音
然後那個音的發音會怎樣
所以呢我就用我的 linguist  knowledge 來做我的 question  set 
然後一路來選
所以這個是 driven  by  linguist  knowledge 
但是另一個另一方面我是 driven  by  data 
因為根據現在我把這堆東西都都拿來的時候它是這麼亂的
然後我到底用哪一個 question 可以把它拆得比較清楚
我是根據 data  driven 
嗯所以根據這個算它的這個 entropy 來算
這是我在算它的 data  driven 
所以呢它是同時 data  driven 跟 linguist  knowledge 
我一面用人的知道一面用 data 這兩樣組合來建一個這樣的 tree 
那於是呢我這個 tree 呢就從頭相 開始向下走
那我用所有的 available  data 
就是譬如說我現在要 train 這個 
train 這個啊的最後這個 state 的所有 data 我都拿來
然後就開始用它的這個長像非常亂的一個長相開始來算它的 entropy 
然後根據這些 question 來看它怎麼怎麼建這棵 tree 
那麼然後呢我一路長這個 tree 呢
當然我最後我要有一個這個停止長 tree 的一個停止再 split 下去的一個 criterion 
分到哪裡應該要停住呢
那當然你可以想到第一個就是entropy  reduction 不小到一個程度對不對
我在哪兒再分下去 entropy 沒有沒有再變小的話就不必要再分了
entropy 的 reduction 小到一個程度就不必了
另外呢就是我的 data 量少到一個程度也不必了
對你可以 define 我這個 data 量嘛 對不對
我的 sample 如果少到一個程度
已經不足以 train 出一個東西來當然也就不要了
所以呢你可以這裡用這個方式來 define 你的這個這個這個 stop  criterion 
於是呢這棵 tree 會長到哪裡
在什麼地方停住
在什麼地方停住是 depends  on 在那個地方的狀況
看它的 data 的量看它還會不會再降低 entropy 等等
於是呢當你這個 tree 長好之後
所有的 unseen  TRI PHONE 你就從頭開始
這個這個延著這個 tree 向下走就好了
走到哪裡就歸誰
我我我現在這個長的這個這邊都是看到的 data 
我這個 training  data 看到哪些
我把看到的 data 拿來放在這裡 然後讓它一路長下來對不對
我讓一路長長完這都看到的
那還有一堆一大堆沒有看到我們剛才講有一半的 TRI PHONE 沒有看到怎麼辦
沒有看到的譬如說這個左邊要這個右邊要這個
嗯就是沒有
沒有怎麼辦
我就讓它從這上面往下走
那每一個地方因為它都它它它的問題都是說我是左邊是怎麼樣的右邊這個都是語言學的知識
所以我可以根據語言的知道來判斷它應該往哪邊走
所以我就可以
雖然沒有看到沒有 data 的 TRI PHONE 
我完全根據這棵 tree 上面的語言學的知識
就走走到它該走的地方
最後就是這個 traversal across tree  by  answering  the  questions  leading  to  the  most  appropriate  state  distribution 
你凡是沒有的
沒有看到的你就你就根據它左邊跟右邊的語言學知識來走
走到哪裡就歸那裡
你就用那個 data 當成那個 TRI PHONE 
就這樣子
所以呢這就是所有的 unseen  TRI PHONE 都有位置都有 model 的個辦法
嗯 那麼嗯這樣子做的話呢
我的 Gaussian  mixture 最後呢就是
哦 就是凡是有相同的 linguist  property 的就會 tie  together  sharing  the  same  data 然後 same  parameter 
就是說你你到到時候這一堆會會長在一起嘛
就像譬如說這兩個就會長在一起
那沒有什麼理由
就是一方面就是我我如果這樣一路走過來的話
表示說它們左邊右邊的這些音
它們的這個 linguist 特性就是很像的
那一方面呢那如果是這樣的話呢那是有理由說是這個
一方面你也等於是說它們的純度是最純的
或者說它們本來就長得像
那那那麼因此呢這麼一來的話呢
那麼這些個 Gaussian 就是所謂的 tie  together 
所謂 tie  together 就是我用同樣的 training  data 最後 train 出這一組來
最後 train 出一組來
那麼這個時候呢就是所謂的 tie  together 的意思
那麼因此呢它們就是用相同的 training  data 來 train 
所以是 sharing 用相最後就用相同的 parameter 啊
這就是 sharing 的意思
所以這樣的時候到到了最後 leaf  node 
到了 leaf  node 這裡的每一每一組 Gaussian 
它們都是一組大家一起 share 的
那麼這是這個一個非常簡單的解釋
那其實要做這個是很有學問
怎麼樣 train 得好還是有很多很多進一步的問題
舉一個例子來講 
tree  PRUNING 就是說你這個 tree 
你有的時候長長得太長得太茂密了
可能最後分得太細不見得好的時候
你有一些 criterion 把它砍掉一點
把它砍掉一點讓它這個比較不要那麼茂密
麼可能效果會更好
這是所謂的 tree  PRUNING 
另外呢你的 question 也可以是所謂的 composite  question 
所謂 composite  question 就是像這樣子嘛
假設你可以把這個這個是 and 這個是 or 嘛
嗯你可以把左邊又是這樣右邊呢又不能那樣這有一個 這個 bar就是否定嘛 對不對
左邊要這樣右邊不能那樣這是 and 
然後呢或者是嗯左邊是這樣這都可以
你這樣就變成一個 composite  question 
那你也可以用這類方式來做
所以這中間的學問還有很多
那我想我們這邊並沒有詳細的講
我這邊只是把一些基本的原理大概我們大概解釋
大概是這樣一回事
那這個 TRI PHONE 的這樣的方式的的 train 是最重要的主流
也就是說我們今天絕大多數的最成功的系統都是用這個方法來 train 的啊
那這個嗯到這裡為止我們大致把這個最主要的 HMM 的 training 的大概都已經說完
那麼我們其實沒有說得很清楚
我們大概只是選擇裡面幾個重要部分把它說得清楚一點
然後還有很多中間有一些地方是沒有說清楚的
那不過嗯我想應該 OK 
我們在後面大概下週或者下下週會給各位做第一題習題
第一習題就就是在做這些事情
那這個習題倒是這你不用擔心這個程式要寫起來不得了
不用你寫因為都有現成的 toolkit 哦
所以你只要是用這個現成這個去上網 download 下來
然後這個主要的那個怎麼操作那個 toolkit 
那麼大部分的的助教都已經告訴你怎麼做
所以不會太難做
你如果認真得去做那一次的話
大概就會了解中間所有東西
現在看是還有很多問題因為很多功能其實沒有講得那麼清楚
因為真的要講清楚是是太複雜
那我覺得我們也不太可能真的把每樣都講得那麼清楚
但是後面就會給你一個這個習題
你如果認真去做一次你大概就會知道裡面所有東西 
OK 好我們今天就講到這裡
ok 我們補課的時候在講的事情就是
怎麼樣做這個tri phone 的training 
那麼做tri phone 最大的問題就是有一堆unseen event 我們說過就是因為有很多個unseen 的tri phone
你所需要的很多東西其實根本就沒有data
所以呢必須要跟別人去share 
那麼share 的辦法呢就是用我們這邊講的這個這個tree 的結構來做這件事情
而這個tree 的發展的這個過程之中呢我們就是讓它每一步都找一個有最大的這個entropy 的變化的那個地方的那個question 來分這個tree 等等
那這個基本原則是來自我們前面講的這個information theory 裡面的entropy 等等
那這部分詳細的這原始paper 是這一篇
所以你如果想詳細地看可以看這篇
那在這本課本裡面這一段其實也在講這件事情是一樣的
那所以呢你這邊都這邊都可以看得到
那前面這邊它是在講一些關於我們這邊說的一些嗯像是這個phoneme 啦
像這些個co articulation 等等的現象它有一些說明所以都是蠻不錯的一些reference
是可以參考的
那麼到這裡為止呢我們大概把嗯所有的h m m 怎麼train 
然後這些東西我們等於是講完一次
那這些東西其那你如果仔細想一想我們其實從第三四點零開始
整個的h m m training
從頭到尾其實是一個非常複雜的過程
中間很多東西譬如說我們講講這個basic problem 三是在講這個用用這個iterative forward backward algorithm 讓它作微調能夠train 得更好
我們後面有講另外一個是segmental k means 是其實怎麼做initialization 等等
那我們這邊講的tri phone 是另外一塊
那我們並沒有真的足夠的時間把整個都講那麼清楚
我等於只是挑裡面的幾塊把它說清楚而已
那你自己可以去想中間怎麼link 起來
那有些地方沒有說得很清楚
那麼我們後面會給各位一個習題
就是把這個東西做一次
那嗯你不用擔心如果這個習題如果這個這個程式如果你自己寫的話會寫死人
這個這個程式複雜到難以想像的程度
不過現在都沒什麼問題因為我們都有現成的工具
所以其實你只要用那個那個工具都可以download 下來
自己上網就可以download 下來然後你就自己可以train 
那嗯所以呢我不曉得今天助教會不會準備好如果準備好的話今天就會給你那個習題
那這個你嗯你只要從頭到尾仔細得走一次
那它的那一整套的htk 的程式就是所謂的我們給你用的htk 是今天國際語音界最普遍使用的一套程式就是h m m toolkit 
那它有一整套的manual 
很厚你如果把它印出來是很厚一本畫一樣
它詳細說裡面所有東西怎麼樣怎麼樣怎麼樣
你如果真的下工夫的話你想要真的弄清楚的話仔細走一遍
裡面所有東西
就其實是可以很清楚裡面所有程序
如果你沒那麼有興趣的話就照我們習題裡面給你的那些scrip 走一次
大概也會了解中間的程序
那嗯應該是這樣講就是說
我們今天在做這樣的事情的時候
那麼如果打個比喻的話好像是我們要蓋一棟房子
那麼當我們蓋一棟房子的時候
嗯如果說是譬如說我們需要有冷氣
我們就去冷氣行裡挑一個我們需要的冷氣放在這裡就拿來用了
我們需要一個爐子我就去爐子的店去買一個爐子來放就可以用了
我不需要從頭去
自己去做冷氣怎麼做把它做好然後爐子怎麼做把它做好其實是不需要
我們做的事情是蓋房子
因此呢像這類基本的程式
嗯我們可以不要自己寫我們都不要自己寫
然後都用現成的
然後這個但是我們要做的事情是要如何把這些現成的工具兜起來做到我們想做的事情
所以這是我們現在的工作是是這樣子
這個跟十多年前是不一樣的十多年前我們做語音的時候因為沒有這種東西所以我們每個學生都要自己寫一套h m m 的程式
那時候很累很累那現在都不需要了這是我們要做的事在不同就是了
那嗯這個htk 的這個程式是這個原始程式是英國的劍橋大學的學生寫的
英國的劍橋他們在九零年代的時候開始很認真地做hidden markov model 
那麼嗯他們覺得做得不錯他們也就去參加美國的這個比賽
結果一比美國人看不起他們覺得他們一定不會
美國人認為h m m 是他們發明的
認為英國人一定很爛一定不會
結果一比果然是最後一名
嗯結果大家就笑說啊英國人實在很爛
結果英國人回去就很認真地從頭寫這個程式
第二年再去比賽就是第一名
打敗美國所有的團隊
包括i b m 啊包括這些a t and t 全部都被打敗
然後它變成第一名
從那個以後這個嗯美國人也就承認英國人這方面最厲害
所以後來他們就把他們的程式變成一套變成一套軟體
當時開了一家公司是賣這套軟體的很貴
當時htk 一套大概是換算成新台弊是幾十萬才買得到
那後來到了九零年代的末期的時候
這個微軟把這家公司買下來
然後就把這個把它這個軟體放在網路上讓大家使用
所以後來這個程式就變成大家都可以用
那我們用的就是這套
那這個那當然事實上微軟放在網路上的是htk 裡面的最基本的版本
是最基本的版本所以這個嗯不是最好的版本
最好的最有效率最快的真正可以拿去賣錢的可以這個的
那一套它其實留著沒有放在網站上
那放在網站上的是效率不太好的
但是是夠夠用的就是了
所以呢這個雖然慢效率不好
但是是是夠豐富所有我們想要做的事情都可以做
所以我們也用這個來做為這個嗯習題的教材
所以如果either 是本週or 是下週
但是因為發現下週又放假了我們實在是很頭大
好容易才把進度補起來
下週又放假了所以呢我比較希望今天如果助教趕得出來就今天把這個習題給你們
