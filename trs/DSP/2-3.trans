好當我有了這些之後這是一個簡單的描述講 H  M  M 是什麼
那麼這裡面最大的特徵應該有兩個
第一個就是他的 state 是跳來跳去的
這個這些都是 random 的
我們並沒有規定他幾個 observation 之後會跳到哪一個 state 去沒有規定
他只是有一個機率會跳而已這是一個機率
他只是有一個機率會跳而已他沒有一定要怎麼跳法
所以這個是 random 的
那麼因此呢你假設這個這是一個零的 model 的話
零裡面到底那一個 s  tate 會怎麼跳我們其實是不知道的因為每一個人講的零都不一樣
同一個人講兩次你可以講零可以講零也可以講零都不一樣
所以呢我們說這個是一個 random 的
第二個 random 是說呢
我 given 在哪一個 state 裡面在這一個 state 裡面他的 observation 仍然不是固定的仍然是一個 distribution
這就是我們底下所說的雙重的
雙重的 double  layer  stochastic  process 他有雙重的這個這個隨機的特性
第一層就是我的 state 本身是 random  transition
為什麼 for  time  wrapping
這個 time  wrapping 我們剛才講過就是
我的發一個聲音的時候我每一個每一個音段的長短是很 random 的
就是說我們剛才講過你假設這個是 san  FRANCISCO
裡面的 s 到底有多長然後 fran 每一個音到底多長是不一定的
你如果把每一個對應到某一個 state 去的話
他的這個每一個每一個 event 每一個 acoustic  event 長短都是可伸可縮可長可短的
那因此我在時間軸上都每一個 event 都可以伸縮的
那這個特性就是所謂的這個這兩個字就是 time 的 wrapping 這兩個字的意思
那我就用那個 state  transition  PROBABILITY 來描述這現象
因此我的 state 本身是所謂的 hidden  state
什麼叫 hidden  state 你在想意思就是說我其實只 observe 到這個
我並不真的知道他在誰在那個 state 裡面我只是假設他在他這裡他在他這裡
但是其實譬如說 o 四 o 五一定要在這裡嗎
他不能在這裡嗎當然也可以我們剛才說只是機率不一樣而已
你可能把他放在這邊來的時候發現這個機率比較小
把他放到這邊機率比較大就這樣子而已
那麼我其實永遠不知道到底誰在哪個 state 裡面
所以這個 state 本身是隱藏起來的我其實沒有看到
那就是我們所謂 hidden  state 的意思這是第一層的 random
第二層的 random 是說呢即使你知道他是哪一個 state 的話
他真的會長怎麼樣還是不知道那就是我們剛才講的這件事
就是我們剛才講的
即使你知道他是在這裡面這個 o 一 o 二 o 三會長怎麼樣仍然不知道
我只知道他的一個 distribution
所以呢我我即使知道這個 state 我也只知道他長那樣
至於長那樣到底是怎樣會是哪一個我是不知道的
所以呢就是這邊講的即使是
知道哪一個 state 我其實仍然有一個 random 的 output 我是不知道
所以是雙重的 stochastic
那麼第一層是說我在這裡是 random 的第二層是說即使在那裡我這邊仍然是 random 的
好那麼在這個情形之下
那麼喔我們真正會怎麼作我們舉例來講
如果我要辨識零到九的十個聲音
我就是為零建一個 model 一建一個 model 每一個都建一個
怎麼建我要有夠多的 training  data 說零這個是零這個是零
我有夠多譬如說有十個人各唸十個零就有一百個零
把那一百個零拿來計算
然後可以求出這裡面所有的參數那那就是零的 model 的參數
有很多參數包括你的每一個 state 裡面的這個 A  I  J 的機率是多少
那這裡面 b 最多了因為我的每一個 gaussian 有他的 mean 有他的 covariance 對不對
我們舉例來講這個我每一個 gaussian
每一個 gaussian 我要算他的 mean
假設這一個 gaussian 的話他的 mean 在這裡
還有他的 covariance 對不對
covariance 可以看成是他的在每一個 dimension 他的肥度就相當於這個這種東西但是我現在有很多很多個
假設我現在是三十九維的話這個 n 是三十九
我的 mean 就有三十九個 mean
covariance 有多少個有三十九乘以三十九個
喔所以這裡非常多個參數這是一個 gaussian 然後我可以有一把 gaussian
所以呢這個參數非常多
我可以把一大堆的零拿來來 train 出零的零的所有的參數那就是這個零的 model 等等
那麼於是呢我怎麼作辨識
我如果有一大堆我可以為零建一個 model
我們現在一個一個 ma 一個 model 我們就用一個 LAMBDA 代表
零的 model 一的 model 二的 model  k 的 model
總共到九的 model
那這時候進來一個新的聲音某一個未知的聲音進來譬如說我們用就是像這種東西我們用一個大 O 來代表
或者是甚麼呃我這邊是用 ok 用大 O 來代表
假設我進了一個新的聲音大 O 的話他就是一堆 observation  vector
那我就算這個東西
算我這個大 O
如果 given 他是零的話機率是多少
如果他是 k 的話機率是多少
然後看誰的機率最大他就是誰
舉例來講我零有零的 model 一有一的 model 二有二的 model 八有八的 model 九有九的 model
今天我如果進來這個聲音是八的話
八的聲音放到零的 model 的話這個機率會很低我一樣可以放進去的
一樣可以放進去因為你永遠可以把前面的若干個 vector 放在第一個 state 裡面
這個仍然放在第二個 state 裡面這當然可以
問題是如果這個聲音是八那個 model 是零的話你放進去機率都不太對嘛
你就會都會放在一些一些 gaussian 的邊緣他的機率都很小
反過來如果那個 model 那個聲音是八
你放在八的 model 裡面的話呢那他們就會都對上於是我的機率就會比較大
於是你的就會分別就會掉在 gaussian 比較靠近中間的地方機率就會大
因此呢我現在一個未知聲音進來我就把他放在每一個 model 裡面
去算這個機率機率最大的那個就是我的答案齁
這是用 hidden  markov  model 來作辨識一個最簡單的解釋
因為這樣子的關係所以我們這邊就會有
一系列的三個 basic  problem 要解的
那麼這個詳細的解法我們事實上就會在下週以及下下週
的兩次上課裡面我們會講這些 problem 怎麼解
那這些 problem 不容易解因為你要把之前之前所有的數學通通用進來
齁那麼然後呢不過我們到時候就會發現其實也還好因為其實我們並不真的用數學解他
而是用 computer 去解他
我們到時候這三個 problem 都是變成一個 iteration 的程式
經過好幾個 iteration 之後答案就出來了啊
不過這個 iteration 的過程是用這些機率來算的就是了
那第一個 problem 就是我們剛才講的所謂的 evaluation  problem 就是在算這個機率
給我一個八的聲音那放在八的 model 裡面應該是機率最高的
放在五還是三的 model 裡面應該是機率是很低的
我用這個來作 recognition 齁所以這是第一個 problem
那這個怎麼算就是我們下週會講的怎麼算這個東西
第二個 decoding  problem 是說假設你這個聲音是八而且我這個 model 也是八的話
那到底我的 state  sequence 是什麼
到底哪幾個 vector 放在第一個 state 裡面哪幾個放在第二個裡面才是最合理的一個 state  sequence
ok 就是我們剛才講的這個問題那麼其實你永遠不知道他在哪裡他在哪裡
那麼因此呢你就只能夠這個這個找出一個比較好的說這些是他這些是他那就是我的 sequence
所以呢第二個 problem 就是說
你如果知道他是八而且這個是八的話那麼到底哪些放在哪個 state 裡面這是 decode  problem
第三個是 learning  problem 就是我怎麼 train 這個 model
假設說這個 model 是八
我也知道這個聲音是八了
這是一個新的聲音我知道他是八那我想把這個新的聲音的八 train 到這個裡面去讓這個學到他的聲音
所以呢我要想辦法作這件事情就是讓調這個調這個 lambda 讓我的這個新的八放進去之後機率能夠調到最大
也就是說讓我這個這個 lambda 裡面的所有參數學了這個新的聲音之後
他的參數會被調一調之後使的我這個新的聲音放進去之後會機率會變大
那這個呢就是所謂的 learning  problem 就我這個 model 要不斷的學習
根據新的聲音進來我要不斷的學習齁
喔那這是我們講的三個 basic  problem
ok 好我們先停在這邊休息十分鐘
OK 呃有一件事情要說一下我們需要討論補課的時間
不過我想我們等一下在十二點下課的時候我們再來討論呃
呃也就是說我們上週放掉一週我們整個進度 delay
那麼我發現下四月初還有一次所謂的溫書假還要放一次那呃
我其實如果我禮拜一知道我我如果第一週知道上週會停課的話我就會希望上週最好是可以是原時間原時原地補課啊
因為否則我們我們現在是是進度 delay 的很厲害然後呃本學期我還會出國兩次
所以會有兩週要停課所以我們的進度是有嚴重的問題
那呃所以我們需要找這個補課的時間不過我想我們在呃下十二點下課之後再來討論
我們先來講這邊的就是說我們說剛才有三個 basic  problem
這個詳細的我們從下週會講這兩個再下週會講這一個
那基本上就是要用剛才我們的這一堆數學的 formulation 來解這些 problem
所以呢你這些數學符號他的意思要弄清楚否則下週下下週你就會聽不進去了
那麼那我們到時候就會知道其實這些這些 problem 的 solution 都是 ITERATION 的程式就寫程式就可以了倒不需要寫的這麼詳細嗯
那麼我們再補充一件事情就是有有很多課本用這個比喻我覺得也不錯他就是說你可以想像一個狀況假設有三個桶子
三個或者五個桶都可以啦假設我有三個桶一桶二桶三
裡邊有一堆不同顏色的球
譬如說有這個紅的綠的黃的
譬如說這些有一些紅球有一些黃球有一些綠球阿白球好了等等
那麼你這三個桶子裡面的各自的紅綠白的數數目比例不一樣
因此你 random 裡面拿一個球的話拿紅色或者綠色或者黃色的白的球的話這三個桶的機率是不同的
那你今天如果有一個有一個幕把這全部擋起來
一個人躲在幕後他可以 random 選任何一個桶抽任何一個球然後告訴你說我的是紅的
然後待會兒呢之後他又可以再 random 再選另一個桶再抽另外一個球他說我這個是黃的
那麼於是呢你就會得到他聽到他說紅的黃的黃的綠的綠的紅的紅的黃的
但是呢你不知道他到底是從哪一個桶抽哪一個球出來
那這個比喻就是我們這邊講的這這個呃這 state 是完全一樣的那每一個桶就是一個 state
那所謂 hidden  state 就是說你其實是抽球的那個人躲在幕後你並不知道他是從哪一個桶裡面抽的球
然後呢他這次是抽這個桶下次是可以抽另外一個桶的你也不知道所以這就 state  one 相當於這邊的這三個 state
然後在每一個 state 裡面到底紅的綠的機率是多少你是不知道的那麼他們就是不一樣就對了
那就相當於我們這邊所說的他這每一個 state
這好像是三個桶他的每一個桶裡面的每一個就是不一樣就是了
那麼你並不知道他是從哪一個出來的今天你得到某一個得到某一個 O  T 的時候
他可以是在這裡也可以是在這裡也可以是在這裡只是機率不一樣而已就像你得到一個紅球他可以是從這裡出來從這裡出來從這裡出來是一樣的只是機率不同而已
那麼如果這個人可以 random 的隨便抓任何個桶來抽的話就相當於我們這邊講的這個有這個 random  state 的 transaction
ok 那然後呢即使他知道哪一個桶之後他仍然不知只知道是哪一個桶的話你仍然不知道會抽出哪一個桶來哪哪一種顏色球來就相當於我們說的這個 B  B 這個東西呢也是一個機率
那如果這樣子講的話你比較容易想像
這所謂的 Hidden  Markov  Models 的意思
什麼叫 hidden 就是他躲在幕後
他在幕後去做這些事情
你只知道紅
紅紅黃白白這樣你只你只知道這些事情
這是我的 observation  sequence  O  t
那我我 observe 到的是這個但是我並不知道這個幕後是怎樣的
在我們的整個 model 裡面你其實只只知道這個
你並不知道他到底是從哪個 state 在哪兒跳的你是看不到的對所以這些都是 hidden 的那這就是這個 H  M  M 的意思
