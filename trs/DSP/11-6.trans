其實跟剛才是一樣的
就是因為powerpoint 我沒有辦法同時呈現兩張
接不起來
所以這一塊其實就是剛才的底下這塊是完全一樣拷過來而已
那這個時候呢這個我的新的那個k k 的space 就是我所謂的eigen voice space 
也就是說我現在不再需要考慮這個四百八十萬維的大空間了
我只要考慮這個五十維的小空間
這五十維的小空間的每一個點也就是原來的那個點
只不過原來那個呢是y 
我現在到這邊的呢是x 
那我們說呢y 呢等於a 的transpose x 
所以呢它們有一個直接的one to one 的mapping 的關係就是這個關係
所以我現在只要在這個上面考慮就行了
那這裡的每一點
其實呢我也只要我把它inverse 回去就可以算出x 來
所以每一點的y 呢我都可以對應到x 
而那個x 呢就有四百八十萬個值
它就相當於所有的phone model 的mean 
對不對我那個x 找出來之後
就相當於那那這些個值就對應到這個mean 
這個值就對應到這個mean 等等
因此呢我這四百四百八四百八十個一出來的話
我其實就已經這個model 就已經有了
好因此呢我現在只要考慮這個我現在只要考慮這個五十維的空間
在這五十維的空間裡面就是我們所謂的eigen voice space 
就是由這些eigen vector 所展開的
然後呢我現在每一點
其實代表整套的phone model 對不對
就是我們這邊講的因為這裡的每一點是y 
y 都可以對應到x 的關係是這個
所以你當然也可以我用這個的inverse 去乘的話
就可以得到x 嘛
那x 就是這上面的點
那這上面的點是四百八十萬維的
所以就對應到所有的phone model 的所有的mean 都在那裡了
所以呢每這上面的每一點
其實都代表整套phone model 的參數
那麼因此呢這個呢等於說是那這個k 個eigen vector 其實代表最重要的speaker 的特性都在那裡了
那我是怎麼求出來的我是用很多的training speaker 
他們的大量的training data 所train 出來的那些一大堆的phone model 
然後得到了這一堆
那這個的每一個呢就代表了我的最重要的speaker 的特性
什麼叫做最重要的speaker 的特性呢
那麼他們研究結果譬如說第一個e one 
你可以猜e one 是什麼e one 就是男生跟女生
你的第一個e one 的vector 
它的一面就是一面就是男的一面就是女的
然後越是越是粗粗厚的男生就是e one 的值越從這邊跑
越是嬌細的女生聲音越往這邊跑
那基本上幾乎就是一半男生一半女生
當然你有的時候有一點點不同
有一些男生的聲音很嬌細的就會跑到這邊來
有些女生聲音很粗厚會跑到這邊來是會
不過基本上這個幾乎就是e one 就是男生跟女生
這是通常我們本來如果你把一群speaker 去分兩群的話通常就會分成男的女的喔
那這個是speaker 最明顯的區別就是這個性別
那其實e one 就是性別
那同理e two e 三大概都可以找到一些物理意義
那麼因此呢這些就是最具有最重要的speaker 的特性就在裡面
那麼因此你大概有五十個到兩百五十個之間你這個space 幾乎就是所有的speaker 在這裡了
那你如果這樣想的話呢
每一個新的speaker 也可以在這邊找到一點
現在一個這這一千個speaker 都是training 的speaker 
train 好這個model 之後
一個新的speaker 來了
他講了一句話
那怎麼辦
我就根據那個speaker 的那一句話想辦法locate 他在這裡他在這個eigen voice space 裡面的哪裡
如果他是這一點的話
那我就可以同樣地用這個inverse 回去
就知道喔它原來是這個上面的這一點
如果是那一點的話呢那一點是四百八十萬維的
於是就已經告訴我所有的這個它的所有的mean 是什麼都有了ok 
那這就是它的基本精神
所以呢一個新的speaker 進來
我只要在這個eigen space 裡面找到一點就是了
那這個eigen space 裡面所以一個新的新的speaker 進來
我就想辦法在這個兩百五十維裡面找到它的那一點
那那一點怎麼找
那一點就是a i e i 嘛
就是我現在有每一個eigen vector 
分別找一個相對於那一個的coefficient 
然後做一個linear combination 
a i e i 就得到我的y 嘛
因此我要找的就是這五十個a i 
那這五十個a i 怎麼找
maximum likelihood 
一樣我用這個式子
那也就是說呢我現在你可以想像是我我只要找到這五十個a i 的話
a i e i 就可以得到我的在這上面的這一點
那這一點呢又根據這個transformation 我就知道它是在四百八十萬維上那一點
就得到一全套的得一到全套的的所有的phone model 
那因此呢given 這堆的model 的話呢
那麼我會看到現在它講的這句話的機率最大的會是哪一個值
所以我還是一樣根據user 說的這句話新的speaker 進來講的這句話
我根據這句話
我要找這一點
怎麼找就是找這些個a i 的值
such that 這些a i e i 加起來之後
所對應到這一點對應到那一點之後的那一那一點所對應的那四百八十萬個model 
四百八十萬的值的那些個model 呢裡面會看到這個的機率是最大的
那這個呢這一樣又是maximum likelihood 這就是likelihood function 嘛
對我就得到maximum likelihood 
然後我怎麼求這個東西呢用em 
還是用em 
所以這個em 是很重要的我們後面會說這個em 
那麼這樣一來的話呢我就是要找所有的a i 
裡面使得這個機率最大的
使得這個likelihood 最大的那一個那組a i 找到的話
那就是我要的a i 
這組a i 一找到
我就把這個a i 對應回去
a i e i 就可以得到這個y 
有了這個y 我就可以對應回去得到x 
有了這個x 我就有了他全套的聲音
again 這裡我有一大堆unseen 的聲音我都一起找到了
這邊雖然只有少數這幾個音
這裡只有這一堆這一堆phone model 聽到而已
我只有聽到這堆phone 
可是根據這堆phone 我找到這堆a i 的時候
a i 所對應的這一點跟這一點
可不是只有這些phone 
而這個是對應到所有的phone 
所以呢所有的unseen 的model 一起看到
那這個就是eigen voice 基本精神
那麼也就是說呢我現在是
我只需要small number of parameter 
這些a i 
這個a one 到a k 就是這k 個a i 的值
我只要有這些個值的話呢已經就足夠讓我可以specify 整個的整個的speaker 
因為我把所有的model 全部算出來了
那這樣子的話呢我可以只需要很少量的data 我就可以很快速地調過去
你只要講第一句話
你只要講第一句話第二句話
這邊就已經非常清楚告訴我這些東西的a i 是什麼值
我就對應到就出來了
所以結果你所得到的情形呢是
比剛才這個如果我現在這個圖上來看的話呢
你可以想像的這個eigen 這個eigen voice 的的斜率是更高的
這個是我們的這個eigen voice 的話
它的斜率是更高的
就是因為我只要最少的你譬如說只有五十個或者說兩百五十個
這個參數非常少
所以呢我只要很少量的data 
就可以讓我把這五十個a i 
或者兩百五十個a i 找到之後
我就可以很快地調過來
那麼因此呢它是一個比起來是它的速度比剛才那些都快
我只要很少量的data 
所謂的rapid 的意思
這個快速是指我需要的data 少
你只要講少數幾句話
我就整套全部學到了好
那麼只需要very limited quanity of training data 我就可以調得很好
那但是呢這也有一個缺點
它是saturate at low accuracy 它一樣同樣同樣的問題
這邊雖然很好
斜率是是最快的
但是呢它又有同樣的問題就是我會又會在更低的地方saturate 
為什麼會在更低的地方saturate 呢
這個最大的問題應該是說因為我的too few free parameter 
我現在參數只有兩百五十個嘛
那我等於說用這兩百五十個或者五十個參數
要對應到四百八十萬個參數去
所以這個matrix 本身的精確度是是一個問題嘛
對不對我現在這邊只有兩百五十個
可是我這邊要對應到四百八十萬個去
所以這個matrix 的精確度是不容易做得很好嘛
那麼因此呢我這邊太少只有只有五十個或者兩百五十個
當你data 再多的時候它有沒有會更好呢不見得了
因為你的data 再多的話你這中間不夠好的話你就好不了了
所以呢它有同樣的問題就是performance 會saturate at lower accuracy 
好因為我too few free parameters 
這是它的一個限制
那既然是這樣於是就有人想說我其實也可以用我們上面所說的tree structure 
或者是這個分群的方法
喔沒錯他們後來他們就有人做了tree structure 的分群的方法
我也一樣地可以把這個做成一個這樣子的結構
就是tree structure 的結構嗯
那當我的data 越來越多的時候
我變成一個一個的
我我的這個這個sub space 變成一個一個的
變成更精細的
我data 少的時候我就只有一個
data 多的時候我就拆成很多個
他們也可以這樣做
你如果這樣做的話呢那這個就可以saturation 這個就會上去嘛喔等等
那這些我們就不講了你如果興趣你自己去找找reference 可以找得到
所以呢這是它的基本上的limitation 
不過也有可以克服它的辦法
讓它的saturation 向上移動
那當然它的eigen voice 還有一個很大的限制就是說它的所需要的計算量跟memory 跟training data 都是比較大的
那你可以想像我要我要做一個夠好的covariance matrix 
需要譬如說一千個speaker 或者多少個
所以我的需要的training data 也是比較多的
然後我要做四百八十萬維的這個pca 
這個計算量是很大的
memory 也是夠大的喔
所以基本上的cost 是比較高的
但它有它的很精采的地方就它可以做一個這樣子的的的結果
使得我可以用很少量的很少量的這個speaker 的聲音
我就可以很快地調回去
就可以由這一個這個點對應到那邊那個點去可以就對應到那四百八十萬個參數去
喔這個觀念是相當相當值得學習的
那這裡面我們再如果再回過頭去看一下剛才這裡的話呢
其實你也有改改進它的空間
就譬如說呢你你這個vector 
不一定要是這四百八十萬維
你也可以用別的來做
譬如說我可以用m l l r 裡面的a 跟b 來做
那也就是說
那這個的point 是說
我們剛才講這裡很大的一個問題就是你這你這兩百五十維
你要對應到這四百八十萬維
所以中間這個transformation 會變成要要要這個transformation 不容易做得做得精確
所以最好這邊不要那麼多嘛
不要這麼多的辦法呢就是我改用a 跟b 
你記得我們我們上一上一堂課講的m l l r 裡面
我把這個空間分成一群一群
這是c one 裡面有a one 跟b one 
這是c two 裡面有a two b two 的等等
那我現在不要拿這些東西來做這個vector 
我用這個東西來做
可不可以也可以
那其實這些a one b one a two b two 其實代表的也是那個speaker 嘛
對不對你如果給我一個x i 的speaker independent model 的話
你給我a one b one a two b two a 三b 三
其實也就一樣define 了那一組vector 
那所以我就不要用這麼多了我就用這個a one b one 
那這樣就少了很多喔
所以呢我也可以用這個方式
就是這個嗯我用這個在這裡嗯就是m l l r 裡面的a 跟b 的column 
譬如說這個這個a 的matrix 我就把這一個一個排起來
也可以
那這樣的話它就不會有四百八十萬維
也許只有譬如說一萬維或者多少
我的dimension 可以大為縮小或者只有五千維呀什麼的
那這樣子的話我比較做起來會比較好做
而且也比較克服一些困難喔等等
那這些都是可以做的空間
那嗯這個是我們這邊講的
那它的基本精神你現在大概可以了解
那我們等於用pca 的方法
把每一個speaker 本來一個speaker 有他的model 
有一大堆參數
那那些參數不管怎樣
不管你是用它的mean 還是用它的a 跟b 
總之排成一個很大的vector 
那我現在呢把這個這個多維的高維的vector reduce 到一個很低維的空間來
靠什麼用pca 的方法
是我想辦法去找那些個dimension 它的variance 最大
是我想辦法去找那些個dimension
它的所有的統這個變化都在這上面呈現了
所以呢我那麼多的變化我就在一個很小的五十維的空間裡面呈現了
就是這個東西
那麼於是呢我就變成一個小的space 
就是我的eigen voice space 
於是呢我現在的每一個speaker 是這上面的一個點
就是training speaker 一二三每一個就是這些點
同樣地每一點你都可以想像是一個speaker 
那新的speaker 進來也就是這裡面的一個點
所以每一個新的speaker 我只要找到它的coefficient a i 
就可以了
那些a i 就對應上這些點我就對應到這全部的東西
那a i 怎麼做
用em 做maximum likelihood 可以得到
那這就是這個eigen voice 基本精神
那你如果要詳細看的話
eigen voice 的原始的paper 是再下來這篇喔
就是這一篇
那兩千年
那這個這個裡面有詳細說
by the way 我這邊講的這這幾個東西都是用em train 的
所以呢在這些paper 裡面你看到一堆數學
看不懂它在說什麼的時候其實那堆就是在講em 
那麼所以呢等到我們講到em 你那堆就會看懂喔
那就像我們這邊講的這裡
我要求這個怎麼求
它會有一大堆數學那堆數學其實就是em 
那然後呢就用那個就可以求出來等等
那這個是講這個eigen voice 
好那我們這邊講的所有的這些都是很好的期末報告的題目
我下週會再講一下期末報告的的規定是怎樣怎麼做
不過基本上就是說嗯你可以完全用讀paper 然後就寫這個reading report 
因為因為paper 很多你可以去找你只要根據一個
光是這一個題目你就可以找到一堆
喔譬如說eigen eigen voice 
你就找你從這個去找的話你就會有有有一堆
你光是看這些就可以就做也可以
那當然你要寫程式也可以你可以做程式
然後可以做這個computer 的這個報告也可以
那當你在做computer 的報告的話
你可能你的data 不夠
你可以用我們所提供的data 就是嗯習題
習題都會給你很多data 嘛你可以用那個習題data 來做
但是習題給你的data 不見得一定適合你要做的題目
譬如說如果要做這種speaker 的題目的話
你需要有很多不同的speaker 
每一個speaker 的量要夠多
然後我才可以做這些事情
那我們給你的data 不見得符合的時候
你可以跟助教討論
那麼在可能範圍之內我們會請助教提供你這些個data 
喔所以你要做程式的也可以
那這些東西都是可以做報告的題目
