好
有了l d a 之後那後面就一樣了我也一樣做這件事
這張圖其實跟我們上面講的那個圖是意思是一樣的
那麼跟我們那邊講的是一樣的
你所謂的這個temporal filter 
就這個c one 變成一個signal 要做一個filter 這件事情呢
你可以想像成是
這個這個convolution 其實就是在把這幾個分別乘上一個coefficient 加起來嘛
所以你就是把這一堆這四個點這四個點這四個點都都當成是一個n dimension 的這些這些點
然後你在這上面去做l d a 我現在是做
剛才這邊是做p c a 我現在是可以做l d a 嘛
喔那等等
所以這邊圖畫的意思是一樣只是說我ok 我這個這個signal c one 當成是一個signal 
然後呢你如果這三個一起乘上一個coefficient 的話呢
這相當於是output filter 之後的第一個output 
你這三個乘上下一個的話呢是第二個output 
這三個再乘上下一個是第三個output 等等所以這個其實就是在做convolution 
那你如果這樣來看的話呢
我做l d a 跟剛才p c a 有何不同呢就是我要先分好class 
假設我要辨識零到九的十個音的話
哪些是零的
我把那些個點點在這裡
哪些是一的我點在這裡
我通通都要分開來
我都要分好class 就是了
所以呢我的這些data 在做l d a 之前我要先分好class 
哪些是一哪些是二哪些是三
然後它們分別知道class 之後
我就可以來做l d a 
然後我把那個eigen value 最大的那個eigen vector 來做我的第一個軸
也就是做我的那個filter 
那這樣得到的結果呢會比p c a 更好
那畫出來仍然是很像的
不過不太一樣就是了
那仍然是這樣大致在幾個hertz 之內的這個range 是最重要的
然後別的部分把它濾掉一些
那得到會更會得到更好的結果
那不論是p c a 或者l d a 來做的話呢
一個特點就是這裡的每一個filter 都不一樣
c one 有c one 的filter c two 有c two 的filter 每一個都不一樣因為它們各自有它們自己的的統計特性都不同
所以你得到的都不一樣就是了
那這個是這個用l d a 來做的temporal filter 
那跟剛才p c a 唯一不同就是我用l d a 
那跟這個很像的還有用m c e 去做的m c e 是我在九點零講完e m 之後要講的
這個我大概下週會講
那麼所以這個呢就留到後面再說
