這個其實l 的x lambda 就是嗯就是底下的這個l 的x lambda  
就是就是這個東西 
也就是我我我那邊寫的那個式子是同樣的意思啦喔 
就是這個意思 
就是這個東西我要把它minimize 
minimize 調什麼參數 
調調所有model 的參數 
這裡面的每一個b c d e 每一個都有它的model  
譬如說譬如說這個v 有它的model  
就是a b pi  
這個是v 的那個model  
那t 呢有t 的model  
a b pi  
有有t 的model  
每一個都有每一個都有一大堆參數 
那所有的參數一起調 
所以這是一個不容易調的東西 
因為我有我整個的model set 這整個大lambda 是所有的model set  
這個大model 這個大lambda 是所有的model 的model set 所以很多很多參數 
我要一起同時調所有的參數 
讓我這個東西會minimum  
這個minimum 的時候其實就表示說我讓它的competing 的分數拉開 
competing 的分數越拉的開那個個分數那個那個就越低嘛 
所以我就把它minimize  
那這個的過程呢我們用底下這句話是最重要的一句話 
就是說這樣的情形之下我每一個training 的observation  
可以change 所有model 的參數 
而不是change 它自己的 喔 
我們之前的每一個model  
每一個observation 進來 
只change 它自己的參數 
譬如說一個v 進來 
我們只會去train 這個這個東西 
調這個參數 
它動不到別的 
一個t 進來我只會調這個t 的參數 
動不到別的 
可是呢我現在的話呢 
因為我現在是在minimize 這整個的東西 
是在算所有東西之間的差距 
所以呢當你一個v 進來的時候呢 
它不是只調v  
它同時會把其他跟它近的東西一起調 
想辦法把它拉開 
那當然你想這個問題是很複雜的 
當你把這兩個拉開的時候可能另外那兩個會被弄緊啊 
所以呢你就變成都有影響嘛 
所以我是把整個的全部的加在一起之後一起minimize  
所以呢我進來一個v 的時候不是只調v  
而是全部都會凡是跟它competing 的都會被調到 
那麼但是你把它們拉開的時候 
不能別人弄緊了一定要都拉開嘛 
你進來來一個t 的時候也不是只調t 自己 
是每一個都會被調到喔 
所以這個是跟我們之前講的的training 不一樣的地方就在這裡 
那這種training 就是所謂的discriminative training  
那這個觀念後來用到非常廣泛 
一直到今天這個仍然是一個非常重要的大題目 
你如果現在去找paper 的話 
今年仍然有非常多的paper 在講這件事情 
就是用什麼用這類的觀念但是有許多更新的方法 
怎麼樣做這這件事情 
讓所有的competing model 能夠拉開 
然後讓它的正確率能夠提高 
讓這些confusing 的或是competing 的不會製造error  
那在那我們現在講的這個m c e 是這個discriminative training 裡面的始祖 
就是當初最早的想法 
是這樣做的 
那現在這個很多很多了比這個進步很多了喔 
那但是我想最原始的一個就是這個 
那這個觀念應該是最簡單而清楚的 
最容易瞭解的就是這樣子 
所以呢我現在就是就是要minimize 這個東西 
但這邊參數很多啊 
我這邊可能有一大把model  
每一個model 都有一大把參數 
那麼我這個多東西全部要一起調怎麼辦 
那麼這裡通常用的是這個 
這是最普通的嗯方法 
所謂的gradient descent algorithm 或是steepest descent algorithm  
那你看這個式子你可能是看過的 
在很多領域裡面都用這個個東西 
那這個式子的意思其實是這個式子 
也就是說假設對a sub 這個a 是某一個parameter  
這裡面有一大把的parameter 有幾千幾萬個parameter  
那那麼假設每一個parameter 叫做a 的話 
那它都這樣子調 
t 是指它第t 個iteration 
我用很多個iteration 去調它 
那第t 個iteration  
變到t 加一是什麼呢 
是根據它的微分 
那這個的意思很簡單的講就是微分告訴我該怎麼改 
假設說這是那一個參數a  
而這是某一個某一個這個這個是我要的object function 
這是我要minimize 這個function l 的lambda  
那它這個l 的lambda 呢它是function of 一大把參數 
我現在選裡面的一個a 而言的話 
譬如說它是一一一萬個參數裡面的一個a 而言的話 
那那那我怎麼知道呢這depend on 如果我現在的a 在這裡 
第t 個a 在這裡 
這個微分在這裡的話 
這個微分告訴我說 
其實我向這個方向移動的時候會更低一點 
所以呢我下一步就會向這邊移動一點 
因為這邊是有一個local optimum  
那因此呢它就會告訴我應該向這邊移動 
可是我今天如果我這個a 在這裡的話呢 
第t 個iteration 如果在這裡的話呢 
這個微分是這樣子而且很陡 
它就告訴我說有一個minimum 在這裡 
而且是你這個地方是很這個很陡的 
所以你其實是可以走的很快我下一步就多走一點就走到這邊來了 
那這邊是因為很平 
所以呢不要走太多 
因為你走太多可能走過頭可可可能會走到不同的狀況 
因為也許這邊可能會上去了對不對 
所以呢這邊如果很平的話你不要走太多 
這邊如果很陡的話表示你可以再走多一點喔 
那麼因此呢這個你就就用這個微分就是這個這個object function  
對於這個參數的其中一個parameter 的偏微分來看 
那它的正負告訴我是應該向這邊還是向這邊對不對 
應該是小一點 
下一步下一個iteration  
應該是小一點還是大一點 
它的正負告訴我是向這邊還是這邊 
它的值呢告訴我應該走多一點還是走小一點 
喔那麼因此呢用這個方式的話我每一個參數都用這個方式來調 
所以呢這個個epsilon 呢是指這個adjustment 的step size  
就是你每一步到底要走多少 
那麼我主要的每一個下一個iteration 的a 呢跟這一個iteration 的a 呢 
就是根據我的微分來決定應該要向哪邊移動 
要動得多快還是多慢喔 
用這個方式來做 
那t 呢就是我iteration 的這個次數 
那這個式子呢是指對一個參數而言 
那我現在如果要把整個參數全全部寫在一起 
那用vector 的寫法就是這個式子 
那其實這裡這個一萬個a 加在一起就是lambda  
就是指那一大把的parameter  
它是在時間第t 個iteration 的時候 
這是這一大把的參數呢加在一起就是這個大的lambda t 加一等等 
那這一大堆對的每一個的偏微分呢 
我們就寫成這個符號喔 
這個你如果學那個嗯數學裡面就有這個符號 
這就是對每一個參數都去做偏微分的意思 
所以這個式子其實就是這個式子的意思 
那你可以想像的情形是是這樣 
我們也許沒辦法畫這麼多維我們如果畫兩維的話 
也許稍微可以畫一下 
譬如說我有一個碗的形狀 
假設我有一個碗的形狀 
那麼在一個這這個在一個這個二微的空間裡面 
假設我的這個是這是我的 
這個這個是我的這個l 的lambda  
在這個這譬如說這個是a one 這個是a two  
在這個碗的形狀裡面 
那假設我現在是在某一點上面 
在這裡的話 
那麼這個時候呢 
在在a one 而言我有一個這樣子的的一個一個斜率 
得到一個這個斜率 
對a two 而言我有一個這樣子的一個我也得到一個這樣子斜率
等等喔
那所以呢我a one 朝向a one 的那個那個方向走
a two 朝向a two 的那個方向走
因此我最後得到一個朝這個方向的
其實就是走向碗底的喔
因此你可以想像這是一個這個很多dimension 的一個這個一個這個function 的話呢
它會朝向這個一個local 的optimum 去走
那麼當然這裡有一個重要的問題就是說它只會走到local optimum 
它不知道哪裡是global optimal 呃
這個是這個only converge to local optimum 
local minimum 
跟我們前面講的e m 也是一樣的
我剛剛才漏掉講e m 有同樣的問題
e m 其實它是每一次向上走只走到一個local 的maximum 
它不見得找到global 的
這裡也是一樣你可以看得出來嘛
喔它就會走到local 那裡
所以你的起始點是非常重要的
那這個這個initialization 如何選擇一個好的initialization 很重要
那基本上來講在這個case 就是你原來那個model 要train 得好
一開始的這些model 都已經train 得非常好的話
那基本上應該是一個比較好的initial 的condition 
這個時候你再向這個這個再在這個地方調的話基本上它可以調得比較好
嗯就這樣的意思
所以我得到的是local minimum 
你就經過很多的iteration 
不過這個程式是有技巧的
就是說這個因為你現在有一大把參數成千上萬的參數一起調
是很難調的
那有本領的慢慢調調出來可以得到一個非常好的結果
那包括譬如說這個每一步到底要走多遠
這裡有一個epsilon 
這是一個step size 的參數
你也要小心的選
你可以想像如果這個epsilon 太小的話
我收斂太慢
一次只走一點點我走不過來
可是你如果epsilon 太大的話很容易一走走過頭
對不對你這個一走走過頭結果就就就收斂不到那個那個minimum 去嘛喔
所以這個怎麼去選擇也是一個問題
然後你可能每一個參數可以選它自己的epsilon 等等
所以有很複雜的方法來做它
不過基本原理呢大部分用這個方式來做呢
可以做到
那它的基本精神就是我們剛才講的
我每一個training data 呢它可以
它是用來調所有的model 
而不是調單獨它自己
一個v 進來的時候不是只調v 
它所有的會跟v confuse 的音全部都調了
一起都拉開來
等於是這樣的意思
那這個觀念就是所謂的m c e 
那m c e 可以拿來做很多事
我們舉一個例子就是m c e 來做feature optimization 
也就是說如果我我原來的如果我原來的這個m f c c 不夠好
我可以做一個新的一組
譬如說我原來的m f c c 我叫做x 
就是這一堆三十九維的
這x 
我覺得這個不夠好
我可以做一個y 是譬如說a 乘上x 
對不對
我用一個a 的matrix 乘上x 
得到一個新的y 
得到一個新的y 
我可以用y 來當當我的feature 
那這個a 裡面就有三十九乘三十九個參數
你要求這個a 可不可以求呢可以
那用m c e 來求
那這個就是用用m c e 去找一個好的feature 
那怎麼做呢你可以想像今天還是一樣
我的原來的要minimize 的東西是這個嘛
我現在呢這個x 變成f of x 
所謂的f of x 就是我這邊的a x 
我這個我把這個這個乘上這個a 
叫做f of x 
那於是呢我就把原來的所有的parameter 所有的這些feature 都變成f of x 
當我這個feature 變成f of x 的時候我所有的model 都改了
是以是以這個f 為基礎的model 
那這時候變成成一堆完全一堆新的東西
然後我也一樣可以去minimize 它
去調所有的參數
調什麼參數
就是調這裡面的所有的參數
因為這裡面的每一個這裡面的每一個參數都把我的這個m f c c 參數改變了
然後也因此把所有的model 都改變了
那我就可以調所有的東西
之後可以調到一個比較好的嗯
所以這裡裡我寫的就是說你可以用這個方法f 是一個transformation function 
把你的原來的x 的feature 就是m f c c 調成y 
那你也可以去那這個時候未知的參數就是這一把
就就去調所有的這這一把參數
那這裡的每一把都去做微分
你也一樣可以得到嗯
那當然因為這樣你改變了feature 所有的model 都改了
那你你就可以去minimize 它你也可以得到一組比較好的feature 等等嗯
那這一類都是m c e 的的基本精神
就是等於是我就是把這一大把的所有的參數拿來把它轉化成為它們的competing 之間的距離
喔我我這堆東西東西等於就是它們的competing 的model 之間的距離嘛
然後我要minimize 
會造成error 的的的問題我就把那個東西minimize 
那這樣子的觀念就是要把所有的會competing 的東西拆開來
盡量把它們拉開來
這就是所謂的discriminative training 
那這個m c e 的方法在九零年代是非常成功
它apply 在這種比較小的vocabulary 
像這個b c d e v t z 這個這種這種東西可以分得非常好
正確率可以提高很多
同樣我們拿來辨識中文的一跟七啊六跟九啊
也可以大幅提高正確率
這都很成功
但是當我們的字彙大到幾萬個字的時候
這個m c e 不太容易做了
因為你可以想像它太複雜了
那麼後來像在最近幾年非常熱門的是很多新的方法
精神也是這樣
但它是比這個還要複雜很多
那也都目的都是一樣的就是要把凡是confusing 的地方
把它拉開來
但是你不能為了你不能把這兩個拉開之後把另外兩個又弄得很近了對不對
假設說這兩個比較近這樣子
你為了把這兩個拉開
結果這一拉開它跟它太近了不行啊你要把每一個通通都拉開啊
你要想辦法做到把每個都拉開
那這個是非常非常重要的一個研究題目
在今天仍然是一個非常精采的研究領域
不過基本精神大概是這樣
那這個是m c e 
那麼m 跟這個m c e 相關我們講一件事
就是我們在講十五的時候
呃我有提我們那時候在講feature base 的十五點零的feature base 裡面有一個地方有用到m c e 的
我們們那時候跳跳掉了
你現在就知道了
就是在feature base 的嗯這裡
這裡我們在講feature base 的時候我們做temporal filtering 
那麼我們那時候就說用l d a 可以做
其實一樣的用m c e 也可以做
那這個你可以想m c e 跟l d a 其實精神是很像嘛
l d a 也是把它一個class 分得很開嘛
分開嘛對不對
l d a 的意思就是把每一個class 要拉開來
那所以當你有了class 的information 你就可以把它拆開
那同樣的m c e 也可以做的嗯
所以我們這個地方如果用m c e 來推一樣可以推得出來
ok 好
那到這裡我們大概把九點零的兩個重要的東西就是e m 跟m c e 都講到了
後面還有一個m m c e m m i e 我們等有時間再講
再那麼麼下週我想我會講的應該是這個十三跟十四吧
十三我看看十四是這個ya 這個spoken document understanding organization 
然後是那十三應該是retrieval 對
我想我們下週會講這個十三跟十四喔
那這樣的話我們大概這個比較重要的一些個topic 大概都會cover 到
在這個學期之內
ok 好我們今天上到這
我的目標標是要在
這兩週把所有還沒有
ok 所以目標是要在這兩週之內把把所有還沒有這上面還沒有說到都要說到
