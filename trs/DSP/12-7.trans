好有了以上這些我們現在底下就可以來說我怎麼來做 language model 
那你可以想像我們這邊所講的 lsa 其實不限於做 language model 
其實是一個很 general 的 concept 
可以拿來分析很多東西可以做很多用途的
那我們可以拿來做 language model adaptation 
當你說到哪裡的時候你知道你在講哪一個 concept 
所以後面的 word 你可以猜它應該是哪一個 word 
我可以用那個 concept 跟那個 topic 
其中考的
你如果看考古題就知道我上面都是期中考的形式都跟考古題一樣啊都是那個樣子的
那我上面是是有寫說這個open everything 
也就是說是open everything 你不需要去記任何東西
啊
那你可以帶任何你要帶的reference 來看
但是呢我有另外一個條件就是說所有文字要用中文寫
啊
那意思是說你如果open everything 的話
如果你可以直接從課本上抄一段
你根本不用懂它的意思你也可以抄一段來因為它是跟課本一樣
所以一定要給你滿分
所以這個不合理嘛喔
所以你必須要看懂了
用中文寫才有分數
ok 
所以我想這個這個規定就是比照去年的考古題是一樣的就是那樣子
那我們的範圍考到八點零我們講過了喔
好我們現在來看最後這一段
就是這個怎麼樣拿來做n gram 
做n gram 的基本精神是說ok 如果我這邊都已經知道的情形的話
我現在怎麼做n gram 
今天如果一個人在那邊講話
他已經講了一堆話了
他已經講我已經辨識出出來他講了這一堆話講完之後講到這裡的時候辨識下一個字
這是我這邊講的w q 
喔
這個地方有一點confuse 就是在這裡的時候這個w q 變成q 是sequence number sequence index 
因為之前的我們的w i i 都是辭典裡面的第i 個word 
i 是在辭典裡面的第i 個word 
但是現在的w q 不是辭典裡面第q 個word 
而是你現在在講話的時候講到第q 個word 
ok 
你講到第q 個word 的時候呢
你前面所講的
從這個d 的q 減一就是你前面的recognize history 
那麼這邊呢叫做d 的q 減一
那你馬上猜得到是怎麼回事了
我就已經辨識到q 減一為止的這個呢當成一篇新的document 
當成一篇新的document 塞進來嘛
我把那個當成一篇新的document 塞進來之後我就可以得到它的八百維的vector 
那我就知道那個八百維的vector 在這裡放到這裡來
那你就把這個d 的q 減一放到這邊來
那你就知道我現在在講的是這個topic 
所以從這那當然在這附近的word 會發生的機率就高了嘛
那這個其實就是對應到這邊來我們說這兩個其實是同一個空間啊
所以我就把它對應到這邊來那你就看這邊的空這哪些word 跟它相關嘛
那你應該會講跟它相關的word 
喔
基本觀念就是這樣子
所以呢我的language model 可以這樣子做
那麼我就是把w q 是第q 個word 
然然後呢q 減一就是到目前為止你所recognize 的history 
那麼因此呢
這件事情其實就是把那個d q 減一把它塞進來
把那個d q 減一這個這個equation 就是剛才這個equation 
就是剛才這個equation 就是fold in 這個equation 
所以這個意思只是把只是把這個d 的q 減一呢我重新放到這個vector 裡面來或者這個vector 裡面來
變成那八百維裡面的那一點
有了那個之後呢其實我現在就可以算
given given 它你前面講的這些話
所以下面的那個出現那個word 它機率是多少
那你可以想像就會就會在它的附近
所以你就可以根據這個來算
其中這個word 你可以算它的u q 
這個d 你可以算它的v 
那麼嗯我這裡稍微有一點點錯
這裡是v 的q 減一的underline
應該是representation of v 的d 的q 減一by v 的q 減一
這邊應該都是q 減一啦是指這個都是指前面這q 減一的到q 減一為止所辨識出來的結果嘛
所以那個是d 的q 減一
然後representation by v 的q 減一這兩個都應該是減一的啦
啊
應該都有減一
然後呢我現在就是算
那麼因此呢我就可以我的這個history 就可以用v 的q 減一來代表
而我現在下一個word 的w q 也可以用u 的q 來代來代表
那這個u 的q 也就是把我要哪一個word 我現在放在這個裡面放在這個dimension 裡面
那就是我u 的q 嘛
那這個是我d 的q 減一嘛
那這兩個其實是同一件事
因為是同一個我們講你其實可以放同一個八百維裡面來看
那你就可以在裡面看它們之間的關係
喔就這麼回事兒
那這個詳細的說法呢是可以說它是可以跟n gram 整合在一起的
為什麼說跟n gram 整合呢
因為這邊講的這個跟n gram 是互補的
為什麼
n gram 給我們local relationship 
而這裡的l s a 給我的是semantic concept 
這兩個是互補的
怎麼講
我的n gram 是告訴我說這個word 後面要接這個word 
我如果遻這是畫隻字詞的話
或者這兩個word 後面要接下一個word 
這三字詞的話
對不對
所以呢它n gram 是告訴我說locally 這些relation 
但是沒有告訴我它沒有去分析這邊到底講什麼話
那我現在這種l s a 講的是你在說什麼concept 
是一個global 的relation 
但是它沒有講local relation 
那exactly 這個後面會不會接它呢這其實是n gram 告訴我們的
這個不見得告訴我啊
ok 
所以l s a 告訴我的是它到底是在講什麼topic 所以它應該出現什麼word 
那n gram 是告訴我前面有什麼東西是後面接什麼東西所以它們兩個是互補的
一個是local 的一個是concept 
那你也可以說
這個l s a 呢比較強調是主要的content word 的關係
而n gram 是把所有的的word 包括function word 一起算
什麼叫function word 就是我們之前講的譬如說的
譬如說他的爸爸
那這個的後面都算進去
那我在算它們的n gram 的時候
我把所謂的function word 就是這東西
或者非常
就是說沒有真正的意思
不真正的代表content 的內容的東西
像非常啦這個什麼這種什麼的啦這都是屬於function word 
那麼如果講爸爸
這就是content word 
或者說李安或者說馬英九這種都是所謂的content word 
所以呢我們這邊講的這個semantics 比較強調的是主key content word 
是content word 裡面的key word 那麼應該會出現哪些東西
但是它漏掉了這些function word 
因為function word 我們一開始就把它拿掉了嘛
那樣才能夠得到一個比較清楚的東西
所以function word 我一開始就拿掉了所以呢function word 其實也是你在算n gram 當然是跟function word 一起算的
喔
所以它它們是是互補的
喔
因此呢
你可以怎麼做
我現在的這個這個這個出現下一個word 的機率given 前面的history 
這個大h 呢是所有前面的history 
我可以包含兩件事情
一個是n gram 
一個是前面的這個
這個d 的q 減一就是我已經辨識到這裡為止
到底辨識多少東西我這個可以放進這邊來看
它是什麼
對這個是d 的q 減一
但是另外呢我還可以這個是是什麼
h 的q 減一的n 呢就是到q 減一為止的n gram 
我可以到word q 減一為止的n gram 前面的n 個
我就可以算n gram 
那這兩個可以一起用
所以呢我可以given n gram 再given 這個之後來算這個機率
那這個詳細怎麼算呢我這邊就不再說下去了
那在paper 裡面有你如果有興趣的話去看那個paper 
同樣呢它們個推導我不認為一定是最理想的所以那個一定還有改進的空間
啊
那麼所以那邊我就不再講下去但是你基本上可以想像是這麼回事
所以我就可以這樣子做
那底下要講的一件事情就是說
你如果這樣子講的話變成我每每辨識一個word 
就有一個新的d q 減一出來對不對我每辨識一個word 這邊就q 加一
我辨識一個word 出來我就得到一個新的document 
對不對我這就放進來了每辨識一個word 我就變成一個新的
那這樣的話我每次都要把它重新再算再放進去不是很麻煩嗎
其實不麻煩
這只是要iterative 加進來就好
我每一次只加一個word 
每一次只加一個word 是什麼
只加那一個word 就好
因為你的那個d 的q 減一就變成一個一個row 一個column 在這裡
對不對
你如果在d 的q 減一的那個你前面辨識到d 的q 減一的時候變成一個column 在這裡
再多辨識一個word 的話
那個word 看它哪裡就後面加個一
如果下一個d q 
如果下一個word q 
它其實是這裡的某一個word 
你辨識出來
你就這邊這邊加一嘛
你就這邊加一就好了嘛
這個加一之後就整個全部都照做就行了
那這個加一的動作其實很簡單就是這邊加一
假設你下一個word 
下一個word 進來是第i 個word 的話你就在第i 個component 那邊加一個一
別的地方方加零就可以了
那這邊為什麼會有這些呢
那只是因為我們一開始算這個的時候
有這個normalization 你記得
我們一開始的時候
我有除以n j 
還要再乘以e 減epsilon i 
我有這個normalization 
所以你不是光是如果光是這個就加一就好啦
可是因為有這兩個所以你要稍微做一點調整
就行了
那就是這邊講的
你譬如說原來的那個東西不是直接加一
要除以q 
要乘以q 減一再除以q 
q 就是全部的長
就是全部的字數嘛
所以q 這個q 其實就是我們剛才那個n j 的意思就是這邊的n j 
就是n j 就是全部的字數詞數
就是n j 
那我現在呢都除了n 減一的都除了q 減一的
都除了n 減一的都除了q 減一的所以我現在要乘上q 減一除以q 因為現在變成q 了
所以就變成要乘上這個factor 
同樣呢我那個e 也不是直接加一
因為要除以q 
還要乘以e 減epsilon 
所以這就是一個normalization 的過程
所以你如果前面d 的q 減一做完之後
下一個d q 很簡單就這麼做
我iterate iteratively 加上去就可以了
那如果d 是這樣做的話
那我這邊要算那個vector 也很容易算照算
所以這就是那個式子
我照算那個就是要算那個vector 或者這個vector 也很容易算就照樣代進去
就是這樣斷就可以了
那這樣我們就得到這個嗯用l s a 來做language model 的方法
那這個就等於說是我用l s a 的分析我來判斷說我現在應該在說什麼話
然後你就
那麼因此呢我的language model 應該跟著它走
那這邊最後這句話就是講這件事情就是說呢
你一開始的時候
你的v q 會在那個移動
會在那個空間裡面移動
最後會settle down somewhere 
也就是說假設
譬如說這個布希發表這個一個演講
就是說這個什麼什麼國情諮聞
他在講某一段在講譬如說在講經濟政策的時候
你一開始它的它的頭幾句話會跳來跳去
因為頭幾句話它不見得exactly 針對這個主題
所以一開始的時候你可能在這裡會發現它會動一動
可是講到若干句話以後
你就會清楚它是在它的主題是那個
所以它就會settle down 在那個地方
ok 
所以你開始講的時候它會動一下
然後呢最後會settle down 哪裡
之後一直在講那個topic 
等到它這段講完
下一段它要講外交政策的時候呢
那又會開始動
動到另外一個地方去
然後過一陣之後會settle down 說它講的是中東
就是中東
等等
所以呢基本上是一個這樣的過程
那這個是l s a 
有了這個l s a 之後呢
我們現在來說一下就是我剛才前面給的reference 
我剛才講的絕大部分都是based on 這一篇
這算是寫得最清楚最完整
所以我想這是一個很好的reference 
那跟這個相關的這是一個嗯special issue 
裡面有很多篇文章
它講的不見得是這個
但是它講的都是language model 有關的
以及linguistic prosody 有關的
還有dialogue 
因為其實dialogue 裡面dialogue 你要跟這個系統對話你要跟user 對話你很多都是要用這個linguist processing 的觀念
所以呢它是把dialogue 跟這個放在一起
有很多篇
這其實是相當好的一個reference 
我想你現在其實是有些東西是可以看的
所以這也是一個很好的reference 
不過我們剛才沒什麼說到
你可以去裡面看很多關於language model 相關的文章
那第三篇呢是把這個再衍伸
其實同一個作者
他從二千年到二千零五年
嗯這個是二千零五年
他重寫了一篇
這個時候他把它已經把它extend 
它把它改名叫做latent semantic mapping 
那現在不再它變成一個mapping 了
所以不再限於詞跟document 
它可以把它extend 到很多不同的對應關係跟不同的應用
所以呢這篇就會比這篇多了很多東西
變成是不同的應用啊等等
那還有它有很多新的reference 出來
都在那個裡面喔
我們舉個例子來講
它一個應用是e mail 的那個垃圾信件的排除
就每一個user 我都收到到一大堆垃圾信件
但是哪些是我的垃圾信件哪些是我要看的
你其實可以把它歸類
然後你會要看的一定是哪些topic 
那那些你不要看那些topic 就是你的垃圾
啊所以根據那個就可以來做垃圾信件分類等等
也可以用這個再來做其它很多application喔 
這都在這篇裡面有講
這是三
然後因為我們這邊講一大堆matrix 
那麼你如果對matrix 不是那麼熟悉的話
很多課本關於matrix 的
那我這邊已列其中的一本
不一定是這一本這本很多啦
