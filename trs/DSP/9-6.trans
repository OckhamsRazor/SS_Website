那麼底下我們就可以來看我們在第第四點零節講的basic problem 三 
其實我們是用了這個的 
你記得我們basic problem 三是幹嘛的 
我們說 
我已經有了一個某一個我已經有了某一個lambda  
譬如說零到九 
假設是這個聲音是八a b pi 
這個是八的的model  
我現在知道這個聲音是八的話 
我把它放進來 
我希望得到一個新的lambda prime 是a prime b prime pi prime  
那得到這個新的之後呢 
我再把這個重新放進去 
再放進來再跑 
那這個這個的iteration 就是我們當時講的forward backward  
這個forward backward algorithm  
那讓它一直跑 
那麼這個過程其實就是用e m 在做的 
那我們當時是沒有講e m 這個名詞 
我們當時是說你怎麼做 
你你q 放進去之後你可以算alpha 算beta  
然後gama 什麼一大堆算出來之後 
我們說a i j 有一個新的值 
等於什麼除以什麼 
這裡面每一個mean  
mu i j  
有一個新的值 
等於什麼東西除以什麼 
然後每一個covariance matrix 裡面 
那你也可以算出出來它是什麼什麼什麼什麼等等等等 
我們有有這堆東西 
我們們在講那個basic problem 三的時候我們就說哦你代進去之後可以得到alpha beta 然後呢就這些東西就是這樣子 
那麼我們解釋說這個裡面的物理意義你可以發現他們是有道理的 
但是其實那樣不是那樣寫出來的 
這個怎麼來的 
這個就是用e m 推出來的 
所以當時我們在四點零裡面算的那堆東西 
是e m 推出來的 
那就就是這樣推的我們這邊簡單解釋一下 
我現在observation 是o  
然後呢那我不知道這個東西啊 
那所以我們你記得我們當時是講怎樣 
我已經有一個initial 的model 在那裡 
我才能夠做嘛 
那initial 其實應該是可以從零開始 
可是零很難做嘛 
所以我們那時候說我前面initialization 要另有一套 
我們用segmental k means  
去先去想辦法做一個比較好的的initial 之後 
然後放進來去求alpha beta 之後 
可以得到這個 
就得到下一個 
然後呢再回來再做等等等等 
那其實就是這樣做的 
所以呢假設我現在observe 到一個譬如說這個是八的聲音 
那麼我的latent data 這個state q 是不知道的啊 
那怎麼辦呢 
那麼我真正要做的東西其實是這一個 
是given 這個lambda 之下 
given 這個state 這個model 之下 
我不但是要有這個observe 的 
還要有沒有observe 到的q  
一起的機率 
這這樣子才好算 
那這個東西呢你可以想成就是這樣子 
那那它呢那那你這個式子很容易看啦 
就是q 是看不到的 
o 是看得到的 
那這這兩個的joint probability 呢就是一個condition 再乘上那個condition  
對不對 
那lambda 反正都在右邊 
你可以不看那個lambda  
那這個就是這個等於這兩個相乘 
那我怎麼做 
我先做e step  
這個這個這個e step 是什麼呢 
就是這個q  
就是我們剛才講的我要的這個這個q  
裡面一個是真正不知道的 
我永遠不知道的那個那個model 的真正的參數 
一個是我現在第k 個iteration 的參數 
那這個東西是什麼呢 
那其實就是這個 
那這個其實你如果仔細看的話 
那就是其實就是就是其實就是這個嘛 
在這個case 就是這個 
那麼你看我是在 
given 現在這個iteration 的model  
就是這個 
然後given 這個observation o 就是given 這兩個嘛 
given 這兩個之後我其實是有一個q 的distribution 在這裡 
所以given 這個之後呢 
我用q 來做這個expectation  
那麼因此呢我就對對這個每一個q 來做這個這個平均 
也就是這個式子 
你看這個式子就可能可能更清楚了 
那這個就是 
呃 
given 這個現在第k 個iteration 的parameter 參數 
就是這個 
然後given 這個observation 就是這個 
那given 這兩個情形之下我可以得到一個q 的機率 
那就是這個 
given 這個q 的機率之後 
我現在那麼在這個q 的機率之下 
在這個q 的sequence 之下呢 
我現我真正的model 我會會看到的這個的機率 
是我是這個東西 
然後我對所有的q 加起來 
那這個式子exactly 就是哪一個式子呢 
就是你如果仔細看我們之前的這個例子裡面的 
就是這個個式子 
是exactly 一樣的 
喔
那只是我們現在這這power point 沒有辦法把兩張疊在一起所以你只好跳過來跳過去 
那這個東西 
你看就是 
我我given 這個given 現在假設的這個假設的這個參數 
然後跟observation  
我有q 的distribution 
然後在這個q 的distribution 之下 
我來算我的這個maxima likelihood  
那跟這邊講的這個是一樣的 
就就是這個就是這個 
那麼在這個情形之下呢 
這就是我的e step  
當我把這個e step 算出來之後 
我的m step 呢就是要對它去找最好的lambda  
那怎麼找呢 
就是對其實就是對lambda 去微分 
那麼我這時候對lambda 去微分的時候有一堆constraint  
因為這裡這個lambda 裡面有很多東西包括pi 啦a i j 啦 
這個lambda 裡面就是一大堆這種東西啊 
這是我們在四點零講的 
lambda 裡面一大堆這種東西 
它們都有一堆constraint  
譬如說它的機率加起來要等於一 
啊 
這些東西加起來都要等於等等把這堆constraint 都算進去之後 
我要maximize 這個東西 
用微分啊等等這個方法去做它 
你得到了的就是下一個 
那那一堆東西就是我們在四點零所得到的那些式子 
ok  
所以我們在四點零的時候我們就說喔這樣這樣所以呢我就用這個來做下一個iteration  
那我們當時是對每一個裡面的式子為什麼長這樣 
我們說他們都有物理意義可以解釋 
但是呢這些東西是怎麼來的我們沒有說其實它不是完全憑空想的 
它是用這個推的 
啊
那因此呢在在這裡面我在做maximize 的時候 
就可以算出它的這些東西出來 
那麼於是呢你有了這個之後呢 
那你就可以得到我們那個時候所要的結果 
然後那也是為什麼我們當時說你每跑一個iteration  
這個機率會提高 
那所謂每跑一個iteration 之後 
每跑一個iteration 之後 
你的這個機率會提高的原因其實就是 
我們這邊所說的這個式子嘛 
就是我們們這邊所說的嗯這個式子嘛 
就是你跑一個iteration 之後 
你的的新的model 看到那個機率一定會提高嘛 
啊
那這樣子呢我們這這樣就得到我們用e m 來推的那個basic problem 三 
好 
所以你現在如果在回去看我們四點零所說的這個h m m 的這個basic problem 三的話 
你現在去看那個課本裡面講的一堆那裡的一堆數學式子你現在看就沒有問題了 
那它就是在做這件事 
它裡面就會跑出一個q function 出來 
那個q 就是這個q  
然後它就用那個q 去去maximize  
之後呢它就就得到這堆式子 
那就是這麼麼來的 
那這堆式子的物理意義我們當時就已經講過了 
那麼因此呢這個就是所謂的e m  
那麼我們的e m 說到這裡 
那你我們可以回想一下我們後面講的很多東西的e m 
好 
我們當時都跳掉舉例來講在十一點零那裡面 
十一點零的的的speaker 那裡面我們說過很多東西 
譬如說 
這個m a p 的時候 
你做m a p adaptation 的話 
你這個東西要怎麼求 
我們說最後答案是這個 
怎麼求我們沒有說 
我們說用了一堆e m theory 所謂的e m theory 就是用剛才那些方法 
你要要maximize 這個東西裡面有一堆堆unknown 的variable  
我就去做e m  
這是m a p 這裡所用的 
在m l l r 這裡 
我同樣的我要做maximum likelihood  
然後呢我怎麼辦 
我要這個東西maximum  
我調這些a i b i 使得這個機率最大 
怎麼調 
用e m  
那這個詳細的也是 
你現在再再看那些paper 那一大那一大堆數學其實就是e m  
同樣呢 
我們後來講的eigen voice 裡面 
eigen voice 裡面你一個新的speaker 進來 
我怎麼算 
我就是要讓這些東西a i 加起來的這個東西要最大 
調所有的a i 使得它最大的那個a i 就是我要的a i  
這個怎麼求 
用e m  
你如果現在再去看那些paper 的話 
一堆數學式子就是在做那個e m  
等等等等 
那包括這裡的 
這裡的這個s a t 
也是一樣這東西是用e m 求的 
喔 
那等等 
喔 
所以我想這個這些我們當時跳掉的部分你現在應該都容易想像它為什麼是這樣 
那同理譬如說 
我們講這個g m m  
這個g m m 就是一堆gaussian  
一堆gaussian 的時候 
這個 
我我我們說你可以想像想像成 
這是一堆gaussian 這是一堆gaussian 這是一堆gaussian 
我們在做segmental k means 的時候我們說ok 我就做v q 嘛 
譬如說你先先做一個v q  
然後呢拆成左右兩半兩個v q 兩兩個 
然後再拆變成四個 
然後分別就求它的的mean 跟covariance 得到四個 
這是一個比較粗的做法 
這樣子做出來的你如果直接用v q 來做的g m m  
不是最好的g m m  
因為它不見得保證你的那個你的那個那個這個東西這種機率是maximum 的 
你如果真的要做一個好的g m m 的話也是一樣用e m 來做 
等等 
所以這些東西都是可以用e m 做的 
那這個 
所以呢我想這個 
我們現在e m 講完你現在再回去看所有的我們提過的東西其實都容易得到答案 
