這大概是這個language model 裡面是中文比較不一樣一點啊
有有有這些不同
好那麼
六點零我們說到這裡
休息十分鐘
我們底下開始進入七點零
ok 我們進入七點零
七點零是我們目前為止沒有真的去討論語音訊號長怎樣
到我們現在要來看語音訊號長怎樣
然後怎麼樣做front end processing 
啊
那這一塊的位置
嗯
這一塊的內容比較複雜就是有各種各樣的東西
所以大概的reference 是包括這一本的前面這些這本的這些跟這本的這些
那這裡面是啊
我想第二個這部分呢是比較牽涉到各種語音各種不同的聲音它訊號會長怎樣等等等等的問題
稍微比較難看一點如果你覺得看不下去跳掉一些應該沒有什麼關係
啊
那這個比較重要這個比較重要就是了
啊
那那中間這個是比較比較啊嗯牽涉到很多不同東西就是了
那我之前提過一下就是說
我在每一個每一每一個chapter 的前面的這一堆reference 是基本上是我希望你嗯會去看的
也就是期中考的時候是會考到這些東西的
啊
那你可能會說啊這個我看到一堆看不懂怎麼辦
看不懂你就跳過去就好
sil
然後過一段時間你再回來一次會發現你看懂了多一點
就這樣子啊
那如果說是嗯
我再回來看我始終看不懂的話那那裡就是不會考的嘛
那我我們這門課因為修課的同學有很多不同的background 
所以我們基本上只能用這種方式來來進行就對了
好那麼
嗯我們現在要講的是哪一塊呢我們如果回到這個剛才講的那一點零的那張圖的話你就知道
就是有這一張圖的話
那我們現在己經講過的是包括這邊
就是你從acoustic data 你從acoustic data 經過acoustic model training 來train 的這是什麼這是h m m 
所以就是四點零跟五點零就是講這一塊
然後呢我們六點零講的是這一塊就是你怎麼樣用文字來train language model 
sil
那我們七點零是講這一塊就是到底input 的語音訊號長怎樣
然後我們怎樣做front end processing 
最後得到我要的那群feature 
我們還差八點零是這一塊
sil
所以七點零的這一塊
跟八點零的這一塊講完的時候我們這張圖才算講完
那那個也就是我們期中考的範圍就是考到那裡
啊
所以大概還要兩三週才會講完這部分就對了
好那我們現在回到七點零
那麼語音訊號長怎樣呢那這個時候我們就得要看一點語音訊號的圖
那這個呢
這一類的語音訊號的圖在任何一本課本上都可以發現很多
那我沒有把它放在網頁上是因為這些因為都是不同的書上的
基本上都有它們的智智財權或者版權的問題如果如果放在網頁上都會有問題
所以我們就不放了
但是基本上呢你要看是你要找的是很容易找得到的
sil
那這個是什麼呢這是母音
e 嗯什麼a 這個啊哦等等
這些所有的母音的訊號長得像這樣
那麼你有沒有發現它們有什麼共同的特性
很容易看到的特性呢
就是它有週期
sil
所有的母音都是有明顯的週期性
你可以看到譬如說從這裡到這裡就是一個週期
啊
那它們不同只是因為一個週期裡面長得不一樣
這裡面如果長得這樣就變成ㄨ
對不對
如果長得這樣呢就變成ㄚ
長得這樣就變成ㄟ等等
這週期性是非常明顯的
這是語音訊號裡面一個很大的特點
但是呢並不是說所有的聲音都是有週期的
也有一堆是沒有週期的
那我們舉例來講呢
像這個子音
這裡面它其實是一個子音被插在兩個母音的中間
這裡要show 的是ㄒ這個音
那前面是ㄜ後面ㄚ
那前面這裡的週期性是非常明顯你看它是有週期性的
後面ㄚ也是非常清楚有週期性的
中間的這一堆是什麼呢
這一堆是ㄒ的音
那你可以看到它是沒有週期性的非常混亂
啊看起來跟white noise 是非常像的一些東西
而這個聲音呢跟前後之間呢是有一個transition 
你看到它的這個這邊的ㄜ有週期性然後這個週期慢慢慢慢開始變化
到這邊還有週期性不過已經變得很厲害最後就變成這樣
這底下開始慢慢出現週期性最後變成這樣
中間這邊是一些transition 
那這兩種的差別就是我們講的voice 跟unvoiced 
那這就是所謂的濁音
我們在中文裡面翻作這個叫做濁音
這個叫做清音
那麼所謂的voiced 就是像這種週期性的
像剛才所有的母音都是這種週期性的
這是所謂的voiced 
那麼所謂清音或者unvoiced 就是像這個
ㄒ這種呢是unvoiced 
那並不是說子音母音這樣子分
母音都是voiced 沒錯
那麼子音並不完全都是unvoiced 
這裡有另外一個例子
這個是這中間那個子音是v 
這個v 的這個音v 
那你看到它中間會變成這樣
它跟剛才不太一樣
它的週期性仍然很清楚的出現在這裡
這是非常清楚的週期性
那這個呢是子音但是它是屬於voiced 
濁音的
它是voice 是濁音不是清音
嗯
那麼它的週期性非常清楚但是你也可以看到它的這個transition 
從這邊的ㄜ的週期
轉到這邊v 的週期性是慢慢慢慢變下來最後變成這樣
你要變成底下那個啊呢也可以
它再慢慢再變過去變成這樣
嗯
那這個是這個另外一個例子
那我們真正的一段語音會長怎樣呢
底下這個例子就會看得很清楚
像這樣
這句話呢是
這句話是should we trace 
那這句你可以看到前面是sh
這個全長是每一段長是一百個mini second 就是零點一秒
啊
這邊是sh 
所以這個是unvoiced 的清音
是這種清音哦
然後呢would 的時候呢就變成一個濁音就是voiced 
週期性非常明顯的
那d 這個d 的是一個子音
但是它很清楚是一個voiced 
週期性仍然非常明顯的
sil
等等
we 這是e 是很清楚的這是一個週期性的這是voiced 
trace 的時候呢這個tr
是一個unvoiced 的清音
那你要發tr 的時候你必須先把聲音停掉嘴巴關起來之後再打開
should we trace 
你這個得再重新所以中間前面是有一段silence 
之後從這邊開始是tr 的音
然後到這邊呢是a 
最後再回到s 
那這樣子的的一個這個這是一個非常typical 的一一句一個語音訊號就是長這樣
那你可以看到基本上就是分成兩大類
就是voice 跟unvoiced 
那麼voiced 的呢那很清楚的就是以這個這個週期為它的最大的特色
然後呢大部分都是母音
但是也有voiced 的子音
像這個d 就是voiced 的子音
那unvoiced 的話呢清音的話呢像這種
sh 或者這個s 或者這個tr 
那它們有一個共同的特性就是完全沒有週期性
然後看起來非常像white noise 
的東西
那當然還有一個很明顯的特性你也可以看到就是
它比較小嘛
所有的這個這個基本上都這個母音的amplitude 會比較大嘛啊
或者voiced 的amplitude 會比較大嘛
那子音的amplitude 都會比較小像這個雖然是voiced 這個子這個amplitude 會比較小嘛
啊差不多是這樣子
好那有了這個了解之後那我們想可以說一下那麼這個我們真正的差別究竟是什麼
那真正的差別其實就是我們的聲帶有沒有在震動
那這個就涉及所有的聲音都是人的嘴巴說的
所以都是人的發聲系統才會發出這樣的聲音來
那你可以想像成是一個這樣子的一個這是我們的發聲系統
那這是我們的氣管
氣流從這邊進來
這邊是我的聲帶
或者聲門
我這邊聲帶在這邊震動之後進入口腔
這個時候depend on 你的其實我們發不同的音就是在這裡
因為你的不同的口型嘛你這ㄚㄛㄜㄟ
差別差在哪裡就是差在口型不同
那麼因此這邊不同結果給我出來一個不同的聲音
如果進去的叫做u 的t 
出來是x 的t 的話
那所謂的這些voice 的聲音
為什麼voice 會有週期性
其實就是因為聲帶在震動
聲帶的震動的週期就是那個週期
也就是說這個u of t 呢是怎樣的
聲帶每震動一下的時候
聲門打開一次
有一堆氣出來
然後呢就關上了
待會再震動一次再一堆氣出來之後又關上了
因此呢我們的聲帶以某一種週期在震動
那麼它就會得到這樣子
這就是我們進去的u of t 
那你凡是這一堆氣進去的時候
出來會變成怎樣呢經過這一堆你的口唇齒舌之間的關係
它會變成一個然後你下邊又出來一個
它就再來下一個
又來一個它就再來一個
那這個就是我的這就是x of t 
於是你這邊看到的週期性就是這邊的週期
那這個週期就是所謂的pitch 
這個pitch p 的話
也就是這邊的週期
也就是我聲帶以這個以這個頻率或者這個這個週期震動的話
每震動一下產生一個氣流
經過這邊就變成一一個週期的waveform
然後這邊再來下一個這邊就是下一個所以這邊就是這邊的週期
這個週期就是你這邊所看到的這個週期
那這樣子的這個震動的狀況其實你是很容易感覺到的
你只要把你的手指頭放在你的這喉嚨這邊
你發一個音的時候ㄚㄛ你可以感覺到麻麻的
它那邊在震動
那個震動就是就是這個
那你感覺得到的那個震動的那個頻率那個週期就是這個週期
那麼其實也就是這個週期
那這個關係當然你可以我們如果把它變成discrete time 
這裡變成一個一個的sample 
因為我們其實是以一個一個sample 在處理
那這就變成x 的n 
那其實這邊也可以看得出來是一個一個的sample 
那這就是u 的n 
那麼在這個情形之下呢我們通常會用一個這樣子的input output 關係來來描述
這是u 的n 
這個是x 的n 
那這個un 到x n 的關係
其實就是從這個到這個的關係
或者是從這個到這個的關係
其實就是你這邊的唇齒舌之間的關係
就造成這個關係
那如果是這樣子看的話
那麼這個是voiced 的時候
會發生週期的原因
那這個這個字你如果是很喜歡音樂你就知道這個字是什麼就是我們音樂裡面講的音高
也就是你音的高低
其實就是我們稱之為的f zero 
是什麼就是p 分之一
我的訊我的訊號裡面有一個所謂的f zero 
就是這個週期的p 分之一
那這個東西就是我耳朵聽到的音高
那麼那這個影響最清楚的
就是我們的四聲
你知道我們中文的有聲調
巴拔把爸
的四聲是什麼
就是我那個f zero 
剛才那邊畫的那那個f zero 就是p 分之一
as function of time t 
你可以得到四種pattern 
如果它大部分是平的這就是第一聲的巴
如果你得到這樣上去的話呢
這就是拔
那麼如果是把就是第三聲
如果是把你就會上來
第三聲有兩種啊
這叫做全這個叫做全上這叫做半上
那麼把
這是全上
你如果只唸把這就是半上前半上
那麼嗯那第四聲呢
第四第四聲是爸
第四聲
那些這其實就是f zero 相對於time t 的不同的pattern 
就是我們這個音高的變化
第四第四聲是爸第四聲那些這其實就是f zero 相對於time t 的不同的pattern 就是我們這個音高的變化
那這是在在我們中文裡面的語音聽得最清楚的
那其實我們真正講一句話的時候不是這麼標準的pattern 
而是很混亂的
譬如說可能是這樣子
那也就是說你除了基本上基本上比較像這個pattern 之外
它是會受到前後音的影響它都會動掉的哦
不是那麼漂亮的pattern 
而是它都會受到前後音的影響
會動掉
然後它的它的level 也會動到
所以真正的你聽到的一個聲一句話的f zero as as function of t 的話呢
你會得到一種像這樣子的情形啊
那這個其實這樣的訊號週期的
然後它的這個f zero 這個現象其實是音樂裡面也是一樣的
你如果拿一個小提琴獨奏或都鋼琴獨奏的waveform 來看
也是這樣
那這個pitch 也就是那個的中央c 還是什麼
那個c d e f g 那個pitch 
啊也就是這個
那麼這個是講voiced 的時候
那麼如果是輕音會怎樣呢
如果發輕音的話就是我進去的時候根本就沒有這麼
譬如說我發我發ㄚ你可以感覺到我這邊是麻麻的我確實是有一個頻率在震動
可是我發ㄒ
你發現我跟本就沒有動
換句話說它這個門根本是打開的
這個根本是打開的於是呢它就是一堆一堆氣在這邊通過
不像剛才是會關起來所以會一那樣子一個一個會關
現在沒有它就整串整串在那在這裡了
那這就變成u of t 
那因此你如果取sample 還是一樣
它就是這樣一堆非常random 的在這裡
那就是u of n 
在那個情形之下
當然經過了這個之後我這邊也變了
但是它還是這副長相
你會得到一堆這種東西
那這個就是我的x t 
那你也可以取得一堆sample 
不過就是長這樣子
那這個呢就是清音
那麼因此哪哪裡是濁音哪裡是清音你是很容易判斷的
那麼那你注意到我這邊現在畫的時候其實應該是這樣
就是中間會斷掉
斷掉的地方就是這個可能是中間可能是輕音
因為凡是如果碰到輕音的話
你沒有週期嘛
所以你求不出f zero 出來
所以你部分就是輕音這部分就是輕音
那你就就沒有f zero 
那這個是濁音跟清音的情形
跟這個相關的另外一件事情我們順便說了就是
這個time 跟frequency 
那麼這個我們修這門課的同學有不同的background 有的人可能非常熟悉有的人不熟悉所以我們簡單的解釋一下
那麼通常我們講一個signal 
有兩種說法
一種是用x of t 
或者它的x of n 來說
這就是time domain 
或者是x n 變成它的sample 
這是在time domain 上來描述這個signal 
但是呢我其實還有另一種方式的表示法是在frequency domain 上
那我得到另外一堆
這個呢我們簡單的寫也許寫成x omega 
這是我在frequency 上面的distribution 
這是什麼意思呢
在這上面的任何一點譬如說這個是omega one 的話
那它所代表的是一個一的j omega t 的這樣子的一個訊號
那這是什麼呢你知道如果取實部的話
它不過就是一個cosine 
我如果取它的實部
就是cosine 的omega t 
這個是omega one 這是一的j omega one t 
這個就是cosine omega one  
所以基本它就是一個cosine 
那那這上面的這個值
x 的omega one 
它是一個虛是一個負數
它有一個amplitude 跟一個phase 
所以呢如果這上面有一個a one e 的j phi one 這麼一個負數的值在這上面的話
意思是說
這個e 的j omega one t 這個東西它有一個這個coefficient 
那麼它所代表的意思呢是
等於說我真正的訊號是這個
a one e 的j phi one 
這個東西乘上e 的j omega one t 
是
就是這個訊號e 的j omega one t 的這個component 它的大小是這個
所以應該是這樣
這兩個乘起來
那這個到底是什麼你如果要比較有感覺的話我就取實部就是了
那就是a one cosine 的omega one t 加上phi one 
因此呢這個amplitude 跟這個phase 所代表的是這個amplitude 跟這個phase 
那這個omega one 代表的是這個cosine 
那你就就知道它所代表的意思其實是一個就是一個cosine 
它的頻率是omega one 
a one 代表它有多大
而phi one 代表零在那裡
它的前後的位置是phi one 
那麼以此類推
我現在在另外一個地方有另外一點是omega two 
那這上面的那一點呢是
它是a two e 的j phi two 
那它也代表另外一個訊號是a two e 的j phi two 
然後那個是那個東西
另外一個cosine 
是另外一個cosine 
那它的大小是a two
它相對零的位置是phi two 等等
那麼當我把這個訊號用這個方式來呈現的意思是說
我這裡的每一個都有這樣都代表了一個那樣子的cosine 
那它們加起來就是這個
ok 
所以呢我現在如果把這上面的每一個任何一個都代表一個這樣子的東西
然後我把這些東西全部加起來的話
就相當於那個
所以這個跟這個是同樣的同一個訊號的另外一個表示法
不過這個表示法是表示在frequency 上面
所以這個是我們所謂的frequency domain 
因此呢我們是任何一個訊號同時有兩種表示法
是在time domain 上還是frequency domain 上
那你如果是用這個來呈現用這個discrete time 的n 來呈現取sample 的話其實也可以
那這邊也不過就是這個cosine 也是取sample 的cosine 嘛
不過就是這樣取sample 的cosine 
這個cosine 也是另外一個取sample 的cosine 
其他並無不同
因此呢這裡面我們說它是e 的j omega t 
就會變成它是e 的j omega one n 
就變成n 就是了
一樣的
那到時候就是這邊變成這個變t 變成n 
這個t 變成n 就是了
那就變成這樣子
那就是我的取sample 的東西
取sample 的東西也一樣可以用這個來來呈現
那麼這兩者之間的關係
就是所謂的fourier transform 
那如果我的不是用t 而是取sample 的話呢
就是加一個discrete 
就是discrete 的fourier transform 
那這個的演算法直接可以把這個算成這個的
那就是所謂的fft fast fourier transform 
我有一個快速演算法可以做這個東西的關係
好如果了解這個東西的話呢我們其實是為了講我們剛才看的這些聲音會怎樣
不管你是voice 還是unvoiced 
你如果去做這樣的一段聲音
或者是剛才的剛才的voiced 
這裡有pitch 
不管是這個還是這個
你都可以去做像剛才那邊所說的那種fourier transform 
把它轉到frequency domain 去看的話
你得到的會是一堆
一堆這種東西
它毛毛的
哦
一堆混亂的毛在上面
但是你可以看到它是有清楚的這個這個peak 的形狀
這些個peak 
這個上面可能非常亂
但是呢你可以看到很清楚的
在某一些frequency 它會高起來
這些frequency 呢就是我們所說的f one f two f 三f f 四
也就是你的聲音裡面的這些個頻率是最大的
因為這些個這些個頻率上面的的這個
啊
那些的頻率上面的內化的就是這個東西嘛
它上面這些cosine 它的是最大的
那這些東西是什麼呢
是我們所謂的formant frequency 
那通常我們最最重最常會看就是這四個formant frequency 
就是這四個最重要的formant 就是這四個peak 的位置
為什麼這個會這麼重要呢
我們舉一個很簡單的例子
這是語言學家早在幾十年前就發現的
譬如說你如果以f one 跟f two 來畫圖的話
拿這兩個frequency 來畫圖的話呢
你會發現
譬如說
ㄚ在這裡
ㄨ在這裡
e 在這裡
等等
我這是隨便亂畫的不過大概意思是這樣
就是說你不同的人唸ㄚ當然點不太一樣
你同一個人唸幾次也不太一樣
不過基本上大概在這一堆
如果ㄨ大概在這一堆e 大概在那一堆
也就是說你只要根據前兩個frequency 大概就可以分出來
它是哪一個母音啊等等
那這個說明
這些個
這些個這麼複雜的frequency 的變化裡面
這些個peak 的位置是非常重要的
這個聲音是什麼聲音的特性
這是所謂的formant frequency 
好那如果有了這個的話
那我現在如果把剛才的一段聲音拿來看
一個很很長的一段聲音拿來看怎麼辦呢
我可以這樣子
就是我取裡面的一段去做一次fourier transform 
得到一個這個
然後呢我待會再取這一段再做又得到一個
再取這一段又得到一個
那你可以看到我這個聲音不斷地隨著時間變化我的音在改變嘛
我音在改變我的frequency 也在改變
sil
你可以得到每一段每一段每一段它的frequency 上面的變化
那麼我可以把它拿來畫一個圖
這個橫軸是時間
縱軸是frequency 
我第一個得到的這一個圖
把它用畫在這個上面
我用這個不同的顏色或者是黑白的強弱來呈現這個圖
第二個得到的我也畫在這上面
那我畫成一條
我也是用顏色或者黑白來呈現它
等等
那我這樣一條一條畫出來的話呢
我最後會得到一片
那這一片就說明說
當我在講這句話的時候
這個聲音一路變過來
它的frequency 其實一路在變化的情形
那或者說就是你在這上面隨便拿一條線來看的話呢
這一條線畫下來是一個這個東西
那你在這裡可以看到它的peak 在哪裡哦等等
那這個圖都是所謂的spectrum gram 
那我們通常是
這個spectrum gram 是最清楚的看到time 跟frequency 這兩個dimension 上面聲音變化的聲音變化的情形
就是所謂的spectrum gram 
好
那這樣我們可以看兩張圖
像這個就是我們剛才看的一個一段voice 的母音
這是voice 的母音你可以看到它有清楚的週期性從這裡到這裡到這裡
那這個voice 的母音如果我轉成frequent domain 就是我畫成這個這就是我那邊畫的那個東西啦
那你看到就是我這邊講的這個毛啦
上面一堆毛毛的
但除了毛毛之外你是可以看到這邊是有一個peak 
這裡面這這這個pick 的位置就是f one 
那麼然後這邊呢
應該是有一個f two 這裡有一個f 三這裡有一個f 四
sil
那這幾個就是我們這邊講的這個這四個formant frequency 
等等
那你如果把這些東西
如果我把它整個的畫起來的話呢
就變成像
以我們剛才的那張那句話為例
這我們剛才講的那句should we trace 
我們剛才畫得很長總共有好多段
但是我現在把它把它這個time scale 收得很緊就兜在一起了
sil
那你這時候看到的就是
這是should we trace 
這是should we trace 
啊
這是同樣這句話
那我把它兜得收得很緊的關係所以你其實看到的這裡從這裡到這裡就是一個週期
就是從這裡到這裡因為它畫得很緊的關係變成兩兩個尖的地方就兩條線
就是一個週期
sil
那這就是它的它的這邊就是voice 週期情形這是這是would 音這是e 的音這是a 的音
那這是sh 這是這個是s 等等
那你現在如果把這裡面的每一小段每一小段分別去做這個fourier transform 
得到這個東西之後
畫成這個圖
就是上面這個
那你可以看到很清楚
這個sh
它的spectrum 是在這裡
對不對它的能量大概集中在這個地方
到了變成would 的時候變成是這一塊
然後呢這個we 
we 本身是一個在變化中的聲音
所以你看到它的這個在變
然後這個是sh
那這個是a 
然後這個是s 
ok 
那也許a 這裡看得最清楚
這就是f one f two f 三f 四
就是說它的這四個peak 
這四個peak 是在隨著時間在變動
啊
f one f two f 三f 四這是看得最清楚的
那這個圖就是所謂的spectrum gram 
那麼因此呢我們就是這個這個在早年的時候是很多人很仔細的研究這個東西之後
那麼發現事實上可以看這個圖就可以看出它在說什麼話
那這個學問就做spectrum gram reading 
那有一個最厲害的人他就是只要看這個圖就知道你在講什麼
那這個
他只要看說這邊長怎樣這邊長怎樣就知道這是什麼音
這邊長怎樣這邊就知道什麼音
所以這樣他就知道你在講什麼話
這是所謂的spectrum gram reading 
那我這裡有一張圖在講這個spectrum gram 可以更清楚一點的就是這一張圖
這一句話是he will allow a real lie 
就是每一個這裡的每一個小段的音它都切出來
這是he 這是h 跟e will allow a real lie 
那這個是它的time domain 的waveform 
這也是收得很緊很緊所以你看起來不太像了哦
那這其實這兩條線的中間就是一個週期
收得很緊很緊
那在這個裡面呢你可以看到的
相對於這個的spectrum 就是下面這一段
的相對於這個就是上面這一段
這樣子
那上面這個呢就是它的spectrum gram 
sil
所以呢也就是你把time domain 的waveform 
就是那個time domain 的waveform 
相對於它的frequent domain 的這個spectrum 
兜起來就是這樣
那這四條白線就是f one f two f 三f 四
它的這四個formant frequency 
那麼隨著時間在變化
那麼不同的音它就是不一樣了哦
就是我們講的其實你你你只你只要看到這四個f 在哪裡幾乎就己經發現它們不同的聲音就是在這邊不同就是了
sil
大概是這樣
ok 
那這個呢
大概我們今天就是把這些圖都看過了之後
那你如果再去看我們剛才講的其實我們七點零最前面的就是講這些東西
啊ok 好我們今天就上到這
ok 喔我們上週在說的我們上週用view graph 有解釋一下用over head 我們有解釋一下這些東西嗯那基本上呢這個喔我們的語音訊號可以初分成為兩大類就是voice 跟unvoiced
那這個voice 跟unvoiced 我們說過呢其實你是可以根據他的這個的這個發生的狀況來分的
那麼如果說這個是進入聲從聲帶進入口腔內氣流叫做u 的t 那麼這個出來的叫做x 的t 
或者呢是u 的n 出來叫做x 的n 的話呢那麼所謂的voice 
我們說是因為這裡的聲帶這個地方他會他會震動他會一動一動的一開一開
所以呢你會看到他有有週期性的這樣一個一個每一次震動就是這樣動一下
那這個週期就是所謂的pitch
那這個的情形其實就反應在這邊那麼你每一每一鼓氣出來他就有一個每一鼓氣出來就有一個每一鼓氣出來他就有一個那麼因此在這邊也是週期性的
那這個就是我們在語音訊號上所看到的週期這個也就是pitch
那這個pitch 的倒數p 分之一就是我們所謂的f zero 也就是你聽到聲音的音高
那這樣的聲音就是所謂的voice
那麼另外還有一種就是所謂的unvoiced unvoiced 的話呢他進他進來的時候呢就會變成沒有這樣子因為聲帶根本就沒有關起來
就一鼓氣不斷的在裡面跑你就會變成一種這樣子的
那這鼓氣到底長怎樣呢就像是white noise 差不多那麼我們不太容易找出它有它有什麼什麼特徵它就是一片混亂
那這個時候呢通過了口腔之後我仍然是一片混亂
那麼只是說不一樣因為你仍然是因為口腔上面的不同這裡的不同而造成不同的聲音
那這就是x n
那麼一個很簡單的 可以了解就是說其實我們在說什麼音的時候這邊的影響不大這邊只是這樣的區別而已
真正差別在這裡
那麼或者說就是在這裡
換句話說呢你發的是嗚還是一還是哦還是阿是你的口腔的口型的不同而造成的
那麼並不是這邊造成的
嗚還是阿還是一都是都是這樣進去的
在這個地方而言並沒有不同不同在這裡那換句話說不同是在這裡
那麼也因此呢你烏還是阿還是一的不同是這一個週期裡面他會長的不一樣
它是會這樣還是會會另外一個這樣還是會這樣是這個問題而不是 它一個週期裡面會長的怎樣
是造成它是烏還是阿那是這裡面所造成的不是這裡面所造成的
那麼同理呢在這裡也是你是需還是輔還是斯是因為你的嘴型不同所造成的而不是這裡不同
這裡反正就一團氣就可以了
那麼是是這裡的不同造成他的不同
那麼在這樣的情形之下呢那我們還還說到另外一點就是你其實很容易判斷它是voice 還unvoiced
你只要用你的手指頭摸到你的喉嚨這邊你就可以感覺到你發音的時候有在麻的有在震動就是就是voice 沒有在震動就是unvoiced
因此你有的時候你也可以whisper 的話
你就是我就這樣子講話
你就會發現它都它都它都沒有在震動
所以這個時候你發的它全部都是unvoiced ok
那麼你如果如果從這點了解的話
那麼我們就可以想像真正的差別是在是在這個地方
那麼這個地方呢我們就可以把他用一個比較更直接的model 來model 它那麼我們就把它可以畫成一個這樣
這是engineer喜歡做的方法
就是把它想像成是一個這樣的東西
那這邊是出來的x n 這邊是進去的u n
那你如果這樣子看的話呢它就變成一個這個這個這個io 的關係
這是input 這是input 這是output
那麼這個就是我們所謂的vocal tract model
那麼等於是用一個model 來model 它
進去是u n 出來是y n
那麼那這個所謂的vocal tract
這個東西呢就是我們vocal tract 所謂的聲道就是指這一段從這裡到這裡這就是我們所謂的聲道
那麼事實上我們剛才講的意思是說你今天你講的是什麼聲音的差別是這裡的差別也就是這裡的差別
那前面的差別其實不大
前面只是一些是這樣還是這樣的區別而已
那麼我們可以做簡單的實驗
就是你如果有一個東西你如果讓它那麼後來人家就用這個來做model 嘛那就就這個圖
就就這個圖就這個model 所謂的source model 就是那既然這樣的話那我乾脆就這邊其實也弄一個generator
讓它產生這個東西
然後呢那這個那於於是我這個generator 等於是等於是在模擬後面這邊讓它generate 這種東西 然後呢你變成這個
當你這樣做的時後呢我們其實是可以人家做過很多實驗就是我同樣的這個東西我只要這邊改成阿還是嗚的口形的model 的話出來就是阿還是嗚並不受前面的影響
同樣你只要那一堆random noise 跑出來之後你前面是ㄕ還是ㄙ你也是在這邊影響就夠了那出來就是ㄕ還是ㄙ
那倒不受前面影響
所以前面其實很簡單只有這兩種
然後呢只有這個東西你要你要看妳的pitch 是多少決定你的音高
我們說過這個東西這個如果是週期越變越大的話表示它的它的f 零在變低表示說它是音調在向下掉等等那是在這裡
那這就是這邊就是這就是我們這邊所講的這個voice 跟unvoiced 然後呢這個pitch 跟音高跟聲道
那麼我們上週還說的另外一件事情就是frequency domain 的distribution
那這點我們這個已經提過
不過很簡單的repeat 一下
就是說呢你任何一個聲任何一個訊號都一樣
你除了在time domain 有它的這個是這是這這個是在在time domain有它的這個呈現的方式以外
那當然你也可以把它變成discrete 變成n 永遠是可以的
你只要在這邊取sample 就變成n了
那麼我們永遠有另外一種呈現的方式是在frequency domain 上
那麼這個東西的話呢你可以想像它是一堆frequency 所兜起來的
那麼這個上面的某一點譬如說是omega one 那這上面有一個值
這個值是一個complex variable
它是一個複數的值
那麼我們如果寫成a 一的j fi 的話
那麼它所代表的其實是一個e 的j omega one t
這個值代表的是一個這個東西
那它的大小是多少大小是這個就是a e 的j fi
那這個究竟是什麼呢
妳如果覺得不太容易有感覺的話我們就取它的實部那就是一個cosine
那你就了解他其實只是一個cosine而已
那麼這個a 代表的是它的大小它的amplitude
而這個phase 所代表的是它的時間的零在什麼地方
那麼這個phase你可以想像它可以在有零到二pi 裡面的變化
我們可以看這個cosine 是從真正的位子是在哪裡等等
那這樣的意思等於是說我現在把這個signal 我把它拆成一大堆的cosine
每一個omega 上面都有一個值都有一個cosine
那麼他們就是有不同的frequency 不同的phase 不同的amplitude
那他們這些東西加起來應該是構成他的
那這兩者之間的關係是所謂的fourier transform
那麼當我們是用discrete 來做的時候變成x n 時候也可以做
那這時候這邊也可以變成discrete 的一個一個
那這個如果這個也變成discrete 的一個一個的話我們通常這邊就變成k 軸
那也變成discrete
那這兩者之間的關係呢就是所謂的discrete fourier transform
也就是所謂的d f t
那麼因此呢我們所有的signal 我們都可以從time domain 來看也可以從frequency domain 來看
那如果是這樣子來看的話呢
你可以發現真正的我們拿一個語音訊號
不管是這種還是這種拿來這個作一個fourier transform 的話
你看到的都會是像一種某一種樣子
那麼基本的架構是這樣
那上面長很多毛就非常凌亂的毛長在上面高高低低的
但是呢你可以看到她有一個基本的這個這個結構
那這個基本結構裡面很重要的東西就是所謂的peak 
這裡面幾個peak 的位子
這就是所謂的f one f two f 三
這是它的frequency domain
那不管是這種voice 還是這種unvoiced
你做transform 都會得到這種東西都會看到她的f one f two f 三這種東西
那這種東西呢其實是那麼根據他們的分析呢這個其實這個基本的結構我們說如果她有一個基本的結構是這樣的話
這個基本的結構其實是由這一塊來決定的
也就是你的口形長怎樣
它或者說妳的口形長怎樣或者說它是巫還是一還是阿其實是這些東西決定的
那這些東西其實就是我們所謂的formant frequency
或者formant structure 這些f one f two f 三就是我們所謂的formant frequency
那麼至於說上面那些毛
它高高低低的那堆很凌亂的毛
那其實就是由前面這些東西所造成的
那麼如果分析的話是可以發現你前面這些東西的的她的特性它的frequency
妳如果拿這個也去做frequency 的話呢
那它幾乎就是上面那堆毛的變化
就是在前面這些東西上面
而它的真正的這些高低呢
是是由後面的這一堆就是這個這塊聲帶或者說是vocal tract 所造成的
那麼在這樣情形之下呢我們所謂的formant structure 或者formant frequency 就是指這裡面的幾個peak
那麼我們說呢這幾個peak 其實是區別聲音裡面非常關鍵的部分
早在很幾十年以前當時的語音學家就已經發現
譬如說前面這兩個frequency f one 跟f two 是非常重要的
那那麼你如果拿f one 跟f two 來做圖的話
你會發現某一堆音就在這裡譬如說阿在這裡巫在這裡一在這裡或者怎樣
那基本上呢你不管什麼人發的阿基本上都在這一堆怎樣發的一都在那一堆你
可以根據這個f one 跟f two 就可以把他們區隔出來
那麼類似情形發生在各種聲音上都是這樣
那麼因此呢這些的peak 遠比其他的部分來的重要
也就是說你到底是f one 跟f two 的值f 三的這些值是多少
這個所帶的information 遠比這邊到底多大多小或者這邊這邊等等的information 要重要的多那這些東西就是所謂的formant frequency
好那麼那我們上週其實還有一件事情
就是說那你真正講一句話的時候是怎樣
呢我們真的說一句話是一大堆phone 接起來的
你說今天天氣很好前面是基後來是一這邊是因然後ㄊ一ㄣ
你中間的聲音不斷的是不同的phone 串起來的
那每一個phone 它有它自己的特性
所以呢你很可能這一段是voice 所以它有它的週期
然後這邊變成是unvoiced 它就會變成一堆unvoiced
然後呢這個之後呢又有另外一個voice 是聲音出來它於是就會變成是另外一個然後之後他可能又接到一堆unvoiced 她又變成一堆unvoiced
那我們上週有看過就是你真正的聲音就是長這樣的所以這是某一個phone 這是某一個phone 這是某一個phone 這是某一個phone 它從一路這樣接過去的
那這個是我們聲音裡面本來就是不斷地在變化
譬如說你可以想像這裡面這前面可能還是一個一個unvoiced
譬如說這是需這是巫這個是斯這個是阿這個是夫等等之類的
那麼你每一段其實可以看的出來那麼這一段是unvoiced 這段是voice 這段是unvoiced 這段是unvoiced 等等
那這是不同的聲音這樣串起來
所以呢那這個時候呢就會有喔你如果直接去做這個這個frequency domain 的話呢
那麼你如果把它整個的全部來一起來做這個東西會變成沒有意義
我們必須一小段一小段來看
那這個話的意思是說呢
在你如果去看一般的課本裡面講的這個這個喔嗯fourier transform 的話呢
它的數學式子應該是要它的數學式子應該是要
譬如說x of p e 的minus j omega t d t 積分積負無限大到正無限大
這是一般數學課本裡面的fourier transform
或者說是discrete 的s n e 的minus j omega n summation n 等於負無限大或者正無限大
那這是一般的課本裡面講的fourier transform
你如果把它你真的要算這個跟這個照說好像應該要這樣子算
可是你如果這樣子算的話就把它全部通通都加起來的話就把它從頭算到尾
你就會把你所有的聲音全部混在一起
把不同的母音不同的子音全部搞在一堆
那這樣其實是是就把你的所有聲音都混在一起了你其實看不出來真正的真正的變化
所以我們怎麼做的就是我們上週在下課的時候提到這個spectrum gram 那它不是這樣做它是每一次只取一段
譬如說我這一次取這個window 那在這裡面去做於是因此我這個summation 或者積分不是積整個只是積一小段
當我只是積一小段的時候呢這個時候才能夠清楚的呈現巫的這個音它裡面是怎樣的
那麼待會我再shift 一個window 慢慢看到由巫變成司那麼我這個window 慢慢shift 我可以看到這個spectrum 慢慢變化的情形
那我唯有這樣子在用一個很短的window 不斷的在每一小段去看她的spectrum 的變化
我才可以看到她每一個音ok 那個巫長的是這樣等一下變成阿的時候會長另外一個樣子那變成需又長另外一個樣子
你那樣才看的到一定要是在一個一個短的window 裡面才看得到的
那麼因為這樣子的關係所以呢我們真正在做這個的時候
我們不是用一般的課本裡面這樣子從頭加到尾或者從頭積到尾而是一個一個小的window 不斷的這樣shift 過來
那就是我們上週下課所看到所謂的spectrum gram
也就是說你看到的辨識變成是像一個像一個畫的很小就好了你每每一段看到的是一些嗯就像這種東西
那我的縱軸是frequency 或者omega 橫軸是time
那我等於是說我這邊有的時候寫f 有的時候寫omega 是因為不同的課本裡面有得有的喜歡用f 作單位有的喜歡用omega
那你知道他們的關係是omega 等於two pi f
那麼depends on 你要用omega 還是用f 其實都可以中間的關係差一個角度的關係
那麼這樣子的圖就是所謂的這個spectrum gram
也就是說你其實是在拿這每一小段每一小段分別去做每一小段每一小段分別去做這個fourier transform 的時候
你分別可以得到一個一個這樣子的代表那個音是巫還是由巫變成司還是什麼的時候那麼他們的spectrum
那麼因此呢你這裡每每一行其實就是一個那個的它的用顏色或者黑白高低來代表這個這邊的高低大小
那這樣的話你就可以的到那這樣子一塊一塊一片的聲音的變化
那麼這個時候呢這裡面的peak 你也可以發現這可能這就是f one 這就是f two 那麼他們隨著時間變化的情形那這樣子就是所謂的spectrum gram
好那這些東西大概是我們上週下課前在講到的東西
那麼有了這樣子之後呢那麼喔
一般而言我們最常看到的是這件事情就是所謂的這個source model
那這個model 最常用在所謂的low bit rate speech coding
那這塊呢其實就是今天各位打你的手機的電話的時候妳的聲音其實是經過一個這樣子的處理變成一個low bit rate speech coding
那這一塊我們平常在我們這門課裡面沒有不太多說所以我們就在這簡單的說一下
那麼換句話說呢你如果把這個我們發聲的這整個的講話的這一部分看成是一個像這樣子的model 的話
我們就可以想像成有一個model 來描述我們的口型的變化這就是所謂的vocal tract model
然後前面有一個generator generate 這個u n 那就是所謂的adaptator的generator
那這樣子的話呢妳u n 經過x n 出來x 就是我的聲音
那如果是這樣子的話呢那麼喔我就可以用這個model 裡面用一些參數來描述它
前面這個generator 參數是比較容易想像的
那麼照我們這邊講法其實就是兩件事情第一個你告訴它是voice 還是unvoiced
那麼如果是voice 的話你還要告訴他我的pitch 是多少
那你只要告訴他這兩件事情大概它就可以產生這個u n 就可以了
那麼vocal tract 的話呢這個比較複雜因為這個depend on 那你那個口型長怎麼樣
所以呢可能需要比較多的參數去描述它
那麼也許是要主要是要描述你其實真正的那個唇齒舌之間的各種關係
那如果把這點講清楚的話呢那你這個model 就可以出來了
那麼在你如果修過譬如說電機系的訊號與系統或者是資訊系的d c c 的話
你就知道那這個時候這這中間的一個簡單的描述方式是我這個中間這個vocal tract model 我可以用一個數學式子g n 來描述它
那如果是這樣子的話呢這樣它們的關係其實是x n 呢是u n 跟g n 的一個數學關係所謂的convolution
那如果你沒有修過並不了解這點其實也無所謂你只要知道這中間是有這麼一種數學關係是可以描述的
那麼我們就是用一個g n 的function 來描述這個vocal tract 然後他們的關係其實就某一種數學關係是所謂的convolution
那如果是這樣的話那其實也可以再進一步就是你這個x n 我可以變成它的frequency domain 描描述的方法就是x n
x 的omega 那這個東西就是相當於我們這邊講的這個我的這個x n 我可以用這個的x 的omega 來描述它
那x 就是這個東西那麼我如果那個x n 可以用這個x omega 來描述
同理u n 也可以用這個u 的omega 來描述
在這個情形之下他們之間的關係變成一個乘法其實這個g n 呢也可以用一個g 的omega 來描述那麼於是呢我就得到一個乘法的關係
那這也一樣如果你唸過這些東西你就知道那麼如果沒有唸過不知道也無所謂你只是了解說這中間其實都可以用frequency domain 的關係來描述的那我中間也是一個數學關係是g 的omega
那這個東西的transform 就是它的fourier transform就是它
那他們他們中間關係是一個乘法
你如果唸過z 的的表示法的話知道他們之前其實還有一個z 的表示法那我想這個比較喔知道就好不知道也無所謂
那這樣的model 有什麼好處一個最大的好處是用在所謂的我們剛才講的low bit rate speech coding
也就是譬如說你今天打打你的手機電話的時候那你知道你講的每一句話其實都要轉成一堆零跟一才能夠傳送出去
那要轉成零跟一最直接的想法其實就是每一個sample 都是一個real number
那麼每一個real number 你都可以變成一堆binary representation 用一堆零跟一去描述每一個點那這樣的話呢它其實就可以變成一堆零跟一傳出去了
只是說如果這樣做的話你呢需要很多個點需要很多個bits
所以呢你每一秒鐘要非常多的bits per second 這就是所謂的bit rate
 這個所謂的bit rate 就是bits per per second 你每一秒鐘的語音
每一秒鐘的語音到底要多少個bits 來傳送那麼很可能會需要非常多的bits 才能夠傳送一秒鐘的聲音
那這個時候呢當你打你的手機的時候呢你就知道你的手機的frequency 是非常珍貴的
它能夠送你你如果唸通訊語言你就知道它能夠送bits per second 是非常珍貴的
所以我們希望能夠用最低的bit rate 來傳送就是所謂的low bit rate
那如何做到這件事呢那麼很直接的想法是你如果這樣子來看一個一個sample 分別當成一個real number
然後用多少個bit 來送出去的話呢你是把它當成一個arbitrary 的reform 當成它是一個arbitrary 我根本不知道它有什麼特性
但是事實上如果是語音的話其實他不是arbitrary 因為我們都已經知道它已經長成這樣嗎它它不管怎樣它就是那個樣子它其實不是arbitrary
譬如說它不然就是長的這種unvoiced 的樣子不然就是長的那種週期性的它也就是就是一定一定的長相
的那其實意思是說我們的語音並不是一個arbitrary 的waveform 它是有一定的形狀的
因為它都是這樣子的model 它都是這種這種器官產生出來的所以呢它都是有一定的model 這個長相的
那麼或者你也可以說呢雖然都是因為它都是人的嘴巴說出來的雖然每個人嘴巴不一樣有的大有的小或者說每個人的舌頭不一樣有的大有的小不過事實上他們差別都不大所以出來的訊號都有一定的長相
那既然如此呢我們如果能夠做出一個好的model 來我只要用這些參數來描述它的舌頭嘴巴等等
用這些參數來描述它的聲帶等等的話那其實我做出來的聲音就是
那麼因此呢你真正的做法其實就是我變成說我只要我把這個model 變成一個程式放放在晶片裡面
那我其實當你要講一句話傳出去的時候我只要算我的那句話裡面它的這些參數各是什麼
當我在講這句話的時候我其實我在做聲音也是一個一個frame 或者一個一個window 算這個window 裡面我其實是voice 還是unvoiced 然後其實她的pitch 是多少
然後這個時候我的脣齒之間呢是一個怎麼樣的function 我如果把這個知道了的話我就把這些東西傳出去就夠了
所以我真正要digitize 然後要傳送的就是這些參數
那到了接收端的話呢妳接受到也就是這些參數但是因為這這這整個是一個程式寫在那個接收端那個手機的晶片裡面
所以呢它就只要把這些參數解出來去操去操作這兩個model 去一算算出來就得到你的聲音
因此呢其實你今天再用你的手機跟你的朋友打電話的時候你說的話進入的你手機做的第一件事情其實當然是取它的sample
當你變成這堆real number 之後不是他們直接把每一個real number 變成一堆bit 而是我們就會去判斷一個一個window 去算它這些個參數
當你算出這些參數之後我就直接把它把這些參數去digitize 然後傳送出去
那到了接受端呢他再把它解回來之後它有一個程式在裡面就把它算出來就得到你聽到的聲音
那如果是這樣的話呢最大的好處是我需要的參數很少而且變化很慢
也就是說在這麼一段這麼一個window 裡面的聲音其實我只要少數的參數
這邊所需要的參數並不那麼多那麼我就而且從這裡變成這裡搞不好沒有太大變化
如果我這一段都是同一個巫或者同一個司那可能變化並不太大所以它的參數也變化很慢那不像在這裡會變化的非常快
因此呢你原來在用如果每一個real number 分別用一個bit 來傳送的話我所需要的bit per second 是非常高的
可是變成這樣子之後我可以變的很低那這就是所謂的這個low bit rate speech coding
那麼那今天其實各位在打的妳的手機電話其實都是以這樣的一個原理在操做的
那這個是這個model 一個最重要的最重要的contribution
那麼對我們今天整個的這個我們的生活日常使用裡面用的最多的應該是這一塊
那這裡面到到我們這門課倒並不是以這個為重點所以我並沒有多說它你如果有興趣的話其實你去去找像low bit rate speech coding 這樣子的keyword 會找到非常多的文獻它都再講這些事情是怎麼做的等等
那我想我們這邊就大概把這些東西提到
好那有了這個了解之後
那麼底下這張圖其實就是在描述把剛才的這個model 說的再精確一點我們說這一塊叫做excitation generator 就是產生這個u n 的
這一塊呢所謂的vocal tract model 就是在model 我們的那個發那個聲音的那個口形的
那你如果仔細的畫的話呢那就是這邊的這兩塊那這個就是產生那個u n 的
那那個u n 呢我們說其實最簡單的解釋是其實他是unvoiced 還是voice
那麼因此呢你其實只要知道是我用一個bit 告訴他說現在這個window 是voice 還是unvoiced
如果是voice 我告訴他的pitch 是多少這樣就夠了
那你再多一個就是一個game 那麼這個amplitude 大小是多少
所以呢你只要這幾個參數那其實就可以產生我要的u n
那當然這個是早年最早的最粗的model 這樣子作
那今天你真的在打電話的時候不是用這個你用的是比這個精細很多的那這個只是我們從最基本的最基本的觀念來講的最最原始的最粗糙的model 是這樣子的不過這個觀念可能是最容易想像的
那麼口型這邊是比較複雜你如果真的要去描述脣齒之間的各種關係的話這東西是相當複雜的
不過後來人們發現其實可以簡化成為這個model
那這個model 這樣子寫用z 來寫可能有點頭大
那你如果不用z 來寫的話那麼喔其實那個是是一個非常簡單的數學式子
那其實你你如果是對於這個z 的表示法熟悉的人你一看就知道了
那你如果不熟不熟悉也沒有關係這個關係其實就是x n 減掉summation 的a k x 的n 減k k 等於一到p 等於u n 就是這樣一個數學式子
那那這樣一個數學式子用用z 來表示變成這樣子就是
那這個意思等於是說其實你只要把x n 的前面若干個點拿來然後跟u n 加起來就得到下一個點
阿所以呢你你其實你看這個這個這個這個加這個等於這個嘛
所以呢你其實只要給我前面這些個點
前面這些個點給我一個下一個u n 我就可以可以把下一個點加出來嘛所以呢其實是一個非常簡單的數學運算
那這樣的一個關係呢可以描述這個我們的這個u n 跟x n 之間的關係
而於是我現在的參數是什麼所謂的parameter 就是這堆a k
那這堆a k 是需要計算的是需要求的是沒有錯
所以呢你就是在給我這堆東西裡面呢我要從裡面算出這些a k 出來
也就是說你今天如果在講一段話的話我每一段聲音我通通都都經過一個演算法去求所謂的參數
什麼參數呢其實所謂的參數就是我們這邊講的這些包括它是voice unvoiced 它的pitch 它的a k 這些東西
那如果這些東西都算出來的話那我到了對方我只要把它東西把這些參數帶進去一跑跑出來就是我的聲音
那麼那這邊所講的其實就是我們剛才所說的因為這些東西其實就是我的vocal tract
所以它其實已經告訴我formal structure
也就是說我們說真正的formal structure 是就是像這個圖上面這個它在哪裡有formal structure 哪裡有peak 的這個長相就是所謂的formal structure
那這個formal structure 很很顯然是由這個決定的
也就是由這堆a k 決定
所以你你有這一堆a k 我其實是可以算的出來它的formal structure 是長怎樣
那這個事實上就是我們到底是阿還是巫還是一還是斯還是夫還是施那真正的關鍵就是這堆東西
那我們這個講的是一個非常粗的model 他只是一個good approximation 而已
當然我們今天用的是比它細的多不過呢這是一個最容易了解的一個圖所以我們用這個圖來講好
那底下這一頁其實大概沒有太多東西就剛才我們都已經說過了
那這邊只是在在畫一個也就跟我那邊剛剛畫的一樣的意思
那就是我們發聲的時候從氣氣是從這個肺在氣管過來
那這邊就開始就就有聲帶震動等等等等最後呢其實我們氣出來的東西一個可能是口腔一個可能是鼻腔
那麼因此呢我們如果真的要model 的話呢那麼這邊這是聲帶在這邊開關那麼然後你進來的時候我有一個口腔有一個鼻腔
那這兩個都有可能有氣出來
那在多數的時候我們發的聲音其實你如果感覺一下你就知道呢多數的時候你講話的時候聲音是從口腔出來這邊是沒有的絕大多數case
但是又有少數case 例外是反過來就是鼻音的時候
所謂的鼻音是我們這邊關起來沒有聲音
聲音是從這邊出來的那就變成鼻音
那你如果發一個嗯還是什麼你就會發現其實我的這邊嘴巴這邊是沒有氣的我已經關掉了那氣是從這邊出來的那就是鼻音
那底下這張圖是真正的去量出來的進入口進入這個氣管從口腔進從這個聲帶這邊進入口腔的氣流
我們剛才所畫的這個東西其實是一個比較誇張的畫法真正的是長的這樣的
就是說你這個就是我們講你你聲帶這個地方一開一關的妳震動那邊打開的時候他一一團氣它大概是長的像這個樣子的
就是我這邊畫的這個圖那真正的長相大概是這樣
那麼底下的這一張沒有什麼不同只是又重畫一次而已
所以呢妳如果看底下這個圖almost 就是我們剛才畫的上一頁的圖
這個我這邊進來的j 這個這個adaptation generator 可以有voice 跟unvoiced
那如果是voice 的話呢不過就是產生一堆這個這個嗯告訴他週期它就產生一堆那種這個週期性的signal 就是了
就像我們那邊所畫的這樣產生一堆這種東西就是了
那如果unvoiced 怎麼辦呢unvoiced 我們就就用一個random sequence generator 產生一堆random number 就好
因為我們說過這個地方其實你只要產生一堆generator 用那個random number generator 產產生一堆random number 就可以了
那這時候你只要這邊用的對你這邊用夫還是司還是雌這不同的口型你只要這個model 用的對出來就是你要的聲音
那倒並不care 這中間是什麼
那你也可以做這個很多實驗你說你把這一段跟這一段因為是random number generator 所以這一段跟這一段長得完全不一樣嘛可是你聽聽看他最後出來的你是是什麼音就是什麼音並不受到你前面是這個還是這個的影響
那麼換句話說你其實在unvoiced 時候你只要用一堆random number 就夠了
那麼其實進入口腔氣就是那樣的一堆random 的東西
那麼你是發的是什麼音是後面這塊的影響
因此呢我們在這裡只要作一個random number 的random sequence generator 就可以了
這也就是在我們上一頁的那個圖裡面也就是這樣子畫的
你看如果是unvoiced 的話就是random sequence generator
你只要產生一個random sequence 就好了
這個並不介意是哪一種random 都沒有什麼關係
那所以呢底下這一張圖almost 就是我們剛才的上面的這一張圖是完全一樣的了
那上面這一張圖是稍微複雜一點
那它把一些更多東東西呢弄進去
譬如說這邊多了一塊這個
這個是什麼呢就是當你聲帶在震動的時候其實那裡不是一個那麼單純的是稍微還有一點別的東西的
它用一個ma 用用一塊來model 那些東西那就是我們看到的這裡嗎它其實是這樣子的
那所以它還有一些有一些這個特別的長相跟變化等等
你如果要把那個也弄進去的話那就會變成這一塊
那還有一塊呢就是這個lip radiation
也就是說當你的聲音真的從嘴嘴巴出來的時候
你可以想像它在這個之前它是受到這邊脣齒各種的constrain 所以他在氣只能在這裡跑
你從嘴唇一出來它忽然變成沒有限制的一個free space 可以自由的完全的這個這個發散出去
所以這中間的變化其實是也是有另外一個嗯數學關係可以寫的
那就是底下的這一塊就是所謂的lip radiation filter
那我們現在只用一塊其實可以看成是一個combine
就是說把這三塊的效果這個才是vocal tract 這個是我們那個口型的
其實你要完整的講的話呢這個口型的在這裡前面有這個後面有這個
但是我們現在講的東西通常就是把這三個合成一個在這裡所以我們可以看的這麼簡單
其實是應該有這個效應這個效應跟這個效應合在一起的
也就是我們之前畫的這個這個數學式子或者是我這邊所寫的這個數學式子
那其實是已經把這三個效應畫在一起了那就只用這一個就只用這一個就可以了
那也就是這邊的這邊的意思那所謂的combine 就是把這三個合在一起了
好那以上我們大概是把這些聲音的這個基本的知識大概稍微說一下
那麼嗯因為我們喔要處理的東西是語音訊號我們必須要知道它長怎樣然後我們才知道我們後面為什麼要這麼做
那麼我們要了解那麼我們的聲音是這樣子一系列不同的phone 串起來的
每一個phone 它either 是voice 還是unvoiced 還是怎樣它都是不同的口型兜兜兜成的
所以呢它是不斷的在變化的
那麼在這個情形之下呢我們底下要做的事情必須要考慮到這些狀況我們才才知道為什麼我們是要這樣子作
那麼因此從這個以後我們現在底下要講的最主要的事情就是怎麼樣子做這個feature exaction 也就是其實就是我們所說的m f c c
所以在下來我們所說的其實就是要求m f c c
那麼我們在這裡會仔細的再講這裡面每一塊怎麼作
那這個我們在二點零已經說過一點
那麼他為什麼要這樣子作
那麼當我們有了在這裡講的這背景之後我們會比較了解為什麼m f c c 是這樣子做的
那我們先來說我們要抽抽這樣像m f c c 這種東西的話
那一個最主要的這個幾個考慮包括呢
第一個我們當然希望找一些參數他真的代表我們聲音裡面重要的部分
也就是說你如果光用眼睛去看的話
這些聲這個這東西變化萬千你實在無無法想像
那它們也就是說你如果直接從waveform上面看呢不容易看出來
我們如果用眼睛看是不容易判斷為什麼這一段是巫這一段是一那他們也就是說你如果直接從waveform上面可能不容易看出來他們的很多重要的東西在哪裡
那因此我們需要找一些feature
這些參數這我們這些參數我們希望它能夠真的描述聲音裡面一些重大的特性的一些參數這是所謂的feature
那這個到底是什麼呢其實我們不知道
那麼這是多少科學家一直在想而不容易找到答案的
那麼他們找比較容易想到的辦法是我們就去學人的耳朵是怎麼聽的
這是人的聽覺系統
那麼他們是怎麼聽的呢
那麼人就是這麼厲害
那麼不管誰說的阿我們一聽就知道它是阿不管誰說的是一我們就知道他是一我們根本不用看那個waveform也根本不要什麼我們一聽就知道了
人的耳朵為什麼這們厲害呢我們希望去學習人的耳朵是怎麼聽的
然後從那裡面找一個類似的model 來做這是基本m f c c 的想法其實是這樣來的
那麼第二個呢所謂robustness 就是我們希望他比較其實這個是很難做到我們只能說我們希望他比較robust
所謂比較robust 就是說因為我們的聲音是有很多外在環境的破壞的
包括雜訊的干擾包括你的聲音通過電話就是通過電話就經過channel 的傳送的變化
不同的人說的阿為什麼不同的人說的阿明明都不一樣為什麼我們聽起來都知道它是阿呢
然後這個transducer 譬如說是麥克風譬如說喇叭
那這一些個東西的每一每一支麥克風的都不一樣妳同樣的一個聲音經過兩支不同的麥克風出來的東西就是不一樣了
為什麼我們人聽還是一樣的聲音呢
那麼就是我們希望能夠僅可能比較robust 一點
那再一個呢就是dynamic 就我們的聲音裡面顯然有很多dynamic 的特性那我們希望能夠抓到
所以dynamic 是指說它是它是在變化的
因為我們不斷的我們不是只發一個阿不是只發一個音
我們是把很多很短的phone 把它串起來
所以我們聲音不斷在變的而且這個變一定是連續的
因為這是一種口型那是一種口型這是一種口型
那麼他們中間是連續變過來所以是一個time variant 隨著時間連續在變化的
那麼因此呢他們在時間上變化的情形應該也是一個重要的
所以這些是一個一些主要的考慮
那麼底下我們來看我們要說的m f c c
那這個圖其實是我們裡面的所有東西我們大概在二點零都已經說過了哦
那我們現在再重新先再來revisit 一次那麼這個時候我們再來想一下裡面為什麼是這樣
那麼這裡面呢我們說m f c c 是我們今天最普遍使用的
那它基本上呢公認是有一個合理的accuracy
我的計算量也相當的小是一個合理的正確率跟合理的計算量
那麼大概在不同的不同的application 不同的task 大概都不錯
那麼其實我們今天真正在實驗裡面在研究裡面用的是比這個多很多種啦
feature 本身是一個很重要的研究題材那有千千萬萬種不同的feature
不過m f c c 始終是少數這個最使用最普遍而效果算是大家公認不管你是辨識什麼
不管是阿拉伯文還是伊朗文不管是阿還是巫還是一不管是在這個電話的還是雜訊的還是不管是什麼基本上大概都還不錯
那麼它的這個整個的計算的過程這個我們大部分都已經說過喔
就是我譬如果我我我底下整個七點零在講的是這個所以我們稍微用看一下它的這個符號
假設我原始的聲音叫做x n 這就是我最開始的這個x n
我做的第一件事情是做pre emphasis 把它x n pron
變成x n pron 之後我開始取window
那這個window 就是我們這邊講的這一個一個
那這個window 我們可以以它的某一個時間點譬如說他的這點作為t 的話
那麼這是t 這是t 加一這是t 加二t 加三等等
這是t 加二這是t 加三如果這樣來看的話呢我就是以這個t 作為我的index
因此呢我經過window 之後我就得到一個x t 的n
那這個時候等於是說
以這個為例的話呢這是x t 的n 的話我其實就是得到這一段
所以你得到的是這一段外面就沒有了
那這樣子的話我就得到這個window 的這一段這就是x t 的n 然後這個時候我拿來做所謂的d f t
d f t 就是我們這邊講的discrete fourier transform
那麼做了這個d f t 之後我就把這個x t 的n 轉到這邊來
但是因為這是discrete 所以他得到的並不是一個連續的而是discrete
所以我的index 變成k 那我就得到這個這個index 為k 的這邊個這邊的一系列的點
那其實就是我的x t 的k
那麼這時候我開始在上面作所謂的mel filter bank
那你如記得的話是一系列的三角形的
我們二點零講過就一個一個三角形
那麼等於說我在每一個三角形取它所cover 的這一些點等等
那這樣子話呢我每我這個時候得到我就把它叫做y t 的m
這個t 還是這個t 就是這個window
我每一個window 得到一個嗎這個t 也在這裡嗎
所以我每一個window 得到一個這個這個就變成我們的這個x t 的k
我這個東西現在橫軸現在叫做k 因為現在變成discrete 的點變成一個個的sample 它也是sample 那我變成x t 的k
然後我在這上面取所以呢這個t 仍然代表我這個window 裡面的東西
那我現在這個window 裡面的東西呢這個m 呢這個m 是我這個這個filter 的index
第一個三角形是m 等於一第二是m 等於二第三是m 等於三等等
那這樣子的話呢我得到一系列filter 的output 是這裡
那這個時候我再取絕對值平方取log 那麼這個時候我們稱稱為他的pron
這個時候我再做inverse d f t 轉回來得到的這個東西就是我們所謂的m f c c
它還是對某一個t 而言的
那這個時候得到的另外一堆index 叫做j 這我們後面會再說到
那通常到這裡的時候呢這就是我們所謂的m f c c
但很多時候我們同時把energy 也拿來這個energy 就是那一個frame 的energy 或者那一個window 的energy
你可以把這一段這一段所對應的energy 全部加起來那這邊這樣你所得到的就是e t
它也是對時間t 的那個frame 而言的
那這個時候呢我們通常譬如說這邊可能取十二個或者十三個再加這一個的話呢大概是十三個或者十四個就是這邊的前面十三個或者十四個
這時候我們可以做微分一次微分跟兩次微分就得到另外的這樣我總共得到三組
如果一組是十三的話那三組就是三十九維我們講的三十九維的參數

就是y 的t /*這句也消失了*/

那這一塊的裡面我們底下現在就是針對裡面的每一塊單獨來看他怎麼做的他為什麼那樣子作等等
那我們先看最前面的pre emphasis
那這點倒是我們簡單我們之前也已經說過了那現在再說一次而已
pre emphasis 只是把我原來的訊號x n 變成一個新的x pron 的n 用的非常簡單的數學式子就是這樣
那這個把它變成這個就是用這個數學式子也就是我們這邊所說的把x n 變成x pron 的n 的這個pre emphasis
那中間的這個動作很簡單就是這樣一個數學式子如果寫成z 的話寫成這樣
那它的意思是什麼呢我們之前說過那個a 是非常接近於一的值零點九六零點九七零點九八這種值
當你a 非常接近於一的那麼一個值的時候它的效果是像一個這樣子的像這個零點九五像這種這種東西
那它是在高頻的部分會變的比較大
所以它是一個所謂的high pass filter
那麼所謂的高頻變的比較大的意思呢我們在這一頁有說我為什麼做這件事
那基本上對voice 的section 而言
你可以看到其實我們聲音裡面有voice 有unvoiced
但是voice 是一個非常關鍵的部分你如果voice 沒弄對當然就不對了所以voice 是所有的母音都是voice voice 非常重要的
voice 有個特性呢就是它是這個有一個slow 會慢慢掉下去
也就是說它的這個format structure 基本上是有一個斜率的這樣下來的
你越高頻它就越低
這個斜率基本上呢roughly 大概是多少呢二十d b per decade
也就是說你每十倍的frequency 就掉二十個d b
你如果從這個一百h z 到一千h z 大概會降二十的d b
一k 到十k 大概又會掉二十個d b
它幾本上是voice 聲音都是會這樣子掉下來的
那這個原因為什麼呢
應該是我們發生了有一些生理的特性就是這樣
那麼因此呢我們人的發聲系統不管是在這邊或者在這邊不管是在這邊或者在這邊總之我們就是會變成我們產生的就是會變成一個這樣子的斜下來的
因此越高頻的部分越微弱越不清楚
那麼因此呢也就是說這個high frequency 的的format 呢是這個它有它的安培會小的多比起low frequency format 來
也就是說你到了f 三f 四的時候會會變的比較微弱那麼比起f 一f 二來
因此呢你的我們做這一塊的目的就是把它拉上去
也就是說你如果本來是這樣的話
那我希望把它拉到上面來讓它可以我的這個原來的format format structure 我希望不要改變它
但是我把它拉上去
把它高頻讓讓它能夠這個到f 三f 四的時候可以差不多一樣高
那這樣的話呢我比較容易抓到所有重要的東西這是一個最基本的原因
也就是說我把這些higher frequency 把它拉高
可以得到比較清楚的所有的format 
那當然也有另外一個說法就是其實人的聽覺對對於一個kilo h z 以上的那些頻率其實是相當sensitive
那麼既然我們人會非常sensitive 的話
那那邊這麼弱為什麼會sensitive 不知道
那我們就把它拉高一點那麼那麼讓我們的比較sensitive 的的感覺能夠在這裡能夠感覺的出來
那這個就是我們這邊所說的這個pre emphasis 他在高頻拉高的這個基本的一個說法
那這個其實一個更簡單的說法也許是說有人做這個實驗之後發現欸辨識效果就是比較好嗎
那麼如果不不是辨識效果比較好就不做了嗎
就是因為不管怎麼樣做這個都是會比較好的所以呢後來大家都都做了
就是了好這是第一個然後第二個再下來呢我們要看的就是我為什麼要取window
取window 的基本的原理其實我們剛才已經說了
就是因為我的一段話是很多不同的phone 串起來的
那我really 要知道每一個phone 在幹什麼這個是巫這是斯這是阿每一個phone 在幹什麼我必須要知道
那我如果不取window 的話我做的東西都是像這樣的
那也就是我們一般在課本裡面所學的這個fourier transform 或者是這個discrete fourier transform
都是這種東西都是從負無限大加到正無限大或者是這樣子加的
也就是把它全部等於是把他所有聲音全部平均掉了
但是在我們這裡是不能這樣子做的因為我們的語音訊號顯然是每一小段是什麼音每一小段是什麼phone 才是最重要的
你如果都這樣做掉就沒有了
所以呢我們必須要取一小段好那這樣的一小段的話就是我們所謂的short time fourier analyses 或者是
這個也就是說我們的聲音是非常的這個none stationary
就是我我們的聲音的訊號是它的特性隨著時間在改變這個是這一個音這是這一個音當然不一樣嗎
它不斷的在改變的所以呢那你怎麼辦你只能夠取一個小一個小的window 來看
在這個小window 裡面你可能可以假設它它是某一種特性
因為它就是巫它就是巫它就是這樣這邊它就是阿它就是那樣那麼因此我可以在那一小段裡面可以看他的特性
那因此我就必須要作這樣子的short time 的這個fourier analysis
那麼因此我們也可以說我們這個語音是語音的訊號是非常的none stationary 他不是隨著時間變化的
但是我們可以假設它是在這個這個很短的時間裡面假設它是stationary 我們才可以做這一件事情就是所謂的short time stationary
換句話說這上面這句話講的意思是說我們一般課本裡面這樣講是假設那個signal 本身有某一種特性
他那個特性並不並沒有隨時間在改變
如果那個signal 本身沒有隨時間在改變的話那麼我可以從頭到到尾整個積分積起來或整個加起來作一個
那你如果每一段有不同的特性的話其實就不能這樣子做了
那麼因此呢我們的聲音不是他這邊所謂的stationary 我們的是none stationary 的
我們的聲音是none stationary 特性
所以呢不能夠把它想像成是stationary
然後可以然後再用這種方法來做其實是不對的
因此呢我們必須要做個假設說其實這個stationary 呢出現在哪裡出現在很小一段裡面
所以呢我如果只取這麼一個window 的話呢我可以說它是stationary
在這個情形下我可以在這裡面作
那因此我就是做這個short time 的stationary
假設因此我就是在做這個short time 的fourier analysis 那就是這段話在講的意思
那因為這樣子的關係我就必須要做window
那這時候就第一個問題window 到底要多長
顯然不能太長也不能太短要有一個一定的長度
那為什麼不能太長你如果太長的話把好幾個phone 都放進來那又失去意義的了
你你只要長到對不對包含好幾個phone 就沒有意義了你顯然是要短到應該只有在一個phone 裡面
那你只有在慢慢shift 過來的時候你會cover 到兩個phone
那這個時候這個window 其實是代表的就是由這個phone 轉成這個phone 的時候它有什麼變化
那慢慢再轉過來最後到下一個phone
你只能夠這麼長不能夠太長
當然你也不能太短太短的話可能短到你不能抓到裡面的特性那也就沒有意義了
所以一定一定要不長不短那差不多是在這個range
然後第二個呢你要隔多遠shift 一次
就是從這裡跳到這裡的時候到底要跳多遠這就是所謂的frame shift
那你也可以想像我們第一個不能跳太遠
如果隔的太遠的話這個跟這個已經完全不一樣了
那這個時候呢顯然中間少了他們變化過去的情形
所以你不能跳太遠
但是也不能跳太近跳太近的話就是我的計算量太大了
你每一個都要算的話計算量會會太大所以呢我顯然是要選一個不遠不近的這是所謂的frame shift
那這個frame shift 就決定了我的frame rate
也就是每一秒鐘要幾個frame
那這個幾個frame 就是幾個window 我們有的時候稱frame 有的時候稱window 是指同樣的東西
那這個就是我們在做window 的時候要做的事情
那麼底下呢我們就要進一步來說明你做了window 之後到底發生什麼現象
那其實是對聲音做了相當多的一些個喔作用在上面
我們這個喔休息十分鐘好了
好
我們接下去要講的就是 你做了window 之後會怎樣
那你可以想到的是說我們其實中間要算這些東西
要做這些個 嗯 m f c c 的時候
要做這種我們是非常非常 careful 它的 frequency domain 
也就是說這些東西
因為這些東西是真正幫助我們決定是什麼音的是這這裡是它的 format structure 
也就是它的 frequency domain 
這個這是影響非常大的
那也是為什麼譬如說我們要做 pre emphasis 
也是為了要把他這個東西修的更容易處理
所以 frequency domain 是非常的重要的
那當我們做了這個 window 之後在 frequency domain 有什麼影響
那就是我們這邊要說的
那麼事實上你做了 window 之後
無可避免的對他做了相當大的破壞
為什麼說做了window 會得到相當大的破壞呢
你可以想像譬如說
我的我原來的 signal 是這樣
然後我是用這一堆
那這一堆是一堆 cosine 
對不對我們說過
這個其實是一堆 cosine 加起來的
他是這樣這樣
等等
很多東西 這些東西 整個兜起來變成這個東西
可是當我做了一個 window 之後
譬如說呢 我從這裡 開始
但是呢我到這邊就沒有了
這後面就沒有了
那麼我等於是把這後面這些東西都切掉了
那你想會怎樣
我原來的這堆東西
原來的這堆東西 cosine 兜起來是這個的話
我現在 我要有什麼辦法可以讓它們兜起來變成 中間是一樣的但是外面會沒有
這個是一個很大的變化
我這邊要動很多東西很顯然 這個地方都被破壞了
這個地方我顯然都要動掉
我都要動掉很多東西之後才有辦法使得它加起來結果是 中間不變外面要變成零
ok 
那 於是你就可以猜的出來是中間受了很多變化
我這個地方 整個都 亂掉

那 這個整個都亂掉之後我變成是跑出很多不同的東西出來
然後也 整個動掉很多東西之後才有可能 讓他們都兜起來 會變成中間不變 而外面呢會變成零
那這個過程呢
我們從數學關係來講的話呢
就是 你乘上一個 window 
那你再乘上一個 window 在 frequency domain 而言呢
就是在做了一個 convolution 的關係
阿那 giving 這個 這個數學關係我們就這樣講
那麼你如果是唸過這些東西你就了解他是什麼意思
如果你沒唸過 嗯 不了解也無所謂
那基本上 我們知道有這麼回事
也就是說 我們剛剛講它真正實質 實質的影響 就是 整個都會被動掉
為什麼會被動掉因為我要把它 我現在要重新變成 他們加成起來要變成中間一樣
但是外面是零
那這個情形就使得我動掉很多東西
那這個動的情形呢 如果用數學來講的話
在 time domain 我這個 window 就是原來的 x pron 乘上一個window 
可是在frequency domain 的話我把他們都分別去做transform 
到frequency domain 去看
他們通通都到這個domain 來看的話
那他們就會變成
他的 transform 跟這個window 的transform 
再做一個convolution 才是我的東西
那這個convolution 變化呢就是我們這邊講的他整個全部都動掉了
那動掉些什麼呢
那麼基本來講
我們如果來看
他是一個convolution 的關係
那麼這個window 的 transform 是什麼呢
我們就看一個最簡單的rectangular 的window 
那麼 rectangular 的window 長怎樣
畫一個簡單的圖來說的話
那麼你如果熟悉就知道
不熟悉的話
你
了解一下就好
那麼它的transform 呢是一個 所謂的 sink 
這種東西
那這種東西呢 它其實他的 他這個兩邊的這個這個東西很快就變的很小
那 我們不太容易真的畫出來他是怎樣所以我們
通常把它畫成一個 一個這個 畫成一個這個圖
這個呢 就是 這個其實畫的就是這個東西
但是我現在是用log 來畫
所以這邊是取db就就是這邊是取log 了
取log 之後呢
我其實是這個地方等於是這個
那它的 這是它的第一個side lobe 下來
這是第二個
它是一個這樣子的
有一個 有
它是一個這樣子的
那麼這個什麼意思呢 我我我我取取log 之後
這個變成零
然後呢這個變成這個這個log scale 在下來
那麼當我掉在第一個零的時候它其實是在取
如果這個是 喔 這個transform 的話
它其實是在取這個然後取log 
取它的絕對值然後取log 
之後得到這個圖
那麼因此呢
我在 這個地方掉下來的時候它就這樣掉下來
掉到零的時候應該是掉到負無限大
因為取log 是會掉到負無限大嘛
然後再來這個的話就會跑出第二個出來
然後呢這個會在 這個是指對應到第二個
那這是指對應到第三個
這樣子
那麼 其中的這一 第一個呢我們叫做main lobe 
這個東西是我們所謂的
就是這邊寫的這個main lobe 
那另外的這一些呢就是我們所謂的side lobe 
就是後面這些東西
這些東西是我們所謂的side lobe 
就是這些這些side lobe 
那所以呢你你
這個圖其實跟上面沒有什麼不同只是因為 你你真的要畫上面這個的時候
你會變的很小很小
不太容易看喔
重畫一個
它是一個是一個這樣子的
那麼你很快就變的非常小
不太容易才看的 不太容易看的出來啦
因此我們通常用用db來畫
這樣的話呢 比較容易看的出來它一個一個
這就是它所謂的side lobe 一個一個side lobe 一個很清楚的在這裡
那就是我們這邊所畫的這一張圖的這個rectangular 這個
就是 這是這是它的main lobe 
然後這是第一個side lobe 第二個side lobe 這樣一直下來
那這個是用log scale 來畫比較看的清楚
得到這個 這 這個情形
那麼這個
那麼因此呢你你如果你的window 的transform 長的像這樣
這個是 我的 這是這兩個之間的fourier transform 
喔
就是你取一個window 
我取一個window 相當於是取 這個 譬如說我從零到l 減一
這中間是一 外面是零 
我如果取一個 這樣子的長度是l 的window 
中間是一外面是零的
他的transform 是長的像這樣或者說是這樣
那它有一 它有一個main lobe 然後有一堆side lobe 
那麼於是所謂的我的這個東西跟這個window 去做convolution 什麼意思
是我原來這裡面有一個這個
我們說原來是這樣的一個
這個東西去跟他去做一個convolution 
那這個convolution process 你如果了解的話你可以想像它的意思就是我這邊所寫的
那你如果不容易了解也就 聽一聽就好
那基本上呢main lobe 的的的結果是什麼呢
你這個東西去跟他convolve 的結果呢
就是它把它通通磨
就是我們這邊所畫的意思
它的效果是 就是把它都磨開來了
變成一片
都把它磨開
來所以呢 它就會 不再是原來那麼精 那麼精確的
會跑出一堆亂七八糟的東西出來
這是 這是main lobe 的效果
這個這個main lobe 
跟他做convolution 
就會把它都弄亂
那就是這邊所講它會sprayed out the y 的 frequency range 
你原來是有一個很清楚的一個 譬如說你本來有一個format 
本來是一個這樣子的
你會會會被他整個磨開來
變成一片
因此呢 會會使得你這個local frequency resolution in formant allocation 造成困難
你本來這個地方有一個formant 
非常 清楚可以看到譬如說這是f 三在這裡的
可是你這麼一搞就把它弄亂了
把它整個磨磨開來
所以呢你就不容易抓到它的formant frequency 了
這個是main lobe 所造成的效果
那side lobe 更糟糕
因為它會把遠距離的 遠距離frequency 攪和在一起
這些side lobe 的convolution 造成什麼效應
可以把這個不同的frequency 搬來搬去 哼
你如果了解這個convolution 效應你就知道
你可以把這個搬來 這個搬到這邊去 喔
把他這個搞來搞去之後
所以會弄得更複雜
這個東西會更散
因為你把這邊搬到這邊來 這邊搬到這邊去
所以會弄弄得更更亂一團
這是side lobe 效應
那麼因此呢不管是main lobe 還是side lobe 都是一個很不好的結果
因此你所得到的雖然我只是乘上一個window 
說穿了很簡單我只是乘上window 我只是為了要取這一塊嗎
我不要把別的音弄進來我不要別的phone 
我只要取這一段
講起來很簡單 其實我做的事情是把它整個都弄亂了
那簡單的解釋就是我們剛才講的
因為你要中間還要一樣
外面又要變成零
就把它整個都弄亂了
那用數學來講
就是我們這邊講的
convolve with 這個window 的transform 
那它長的這樣的結果
就使得我的main lobe 跟side lobe 
都有一堆效應
使得我整個都變的很糟糕
那這個就是我們平常取的window 不得不做
但是你做了之後
我們必須了解我們取了window 之後會變成這樣的情形
那因為是這樣的關係呢 所以呢
但是反過來你又不能不做
因為你如果不做的話是不行的啦
喔我們說過非作不可嗎 阿
但是你你你想要沒有這個效應除非你那個window 等於一for all n 
你除非你得到一個window 等於一for all n 的話
那全部都是 那那個時候就沒有不會有這種變化可是那樣你就抓不到你的這一段了
你就變成全部不行了嗎
那麼因此呢 我就我們這邊講的就是這個意思
所以呢你沒有辦法做這件事情說
我讓window 等於一for all n 
我必須取一段
你一取一段之後
譬如說用rectangle 就會就會產生這一堆問題
那因此呢我們這個window design 的問題就是說
我希望main lobe 越窄越好
side lobe 越低越好
我希望這個side lobe 越低越好
如果這個side lobe 可以壓到很低的話
使得我們剛才講的這個side lobe 的問題
這個他就搬來搬去這個問題
可以效應可以降低
所以我希望把這個side lobe 僅可能壓低
反過來呢我希望這個main lobe 越窄越好
main lobe 越窄的話也是使得我這個本來這個磨平的這個效應呢 可以可以比較小
所以呢我的目標就是希望main lobe 越窄越好
side lobe 越低越好
但是呢這個是做不到的
就是說你沒有辦法同時做這兩件事
你如果把這個side lobe 壓低的話呢通常main lobe 就會變寬
你如果把main lobe 變窄的話side lobe 就會高
那為什麼會這樣我想這個也不在我們這裡解釋
那你如果去修相關的課
會學到這些事情
不過我們這裡我們只是這樣說一下就是了
就是說你要同時得到這兩個的話是無法得兼的
所以呢中間存在一個trade off 
那我們今天最常用的window 呢是哪一個呢
是hamming 
hamming 跟這個有點不一樣
那我們之前也說過hamming 是我把它兩邊都壓低了
把它變成一個cosine 
我把它變成一個 一個這樣子的cosine 
那變成這樣cosine 的話在這上面效應是什麼呢
是比較成功的把side lobe 都壓低了
所以我比較不會造成這些frequency 搬來搬去胡搞一陣的現象
可是我無可避免的win 這個main lobe 會變的更寬
所以它其實他也會磨的更利害
那這個效應就是在我們這個上一頁的圖的這裡
我這邊畫的另外一個就是hamming 阿
那你看到從上面這個是rectangular 
底下這個是hamming 
那它明顯的把side lobe 壓低很多
從這裡壓到這裡
這壓低多少呢你如果看這邊大概幾乎壓低二十個db
二十個db 是一百倍的意思嗎
所以呢基本上是把它side lobe壓的非常低
這是從rectangular 走上的window 的 走上hamming 的成功的地方
但是無可避免的我的main lobe 是變寬了
是變寬了的
所以呢它有有好處也有壞處
但是這個好處是很大的
就是你看到side lobe 降低了二十db 阿
那麼因此呢我的這個有一些效應是減少
雖然有一些效應是無法克服的
那這個就是我們選擇這個hamming 的情形
那hamming 的這個式子我想我們從前就已經看過了 阿 這就是就是這個一個cosine 
那麼關於這一點呢其實我們在講二點零的時候曾經說過另外一個說法
其實也是同一件事
我們當時講
從從這個rectangular 變成hamming 我們有另外一個說法
就是他在time domain 上的說法
那其實意思是一樣的
是講同一件事
我們當時的說法是這樣說的
就是你的訊號譬如說我的這樣子
我現在如果我取一個window 
取這樣的話
取這樣的一個window 的時候
我就說應該長一點
取取取這裡好了
取這樣一個window 
那等一下我如果shift 一下到這來
會發生什麼情形呢
我這邊丟掉了很大的東西
這邊只收進來很小的東西
所以這個變化會很大
但其實我這個語音訊號從這裡從這個window 到下個window 有變化那麼大嗎 沒有
它明明是非常穩定的
對不對
從你可以看到從這個邊其實是非常穩定的東西
但是你從這邊變到這邊的時候呢 你這邊就會 可能丟掉很很大的東西 這邊只加進來很小的東西
使得我那個值會變化很大
是不合理的
那為了避免這個現象呢我的辦法呢就是ok 我變成是一個cosine 的形狀
那他兩邊被weight 的很
如果是這樣的話呢你可以看的出來我
這個時候
我就讓兩 兩邊的這個 這邊會丟掉 這邊會增加 這個變化呢
都被weight 很低
我強調是中間
那這個時候我就比較不會影響
那這個說法其實跟我們剛剛講的這說法是一致的
那只是這個是一個time domain 的說法你也許比較容易想像
那我當我從這個變成這個的時候因為我把兩邊都壓低了
所以它的影響很小
那這個效應其實就是我這邊的side lobe 的效應降低了
我side lobe 就不會發生那些現象
那就就比較好一點
這個是我們選擇hamming window 的一個原因
所以這邊講的是
講這個window 的process 
有了window 之後呢再下一步是幹麻呢 我們回過頭來看一下
我window 完了之後我現在就得到這一段了
我得到這一段了
我先來開始做discrete fourier transform 
那我們知道我transform 的時候中間會跑 因為取了window 會跑出一 一些
亂的現象
不過不管怎樣呢我們得到了一系列的sample 
就是這些個
這些一個一個東西
那這些東西就是我們的這邊所寫的這個x t 的k 
我現在是在k 的軸上
那麼我會得到一些這樣子的
這些東西這就是x t 的 嘶 喔 大寫吧
他是一個大寫的x t 的k 
那這個橫 我們說橫軸的k 其實就是frequency 
然後呢那大寫表示我現在是在frequency domain 上面
那 我是一個一個的
仍然是取它一個一個的sample 因為我的d f t 算出來的就是一個 一個一個的sample 
那這個時候呢
我們現在來看我後面要做的事情
其實就是這個mel filter bank 
那這點其實我們之前也已經說過
我們現在重新看一次
第一件事情我們要說的是
你因為我的signal 是real 的signal 
你如果去了 了解這個d f t 的話
就知道
不了解也沒關係
我得到的東西應該是會是對稱的
他是symmetry 
所以會變成只有一半有效就行了
換句話說你譬如說原來是l 
我這個window 這是l 是window 的長度譬如說五百一十二
我這個window 的長度從這邊的這個五百一十二點
我做d f t 之後
還是五百一十二點
但是其實因為它是左右對稱
我只要兩百五十六點就夠了
所以變成二分之l 
ok 
所以我這邊只要兩百五十六點
二分之l 就好了
而這兩百五十六點上面我來做這些mel scale 的這個filter 
那一個個的三角形我們把它畫在這裡你就看的出來它的意思
那麼depends on 
譬如說就就這個三角形而言
你可以想像是這幾個值
就是這幾個值
是被他所cover 的
那麼分別乘上這些個三角形的的weight 
然後加起來得到一個值
那我就等於假設是取這一塊
喔 等等喔
那麼我這一個個三角形就分別去取每一塊每一塊
分別就把這些個點乘進去
用這些三角形的weight 乘進去之後加起來得到一個值
那這點我們在之前就已經說過
那其實是在模擬
人為什麼可以聽其實我們始終不懂
那麼但基本上你可以想成是在frequency domain 上面
那一組聽覺神經就是管這堆frequency 
那另外一組聽覺神經就是管另外一堆frequency 
他們會overlap 
他們會overlap 
但是呢顯然overlap 沒關係
那他們就是這麼做的
那怎麼辦呢我們就用成這樣子的三角形
那這一點呢就是在我們底下這一 這邊所講的意思
喔
就是說 當你有一個 一堆複雜的聲音 在某一個frequency bandwidth 裡面的時候
你可 你的耳朵其實分不出來
這就是指這一堆 嗯
譬如說這一堆
這一堆frequency 
這一堆這一堆frequency 裡面的
譬如說有的聲音 有有一個這個frequency 跟有一個frequency 的話其實我耳朵分不出來
因為我是同一組 聽覺神經在那邊做這件事
所以你如果在這一堆裡面的話這個跟這個其實我是分不出來的
這是這句話在講的意思
這是講我們聽覺的現象
你在一個complex sum 的某一個 在某一個certain bandwidth 裡面的時候
它我其實是我我分不出來的
可是呢你如果是在另外一個的話你就分的出來了
譬如說如果一個frequency 是在是在這裡
另外一個frequency 是在這裡的話呢
這兩個我反而分的出來
為什麼會這樣因為這個是
它在聽這個是它在聽
所以結果我在不同的地方我反而分的出來
嗯
那其實說穿了就是我們這邊講的意思
就是說我其實是一堆一堆的
這一堆在聽
然後這一堆在聽這一堆
然後這一堆在聽這一堆
它就是這樣子
它是一堆一堆在聽的 喔
那所以第二句話的意思就是說你如果是在兩堆裡面的話你聽的出來他們的不同
那這樣的每一堆呢在他們講的這個聽覺裡面他們稱為critical bank 
所謂的critical bank 就是指這樣的一堆東西叫做一個critical bank 
那因此呢我們現在的 現在用的這個三角形的這種的這個filter bank 呢
其實是在模擬那麼一個現象
那麼如果是這樣的話那我們現在再來說另外一件事情
就是 這我們之前講過的 就是
你在 一個kilo h z 以下它是uniform 一個一個
但是在以上的時候它會變成什麼呢 會變成一個log scale 
越變越大
它是一個log scale 
那麼為什麼會這樣
那麼當然這個也是我們聽覺就是這樣的
那我們可以用很多別的佐證來說這件事
譬如說我們人聽的音高本來就是log 的 frequency 
我想這個是 喔 我們之前大概也說過
那麼你可能在別的地方 也聽過這個 常識
一個最容易想像就是你學音樂的時候
do ra mi f a so la si do 
你知道它的這個 do 跟ra 之間的距離 ra 跟mi 之間的距離都是一樣的
這叫做全音
mi 跟far 之間的距離是半音
是他們的一半
然後so 跟la 又是全音
中間距離又是一樣的
到si 跟do 又變成半音
是他們之間的一半
這是什麼距離
這個就是log 的frequency 
或者說那個pitch 的p 分之一
這個東西
也就是說你 你如果把這個pitch 
你音樂的你
這個如果是交響樂很複雜你看不出來那個音樂非常複雜
你如果是單一的樂器譬如說是鋼琴
或者小提琴
你你去把它的wave form 拿出來也是長的這樣
也是這個樣子的
然後他也有週期那個週期就是pitch 
就是音高
但其實是什麼音高你如果把那個pitch 的那個音也是這樣一個peak 
你把這個pitch 的p 分之一
也就是我們講的f zero 
你把這個東西取log 之後
就是exactly 這樣的scale 
你在那上面會看到do 跟ra 的距離 跟ra 跟mi 的距離是一樣的
mi 跟far 的距離是一半都是這樣子的
那麼你知道從do 變成do 的時候是exactly 這個東西變成兩倍
它就變成這樣子
那麼因此呢 所謂變成兩倍的意思呢
你可以想像就是
如果講它的一個cosine 的話
這是一個這是一倍這是兩倍嘛
這樣的嘛
所以一個一個是do 一個就是do 嘛
那麼也就是說呢
我們本來所謂do ra mi f a so la si 這個音階其實就是把這個這個range 平分出來的
但是我平分在什麼平分在log 的frequency 上平分
所以我們人的聽覺本來就是
這個human percipient 對於音高的感覺本來就是跟log of frequency 成正比
那麼這可能是一個簡單的原因說
我們為什麼在在這個在這個這個地方
我們後面這個這些三角形是以log scale 去分分配
那麼越到後面是一直都以log 來來來分配的原因其實就是
那也可以說其實你分析這些個critical bank 
他本來就是以log 的方式
接近log 的方式來來呈現的
那就是這邊所講的意思了啊
那這另外我們也可以這樣講就是
這個這是另外一個理由就是說呢low frequency 本來就是這個important role in human ear 
我們剛才看到最重要的是f one 跟f two 
f one 跟f two 是最明顯的區分所有的音的 是f one 是在log frequency 這邊
那也因為這樣所以呢我們在我們說在low frequency 的時候呢我們就給它uniform 
就是在一kilo h z 以下就是用很精密的一個一個
對
嗯
我們就讓它很精密的一個一個三角形
這樣子
那
為什麼要那麼精密
那是因為在一個kilo h z 以下是最重要的部分
我們希望做的很精密嘛
那然後呢
那麼為什麼它的每一個 到後來三角形越來越大
那這也是這個critical bank 本身它的bandwidth 就是跟他有跟他的center frequency 有關
也就是說這個東西越到高頻就是會越大
它的寬度就是越到高頻會越來越大
那他跟他的center frequency 就是它所位置那個frequency 是有一個正比的關係
大概在一個有一定的比例之內
所以當你到了高頻的時候
它就是就是會變大的
所以呢我們這些都是都是這些個filter 會長成這樣的原因
ok 好
那這個是我們說的這個這一塊
就是我做這個mel filter bank 
那於是呢我的出來變成是y 的t 的m 
那麼這個y 的t 的m 就是指第m 個filter 
所以呢如果第一個filter 是m 等於一
第二個是m 等於二 第三個是m 等於三等等
那我每一個做出來得到一個
那就是我的y 
t 呢仍然是我這個window 的index 
那麼當我得到這個之後下一步幹什麼就是取絕對值平方然後取log 
關於這點呢也是有理由的
那就是寫在底下
就是我們這邊所說的
當你這個假設這是m 就 總共是 這個橫軸是這個filter 的index 小m 
m 總 這個filter 總共是大概是大m 個
大m 個可能是二十幾 嗯
不同的人作法不一樣大概從二十三到二十六到三十大概
那這個數字大概也就是我們一般講的這個聽覺的這個critical bank 的數目
大概是 二十三到二十六 二十七
差不多這樣的數字
那你每一個得到的一個值之後呢
你把它絕對值平方取log 就變成pron 
然後我們要做inverse d f t 
為什麼絕對值平方取log 呢
那這邊也有一堆理由
你第一個呢所謂的絕對值就是把phase 丟掉了
因為你本來的每一個東西 都是有這個有這個的
那有有這個amplitude 有這個phase 所謂phase 就是這個東西
那這個phase 的影響是什麼
就是我們說那個cosine 嘛
那個cosine 它的零在哪個位子
就是這個phase 
那不同的phase 讓我這個cosine 可以前後移動
但是我現在如果取絕對值的話是怎樣
取絕對值就是把這個拿掉只保留a 
這個沒有了嘛
所以就是把這個phase 丟掉了
phase 為什麼可以丟掉呢
那 我們的了解就是一般人的聽覺就是不太聽phase 的
我們人的耳朵就是不太聽phase 的
那這一點呢你很容易做個實驗就是
你如果把phase 丟掉
你 放一個random 的phase 進去的話
你再transform 回來你聽不出有何區別
這很容易做的實驗 就是你如果拿某一段聲音
我做了transform 之後
我把這個phase 全部丟掉
把random 的number 放進來
那這個phase 完全random 之後
其實就是我這邊的每一個都前後都都動掉了
我再加回去再inverse transform 回去
我得再把他加回來這個這邊長的完全不一樣了
但是我耳朵聽起來幾乎是相同的
那這是我們的聽覺一個很奇怪的現象
喔
喔
這個如何解釋不曉得 但是這是一個大家所公 共同知道的一個現象
我們也就是說當你把它的transform 過來的時候
本來你每一個frequency component 上面都有一個phase 
他告訴我那個cosine 的位子在哪裡
但是我可把他這個完全丟掉之後完全加random 的number 進去
等於把他每一個都全部的前後都全部動掉
我再加回去
那這邊當然就完全不一樣了
因為你這邊每一個component 都全部都動掉 再加回去這就完全不一樣了
可是你耳朵聽起來是完全一樣的
那就表示說我們好像不聽這些東西
既然我們不聽呢 不如把他拿掉
那這個是一個簡單 簡單的解釋
為什麼把這個東西只取這個
喔 不是完全聽不到
所以呢 他們今天有不少的研究 其實是怎麼樣把這個phase 好好的處理
那是有幫助的
不過呢大多數情形我們所了解是不太聽這個
所以呢我們就可以把這個phase 拿掉
那然後呢 為什麼要平方
這裡還有一個平方喔 喔
除了把phase 拿掉之外我還要平方
平方也是因為我們的聽覺
基本上是sensitivity 是 跟energy 有關
也就是說 你如果 這個變成 兩倍的話
你耳朵聽起來是四倍的四倍的強度 阿
也就是說我們耳朵聽的 是它的 能 是它的energy 
不是它的amplitude 
這個變成一半的時候我們耳朵聽起來是只有四分之一的強度
那麼因此我們聽的是它的 是它的energy 
然後為什麼要取log 
還有取log 取log 有沒有道理呢
取log 的道理也是有的
log 就是把dynamic range 壓小
那他本身就是一個壓小的目的其實就是可以怎麼樣呢
可以less sensitive to variation 
那這個是 本來就是我們的 也是我們聽覺裡面一個自然的現象
也就是說
你知道log 是怎樣的log 是這樣的
因此呢你如果這邊變的很大的話呢
這邊變的沒有那麼大
他在 也就是 當你的 當你range 小的時候 這邊比較像是linear vari variation 是差不多的
比較像是linear relation 
可你如果變大的話呢
它會被壓的比較小
ok 它不是這樣上去而是被壓下來的 嗯
所以log 本身是有這樣的特性 這個特性有它的道理
就是說當你如果有一個很 如果那麼大的話
八成是一些雜訊阿 或者什麼
那那些東西的話我們不要聽那麼大 我們就把它壓小了
所以這個是一個 也是在我們的這個human hearing 自動發生的一個現象
雖然那個現象不見得exactly 是log 
但是這用log 來做卻是more or less 描述這個現象
那它呢 是可以這個 使得我們這個抽的feature 可以less sensitive to variation 
你一些大的noise 進來的時候會會被壓小
那因此我們也一樣做這個log 
是有這個原因的
那做這個log 還有一個好處寫在底下
不過這個我們後 後面再一起來說
好 那麼這個是講這個 做這個 絕對值
取 把phase 丟掉平方取log 
當我得到這些東西之後呢 我現在就可以做inverse transform 
我 做到這邊之後我現在再transform 回去到這邊來
那這是inverse discrete fourier transform 
那你本來transform 回來應該是得到相同點數
你這邊有m 個點
這邊應該得到了 得到m 個點才對
那我們通常會丟掉最後的這些點
保留比較少的點
這個為什麼我們底下也會解釋
那是為什麼我們後來這邊只有十三個嘛
十二或者十三個
那本來這邊就是二 二十三或者二十六個
二十多個為什麼變成十多個
其實我們丟掉了一些 阿
那丟掉的原因我們待會會解釋
那同樣呢 我原來這個地方的是 frequency 
我再transform 回來是什麼呢
那個是 這我們這邊叫做j 
叫做quefrency 
那這個東西其實 阿 是一個很奇怪的東西
那麼你可以想像我如果沒有做這個log 
沒有做這個的話
其實transform 回去就是time domain 
就是你本來是transform 過來inverse transform 回去
應該就是time 才對
但是因為我現在在上面做了一堆奇怪的東西我又取了絕對值又取了log 又平方什麼東西
之後這個東西已經不是原來的frequency domain 的東西了
是一種 而且你看 我是這樣子一個一個 嗯 是另外一種東西
所以transform 回回去的時候呢
這個其實不是原來那個time 
是另外一種東西很像time 但不是time 的東西
那當時的 發明這個的人
它就自己取個名子他說我不知道如何取它
我就把frequency 的四個字母倒過來 阿
其實這個就是原來的frequency 
這個f r e q u e 
把這兩個字母倒過來
他就叫做quefrency 
那它只是這樣的意思
那這樣過來之後的這個東西呢
那他也取另外一個名子
那本來的這個frequency domain 這個東西我們有一個名子叫做spectrum 
這個字spectrum 
我們中文通常翻做頻譜
就是指我的訊號在頻率上分佈的情形
這是所謂的頻譜
那當然你這樣轉回去的時候 這又不曉得是什麼東西
它已經取了一個很奇怪的名子叫做quefrency 了
那這東西它也取了一個名子呢
它也一樣 就是把它就叫做 cepstrum 
那這所謂的cepstrum 也也不過就是這前面這四個字把它倒過來
他把前面這四個字母倒過來就變成cepstrum 
那是這個字的由來就是這樣子
那麼 喔 所以我們現在所謂的m f c c 
你如果回憶起來我們當初講m f c c 它的全名是什麼
就是mel frequency cepstral coefficient 
mel frequency cepstral coefficient 
這是它的全名
這是m f c c 的全名
那麼這個 這個字是什麼東西
這個字就是這個字
那它只是 他只是把他字母倒過來而已
那這個東西 當時在十多年前 
我們在想這個字應該怎麼翻成中文
因為spectrum 叫做頻譜嘛
那這個字應該怎麼翻
喔 不知道
阿那曾經有人建議我們就叫他譜頻
我們就叫他譜頻
那這樣的話呢就跟他的原意好像很接近 嗯
那 那 不過當時也有另外一個人有有另外一個翻法叫做倒頻譜
它加個倒字阿
前面加一個倒
那意思是一樣我們把它倒過來就是了啦 阿
那你就為什麼叫做倒其實也就是因為它是 它是字母倒過來的就是了 阿
那麼 不過後來好像用倒頻譜的名的的比較多就是了
所以所以呢 所謂倒頻譜也就是指這個cepstrum 
也就是指這個m f c c 啦 嗯
這個解釋一下這個名子的由來
好 那底下我們現在來說我現在為什麼要做這個inverse d f t 呢
就是做這件事
那麼基本上呢這個 基本上我們在做的事情是inverse 的d f t 
但是其實呢
這個inverse d f t 呢會變成一個discrete cosine transform 
那麼為什麼呢 因為我的log power spectrum 本身是real 而且symmetric 嗯
那關於這一點我想細節我們不說
你如果有學相關的數學那些東西的話就知道他這裡面講的意思
如果沒有學不了解 也就無所謂阿
我們就是了了解這這件事就是了
就是說我這邊做的其實是d inverse d f t 
就是這一個inverse discrete for fourier transform 
就是這個這個轉回去的轉回去的這個過程
但是我其實因為我現在這上面所做的東西
其實是 不是這樣子畫的random 的
而是它有一定的特性的
這個特性就是我們這邊說的他是 real 而且是symmetric 
在這兩個特性之下那個inverse d f t 呢
會變成一個discrete cosine transform 
什麼是discrete cosine transform 呢
跟fourier transform 是很像的
只是我的basis 就直接是cosine 嗯
也就是說我我們這邊講的時候我這邊的每一個東西
都是用 我這邊的每一個
我都都說它其實所代表的是
一的j omega one t 等等
都是這個東西
然後我的transform 呢 也都是把他寫成
譬如說x of t 
e 的minus j omega t d t 我的
積分都是這樣積的
或者說我的summation 都是我剛才寫的這邊的啦 哈
就是x n 
e 的minus j omega n 
summation over n 
就是這種東西
那基本上呢我都是以e 的j 這種東西
作為我的基本的basis 來做這些事情的
那cosine transform 唯一的不同
只是這些東西我都變成cosine 就對了
那其實我們說過這些東西你如如果取實部就就是cosine 嘛
那它就 根本不 這邊就 這邊就直接改成cosine 了
我這邊就直接改成cosine 
我如果直接把cosine 放進去的話
這種東西就是所謂的cosine transform 
那cosine transform 跟這個 跟這個原來的這個fourier transform 之間
是有非常密切的一堆關係的
那我想不在我們這門課要說
你如果有興趣的話在相關的文獻相關的課本修相關的課會學到
那我們這邊不講我這邊只說這句話就是說呢
阿 你去 你去查paper 可以查的到
它可以證明
這個cosine transform 有個很大的特性
就是可以把這個 這個中間的correlation 降到最低
使得你得到的最後得到的東西是highly un correlate 
那麼換句話說你如果做了這個cosine transform 的 的話
你所得到的這些
最後這些東西他們彼此之間 的 幾乎是沒有太多的correlation 
它們幾乎都是 非常independent 的component 
那這樣有什麼好處
這樣最大的好處是
我到後來我的那個gaussian 那個gaussian 裡面
我都可以用對角線的matrix 
這是指什麼
你記得我們的 我們的hidden markov model 裡面 h m m 裡面
譬如說 這個state 裡面是怎樣的
我們說是一堆gaussian 對不對
譬如說這是一個gaussian 這是一個gaussian 這是一個gaussian 
我用一堆gaussian 來來描述說
當我的訊號在這個state 裡面的時候它的distribution 是這樣的
那這每一個gaussian 你如果去寫的話
它是會有什麼東西
e 的minus 什麼東西
那這裡面會有這個 譬如說 這一類的東西
那它有一個covariance matrix 
我現在這個東西是三十九維
所以應該是要有一個三十九維乘三十九維的covariance matrix 
那 那個matrix 非常複雜
那在它的對角線上
是相當於每一個自己的variance 
但是這外面的每一個點呢是他們各個component 彼此之間的correlation 
那麼我現在如果說這些東西都變成highly un correlate 的話呢
那他們等於說你可以想像
我大概比較可以假設他們是zero 
我如果假設他們是zero 的話呢
那就簡單很多 我這邊都是零
於是我這個matrix 只有三十九個參數
你否則要三十九的平方的參數就很多很多 喔
那事實上我們後後來真的在做的時候
在很多的情況之下
我們都會做這個假設
說 它的每一個gaussian 的這個covariance matrix 
我們都說它是 我們都說它是diagonal 
你就讓它是假設外面是零
那這個假設為什麼可以成立
就是因為我這這邊是做了d d c t 
等於是經過了這個d c t 之後
我們比較可以做這個假設
因為它transform 出來的東西比較是un correlate 喔
所以呢我們可以 可以做這個假設說他們是un correlate 
所以這些東西呢就可以假設它是零
在很多時候是可以這樣做的
當然在某些情形你如果要做特別精細的話
你可能還是不能做這個假設
你還是要讓他這個所有的維都跑進去
但是很多時候我們是這樣做這個假設
好那這個是 這個 阿 阿 這個 我們做這個 這個 dis 這個d c t 的其中一個原因
那還有這個地方我想有一點寫錯
喔 因為我們其實應該是 你如果看這個圖就知道
我們應該是這個y t 的m 
取絕對值平方 取log 
再做這個cosine transform 
所以呢看起來這裡應該是
取絕對值平方應該是掉了一個平方
有了平方之後再取log 
之後這個東西
乘上一堆cosine 加起來
這個就是cosine transform 
就是我現在把這個東西改成cosine 嘛
就這個東西乘上一堆cosine 加起來嘛
所以應該是 這個地方應該是有一平方的漏掉了
ok 那麼底下的這件事情
是跟我們前面的 連 連在一起的
我們跟前面這件事情連在一起來解釋
我我們休息十分鐘好了喔
喔 我說一下我們的這個期中考的schedule 喔
我們現在今天是四月
今天是四月十八
下週是二十五
再下週是五月 二號 九號 十六號 
我們期中考的範圍是到第八點零考完
那麼我估計大概今天可以把七點零講完所以下週大概可以把八點零講完
所以大概是 期中考的範圍大概到這裡
是大概講完
所以呢過一週兩週
合理的期中考時間可能是在九或者十六
那我的我的這個plane 是在
這一週 因為這一週我出國
那麼如果在我出國的時候考期中考
可能是最不需要補課的一個狀況
喔 所以呢我現在的plane 是
就是我們在今天把七點零講完下週把八點零講完
這個時候我們把basic 全部結束
這是我們期期中考範圍到這裡為止
ok 
我們現在來解釋剛才這邊沒有說的一個就是這個嗯
嗯在做log 的時候我們底下這一段話在說的東西跟
這邊的我底下也有這段話
那這兩段話在講的意思是什麼呢
那就是我們剛才在這邊所講的啊已經擦掉了
就是我們的這個
你可以想像成是
這個你可以想像成是一個進去是u n 
然後出來是x n 
那這是我的vocal tract 我的發聲的這個脣齒口型的變變化我們叫做g n 
那其實x n 呢是u n 跟g n 的convolution 的一個數學關係是這樣子的
那麼這個意思你等你可以想像呢是說
我們所聽到的每一個聲音
是有兩個部分的現象
一個部分的現象是這個excitation 裡面發生的
一個現象是在這個g n 裡面發生的
那麼那麼這個g n 裡面的東西呢等於是在描述我的口型的那些東西
而excitation 是在描述我我進去的這個氣流的
那這兩種現象呢在我們的聲音裡面其實最後是混在一團
對不對
經過這個運算混成一團
在time domain 上得到一堆像這樣子
或者說frequency 的到一堆像這樣子
那都是把它們這兩個混在一起了
那麼你如果是這樣子來看的話
那麼在time domain 上是一個這就是u n 跟g n 
就是這個是這個excitation 的氣流
跟這個g n 是我的那個那個喔vocal tract 
那這兩個是混在一起變成我的訊號
所以我的訊號是這兩個混在一起的
那這個時候你如果從frequency domain 來看的話呢
這兩個會變成相乘的意思
如果他在frequency domain 上面是用這個來呈現
它是用這個來呈現的話它也可以用這個來呈現
他們是相乘的關係
在frequency 上面是相乘的話呢
妳取了絕對值還是相乘
取了log 最後會變成相加
那這個意思是說呢
你如果是看在這個地方的話
我其實是有取絕對值有在取log 
所以雖然在原來的訊號裡面他們是有這個convolution 的關係
到了這邊之後呢其實是已經變成相加的關係了
所以呢在這裡的時候其實他們已經是一個相加的關係
那這個時候呢雖然是相加不過還是兩個混在一起
加法仍然是混在一起的
所以你可以想像呢
就是兩種東西一種是描述我的口型的
一種是描述我的excitation 的
這兩種東西呢是加在一起
加在一起之後譬如說一個是這種東西另外一個可能是這種東西
它可能會還是加在上面
但是基本上呢
雖然是加在一起可是呢
我如果是看他們變化的速度的話呢
一個是vocal tract 變的很慢
而excitation 是變化的很快的
那麼也就是說呢你如果在看他的這個譬如說你以這個unvoiced 這個為例
這個是變化的很快的他的這一瞬間跟這這不一樣的東西的
或者說你看這個的話呢其實他的音高一會高它是有變化的是蠻快的
所以基本上來講他們是變化快的東西
而vocal tract 呢是變化很慢的東西
因此呢你這個時候如果做個inverse d f t 的話會怎樣呢
會把它們換到兩個兩段去
喔
也就是說我如果回到剛才的那一張來看的話
這裡
我我做inverse d f t 回到這個quefrency 的這個domain 的時候
這是那個j 就是quefrency 的那個domain 的時候呢
那基本上在這上面的時候他們是加在一起的還是加的啦
就是雖然一個變化慢一個變化快還是加在一起的
可是經過這裡之後呢
一個變化快一個變化慢就會跑到兩個不同的區段來
就是這個就是在我的這個j 
就是這個quefrency 的domain 上面
在這個上面的話呢那麼
你變化比較快的一堆在這裡
變化比較慢的一堆在這裡
那麼其實呢
這堆呢跟這堆因此可以拆開來
那其中呢你可以想像成
這一堆的information 就到了這邊
ok 
然後呢這上面的這些information 
就這樣的意思
sil
那麼
當我這邊所畫的是把它畫在frequency domain 上
那其實呢我們是從是從這個domain 轉過來的嗎喔
那基本上在這個上面的時候我們講在這上面的時候其實這兩種現象仍然是加乘在一起的
但其實加乘在一起的時候他們其實一個是快一個是慢的
所以呢當我transform 的時候
其實會把快的transform 到比較高的j 比較大的地方
那麼這個慢的會transform 到j 比較低的地方
所以會變成這個兩段
基本上他們就somehow 雖然還是這兩個加在一起啦
你可以看成是還是加在一起不過就是說一個是在這邊嗎一個是這邊這邊比較比較沒有
一個是在這邊這邊比較沒有
好你是是是這兩個東西
還是都是整段啦還是相加的啦
只是說一個比較都在前面那一段那這邊沒有
一個是比較在這一段後面沒有
因此你這樣transform 回來的時候其實是這兩個是幾乎可以拆開來
那麼這是為什麼你在我們在這裡後來我們說
我這邊是取m 這邊只取j 
這邊本來是二十多個二十多個filter 二十三到二十七個filter 
可是這邊我只取十二十三個為什麼
其實就是在做這件事
我就取這個就夠了我這個就不要了
因為我們現在要的是這個formant structure 
我們真正要分辨的是在這個formant structure 我要要的是這個東西
這個東西就是被這些毛我們講這上面這些毛搞的很亂
所以你不太容易抓的到它的formant 在哪裡
他這個毛非常亂
所以呢你不太知道他那個peak 在什麼位置
但是呢你如果其實peak 在這裡譬如說
那你如果把這個毛拿掉的話
就毛在這裡我把這個毛拿掉了之後
剩下這個東西的話
就得到裡面這個
就得到這個
你反而清楚知道ok 這個f 這個你的f f f 三在這裡或者什麼
sil
那等於是這樣的意思
所以呢
我們大當這裡這裡做inverse d f t transform 的時候
是有這個原因就是為什麼我只取前面後面不要了
那其實後面因為後面more or less 就是那堆毛
那麼我其實可以把它丟掉後前面反而清楚
我可以得到一個比較清楚的東西
那這個解釋我們剛才講的就是說你中間為什麼取log 
這邊是在解釋為什麼取log 
取log 是有這個好處就是取log 之後可以讓這兩個東西由convolution 變成加法
那麼convolution 是把整個的把整個東西完全混在一起
可是變成加法之後
是有可能像我們這邊所畫的這個情形
就是你只要他在不同的區段的話其實是拆的開來的
sil
這個就變成這樣子於是我拆開來
所以他這個有一個log 除了我們前面這些原因之外還有這個原因是把
乘法變成加法
把convolution 變成加法
那麼這邊講的也就是這件事情所以你可以把interference of excitation on formant formant structure 把他拿掉
也就是說我要的其實是這個formant structure 就是我們這個紅色的
這個紅色的這個東西
那你這堆藍色的東西毛呢你可以看成是一堆interference 
是我的這個excitation 在上面的那一些破壞
我可以把他拿掉
那這樣子話呢我就可以比較清楚的得到我所要的東西喔
這個是這邊講的意思
好
有了這些之後我們現在m f c c 就出來了
就是回到剛才這張圖
就是回到剛才這張圖
那這樣子我這邊得到這個y t 的j 就是我的m f c c 了
那我們很多時候
同時也取energy 
就是把那一個frame 的把這個window 的所有的energy 
就是把那一個frame 的把這個window 的所有的energy 加起來就是平方相加的這個energy 也拿來
所以呢這個變成如果這是十三個加一就是十四個
或者這個是十二個加一就是十三個
或者這個是十二個加一就是十三個
那這就是我們的前面的這一組
然後我可以做一次微分跟兩次微分
那底下我們來說這個微分
微分我們之前做很簡單的解釋是說他們相減相減
這樣也可以
sil
那我們現在這個就是每一個frame 的十三維
那這個就是我剛才的那個quefrency 的j 
所以呢就等於說是我們剛才這裡的零到十二嗎
對不對零到十二嗎這樣就十三維
那這個這個軸零到十二這個是quefrency 這個j 的軸
就是這邊的這個軸這零到十二
所以呢我一個frame 
這裡如果我我符號沒有統一喔符號應該是用t 才對
所以這個是t 
t 加一t 加二t 減一
也就是說我是在時間t 的時候取的這個取的這個window 
那如果這是t t 加一t 加二的話呢
我每一個t 的時候我得到那十三維
那這樣呢就是我的這些東西
那這個時候
我怎麼做這個微分
我們之前的說法是說ok 
它減它得到它
它減它得到它對不對
它減它得到它
它減它得到它
這樣做當然是可以的
這樣做出來也不錯
不過後來有人發現其實有更好的方法
那麼我們今天多半都是用這個式子
那這個式子是什麼呢我們來看一下
它是把前後的二p 加一個點
來做一次得到一個
以p 等於一而言的話就是前後三個
所以這邊所話的就是p 等於一個時候
就是你把這是t t 減一跟t 加一的這三個合起來
做一個這一個
那它怎麼做的呢是你看他分別乘上t 減m 
乘上m 
相加然後除以m 的平方
那這是什麼呢
你可以想像這個是我的這個喔m 
這是零
負一嗯零正一正二負一負二
m 的平方呢就是零一四一四
所以呢如果說p 等於一的話
就是只取前後三個
p 等於一的話這是t 
t 減一t 加一
所以就是t 減m t 加m m 從負p 到正p 嗎
對不對
所以如果m 如果p 等於一的話呢就是從
從這個t t 減一t 加一這三個
這三個裡面呢那就是取這三個
那麼分子是什麼呢
分子你看就知道是t 加一減掉t 減一
對不對它減它嗎
所以呢等於是這兩個相減的差
除以二
對不對
就是就是這個式子
它乘以正一然後呢
t 加一乘以正一t 減一乘以負一其實就是它減它嗎
就是它減它然後除以這個平方相加就是二了
所以呢就變成是它減它除以二得到它
然後呢它減它除以二得到它
等那當然p 不一定是一p 等於二如果p 等於二的話呢就是就這這樣子
如果p 等於二的話呢就變成是
它減它然後呢還有呢它減它然後呢它減它
對不對
所以呢就變成是它減它
然後呢它減它要乘以二
所以呢就會變成這個二
然後除以多少呢除以十嗎
那這些是什麼東西呢
那這些公式其實是來自linear regression 
linear regression 你一定聽過的
那就是說假設我今天有一堆點
如果這堆點是x i y i 的話
我希望找一條直線
這條直線是y 等於a x 加b 
使得他跟他們之間的距離最接近
這個所有所有的distance 
所有的不是真的垂直距離而是他們y 的距離最接近
也就是說我要讓這個喔
a x i 加上b 減掉y i 的平方等於minimum 
ok 
就是我我讓這個這些個點跟y 的距離
讓他們的這個東西可以可以最接近的這條線的斜率
這條線的斜率就是這個公式
那當然我現在的時候是有一點不太一樣
就是我現在這些點
不是任意的x i y i 
而是我的橫軸是整數
這個是譬如說這是t t 加一
t 加二
所以呢我這邊橫軸是整數
只是縱軸是比較不同的值
那我譬如說如果是那邊的p 等於二的話
意思是前後五個點
我把前後五個點拿來找一條直線來讓它們距離最近
那這條直線的斜率就是最後算出來的這個東西
那公式就是這個公式
那所以跟這個稍微有點不一樣的我現在y i 不是任意的
y i 是整嗯x i 不是任意的
x i 是整數
x i 是整數
那在這個情形之下我們來做這個linear regression 的話呢
我得到是這個式子
那麼因此呢他這個比比我們原來說的它減它得到它要來的更精細一點
等於是我用前後的前後的三個點五個點或者七個點
來得得到一條這個這個這個直線的來趨近他
然後那條直線的斜率
那當然這個意思是跟作微分是差不多的
那當有人用這個方法做的時候發現這個效果比我們原來講的這個兩個相減得到它要來的好
所以後來多數人要做得精細一點
都用這個方法來做喔
這就是我們作微分的方法
那麼因此呢你可以看到說是在這個case 在這裡畫的就是p 等於正負一的話
p 等於一的話
這兩這三個做起來得到這一點
那這三個做起來得到這一點
對不對這三個做起來得到這一點
等等等等
那這樣這十三維就得到第二個十三維
那我這邊可以再來一次
這個公式你看是完全相同的公式
所以呢我這邊再來一次就得到再下來十三維
那這樣的話我就全部三十九維都出來了
這個是我們的這個微分的做法
那這裡講的大概是差不多的事情
就是喔你你為什麼要做這個微分
我要有dynamic characteristics 
也就是說我們我們本來你的聲音不是一個一個discrete 這個ㄚ這個ㄧ這個ㄨ不是discrete 
我們本來就是在連續的變化的
你的嘴型不會在一瞬間變成另外一個
你是連續變化所以這中間變化的這個time variation 
time rate of change 
是非常重要的information 
那你問題你這個p 要怎麼選
p 不能太大不能太小
那這個意思也是你可以想像的到的
如果p 太小的話
你就沒有真的抓到它的斜率了
對不對
你如果p p 太小的話
你只取三個點的時候
這個斜率是不太穩的你可能抓不太對只要有一點不對就不對了
所以當然要長一點比較好
喔可是你如果p 太長當然也不對
因為你如果太長其實你已經變成另外一個state 
應該是不同的了
嗯所以呢你不能太太長也不好
你會把不同的state 東西弄進來
太短也不好你會抓不對
所以呢你這個p 要有好的選擇
那這個時候呢其實做這個delta 還有另外一個好處
就是我們底下所說的
你可以把convolutional noise 消掉
那這話怎麼講呢
這話其實是跟我們之前講的另外一件事情是很像的
因為我們很可能碰到這個狀況
就是我我我對麥克風說的話
說的話之後我不是立刻拿去做recognition 
而是經過怎麼樣的處理之後傳送出去了
到了對方收進來之後
我才去做recognition 
譬如說這個可能是server 
我其實是在手機這端講完話之後在那邊才做recognition 
那中間經過這個過程話我的聲音已經破掉了
這個是x n 
你這邊進來是x n 
可是你到這邊的時候已經變成y n 了
y n 是什麼
是他跟某一個東西的convolution 
也就是說你如果中間經過某一些process 的話
我的訊號可能已經改變了
被破壞了
其實我們通過麥克風本身
麥克風也有一個破壞的的process 也好比有一個這個東西一樣
也是妳得到已經不是真實的原來的聲音已經被破壞了
你如果中間再經過別的process 
再傳送什麼
都是被破壞了之後
那麼所有的這些破壞我們都可以用一個用用這東西來描述它
就是它經過某一種數學上的convolution 
變成另外一個了
那如果是這樣的話會怎樣
用我們剛才的這個地方你就會看到
其實
你如果是經過某一種convolution 
我最後得到的這個m f c c 是什麼
是相加的
因為我這裡面的convolution 我在m s m s 中間呢是在frequency domain 是相乘的
然後我又取了log 變成相加的所以最後是相加的
因此呢
你如果是這樣來看這邊的問題的話
你如果y n 是是這兩個的convolution 的話
我經過了m f c c 的處理之後
其實這邊你可以想像是某某某一種x 的m f c c 跟g 的m f c c 是相加的
那麼得到y 的m f c c 
ok 
那麼因此呢
這個m f c c 是有這個好處
就是把convolution 的這種複雜的東西呢轉成一個加法
當你變成一個加法的時候有一個好處就是像我們這邊講的
我這邊如果取取斜率
這邊是在取斜率我們現在講的微分是在取斜率
那取斜率的話就自動把那個加的東西拿掉
如果你加的東西差不多一樣的話
那你可以假設成是說這個
譬如說我講了那一句話在我講了那一句話裡面這東西是差不多的喔
如果說我是經過某一個電話的程序到server 這端去的話
那也許在我講那一句話的時間裡面這個沒什麼改變
所以這個東西是一樣的
如果是這樣的話
那我其實通通都加那個東西我現在寫成h 
都是一樣的h 的話
我在作微分的過程就是把它消掉了
所以呢這個channel effect h 就自動消掉在delta 的程序裡面
那你也可以說是
我在取這個斜率好了
你如果都加了一個h 的話我只是把它整個向上搬嗎對不對
我如果都加了一個h 的話只是把它整個一起向上搬動
所以這個斜率是相同的嗎
也就是說如果這些點它所受到的那個破壞的那個h 是差不多的話
在一個很短的時間裡面沒有太大改變的話
這個加上去的東西在這裡是整個一起加的
因此呢我如果算斜率的話呢是不受影響的
所以算斜率這種就把這一部分拿掉了
那麼有了這個好處之後我們就可以了解說你這個m f c c 裡面如果多了一堆dc part 都可以自動消掉在這裡
那麼也就是說在剛才這張圖來看我們這種共三十九維
三十九維這上面這十三維是會被這種東西破壞的
這種東西是破壞是會把它破壞
可是到了這十三維跟這十三維的時候
這些h 這些g 都自動沒有了
因為我都消掉了
所以這個是它用這個的另外一個好處
好我們現在剛才以上我們把這個所有的m f c c 的東西都講完了
我們應該是把這張圖裡面的每一塊都走了一次喔
就是這張圖的這裡面的每一塊大概都走了一次
那我們現在最後一頁講的是另外一件事情
這也是這個front end process 非常重要的就是end point detection 
那所謂的end point detection 
是說我們其實在說話的時候有非常豐富的時段是沒在說話的
也就是說我講話鐵定是這段時間在講之後呢這段沒有在講
這段在講這段沒有在講
那麼我沒有在講的時候是怎樣呢
中間一樣會有訊號的
為什麼就是noise 
只是說呢你這邊有的時候我的noise 繼續加上來就是了
也就是說我們在講話的時候
如果說我的我的voice 是這樣子的話
我沒有講話的時候呢我一樣有雜訊
這個雜訊在這邊繼續加上去
我們其實碰到是一個這樣的狀況
那麼
那所謂的這個end point detection 其實就是在你要知道哪裡是真的你的聲音哪裡是雜訊
那就recognition 而言
當然我們不能夠把不應該把這堆noise 當成是聲音拿去跑h m m 的話你跑出來就不對了
所以我們當然要知道把這個是在哪裡
那其他的原那其他的事情也是一樣
譬如說你打電話你的手機打電話的時候
你知道打電話時候是這樣子的
你講話的時候
你講話的時候
對方應該是聽的
除非他跟你吵架
然後呢
對方會講話呢應該是在你講完停下來的時候
這個時候你是聽的
然後等到你在什麼在在他講完後你才會在講
所以呢如你如果是打電話的話應該是一個這樣的情形
這兩個signal 是大概有超過一半的時間是silent 
那麼因此呢我們要降低我傳送的bit rate 很重要一件事情就是我要知道哪裡是在講話
然後我就送這一段
這段我就不要送了
我不但節省bit rate 而且我沒有不要送noise 
那像這些都一樣需要做這件事情就所謂的end point detection 
那這有另外一個名子就是所謂的voice 就是vad 就是這個voice activity detection 
你要在一堆noise 所充滿noise 環境裡面你要知道哪裡真的是voice 哪裡不是
那哪裡是真的有voice activity 哪裡沒有
喔
這就我們這邊要說的事情就是所謂的end point detection 
那這些並不好做
那麼一直到今天仍然是一個很重要的研究課題因為一直沒有標準的答案
沒有很好的solution 尤其如果你背景雜訊很厲害的時候
你的在在街上在什麼很吵雜的地方的話這非常難做的事情
那麼怎麼辦
基本上可以想像有人想簡單的辦法就是push to talk 或者是push and hold to talk 
一個簡單的辦法就是你讓你的東西有一個有一個按鍵
你一按之後開始講
那它就知道那算是聲音嗎
然後呢我講完再按一下
表示是關表示我不講了
sil
那這是用人手來幫助解決的辦法
那另外的辦法就是push and hold 就是你講的時候我按下去講完了放下來
那這個其實是很簡單但是有效的辦法
雖然讓user 很麻煩
那你知道這個我們今天這個電話打電話的時候這個這個這個語音信箱
據說從前一開始你要做語音信箱的時候有很大的困難
就是你你把你錄音錄進去的時候人家不曉得從哪裡開始
所以不曉的是哪一段
那過了很久有人很聰明他說
嗶一聲開始
那個那個從嗶一聲開始錄音那個嗶一聲就是告訴你end point 在哪裡
那從那個以後就語音信箱就可以解決了
那我們這裡也是一樣那那那個那個point 怎麼怎麼辦
那當然最好的辦法是讓它可以自動去聽嘛
所謂continued listening 就是你繼續在錄音但是我隨時自動判斷這裡開始
sil
如果可以自動判斷是最好啦
那怎麼做自動判斷呢我們這邊底下講的是一些自動判斷常用的方法
最簡單的就是energy threshold 
我就是取energy 
那假設有語音的話energy 比較大
沒有語音的話energy 比較小
noise 比較小所以我就可以就是用一個window 算那個win window 的energy 
然後那個window 不斷的移動過來
那我這個可能得到一個window 在我得到這個energy 在這邊的時候很小
當我window cover 到語音的時候就會慢慢變大
然後呢可能會比較大一點
如果是這樣的話我可以定一個threshold 
超過這個threshold 就算是語音哦
這是一個最簡單的辦法來做
就是energy threshold 
但是呢這個常常呢就是說到底哪裡是一個哪裡是哪裡是是這個threshold 呢
你可能是需要adaptive 
因為這個顯然是跟背景雜訊有關
我要在這個有一開機還沒開始講話的時候先錄當時的背景雜訊來計算一下那麼應該以哪個threshold 為準比較好
那這個辦法也有它明顯的弱點就是當我的背景雜訊變的很強
當然很可能背景雜訊很強跟這個一樣強你就沒有辦法分辨了
所以這是最簡單但是效果不見得好的方法
那不管怎樣這裡面有兩種可能的error 
一種是false accepts 
一種是false rejection 
什麼是false accepts 呢
就是
我們有兩種一種是false accepts 
一種是false rejection 
false accepts 是說
明明不是聲音而我以為是
譬如說我把它切成我把它切成從這裡開始
我就把這一堆不是語音的我把它accept 成為語音了
這就是所謂的false accepts 
那false rejection 是反過來
是說明明是語音的你把以為切掉了以為這個是noise 
那這個呢就是false rejection 
這是兩種不同的error 
那常常這兩者不容易得兼
那麼你如果這個把threshold 定的高的話
你很可能是會把聲音切掉
就是產生這種情形
你把threshold 定的低的話呢很可能會把noise 收進來就是這種情形
所以呢常常不容易得兼
那這時候怎麼辦呢
我們這邊講的就是說基本上我們是覺得false accepts 比較沒有關係
因為你如果把它弄進來的話我還有辦法把他拿掉
可是你如果這邊把它切掉就沒有了
所以這兩種error 在這裡是不效果是不同的
你把聲音切掉就沒有了你那個就一定沒有了你就一定是發生error 了
可你如是把noise 弄近來你也許還有別的方法再來處理這塊
所以呢就是false accepts 還有辦法來救回來
可是false rejection 就不行
因此呢我們基本上會希望呢是這個發生這個不要發生
所以我的rejection rate 低一點
我就盡量的這個這個threshold 低一點
那這樣的話呢我我盡量讓他這個發生而這個不要發生
這它的一個基本原則
那這個時候這個怎麼辦
一個最常用的辦法就是你加一個model 
叫做silence 或者noise 
就是說你的本來的每一個word 都有一個model 
你還有一個model 叫做silence 
那麼這個時候你就可以想像假設它是它是這裡面講的三個字的話講的三個word 
這是一個model 這是一個這是這三個word 其實你就把它當成前面還有一個silence 
所以呢我就可以讓這個這個我用這個noise 或者silence train 一個也是一個h m m 
那你就train 一個這個model 來
然後呢那麼讓它也是變成一個可能的word 可以接在word 的前面或者後面
那這是一個常用的辦法
那這個通常可以handle 一部分這樣子的問題
那底下的這個呢是在是在課本上它有一個他講的一個方法那蠻好的喔
不過這個這個在在這裡我這裡的reference 的這個的九點三的就是這個
這個就是我剛才講的這個
那它有一個方法
那麼我們簡單解釋一下就是它當成兩個state 
就好比是h m m 只有兩個state 一樣
這兩個state 就是一個是speech 一個是noise 
那你每一個frame 可以去算它是屬於這個還是屬於這個
如果現在是speech 的話呢下一個frame 可能還是下一個frame 可能還是
但是一講完我就會跳到silence 去
是silence 我可以繼續記住silence 然後我會跳回來
這樣就變成兩個state 的h m m 
你就可以去跑它的viterbi 你一樣跑viterbi 
那你就可以分出來對不對
你就是譬如說在這一段裡面它就是在這一個state 裡面跑
這邊就跳到另外一個state 去
這邊在跳另外一個state 過來
你就兩個state 的viterbi 
你可以讓他這樣子來跑
那這個時候呢你這兩個state 分別都可以train 它的gaussian 
用一堆gaussian mixture 
就是一堆gaussian function 
用我們平常的做法一樣
你都可以train 出
你現在只有兩個state 
就是只有兩個state 
只不過他們各自都可以回到他們自己來
然後它可以回來
這兩個state 
那每一個state 呢基本上都是一堆gaussian 
一樣的你就可以train 一堆一堆gaussian 
那這是另外一堆gaussian 
那那這些gaussian 裡面的的用什麼用energy 啦log energy delta energy 這些都這一類的東西當它的feature 
你就可以train 這種東西
然後你可以調裡面的parameter 
這些mean variance 都可以調
然後你就可以看到
他們跳來跳去
用這樣來來切這是一個比較比較複雜但是相當不錯的方法
你如果要看詳細在那個我剛剛講的那個課本那裡
ok 好我們今天上到這裡
那麼喔底下有這個就是說嗯這個是我們去年語言所有一有一位語言學家它提供我一堆網站
因為那個時候我們我們說這個
那些關於語音波形啊什麼那些因為都不能放在網站上
它就提供我一些這些網站都是跟語音學訊號波形這些有關的
你如果有興趣可以去去那邊看
這個這作為reference 參考
ok 好
這樣我們這一部分講到這裡
