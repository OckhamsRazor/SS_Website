那那我們怎麼來想這件事情呢
那我們就來define 我們要做的事情其實就是要算這個entropy
因為所謂的你希望直的切下來
你希望每一個直的切下來是意思是什麼
就是我希望純度
就是我希望這個純度啊我要
我原來是亂度
對不對我一開始的時候是亂度最大上面這個case 嘛
我一開始是最上面的是最亂的
然後呢我希望把它如果這樣子一刀一刀切下來的話呢
越到底下我越純
越純的話呢就是越好
那從entropy 來講呢就是我的entropy 要越變越小
我entropy 的越小的越多就是越純嘛
越小的越多就是越走向純嘛
所以呢我就是要把entropy 降得越多越好
因此怎麼辦呢
我就是為每一個node 去算在這裡的entropy
我這個可能是在這上面這裡
我就可以算在這裡的entropy 
然後今天如果有有有一有一個辦法讓我切一刀的話
那我不知道我這一刀切出來到底是切成這樣還是切成這樣
還是切成這樣
那我就是算這兩個的entropy 
那就是底下這一頁所講的事情
也就是說這一個node 我算它的的這個什麼東西p 這個p log p 
其實就是entropy 
這裡的不太清楚這這個c sub i 這應該有一個下標是i 
c sub i 除以n 就是percentage of data sample 
for class i at  node n 
也就是說
也就是說嗯在這裡
這裡每一個就是一個每一個cross 就是一個i 
它的data 的它的有幾個人
我們算人數好了就是data sample 的數目就是有幾個人嘛
那就是c sub i 
那我總數呢就是n 
total 的人數是n
嗯如果我們用剛才的人數來算的話
total 的人數是n 
這是第i 個人數的cross 是c sub i
你如果這樣子看看的話呢那麼這個c sub i 除以n 
其實就是一個機率嘛
所以它說percentage of data sample for class i at node n 
嗯嗯嗯等一下
這個n 不是總數
n 是指這個node 的意思
c sub i 是這裡的data 是沒有錯
然後n 是指這個node 這個node 
所以呢所以這個n 呢就是在這個node n 這邊在node n 這邊的所有的data 
然後我就就去算對cross i 的人數的percentage 就是一個機率嘛
所以呢就是這些機率
所以這裡的每一個就是所以這裡的每一個機率其實就是p 的c sub i 
at node n 就是它的機率
因此呢我這邊所謂的這個p 然後乘以log p 再加起來這件事情
就是在算這裡的entropy 
當這裡有個entropy 這裡這樣算之後我現在來切
用某一個方法
根據某一個criterion 把把它切成兩個的時候
那個depend on 我怎麼做對不對
我們說最理想的是這樣切
這樣的話呢就左邊跟右邊這樣是切得最乾淨的
但是你很可能做不到這樣
那你做到的可能是一個這樣
譬如說這樣
於是呢你左邊呢可能會變成是這邊這樣然後很小了
右邊這個會變成是這樣然後這邊很小了
這邊都不是沒有
只是少而已
那這樣也不錯啦這也是進步啦對不對
所以呢我這邊可以得到一個這個b 的
這邊可以得到a 的
啊都一樣
這就是在node b 這邊有一個distribution 
我也可以算它的entropy 
這邊的在node a 這邊有一個distribution 我也可以算它的entropy 
那我希望這個entropy 從這裡到這邊呢是降下來的
那降得越多就越好嘛
那降得越多就表示我這一刀切得越接近這樣子切法
對不對你如果這樣子切的話就等於沒有降低
那就等於是白做了ok 哦
那麼因為這樣的關係我現在就可以定義這個delta 的entropy 就是entropy reduction 
就是在node n 減掉node a 加b 的
就是我本來在這裡的時候entropy 是多少
變成這樣經過這個criterion 切下來之後
變成這兩個那它的entropy 是多少
那這個時候呢這兩個entropy 的差就是我的delta 的delta 的這個這個的這個entropy 就是entropy reduction 
看看我減多少
那麼於是呢我就可以選擇一個在每一個地方我都選擇一個最好的切它的方法
就是一個所謂的question 
當然不見得是所謂的question 應該就是一個criterion 你怎麼切它的一個切它的一個criterion 
不過在我們剛才的例子裡面我們都用entropy 都用question 來解釋
譬如說年齡是不是大於十二
他的的他的職業是不是職業籃球隊的
它每週是不是喝這麼多牛奶
這都是用一個question 哦
我們就稱它為question
那其實不見得是一個question 就是一個criterion 就是了
當我把它用用這個一堆我有一堆存在的criterion
但是到底用哪一個criterion 在哪裡切
我也可以一開始就先選男生還還是女生
我也可以一開始就看根據它的牛奶
我也可以一開始就根據它的職業
嗯你憑什麼要先選哪一個呢
那就根據看我在根據這裡來看
我從頭開始
到底我手邊有的data 裡面
哪一個讓我一拆開來
entropy 降低最多
就表示我的程度增加最多
那到了這裡我再來看還有什麼東西讓我降低最多的
我就從那裡來拆
這裡我再看哪個讓降低最多我從那裡來拆等等
所以我每一次都選擇我每一次都選擇entropy 降低最多的那一個
來做這件事
所以呢我這個h n  的entropy 減掉a 跟b 的entropy 
做為我的 entropy reduction 
然後它是一個function of q 
就是我的question 
或是我的criterion
因此我每一次呢就在所有的q 裡面去選擇那個降低最多entropy 的那個question 從那裡開始切哦等等
那這裡還有一個小問題
就是你如果光是這樣做的話會發現不對
為什麼不對呢
這個我們舉個簡單的例子
如果說如果說一開始是完全uniform 的
總共是m 個
那這個就是log m
這個entropy 就是log m 
假設我選了一個非常笨的方法
它這一刀是這樣子切的
那切完之後呢在這邊也仍然是一個log m 
在這邊也仍然是一個log m 
對不對所以這邊也是log m 
這個也是log m 
這應該表示說這個這一刀是白切了
entropy 完全沒有改變
可是你現在如果算一算的話呢
這個h a 加h b 是兩倍的log m 
上面是一倍的h m log m
嘿它還增加了
這邊是一個log m 這邊變成兩個log m 這不太對吧
那為什麼不太對呢你仔細想一想呢其實是應該怎麼樣
我這邊假設這邊有一百個人
其實是六十五個人到這來三十五個人在這裡的話
應該是這個乘以零點三五
這個乘以零點六五
這樣比較合理嘛
對不對
也就是說假設這邊有一百個人你這邊後來是六十五個人到這裡三十五個人到這裡的話呢
你應該是這邊兩個雖然都都等於白切了
是兩個都是log m
你不可能變成兩倍的log m 嘛那變成還變大了嘛
沒有變大只是一樣而已等於沒有做而已
那其實這邊的entropy 應該是它乘以零點六五它乘以零點三五加起來
那就是跟它一樣
表示沒有表示沒有做任何事
那麼因此你後面還要乘這個東西
還要乘這個比例才對
那後面的這個比例就是這個p 的n
ok 所以p 的n 呢就是prior probability probability of n 
也就是說有在你那個node 那裡
到底out of total number of sample 有多少
就像total sample 是一百個人
你現在有六十五個人在這裡三十五個人在這裡的的話呢
這個零點六五跟零點三五就是這個比例
就是這個p 的a 跟p 的b 
這個就是p 的a 這個就是p 的b 
那這樣的話呢乘在這個裡面這樣就不會錯了
嗯所以呢我們這邊還多了一個這個東西
所以這個就做weighted entropy
我還加一個weight 
那這樣的話呢就合理了
那底下這段沒有什麼特別
它只是說呢這個delta entropy 就是這個entropy reduction 這個東西呢你可以証明它其實是這個式子
那這個東西是什麼呢這個東西就是我們剛才所說的那個cross entropy
我們上一頁不是說過這個嗎
嗯再上一頁
就是這個東西
你今天如果給我兩個distribution 一個是p 一個是q 的話
那麼它們的cross entropy 就是p log p 除以q 的這個東西
或者p log p 減掉p log q 的這個東西叫做這個東西
你現在如果那這只是一個式子它說你可以証明它
那麼這個entropy 的reduction 是跟這個有關的
那這個a of x 就是到了這邊的的distribution 
b of x 就是到了這邊的distribution 
那麼它們分別跟n of x 就是
原來在這裡的distribution 
它們到底改變了多少
這個就是剛才講的那個那個entropy 的這個cross entropy 
或者說是它的k l distance
從這邊變到這邊到底distance 改變了多少了那個distance
那你其實就是這兩個distance 
那你這個這個可以想像
這個物理意義是合理的嘛
就是我原來原來這邊是n 啊
原來這個就是n of x 
當我這樣一切之後這個叫做a of x 
這個叫做b of x 
那因此呢我就是算a 跟n 差改變多少的k l distance 
跟b 跟n 改變多少的k l distance 
然後分別weighted by 它們的這個p a p b 就是這個東西
就是它們的weight 
那這就是我真正改變的entropy
那麼如果是這樣子的話呢那那那我就一路去把它降低就對了
那它這邊也提到說我這邊都每一次都算這個p n
我這邊算p a p b 就算這個零點六五零點三五這個東西呢
它有另外一個意義也是很合理的
就是我其實是是把reliability of the data 算進去
我量越多的數目越多量越多的這個這個統計特性越可靠
所以我就weight 比較重嘛
weight 比較數目比較少比較不可靠我就weight 比較較輕嘛
所以這個其實也是把把這個statistics 的reliability 算進去
那這樣的話呢你就可以發現我現在每一步
都在讓我這個tree 不斷不斷地這個repeatedly reduce 我的entropy 
那也就是說我等於是每一次如何split 用什麼方法來split 都是選擇一個 entropy 降低的最多的那個來拆
然後這樣一路拆下來一路拆下來
所以呢我每拆向下走呢我的entropy 都在降低之中
那這棵樹
整棵樹的的entropy 呢
就是它的最後的terminal 的entropy
那最後一路拆拆拆拆到最後的這個
在terminal entropy 就是我整棵樹
那當然我這個terminal 如果如果到最後的每一個純度最高的話entropy 一定最小
那這樣是最理想
所以呢這就是這樣的意思
好那如果這一點比較了解了的話
那我們現在可以來看我們現在怎麼來做tri phone
那麼要做tri phone 的時候呢
這個好像該休息哦我們休息十分鐘吧哦
 OK 我剛才漏掉說一件事就是 哦
就是這個 cross  entropy 的這個東西是不對稱的
嗯這 ASYMMETRICAL 
也就是說
這個 p 跟 q 的位置如果是反過來是 答答案就是會反嘛
你會看到因為這是 p log p 除以 q 嘛
所以如果這兩個倒過來的話就變成 q log q 除以 p 是不一樣的東西
哦 所以它是一個不對稱的叫做 cross  entropy 這是一個不對稱的 measure 
那麼 但是呢有人把它拿來當做 K L  distance 當成 distance 的時候呢 distance 常常覺得應該是對稱的啊
哦這個 a 跟 b 的 distance 好像應該 b 跟 a 的 distance 應該是一樣應該是對稱的啊
所以你當然也可以去定義一個對稱的也可以
那麼最常用的一種辦法就是讓它兩兩個加起來除以二嘛哦
那這樣就會變成對稱嘛
在這個定義裡面它是不對稱的
但是你可以定義另外一個東西就是 d 這個 d 的 p  of  x  q  of  x 
加上 d 的 q  of  x  p  of  x 
然後除以二對不對
你只要定義一個這樣東西的話它就變成是對稱的
所以你如果要一個 symmetric 的的 distance 也可以
就這樣就變成 symmetric 了
那至於說嗯我們這個地方剛才講說這裡這個當你一個 n  split 成為兩個 node  a 跟 b 的時候
它們的這個這個 entropy  reduction  turns  out 剛好是這個式子哦
那這個式子基本上其實就是數學推出來的
那我剛才講過這個你意義上也可以解釋
就是你是這個是這個這個嗯 n 跟 a 差多少
n 跟 b 差多少對不對 嗯
那這個地方如如果你覺得有點這個其實這個整個的式子的推導是來自這篇 paper 
你如果詳細去看的話
在那個 paper 裡面會講得很清楚
就是這一篇啦哦
這一篇嗯我在上次講有提到過
這一篇 paper 的作者其實是我們台大資訊系的一位學長從前的博士論文
當初是他做的
後來變這個變成經典之作
所以這邊拿來講就是
值得你去看一下你如果要看的話哦
那剛才那個推導或者那裡面相關的討論在那個裡面有
那基本上你可以看成就是我的 distance 我的這兩個 distribution 
a 跟 b 跟原來的 n 差多少的一個關係就是我這個 entropy  reduction 嗯
好那麼再來我們來講我們真的怎麼做這件事
