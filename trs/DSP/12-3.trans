那這就是在matrix 的代數裡面很重要的一件事情叫做singular value decomposition 
我們真正得到的是這個式子
這個式子是什麼呢是
你如果回頭看我們這邊的式子的話
意思是我取左邊的一半
我上面這個關係我取取左邊的一半
就是這個u 跟這個s one 這個s 的兩個
我本來是我取這邊的u 跟這邊的s 
取左邊的這個
我左邊取這個u 跟s 
我右邊取這個s 跟這個v transpose 
ok 我右邊取這個這邊的v transpose 
左邊取它的u 
那中間這個我也只取一個
這邊都是平方哦
這邊本來是平方我現在只不要平方我只取一個
我現在不要平方我只取一個
那就是我下一頁這邊的這個情形
你如果仔細看的話
左邊取的就是u m 乘r 
u 的m 乘r 就是這一個
我左邊取的這個u 的m 乘r 
也就是這邊的這個u 的m 乘r 
右邊取的這個是v 的transpose 
就是這個v 的transpose r 乘n 
就是這個這個那也就是這個r 乘n 
那中間這個呢我只取一個
剛才這邊我都要平方
這個是相當於s one 的平方嘛
這個是s two 的平方嘛都是有個平方的嘛
那我現在不要平方了我都只取一個
所以本來這邊寫s one 平方s two 平方我現在都不要平方了
所以呢就是s one s two 沒有平方了
那這就是s 的r 乘上r 
我就這三個相乘其實就approximately 就是原來那一個
ok 我們再看一次
這個意思是說
我剛才的話呢是要我必須要把它跟它的transpose 相乘
我才有辦法做這個eigen value 的eigen vector 的的的分解
但是這個其實都是兩倍的嘛
因為這兩次方的意思因為它它它跟它兩個它跟它自己相乘嘛
對於是一個平方的東西嘛
那這個也是一樣我也是transpose 相乘所以其實也是平方的意思我可以這樣做嘛
那因此我現在如果只做一個我不要平方的話呢
就是這邊只取左邊
這邊只取右邊
中間只取一個
不是平方
那就得到我們底下這張圖所說的
這邊只取左邊的
所以就是取這個
對不對
所以你可以想像我現在是左邊嗯不對左邊是取這個
中間取這個
右邊取這個
那中間的這個你也可以算成是這個也沒關係因為這個跟這個是一樣的
對不對
所以呢你如果看左邊這兩個的話
看左邊這兩個相當於是它乘上它
你如果看右邊這兩個的話相當於是它乘上它
不過中間這個就是中間這個
它都是一次方只算一次了
我剛才是兩次現在都沒有了那個平方都沒有了
喔這裡平方都沒有了剩下一個s one 跟s r 
那就變成這樣子
那這三個相乘你可以證明它其實就跟剛才這個等這個approximation 是一樣的
你這三個相乘的話
是跟原來那個w 很像的
不是exactly 一樣
是把後面這些丟掉了
是我把這堆東西
後面的這堆東西都丟掉了
或者把這些東西都丟掉了
我把這些個不重要的eigen vector 所代表的那些dimension 都丟掉了
我剩下一個比較簡單的了
那麼這樣子做的把一個長方形的matrix 拆成這三塊
是相當於很像原來的正方形的matrix 拆成這三塊
這非常像的
只是因為它是長方形所以必須拆成這樣的拆法
那這樣的拆法是所謂的s v d 
就是singular value decomposition 
那這個時候你所得到的這些東西叫做singular value 
那麼跟eigen value 有一點不像啊對不對
它是原來這個東西eigen value 的square root 
那叫做singular value 
不過我還是一樣按照大小順序來排列
那這些東西呢左邊這個u 呢叫做我的左邊的singular matrix 
右邊這個v 的transpose 叫做我的右邊的singular matrix ok 
那這樣的話我現在就把它展開就變成這樣子了
當我變成這樣之後這回我們底下就會看到有很多豐富的意義就出來了
那麼其實為什麼這樣子可以得到它裡面的concept 
這底下我們就可以看得到嗯
那嗯我們先停在這裡休息十分鐘
ok 我們下週期中考啊
下週我出國我不在
所以我們除了考試之外不做別的事
因此呢我們考一百二十分鐘就是後面的一百二十分鐘從十點十分到十二點十分
那麼只考那一百二十分鐘
那麼前面不上課啊
ok 我們現在回過頭來說我們這邊講的這些東西到底在幹嘛
那麼我們剛才說這個上週我們講的這個這個eigen voice 的觀念是
是說我有一個四百八十萬dimension 的一個很高維的空間
然後呢我想辦法把它reduce 到一個五十dimension 的的空間
那麼本來這個四百八十萬維的空間裡面每一點代表一個speaker 
其實不要那麼多維啦
我只要找到五十維的一個subspace 
其實每一點投影到上面來
它就在這個五十維裡面已經代表每一個speaker 之間的關係了
這樣的意思
那我現在做這件事情其實是很像的
所不同的是現在我本來每一個word 要多少十萬維
我們說這十萬個dimension 
也就是說每一個word 在這十萬篇文章裡面
分別出現的次數為基礎所得到的這個vector 
代表那個word 的特性
所以呢就word 而言呢我似乎應該我也可以想成是這樣子
我有一個十萬維的空間
這是十萬的dimension 
那這裡面的每一個點是十萬維的vector 
就是一個詞
譬如說這個是某一個w i 
這是某一個詞w j 
那麼基本上是這樣子
可是真的需要十萬維嗎不見得
我現在想辦法做一件事情
就是把這個十萬維reduce 到變成八百維
那怎麼reduce 跟這個情形是一樣的我在這裡面找一個八百維的subspace 
那也有點像是這樣子ok 
我這邊也有也找到一個一個八百維的subspace 
我這裡的每一點都投影到這八百維的上面來
然後呢這些八百維的點其實就跟原來十萬維的是一樣的
那我變成一個新的space 
這邊只有八百維
我在這邊的每一個原來的這裡的每一個點
都投到這邊來變成這個
我其實這八百維其實就是原來的這十萬維
於是我的每一個詞都只要八百維
每一個word 都只要八百維就可以描述了
這是一個什麼樣的關係呢你如果如果回過頭來看的話
就是這個關係
不過我現在需要把它擦掉了嗯
恩我把這個擦掉了
我要畫就是右右黑板那個那個powerpoint 上面的這個圖啦
就是這個圖
這個圖你現在是變成這樣子的一個然後乘上一個這個
再乘上這樣子一個對不對
這三個乘起來相當於原來的一個w 
所以這個是我們所謂的u 
這是我們這邊所謂的s 
這邊是所謂的v transpose 
這三個東西相乘會變成原來這個w 
這個是我們在這邊所說的這個singular value decomposition 的意思
這個u 乘上這個s
乘上這個v transpose
會得到我原來這個這個w喔
不是真的exactly 不過就是一個approximation 
會得到這個
這到底是什麼意思呢
我們現在可以來看
譬如說我們如果前兩個相乘你想會是什麼
這個跟這個相乘其實仍然
這兩個相乘仍然得到一個
這個乘這個對不對
這個是m 乘上r 
這個是r 乘上r 
所以乘完之後還是m 乘上r 
這個是u 乘上s 的一個matrix 
那這個u 乘上s 的matrix 呢
它的每一個dimension 是什麼東西
譬如說這裡的每一個row 
這個是第i 個row 的話
這裡面只有第八百個dimension 
這裡的每一個row你想想看其實是什麼呢 
跟這裡的每一個column 去做內積
就得到這邊的那一個row 
我們我們再講一次
就是這個這裡的譬如說這裡我第i 個row 
我第i 個row 這邊有八百個element 
它是不是這八百個乘上這八百個相加得到第一個值
這八百個乘上這八百個第二個相加得到第二個值
對不對它乘上第三個八百個相加得到第三個值等等
那我這邊有多少個有十萬個
結果我就得到這十萬個
對不對所以呢我原來的這十萬個這十萬個呢
其實就是第i 個word 
在原來這個dimen 在原來的這個十萬維空間裡面的那一個點就是那一個點
那一個點的那十萬個就是這邊的這十萬個嘛
那這十萬的每一個你可以看成是這些東西乘上這些東西加起來對不對
這些東西乘上這些東西加起來等等等等
你如果這樣看的話我們是不是可以回過頭來想
這個東西是什麼
這個東西其實是我們這邊所畫的
e one prime e two prime 等等
所以呢這裡面其實它的每一個row 
這個row 是e one prime 
這個row 是e two prime 等等等等
也就是這個的這個的eigen vector 
那如果我把這個值叫做第一個值叫做a one 
第二個值叫做a two 等等的話
那這個vect 那這個vector 其實是不是summation 的a i e i prime 
summation over i 
這個就是這個
再講一次喔
你仔細你要想一想才能夠想清楚這件事喔
就是說你可以想成是這個eigen vector e one prime 乘上這個a one 
再加上這個eigen vector e two prime 乘上這個a two 
這個e 三prime 乘上這個a 三
到這個e 八百乘上這個a 八百
這些東西是乘加起來其實就是這個嘛
這中間關係其實就是這樣子
因此才會這裡的這裡的八百個跟這八百個相乘得到第一個
就是它們的所有的第一個dimension 
這些個eigen vector 第一個dimension 分別乘上這個
得到這個的的第一個dimension 
這些eigen vector 第二個dimension 分別乘上這個
加起來得到這個第二個dimension 等等
那這樣我有十萬個嘛我十萬個就這樣得到了
這樣的關係其實就是e one 的eigen vector 
e one prime 乘上這個a one 
e two 的vector prime r 一個vector two prime 乘上a two 等等
加起來就是a i 的e i prime 
就是這個vector 
那這個的意思其實是不是相當於這八百個e i prime 
就是這八百維
所以現在這個個其實就是第一維就是e one prime 
第二維就是e two prime 
第三維是e 三prime 等等
我沒有辦法畫更多但其實這八百維是在這裡
而這裡的每一點呢
就是那些個a one 到a 八百
也就是說
我現在的a one 到a 八百是什麼
是這裡的一個row 
就是這裡的這兩個相乘的那個row 
換句話說
我原來的這個大matrix 裡面
這十萬維的vector 
代表一個word 
我現在在這個matrix 裡面我只要八百維就代表了
這八百維跟這十萬維是同樣的事情
只是
這八百維你如果分別
每一維其實代表這八百個每一個分別代表這裡的一個eigen vector 而已
ok 
所以呢我等於是
這八百個我等於是在這本來這十萬維的空間裡面
我找到八百個十萬維的vector 
它們是orthogonal 的
構成一個八百維的子空間
那這裡面八百維的子空間裡面每一點其實就是這裡面每一點對應過來的
而對應過來之後呢
我只要
這裡的每一個component 其實就是這些東西的weight 加起來
就得到原來這一點了
那這個意思跟我們原來這個意思是完全一樣的
我等於把十萬維的空間所描述的所有的
word 所有的詞之間的關係reduce 到一個八百維的空間裡面的關係
其實它們關係是完全一樣的
有一點點error 但是error 很小就是了
ok 
那麼這個情形就是我們在在這邊講的
那這這八百個其實就是那十萬維裡面的八百個vector 
我在那十萬維裡面找到這八百個
每一個都是十萬維哦
這都是十萬維的
但是我現在有八百個這十萬維的的東西構成一個八百維的空間
那你裡面的每一個原來這裡的每一點就變成這八百維的那個點
因此它們這些裡面的每一個值
就代表他們的weight 
就是這種weight 加起來
就是這個東西
那這個話就是寫在這邊的這個
你現在所謂的u 這個這是這個u 乘上這個s 
得到u 跟s 相乘得到的這個matrix 
u 跟s 相乘這個matrix 裡面的每一個row 
我叫做u i 的
加一個下下面加一個bar 
一個underline 的u i 
就是
所以這個東西就是我那邊講的那個u i 
有一個underline 的u i 
那你的這個u i 呢
就是原來這個u i 乘上s 嘛
對不對
就是這邊的這個u i 
這個這個八百維的也就是這邊的這個的
的第i 個row 
這個u i 乘上那個s 
就得到這個u i 的bar 
的underline 的u i 
這個東西呢就是一個vector 
它的dimension 由原來的十萬reduce 到現在只有八百
而這個東西呢
你可以看成什麼呢
那這八百個row vector 
那其實呢
就是這八百個row vector of v t v transpose 
就是這邊這八百個row vector 
其實也就是原來v 的八百個column 
這八百個row vector 就是這邊v 的八百個column 是一樣的
那這些東西呢也就是原來這個w transpose w 的
eigen vector 裡面最重要的八百個
構成一個orthonormal basis 
那那個basis 就是我們所謂的latent semantic space 
就是那邊的那個八百維的那個空間
就是一個潛藏的語意的空間
它的dimension 呢就是八百
而在這裡面呢
我每一個u i 的underline 的u i 呢
就是一個詞
也就是說我在原來的要十萬維的那一個詞
現在變成只有八百維就夠了
那也就是說我把這個十萬維的裡面
我找到一個八百維的把它通通都投到這八百維上面去了
那或者說你可以想像我現在有一個八百維的
那這裡的每一個維都代表了一堆東西了
那麼這些究竟是什麼東西呢
其實你是可以這樣子想的
這個我寫在下兩頁哦
哦我想想看在這裡
在這一頁裡面的這句話
每一個component 在這個reduce 的word vector 裡面
就是association of the word with the corresponding concept 
什麼意思
我沒我我現在說這裡的每一個eigen vector 代表某一個concept 
你可能很難想像為什麼這是一個concept 的one 
這個是concept two 
這個每一個代表一個concept 
代到底到底是什麼concept 呢
你可以看
這個eigen vector 就是這個eigen vector 
那如果在這個eigen vector 裡面你可以發現譬如說它這個詞
佔了零點零三
這個詞佔了零點四五
這個詞佔了零點三一
等等這些加起來之後
就是這個vector 
那你去看欸
這邊講的這個零點四五這邊是恐怖攻擊
這個零點三一這個是賓拉登
這個零點零三這個是布希
這個零點零一的那個是白宮
你發現它們加起來其實它代表就是
九二一嗯就是九一一恐怖攻擊的那件事情
它們
ok 你了解我的意思喔就是說你現在如果去看這個eigen vector 它裡面的值因為它是一個normalized unit vector 嘛
它某一些一堆相對於一堆詞都是零
因為它跟那些詞沒有關係
它跟某些詞有關係
如果這個是恐怖攻擊它有零點四五
這個是賓拉登它有零點三一
這個是白宮它有零點零三
那個是紐約那個是零點二什麼東西你加起來發現你會發現
它其實就是在講那個event 
那那個就是一個concept 
那同理呢你如果看一一三的話它有另外一堆是零
另外一堆有數字
你可能發現這個是講台北市政府
那個是講國民黨
譬如說這個有零點二一
這個講台北市政府
這個零點零五
嗯這個零點一三
這是講國民黨
那這個是零點一二
這個是講這個什麼選舉
你把它加起來發現這個就是是馬英九
那如果這樣的話那這個其實就是e two 這個就是馬英九
啊等等
所以你可以這樣想的話呢
那麼其實你只要看它裡面相對於每一個word 的weight 
你可以看得出來它其實代表
某一種concept 
那如果是這樣的話那現在這個word 是什麼
這個word 是這個concept 有零點幾這個concept 有零點幾這個concept 有零點幾加起來的結果
就是這個
於是你會發現說
現在如果這個這個word 
你會發現ok 它
馬英九有零點三五
陳水扁有零點二一
這什麼什麼都有結果發現它什麼
它其實是對美外交
啊等等
ok 
那所以這個word 其實就是對美外交
那它就是跟這些個concept 都有關係
那麼它們的關係就是這個weight 
就是這些a i 
ok 
那就是這邊講的這個意思
就是每一個component 在這個reduce 的vector 
u u j 的這個嗯underline 的u j 就是這個東西裡面的這裡的每一個a i 
這些東西其實就是它的
這個word 
這個word w i 我們講譬如說這個是這個對美外交
那這個word 的話呢它裡面的
譬如說這個跟這個陳水扁有多少的關係
跟馬英九有多少關係
跟外交部有多少關係跟等等等等等就剛好那它就是它們的
相對於每一個corresponding concept 的association 
那就是這些東西
ok 
那這樣的的話呢嗯
恩我們等於是把把這個word 的所代表的這些concept 
我們把它抽象的具現出來
發現這些東西是e one e two e 三到八百個
當然剛才舉的例子是比較具體一點或者應該講誇張一點
你真的是不見得看得出來啦喔
你如果真的去做這樣的分析之後
你要看每一個e i 代表什麼concept 
不是那麼容易看
但是有一點這樣的味道
那基本上呢你可以發現
大概每一個分別因為它們都是orthogonal 的
基本上是都是不同的東西
代表不同的concept 
那如果是這樣的話呢我上面這句話的意思
就是說我現在的word 在這個空間裡面所代表的是
只要越接近表示它們的是比較相關的
因此呢你譬如說這個如果是
一個陳水扁一個總統府
那這就會比較就會在附近
你如果是賓拉拉登跟這個阿富汗也會比較接近
那所以呢在這個地方
你在這邊也會比較接近不過這地方難看難看因為有十萬維你搞不清楚
但是這邊呢會清楚很多
相關的詞彙它們相關的詞它們的concept 接近的話
它在這些dimension 上是會接近的
因此呢如果有有類似的similar 的這個語意上的關係的話
它們在這個空間裡面的relation 應該是比較接近的
而且這裡有一個很大的好處是現在
它們只要有appear in similar type of document 就可以了
不需要exactly in 不需要in exactly same document 
什麼意思呢
就是說你如果在原來的這個w i 裡面
原來在這個w i 裡面
你如果要賓拉登跟這個阿富汗有關的話
它們必須出現在相同的文章裡面
這篇文章裡面賓拉登也出現好幾次阿富汗也出現好幾次
在這篇文章裡面也是
必須它們在同樣一篇文章裡面一再的同樣的出現
我才知道這兩個賓拉登跟阿富汗是有關的
可是我現在不是不再有十萬個了
我現在只有八百個
那每一個八百個是concept 不是那篇文章啊
我等於把很多篇文章reduce 成為一個concept 
所以呢這個時候我不再需要它們要出現在同樣的文章裡面
我只要讓它們出現在類似的文章就可以了
similar type document 
那很可能這些文章merge 成為一個
這些文章merge 成為一個
嗯你可以這樣想
所以我才會變成由十萬變成八百嘛
所以它們只會出現在類似的文章裡面
它們就會發現在那八百維的空間裡面是很接近的
雖然在這邊沒有出現在同一篇文章裡面也可以
那這是它一個非常大的優點
也就是本來我們講的某一個concept 
不是一定要哪個詞才是那個concept 
當你每次講對美外交的時候不是一定要有陳水扁也不是一定要有外交部
你凡是講到
布希你凡是講到什麼
凡是講到這個什麼其實它們都是講同一件事
那麼因此呢你不見得要有同一個詞
也不見得要在在同一篇文章裡面
那麼因此呢它現在就是這樣子
那麼我現在就是就是這個這個它們不見得一定要出現在同一篇文章裡面
只要在相類似的的文章的type 裡面我就可以把它們抓到發現它們是很接近的
那這個大致就是我們剛才講的這個這一頁的意思
因此呢我現在要代表一個word 
原來是要這邊十萬維
現在變成就是這個u u i 的bar 
就是這兩個相乘的
這個東西的u i 的bar 
我只要八百維就夠了
那麼這個意思有一點好像是說我把原來的這個十萬維
這十萬維呢是等於是discrete 
由n 個document 所define 出來的
我有十萬篇文章對不對
我有十萬篇文章所define 出來的十萬維
我現在reduce 到變成只有八百維
而這好像是變成continuous 的了
因為這裡的這八百維是這八百維
這八百維是這它有零點幾這個零點幾這些加起來
所以好像是一個好像continuous 的東西
把一些東西reduce 成為一維
把一些東西reduce 成為一維這樣子來看
那麼因此呢那這八百維的的每一個每一個dimension 是什麼
就是我這邊的eigen vector 
那也就是也就是我現在我的vector 就在這個上面表現喔
那這個觀念如果你可以想像的話
