那這個jason inequality 只是我們後面要用的一個不等式
那沒有什麼特別就是這個p log p
就是我們算entropy 的式子
給我一個機率
一個機率的分佈一個distribution 就是這個東西
我取log 之後分別對每一個p 去加起來
這就是p log p
所以這個式子其實就是我們剛才的
這個h of s 這個entropy 的式子就是這個式子
那麼它說呢
你如果把這個p 換成另外一個q 的話
一定會比較大
嗯就這樣子意思
我們算entropy 是p log p
可是你如果換成另外一個distribution q 的話
一定會比較大
q 是another probability distribution
所以q 一定也是要加起來等於一的
加起來要等於一它也是正的
所以是另外一個機率的distribution 不過不是p 就是了
那你只有把這個p 換成一個q 的話
這個p 換成q 的話都會比較大
什麼時候等號成立
當q 等於p 的時候等號成立
這個叫做jason inequality
那底下沒什麼特別它這只是在証明
這個証明其實很簡單這個証明其實就是用這個式子就是log x
永遠小於x 減一
這個是沒有什麼特別
你在學數學的時候就學過這個
就所謂的log x 永遠小於x 減一
其實有很多種証明的方法
然後等號成成立是在
只有那一點等號成立
它就這樣了這個是log x 
這個是x 減一
x 減一是一條直線
那然後呢它們兩個只有在一點等號成立
就是x 等於一的時候等號成立
那別的時候它都比它小
那就用這個式子
那麼我現在的x 呢
就把它寫成q 除以p 
q 的x i除除以p 的x i 
當成是x 的話
那麼log 的這個x 呢就會小於等於x 減一
然後我分別都乘上p 的xi 去summation over i
它仍然是小於等於它
ok
然後呢
你右邊的這個式子就是等於零的
原因是這個這個兩個一消掉了嘛
就是q q 的x i 減一的summation
那個加起來這個q x i 加起來也是是一嘛所以就等於零了嘛
所以呢
左邊這個式子就小於等於零
左邊這這個式子就是p log p 減掉p log q 嘛
嗯p log q 減掉p log p 嘛
那就得到上面這個式子哦
所以這個沒有什麼這只是一個証明而已
它就是用這個log x 小於等於x 減一的這個式子
然後只有一個等號成立就是x 等於一的那一點
它用這個
然後把x 代q 除以p 代進去
然後就這樣可以証明它等於這樣的意思就是了
那它的簡單的解釋它的說法就是說呢
你本來算entropy 應該是p log p 的
但是你如果把那個p 算做另外一個機率的話
你弄錯了用另外一個機率來算的話呢你entropy 一定是增加的
或者說呢你用一個估計不正確的distribution 去算
你得到的degree of uncertainty 是增加的
你可以用這個方法來解讀這個式子的意思
不過基本上這只是一個一個不等式我們後面會用它來証明一些東西就是了
我們只是這樣而已
那同樣的有了這個東西我們也可以寫成另外一個式子
你看這個底下這個式子的話呢這個p log 的的p 除以q 
其實就是這個一樣的不過倒過來寫就是了
那我們剛才証明這個東西小於等於零現在這個倒過來就這個大於等於零
那這個p log p 減掉p log q 
它一定是大於零的東西
這個東西呢我們給它一個名字
叫做cross entropy 
或是relative entropy 
就是如果你現在有兩個distribution 一個叫做p 一個叫做q 的話
那我們就可以算它們的這個cross entropy
或者relative entropy 
就這樣算
那這個其實是在算兩這是算兩個distribution 之間的distance 
這個有一個名字就是所謂的k l distance 
那麼換句話說其實
你給我兩個distribution 我可以算很多東
算很多種distance
假設這是一個這是一個p of x
那我給你給我另外一個q of x 
它是另外一個
這是q of x 
那麼q 跟p 之間是有是有差距的
到底它們的差距是多少
怎麼算p 跟q 的distance
這有很多種算法你如果去查
查書的話
given 一個q 跟一個p 怎麼樣算它的distance
有很多種distance 的算法
有很多種definition
各有它不同的意思
其中的一種叫做k l distance
就是這個
kull back leibler 
這個這個distance
那這個distance 算法就是用這個來算
那其實即使是這個distance 也有好幾種算法
那有很多個版本
那這個distance 那你看你可以看得出來它的意思就是說
你去算它的entropy 
因為給我一個distribution 本來就可以算entropy
那我如果算entropy 的話那我當把其中的一個
當成另外一個的時候
它的它的會entropy 會差多少
就表示它們之間的就表示entropy 會差多少嘛
就是p log p 跟p log q 會差多少
那就是它們之間的差異
所以這個呢是等於是這個它們的它們之間的這個這個uncertainty 的差異
那麼這個
那麼做為一個它們之間的這個這個distance嗯
這個也是我們後面會用的就是了
那麼嗯在這裡的話呢當然我們這邊都是用discrete 的版本來解釋
當然這個都可以變成continuous distribution
你現在給我我一個continuous distribution 我也一樣可以做這件事情
那這個我們就不多講就是了
好關於這個我們就就說到這裡
底下我們就要來講我們真正要用的是
用在底下這裡
