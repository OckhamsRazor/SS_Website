那底下我們應該要進入的是第
我看我們還可以講個兩三分鐘我們稍微開個頭
就是十點零我們還沒有講
就是
utterance verification 
跟key word spotting 
那這個其實就是
我們剛才講的
譬如說我辨識出來的每一個答案
我都可以給它一個分數
怎麼樣做這個分數
才是可靠的
那這個分數告訴我
哪些可信哪些不可信
那就是所謂的utterance verification 
verify 它
那然後這個
我們做key word spotting 其實是用這套東西來做的等等
那這個reference 我後面再說
那基本上
這裡面一個最基本的精神是來自所謂的likelihood ratio test
那
這個東西是
最古典的通訊原理裡面
做雷達的時候
恩
這個
他們所謂的detection theory 裡面的核心
那detection theory 是數十年前的
最古典的但是也是到今天為止最重要的
之一
所謂的detection theory
那這在幹嘛呢這個是
雷達裡面偵測敵機有沒有來
那你知道呢我基本上是放一個訊號出去
如果前面有敵機的話它會有一些反射的訊號回來
我從反射回來的那些訊號裡面偵測
有沒有敵機
那麼在另外一個例子呢就是
這個
海上的冰山
你知道這個這個在古代
在北大西洋航行的船隻
最怕碰到冰山
那在海面上這個這個大海裡面如果有一個冰山你不容易看得出來
用人眼睛看不太到
所以呢你除了用人眼去看之外就是用個雷達
你放一些訊號出去
然後如果是海面它也會反射
如果是冰山它也會反射都不大一樣
因此你就可以判斷
用根據那個來判斷它有還是沒有
那不管是有沒有冰山還是有沒有敵機
這種detection 就是都只有兩個hypothesis
就是有還是沒有
有敵機跟沒有敵機有冰山跟沒有冰山
所以你收到的某一種observation 
它會有兩種distribution 
譬如說這個是這個是零是沒有敵機
或是前面沒有冰山的話
我收到的很可能是一個非常random 的某一種樣子的observation 
那h 零就表示沒有敵機或者沒有冰山或者前面沒有我要我擔心發生的事情
那可是如果是有的話呢
如果前面有了敵機或有了冰山的話呢
它可能會有另一個長相
這樣子
那麼於是呢我就要靠這兩種不同的distribution
我今天如果抓到一個收到一個signal 是這樣子的
那我應該判斷它是有還是沒有呢
如果收到一個在這裡的話
我應該判斷它是有還是沒有呢等等喔
那這就是所謂的dictation theory 
也就是所謂的hypothesis testing 阿
那我基本上就是兩個hypothesis
就是有跟沒有
那麼它們各自可能有一個prior probability 
就是有本來會有多少機率沒有本來會有多少機率
然後我的observation 是x 
那這個x 呢我有兩種distribution 
有的時候跟沒有的時候的distribution是不一樣的
那於是我現在就根據要根據這個x 我要判斷
我根據這些假設喔我現在知道這個
假設這個是知道的這個是知道
那我要根據這個來看
那怎麼辦呢
最基本的原則還是我們所熟悉的m a p 
那也就是說你看這個其實就是m a p
就是posterior probability
當我observe 我要的是這個嘛
就是當我observe 到x 的時候
零的機率跟一的機率誰大
那這個其實就是a posterior probability 
就是我observe 到這個之後我的我的想要知道那件事情的機率
那麼如果說是observe 到這個x 之後
零的機率就比較大
表示說呢我想可能是沒有
譬如說我observe 到這個話呢
零的那個機率比大
一的機率比較小
那我猜沒有可能比較合理
可是如果我observe 到這來的話呢
那一的機率比較大零的機率比較小
那我就應該猜它來了
那於是呢我這個就就可以根據這個這樣來判斷
如果這樣來判斷的話就是把這兩個m a p 的機率除一除求它的ratio 
如果它大於它的話這個就大於一小於一嘛
那我如果這樣的話呢我那拿這兩個a posterior probability 的機率的的ratio 除一除
看它比一大還比一小來判斷
那這就是這個m a p 的principle
那但是這個機率怎麼求呢again 我不會求
跟我們之前講的都完全一樣我就倒過來
所以我不會求這個機率我會求反過來的
因為零跟一的這個是零還是一的這個機的這個機率就在這裡嘛
我如果倒過來的話這兩個是已知的嘛
我至少可以可以先量好如果前面有冰山的話它會怎樣
那如果前面沒有冰山的話我這個可以先量出來得到一個機率嘛
所以呢我如果倒過來的話呢這個機率就是這個機率就是有的嘛
然後呢再乘上那個除上那個
這就是我們講的那個bayes theorem 就倒過來的
然後呢你如果這樣的話呢
我這個剛才的這個式子就可以重寫成為這樣子
於是我就變成為我就會變成拿這兩個機率比一比
那就兩個機率其實就是這邊講的這個機率跟這個機率或者這個機率跟這個機率
那這兩個機率比一比
我跟一個threshold 去比
就可以判斷了
那這兩個機率是什麼就是likelihood ratio
因為它們各自是likelihood function
這兩個就是就是likelihood function 
那因此這個叫做likelihood ratio test 
那這個是古典的雷達理論裡面最核心的一個東西
所以這兩個ratio 就是 likelihood ratio
就是用這個方式來判斷喔
那我想這是一個簡單的introduction
那我們下週從這裡開始
我們講用這個觀念來做我們要做的事情
ok 好今天提到這
ok 
我們今天是本學期最後一次上課喔
我們要把所有沒有講完的都在今天講完
喔我們先進入第十點零
就是utterance verification 
那麼我們上週開始講到這件事
就是說呃utterance verification 基本觀念來自從前古典的通訊原理裡面的detection theory 
那這個原來是用來做這個雷達的application 裡面用的
它就叫detect 
就是要detect 說前面有沒有某一種我知道可能會發生的事情
那這種有沒有發生的情形就是所謂hypothesis 
那麼換句話說呢假設你要判斷前面有沒有敵機
敵機來了就是一
沒有敵機來就是零
或者說是這個海上航行的船隻要看前面有沒有冰山
那麼有冰山就是一
沒有就是零
那麼這個時候呢我有兩個prior probability 
就是有敵機跟沒有敵機的
或者是有冰山沒有冰山的一跟零
這兩個prior probability 
那麼於是呢我的這個靠什麼呢
我靠傳一些個訊號出去
然後呢它會碰到前面的狀況之後
會有一些反射的訊號回來
那不管不管是海面有沒有冰山的話呢沒有冰山海面還是會反射一些一些訊號回來
但是如果有的話呢
反射的東西會不一樣
那麼我們舉例來講用最簡單的例子來說呢假設我收到的x 
那麼這個x 呢這個喔它在如果說是如果說是海面上沒有冰山的話呢你就收到一些這樣子的
這個那這個就是probability of x 
那麼given 沒有的時候的情形
那如果是有的話呢
如果有的話它可能會有一種譬如說這樣子的
那這個呢就是probability of x 呢
如果有的話的distribution 
當然實際上不是那麼簡單我這只是用一個one d 的簡單的呈現的方式來說假設它們有這樣子不同的話
那麼因此呢我可以depends on 這個我收到的訊號來判斷
譬如說我如果今天收收到一個訊號是在這裡的話
收到一一個訊號在這裡的話
那麼顯然沒有的如果有有一個前面有某些事情發生了的機率是比較大
而前面沒有什麼狀況的話呢機率比較小
反過來呢我如果發生某一件是收到某個訊號在這裡的話呢
那麼表示說呢顯然是沒有什麼事情的機率比較大
有的是有什麼事情的機率比較小等等
那這個想法呢也就是所謂的其實最古老的m a p 來自那個年代
就是a posterior probability 呢
我看這個東西
那這個也是在看這個
那麼如果說是我收到的這個x 
那given observation 這個x 之後
如果沒有的機率比一比有的機率大的話
那我就假設是沒有
那就是像這個情形
沒有的機率比有的機率大的話
我就假設是沒有
反過來呢如果有的機率比沒有機率大的話我就算它是發生了
那就像這樣的情形
那麼因此呢我就是那這兩個機率是什麼呢
這兩個機率其實就是a posterior probability 
也就是在喔given 這個observation 之後
那我要的事情只有我我想要知道的事情只有零跟一嘛
有跟沒有
那就是在這個given observation 之下
前面有跟沒有的機率差多少
那麼它們的誰大誰小
那既然是這樣只有這兩個誰大誰小我就可以做個ratio 來看
那麼如果是大於一呢就是沒有小於一就是有等等
那這個a posterior probability 呢不太好算
但是呢我們可以把它倒過來
這就是我們之前所一再使用的是完全一樣的做法
就把它倒過來
倒過來之後呢
那這個機率就是我們這邊有的機率
也就是這邊這兩個機率
那這兩個機率其實就是likelihood function 
所以呢我就有了likelihood 在這裡了
那麼我有了likelihood 之後呢這個我本來就有
假設這個是這個可以預先量好準備好的
同理呢這個東西就是prior probability 我可以假設本來就知道的
那於是呢這些都可以知道了
那這個呢這個其實不不需要
就像我們前面說的一樣
因為你反正是given 一個x 
所以這個可以不用管
我只要看哪一個i 比就大
是零大還是一大所以這個沒有關係
於是呢剛才這個ratio 又變成這樣子
當你變成這樣子的時候這兩個就是likelihood function 的ratio 
跟這個剛好倒過來
那這兩個likelihood likelihood ratio 的function 呢就是所謂likely ratio 
那麼那麼既然這個是likely ratio 的話就拿這個去跟這兩個prior probability 的ratio 去比
那這個就是threshold 
那麼因此呢這個就是古典的通訊原理裡面或者雷達的problem 所謂的likely ratio test 
我就是看這個likely ratio 來決定有還是沒有
那麼這個東西呢
這個原來是在這個古典的這個雷達裡面的觀念
那我們現在怎麼拿來做utterance verification 呢
這個意思是說
喔假設某一段聲音
我們現在判斷它是某一個word i 了
那倒底是不是呢我要不要相信它呢
因為我們知道我判斷的每一個word 都會有error 嘛
所以呢你很可能我有我判一句話判斷出來有word one word two word three 
這樣子到word n 
我有得到這堆word 
那麼倒底他們對不對呢
我們上週曾經說過
我們可以給它每一個recognized 的word 一個confidence measure 
就是給它一個分數
譬如說呢這個我判斷是零點九
這是零點七這是零點二
然後這個是零點四
那這些這些東西呢
你可以假設我們讓它是介於零跟一之間
當然你也可以讓它介於不同東西之間
假設零跟一之間的話呢這個高又表示我相當相信這個答案是正確的
很低就表示呢八成這個是錯的
那當然這有很多種方法來做它
那麼我們這裡講的這個是其中的一種
最簡單的做法
那麼今天其實要做這個有很多種方法
這種東西就是所謂的confidence score 或者confidence measure 
就是我判斷我的recognize 的result 出來之後我給它一個分數
我倒底要不要相信它
那麼你可以猜得到呢如果是這樣的情形的話呢
你要相信的可能是這個
你不要相信可能是這個
那麼呢這個要不要相信就就depend on 狀況了
因此如果在一個dialogue 系統裡面
它說我要去我要買飛機票或者我我要幹什麼的話你可以判斷你你所recognize 出來的那些word 
它的分數高不高
然後我要不要相信它
那如果不太可靠的話呢我寧可如果它說的這這個key word 在這裡的話呢
我不要太相信我寧可重新再問一次等等
那這樣子的觀念就是所謂的utterance verification 
那因此呢我要的是說假設某一個聲音這是一個x 是我的observation 
我把它recognize 成為w i 了
我認為它是w i 了
但是到底是不是呢
那我就做一個這樣子
那你現在看這個式子就知道這個式子其實跟這個幾乎長得一模一樣
就是如果它真的是w i 的話
啊其實是這個式子啦
啊哦是那個就是那個
就是說我真的是w i 或是不是w i 的機率喔
那麼因此呢那就變成說是這個如果我真的是w i 的話
那麼我我會看到這個x 的機率
除以不是w i 這個底下這個東西w i 的bar 
等於是說不是w i 的時候的機率
那我拿這兩個來ratio 一下
那這個意思其實就是跟這邊一樣
為什麼會有這個是跟不是呢
其實就是原來這邊的有跟沒有嘛
就這邊有跟沒有是一樣的意思嘛
那麼因此呢你實際情形就是說
我現在的這個w i 是那個你所判斷出來那個word 的的hidden markov model 的話呢
那分子的這個東西
其實也就是我們平常所說的那一個
是一樣的嘛
given lambda 
就是你如果是那個word 的話那個word 的hidden markov model 
然後在那個model 之下
given 那個model 這個的的情形之下的話
啊我會看到這個聲音的這個observation 是多少
就是這個東西嘛
這也就是我們basic problem one 所求的這個東西
其實也就是這個嘛
這個w i 就是這個lambda 嘛喔一樣的
然後這個就是這個的observation 嘛
那我現在分子再多一個呢就是不是w i 的
那麼不是w i 那是什麼呢
這個有很多不同的做法
基本上我們稱之為anti model 
或者說是background model 
或者是類似這樣的名字
譬如說它叫做這個alternative 這裡寫錯了應該是alternative hypothesis 
那總之就是不是w i 的意思
那有很多種做法
那麼並不確定哪一種才是好的
因為不同的實驗做出來的的情形可能不太一樣
那麼你可以猜得出來就是depend on 這些是什麼word 
然後depend on 你你到底判斷它是怎麼情形
那麼可能在不同的狀況之下不同的做法各有它的好處
所以呢我們其實並沒有真正的標準答案說哪一種才是最好的
這邊有幾個例子就是說你你這個是不是w i 這個model 是什麼
你可以trained with undesired phone units 
所謂undesired phone units 就是說不是w i 的unit 
譬如說假設說你現在是我現在我要算這個w one 的話
它是phone one phone two 到phone k 
所排成的這個word 
那所謂的un undesired phone units 就是
除了這些以外的其它的phone 
那那suppose 這個如果是w i 的話剛好跟這些phone 都一樣
不是的就是其它的phone 
因此呢我可以用所有其它的phone 
train 一個model 
這是一種做法
那你可想而知它的意思等於是說
它的這個哦它的這個變成是probability of x 
given w i 除以probability of x 
given 不是w i 
那麼因此呢基本上來講
如果這個這個這個x 真的是w i 的話呢
這個分數會比較大一點
可是呢這裡面都不是這些phone 
所以呢這個放到這裡面去呢
應該是積分數會很低
應應該是很小
所以呢這個大的除以一個小的越除之後會越變得更大
所以呢如果說它真的是它它真的是這個word 的話呢
基本上本來這就會比較大
而這個因為都是其它的phone 嘛
就會更小
那麼這兩個一ratio 的話呢就會使得它變得更大
反過來如果它不是這個這個word 的話呢
基本上這裡會比較小一點
如果這個聲音不是這個word 的話
它會小一點
可是呢它不是這個word 它就會有一些其它的phone 
所以呢這個在其它的phone 裡它的分數會大一點
那這個一除之後呢小除以大呢會變得更小一點
因此呢我們平常只看這一個嘛
對不對
我們平常講好像是看這個
那麼這個大看誰大
那現在除了這個之後呢
其實是使得大的會變得更大
因為它要除一個比較小的
小的會變得更小
因為它要除一個比較大的
那麼因此它會把大的跟小的分得更開一點
那麼你更容易判斷這樣它倒底是不是
用一個這樣的觀念
那麼因此這個是講說我可以用這個undesired phone units 
來train 一個這個這個anti model 
或者叫做background model 就是要除的這個東西
那也有的時候人家用所謂的cohort set 
所謂cohort set 的意思是說
假設我算出來的時候
這個w i 的分數是最高的
所以我判斷這個是w i 
那麼其它的其他的word 呢
有差很多很低
可是有一些是比較接近的
w k 或者是w j 
這些是比較近的
也就是說這些個word 比較會跟它confuse 
這些個就比較不不會了因為差很多
那或者說這些裡面其實它有比較接近它的phone 
會造成困擾的
那這樣話所謂的cohort set 就是
我把跟它比較比較接近的這一堆
其實也就是我的confusing 啊我的competing competing words 
我把這些東西拿來
把這些個word 拿來train 一個model 
那基本上就是我希望能夠跟它分開的地方
我拿那些東西拿來拿拿這些比較接近的這些東西就是所謂的cohort set 
就是跟它比較像的這些word 
把這些word 的聲音拿來train 一個model 
那這個呢放在這個東西
那那這個意思呢你可以想像跟剛才這個是有像但是不太一樣
它變成是說呢
我現在我那個聲音如果是這些word 的話
基本上是會跑到這裡面來分數比較高一點
那在跑到那放到這裡來分數就會低一點
所以呢我這個就會變成一除之後比較比較小嘛
那反過來呢我如果是它的話呢
那這個會比較大嘛
我等於用這些來做來做對比
然後make sure 我我不會弄錯我我是這個還是這裡面的
我要把這些東西是它的還是這些要分清楚
喔那如果這樣做就是所謂的cohort set 
那competing units 是說我這個其實我也可以把它變成units 變成phone 
譬如說我的我的p 這個這個phone one 我有一堆它的competing 的cohort set 
譬如說這個p 這個phone 的k phone 的j 
啊這些東西是跟它比較比較容易混淆的
phone two 的話有這些東西跟它比較混淆的
那我拿這些東西來train 
那我等於說是拿這些competing units 
來弄一個model 
喔的意思差不多是這樣子的
等等所以這裡的到這個這個background model 到底要怎麼做其實並沒有一定
那麼也不見得哪種一定最好
那麼在實驗裡面顯示是不同的case 不太一樣
所以我們不太知道exact 到底怎樣
但基本上這都是很常用的做法
那於是呢我現在不管怎樣我就是有一個這個真正的model 跟一個competing model 
一個或者anti model 或者background model 
那這兩個之之間的關係
我拿這個ratio 來做其實就是原來這邊的likelihood ratio 的意思
就是一個有是那個word 或不是那個word 
好那這樣子做之後
那這樣子所得到基本上就是我們所謂confidence score 
或者confidence measure 
那麼這樣做的話呢我比較可以判斷它啊用這個判斷它好還是不好
那麼它的正確機率倒底是多高
當然現在這樣做的時候不一定是介於零跟一之間
你可能還需要一個另外一個呃另外一個mapping 
或者是一個normalization 
你如果希望它是在零跟一之間的話
那當然也不是一定要
那這個是早年最最基本的做這個verification 算這個confidence score 的方法
那當然到到了近年做的方法非常多了
不限於這一種喔
那有很多其它的方法可以做的
那你如果有興趣的話在很多的文章都會看到
他們怎麼樣做這些東西
那在在古古典的這個裡面這個threshold 怎麼算的呢
是它們的這兩個prior probability 的ratio 
那在這裡我們當然不太知道這裡倒底是多少
不過事實上呢我們不太需要真的算這個標準的來
而這個是depend on 我們要它的error 是多少
那這就是所謂的這個type one error type two error 
這個這個名詞也都是古典的detection theory 裡面講雷達所用的名詞
就是所謂的missing 跟false alarm 
那你知道所謂的false alarm 的意思就是說敵機沒有來
但是你認為它敵機來了
所以呢我就這個這個這個拉警急警報
大家虛警一場
後來敵機沒有來
這所謂的false alarm 
那或者說就是false detection 
那什麼是missing 呢
是說敵機來了你不知道
那這個是missing 
那這兩種在之所以稱為type one type two 
是因為這兩種error 並不對稱
它的後果不同
那麼也就是說在如果你是其實判斷後來做這個零跟一的數位通訊的時候
也是用這個做的
數位通訊的基本原理也是靠這個來判斷零還是一
可是在數位通訊而言零當成一或者一當成零是對稱的
都發生一個error 
所以沒有什麼不同
可是在這裡的話呢這兩個代代價是不同的
如果前面有冰山而你沒有發現冰山的話
很可能因此船就撞到冰山船就沉了
這個false 這個missing 的代價非常大的
反過來呢如果你沒有冰山而以為是有的話呢拉個警報
大家虛警一場後來呢就繼續這沒事
所以這兩個代價不同所以它們當成兩種不同的error 來看
就是所謂的type one 跟type two 或者missing 跟false alarm 
那麼在這個情形之下的話呢
你就會有這個錯誤率
false alarm rate 或者是這個false rejection rate 
那或者說是這個missing rate 等等喔都有
那麼這些個
那在我們這裡的話呢我們可以把它看成是
是一個跟我們之前講這個retrieval 的時候很像的一個問題
那麼假設說這一堆是正確的字
correct word 
那麼這堆呢是這個verified word 
那麼因為我現在是在做verification 
我要verify 它對不對喔
那你會發生這種情形
這什麼意思呢就是說這個當你判當你得到某一個word 的時候你要再verify 一次它是不是
如果呢它的分數確實夠高而我認為是了
那正確的字而且我認為是了在這裡
但是也很可能它雖然是正確的
這個字其實辨視是正確的
但是因為我的這個分數不夠高
而我就放棄了
我寧可當它不對
我放棄掉了是這塊
那反過來呢這一塊呢是說其實那個word 是錯的
那個word 是錯的但是呢因為它的分數夠高
我當它對了
結果這個是弄錯了
那如果這樣的話這裡也有三塊就是a b c 
因此你有兩個正確率
就是a 除以a 加b 跟a 除以a 加c 
那a 除以a 加b 的呢這個呢
其實這個就是相當於我如果一減掉的話那這個其實就是喔missing 
那如果一除以a 減a 加c 的話呢
那這個就是相當於false alarm 
也就是說這個所所謂的一減掉這個a 除以a 加b 
其實也就是b 除以a 加b 嘛
那這個意思就是明明是正確的word 而且你已經辨識出來了
但是你因為分數不夠高你就放棄了的話
那這個是你miss 掉的東西
那這個除以a 除以a 加c 其實也就是也就是c 除以a 加c 了
那就是false alarm 
就是明明是錯誤的字
錯誤的word 但是你把它當它是對的了
那在在你total 裡面你當它是對的有多少
這是false alarm 
那麼你如果這樣看的話呢這就是這兩個false alarm rate 跟這個missing rate 之間的關係
那如果是這樣看的話
這個跟我們在那這兩個圖你看就跟我們講retrieval 的時候的record 跟precision 
其實是非常像的
那我們說的這兩個其實就是一個是record 一個是preci precision 
這個其實是record 
這個是precision 
因此呢其實我們講這些這些正確率錯誤率
跟record rate precision rate 其實都是同一回事
那麼這些東西都有其實都有一個關係的
那我們那個時候說record 跟precision 你可以畫一個圖
那在這裡其實也是一樣
record 跟precision 你用一來減就變成missing 跟false alarm 
差不多的東西
那麼因此呢我們也可以畫一個圖
我們舉例來講我如果畫成這個false alarm 在這裡
這個是一減掉missing 的話呢
那麼絕大多數的時候你所得到的curve 是是像這樣子的
那麼換句話說呢
你如果如果你要false false alarm 低的話
那麼missing 就會高
所以一減missing 就會低
那反過來你如果missing 要低的話呢
就是就是這個要高
那於是呢你的fall 你的你你的false alarm 就會高
那麼那麼事實上你可以想像這個就是選擇這個threshold 的問題
當你如果threshold 選得低的話
threshold 選得低的話
那麼動不動你就會認為是是對的
動不動你就覺得是有敵機來了
那麼因此false false alarm rate 就會高
可是我就比較不會有missing 
那我如果threshold 選得高的話呢
我就不太會發生false alarm 
因為我選得很高大部分都不會發生嘛
所以false alarm 就會就會低嘛
可是我的這個我也會missing 
來了我也沒有會沒有看到
所以呢你就是depends on 你選擇threshold 的時候呢
那麼那麼這個false alarm 跟missing 的rate 變化的關係是像這樣
那麼當然ideal 的case 呢是是是在頭上
這是ideal case 
那也就是所謂的ideal 就是你不管不管這個如何
你的false alarm 永遠是在零
我的missing 也永遠是在零喔
那這個是ideal 
那你真正做的情形就會變成這樣子
那你越往這裡面越往那個地方貼近的話就表示越好啊等等
那這個跟我們在講retrieval 的時候的那一個喔喔我們有一個就做這個record precision plot 
把這二個rate 畫做一張圖是一樣的意思喔
那現在是在我們用這個false alarm 跟這個missing 來畫的話就是這張圖
這張圖其實有個名字的這個叫做所謂的r o c curve 
在古典的在古典的這個detection theory 裡面
這是所謂這r o c curve 
就是這個喔receiver operating characteristics 
也就是你你在你那個你那個雷達或者你那個你那個receiver 你操作的真正的特性就在這裡
那你如果是這條curve 畫出來越貼近於這個邊的話就表示越好
你離那個越遠就表示越差啊
這個等於是
這樣的關係所以這就是這邊所講就是說你的一個threshold 呢是是這個你真正怎麼調不是根據這個來調
其實不是跟據這個來調
那怎麼調呢就是就是看你要怎麼樣的performance rate 
看你要這兩個precision 跟record 
或者說是false alarm 跟missing 
那他們之間你要他們的關係是什麼來來調這條curve 
好那有了這個之後呢
我們剛才這個是講的是對每一個word 可以做這樣的事
