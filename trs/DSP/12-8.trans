你可以找一本來
那底下我要說一下第五個
就是probabilistic 
也就是說這個剛才講的這些都沒有太多機率
它有統計啦
其實裡面的每一個這個element 是統計嘛
是還有entropy 還有什麼
是有統計
但基本上它整個不是靠機率來算的
它是用matrix 來算的
那後來就有人發現其實這樣不夠好
更好的應該是整套都用機率
那就好像h m m 一樣
整套都用機率有什麼好處
就是它可以容易學習嘛
你可以把h m m 那一類的所有的那些個training 的方法都拿來
然後可以讓它用各種的machine learning 方法來做
然後這個你新的data 進來可以不斷地學習喔
很多很多好的方法好的情形在有了機率都可以
所以後來就有人說我應該把機率來重新formulate 這個問題
那這個就是所謂的probabilistic latent semantic analysis 
或者說index p l s i 
那這一篇是p l s i 的原始paper 
最早出現的一篇是在a c m 的sig i r 裡面
一九九九年
倒不是最好看的一篇
那不過你可以去找就是說
嗯在這個之後會有好幾篇
裡面有寫得非常完整的會比較好看的喔
那我們來說一下這個是什麼
這個就是在我剛才的再下一頁
基本上還是一樣的事情
我有一堆文件有一堆詞
不過這裡我用的詞term 不太一樣
我這個詞現在叫做term 
所以呢這個term t j 
term 就相當於我們原來這邊的word 
我這邊的word 其實它叫做t j 叫做term 
那這邊的呢就是document 
變成d i 
不過我現在變成大寫的就是
所以符號有點不一樣不過你大概知道我還是這兩個東西
就是這個詞或者是term 
跟這個文件document 之間的關係
然後我希望在中間找一堆就是我所謂的topic 
這還是一樣是topic 
所不同的是我現在全部都變成用機率的關係
什麼機率的關係呢
你可以看到
這個就是說我看到一篇document 
它會是在講哪一個topic 
有一個機率
我假設這邊有八百個topic 的話
我看到一篇東西
我可以算說它有多少機率講這篇
零點三的機率講這個topic 
零點一的機率在講這個topic 
這些topic 它機率是零或者怎樣
所以我看到一篇文章
我可以去分析它談每一個topic 的機率
同樣的呢
我如果知道它是講某一個topic 的話
ok
會在這個topic 裡面會講到這個word 
或者這個term 的機率是多少
我可以算這個term 的機率
所以呢在某一個topic 裡面
某一個term 會用到的機率是什麼
是這個
當我有這兩個機率之後
我現在變成這個式子
也就是說
我現在如果在一篇文章裡面
要看到那個詞的機率
是一個機率
不是直接數的了
我們剛才在這裡的話
在這一篇文章裡面它出現幾次
我數一數就知道是幾嘛
這是一個deterministic value 
對不對
它文章裡面它出現幾次我數一數就好了
現在不是了
現在是一個機率
在它裡面會出現這個的機率是多少呢
應該是這樣算的
就是呢你你如果看到它
你可以分析它可能是哪一個topic 的機率
然後在那個topic 裡面它會講到這個term 的機率
然後你現在這個乘起來之後把所有的topic 加起來
這才是這個機率
那你現在要要train 這個東西
就是要讓這個東西跟真的在這裡所看到的這個數字要像
那也就是我這個機率
那這個是真正的frequency count of term in document 
我要這個東西
那我要把這個東西maximize 
這個就是一個likelihood function 這是一個likelihood function 
在這個document 裡面看到這個term 的like 的likelihood function 
然後我現在要maximize 這個likelihood function 
我用這個來train 
所以我有一大堆文件
一大堆詞我可以train 這個東西
那這整個的數學的formulation 跟這個完全不一樣了
因為它不再用什麼vect matrix 這些都沒有了
它完全用機率所以數學的formulation 完全不一樣了
它的觀念是很像的
這個觀念幾乎是相同的
啊它完全用數學來做
那這怎麼train 
又又是用e m 
那e m 我們考完期中考之後我們會講e m 
那你就會清楚它裡面的一大堆數學
那你如果有興趣的話這也是一個很好的這個我們這邊講的這些都是蠻好的這個寫報告的題材喔
所以我想這一段應該是到這裡
ya 我們十二點零講的就是這些東西
那我想這個
ok 我們今天就上到這裡好不好
ok 嗯本來今天我說是要講的是第九點零的e m 喔
不過我後來決定說我應該再多講一個十五點零的這個robustness 
那原因也是一樣就是啊這是另外一個非常重要的大領域
那啊有非常豐富的研究主題在裡面
所以呢我覺得好像應該我們先講這一個
這樣子讓各位可以早一點可以接觸這些東西
那啊你可以早一點想可能的研究的題材
那我們如果把這個十五點零講完
我們再回去講九點零我想應該是ok 
那如果這樣的話呢我們你可以這個早一點開始多想一點其它的可以做報告的題目
那當然另外一個原因是因為其實十五點零
跟我們之前講的九十一點零跟十二點零是啊啊都是屬於這個adaptation 系列的
那其實這個觀念都是相通的
所以呢也許我們是可以一路接下來講
是比較順一點
