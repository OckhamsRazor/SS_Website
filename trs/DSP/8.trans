ok 喔 我們今天要講的八點零就是這裡要講的最後一塊還沒有詳細說的
就是這個我們一直在還一直圍繞著這一塊在說
那麼我們上週的七點零講的是這塊就是 frontend processing 包括怎麼求這些 feature
六點零講的是 language model 是這一塊
然後呃五點零四點零講的是 h m m 是這一塊
那現剩下就最後這一塊那這一塊其實並不容易
因為你現在得把所有的這些東西全部整合你包括要把這些的 feature vector 這些個 h m m 跟 language model lexicon 全部要整合所以呢這個地方其實是相當複雜的一塊
那就是我們今天八點零要說的事
那麼這個我們有另外一張圖其實是一樣的意思那麼在二點零裡面
對那麼我們那時候在說的事情是這件事也就是說當你的acoustic model 給你一堆h m m
language model 給你一堆n gram 之後
你這個search 在做什麼事情你可以想像成是given given 一個這個sequence of feature vector x
你希望找一個word sequence w 那能夠maximize 這個機率也就是 m a p 的principle
那這個機率呢我們可以把它變成這樣子因此呢你要算這個機率是用h m m 算的這個機率是language model 算的
那你要找一個word sequence 這兩個乘起來最大
這個也這個也就剛才那個圖這樣講起來好像很容易其實並不容易
為什麼你只要想我這個word sequence 每一個word 假設我有六萬個word 的話辭典裡面有六萬個可以用的word
那麼你這邊有每一個都有六萬個可能
所以呢你一開始的第一個你的第一個word 就有六萬個可能第二個word 又有六萬個可能每一個word 都有六萬個可能所以呢你其實就有六萬的r 次方種
那其實應該講你的第一個phone 就有假設你有六十個phone 的話第一個phone 就有六十個可能第二個phone 又有六十個可能所以你如果要把所有的通通都這樣子來做一次的話來maximize 的話其實是非常難做到的事
那也就是我們現在八點零要說的那麼在八點零裡面我們倒底用什麼辦法來做這個search 這就是所謂的search process
好那我們現在來看八點零那麼這裡所說的呃這個其實是recognition 的一個一個chord element 因為你如果沒有這塊的話你其實是沒有辦法真的執行
那麼在所有課本裡面都會講這一段大概相當於這本課本的這些跟這些或者這些
我這邊都寫or 意思是說你只要選擇一個你覺得喜歡看的就可以了
那麼這裡面呃講得最完整的可能是這個
那我這邊講的大概也是從它裡面我大概也是以它為這個主要的reference 來講的我主要是從它來做的
但是呢它講的比我講的多很多所以我只是從它裡面抽一些少部分的我來講而已喔
那它的缺點是說它寫得多而不見得那麼好看
那寫得淺而比較好看的大概是這一個這是寫得淺而比較好看的
那這個呢它講得其實相當不錯只是說它畢竟不是一本教科書它只是他的演講稿所以呢稍微難讀一點就是了
不過這三個我想你只要選任何一個來讀都可以
四十一篇paper 哦是算寫得非常好的完整而詳細的一篇paper 把這邊所有東西都講得很清楚
呃缺點是說這篇不好念就是了相當長而且不好念但是你如果認真讀這是相當好的一篇
那五的這個倒是不是我們這邊講的search
而是我底下要講的這個我們一開始先講一個古代的東西
就是所謂的dynamic time wrapping d t w
那這個東西是在還沒有h m m 之前最成功的技術
那麼其實到今天仍然有它的價值所以我們稍微提一下
那它的觀念其實也就是在做search
所以呢我們可以可以把它看成是這個search 的基本精神可以可以從這邊來看
那麼因此呢我們會先講一下這個東西
那這個是現在用得比較少了但是其實呢相當值得學一學它們的觀念所以我們稍微說一下
那這個的reference 是我們剛才的這個
那這本書是比較比較早的書九三年的書所以我們現在不太reference 它因為它講的很多東西其實之前已經不太用了
但是它的這段其實講得非常好因為那個年代其實他們花了很大功夫做這些東西喔
在還沒有h m m 他們做做得不錯的所以你如果要了解詳細的話是可以reference 這一個
好那麼我們現在說什麼是d t w
我們說過這個是在pre 我所謂的pre h m m 的時代也就是說在古代h m m 還沒有成熟之前
當時已經有非常多的語音辨識的研究跟技術
當時用的最成功的方法就是這一個
那這個這個方法even 今天仍然很好
如果你只是做small vocabulary 跟isolate word recognition 的話
那也就是說呢今天仍然有一些其實有相當多的isolate word small vocabulary 的應用是用它做的
那麼因為用它比用h m m 簡單如果你是一個這麼簡單的problem 的話
那舉個例子來講你如果去看玩具有的玩具是可以聽聲音的
你說這個這個turn to the right 它就向右邊轉你turn to the left 它向左邊轉那 sing a song 它就唱個歌它能夠聽少數的字的這種就是isolate word small vocabulary
那它怎麼做的一片晶片在裡面
它那片晶片其實裡面做的是這個因為在這種狀況之下其實這個比h m m 要簡單
但是呢你如果複雜一點的話呢它就輸給h m m 了
當你的字彙很大的時候我們說過h m m 最大的好處是它可以把小的串成大的
所以你只要有有phone 的model 就可以串成word 的model word 的model 就可以串成sentence model h m m 的好處是可以直接串起來
那它這個就沒有這個好處所以你要連起來不是不能連連起來困難比較多而且比較做不太那麼好
那麼因此呢他要做continuous speech 就比較難它比較適合做isolate
另外呢它大概vocabulary 不容易大也因為這樣就不容易大喔那麼等等這是它的缺點
但是即使如果只是做這種小問題的話
它到今天仍然是跟h m m 一樣好
大概比起正確率是完全相同的是一樣好的
那它的基本精神是什麼
就是找一條optimal path 來match 兩個template with different length
這個講different length 有點簡單化了一點其實不只是different length 而是different sequence of events
嗯我們舉個例子來講譬如說san francisco
你要辨識一個san francisco 它可能這麼長
等一下你再念一次san francisco 的時候呢san francisco 就只有這麼短
那麼你你怎麼怎麼辨識這中間的的東西呢
那麼第一個它們長度本來就不一樣你誰跟誰去比呢
而且你要講這裡面的譬如說某一段音斯它在這裡它在這裡
它也比較長它也比較短那你怎麼知道它跟它比呢
另外一個可san francisco 的這個可在這裡那它在這裡
那你怎麼知道它在這裡它應該它跟它跟它比呢
那那如果你這個都不能知道的話呢我怎麼比呢
那我們知道今天h m m 怎麼做這件事情
h m m 做這件事情是是我們用的方法是就是它的state transition
因為這些都是random 的
對不對所以它可以state 在這邊比較長也可以跳下去
所以每一個state 的的都是可長可短h m m 是用這種方式來handle 這個問題
那它這裡怎麼辦呢它就是想辦法去找一個optimal path 來match 它們
那麼我們舉例來講假設你這個叫做reference 這叫做test pattern
那我的 reference pattern 在這裡假設是比較長
test pattern 在這裡比較短
那怎麼辦呢那你可以想像的是我如果拿裡面的某一個參數來說
假設某一個parameter p one 或者p k 它這邊的值是這樣子的
那那個p k 在這邊的值呢是這樣子的
那你大概可以可以想像那麼很可能應該這一點是對應到這一點的這一點呢是對應到這一點的那麼這一點應該是對應到這一點的
同理呢起點對應到起點終點對應到終點
那你真正要找的應應該是這麼一條path
我如果可以找到那麼一條path 那條path 告訴我哪一點對應到哪一點哪一點對應到哪一點哪一點對應到哪一點
那如果是這樣的話呢我現在就可以在這延著這一條path 來比對
那這個時候就知道它應該對應到它來比它應該對應到它來比等等
喔那這個就是這個d t w 的基本精神就是這件事就是找一個optimal path mention to template with different length
那麼我剛剛講不一定是different length 而且還包括這個這個different distribution of events
那這個event 在這裡這個在這裡你要對應到它們各自應該在哪裡等等
好那麼這個基本精神是這樣真的要做的時候其實是有非常多種方法
所以我們剛才看到在還沒有h m m 的年代呢他們下了相當大的功夫喔所以你詳細你要看的話是看這一這一段
喔那我這邊大概簡單的說一下我們舉一個例子這只是一個example
不見得是最好的但是呢我們稍微看一個例子看它是怎麼做的
那基本上呢你第一件事情就是我的一個是我就是我那邊畫的一個是reference 一個是test 喔
所以一個是test template 是y 然後reference template 是x 那我怎麼樣去去這個調它們呢
那麼這個是這個x 是x i i 等於一到m
所以呢譬如說這個是x one x two 一直到x m
這是我的這個喔reference 這是我的reference template
然後我的test template 是 y j j 等於一到n
喔所以這個軸是i 然後這個軸是j
y j 呢這是y one y two 一直到y 的n
那當然 m 跟n 是沒有理由會一樣它們是兩個不同的integer
那我現在要把它們來做剛才講的那件事情找那條path
怎麼找呢我現在在這個方法裡面這只是它有諸多方法裡面的一種在這個方法裡面它說呢我先想辦法
把它們都對應到一個共同的長度l 去
那麼因此我都做一個mapping 讓它們對應到l
換句話說呢我不論是這個test template 還是這個reference template 我都重新畫一根軸
那這根軸呢變成l
這邊變成一二到l
這個軸呢叫做這個x f x 的i 對應到m
所以我這邊就變成一個 m 軸這個是一個m 軸
那麼我重新把它調成長度是l 這個index 叫做m
那麼以別於剛才那個叫做 i 的現在是m
那同理呢y 這邊我也重新做一根軸我也是一到l
那這軸叫做n 以別於剛才的那個j
那這樣子之後呢這就是我這邊寫的這個
我有一個這個function 把i 對應到m j 對應到n 那m 跟n 都變成只有長度相同都是l
那這個時候當然你有一些條件你這個mapping function 怎麼定義呢就是這邊所講的那一對應到一最後都對應到l 對不對
我一都對應到一一都對應到一那這邊的話呢m 要對應到l 這邊的話呢n 要對應到l 那就是這一排所謂的n point constrain
之後呢所謂的monotonic constraint 是說呢這樣子
那這個意思是說當然你現在這個這個m 不見得等於l 嘛你可能是要拉長或者縮短嘛
那你如果要拉長或者縮短的話
你可以想像會變成這一點對應到這一點這一點對應到這一點這一點對應到這一點是可能的我中間丟掉一些這是可以的
但是呢不可以說是這個對應到這個之後這點給我對應回來這個不可以
好那就是所謂的monotonic constraint 那你看這個意思就是這個意思喔
就是你你如果是你你不能這個你可以往前對應過去如果它比較長的話你要丟掉一些東西嘛
反過來如果它比較短的話你也可以上面丟掉一些東西嘛阿你可以這樣子嘛這個都可以嗯
但是呢你不能說這樣子嘛這個是不行的嘛就是就是就這樣的意思喔
所以這個是monotonic constraints
那有了這個之後我現在都都對應好了然後呢它最主要的動作就是底下這一步
那其實所有的這個我們說這個做法千變萬化有很多種不過基本精神就是這個這是一樣的
那這個的意思其實跟我們在跟我們在嗯四點零講的viterbi algorithm almost 是完全相同的觀念
你如果回想一下我們那時候講的viterbi 是怎樣的
如果還清楚的話我們現在說這個就非常簡單因為跟那個是一樣的意思
你記得viterbi 是怎樣的嗎
我今天如果有一個這個在時間t 要在某個state i 上面我定義一個東西叫做delta t 的i
那是指說我在時間t 走到i 的時候呢這個的某一條path
那條path 走到這裡的時候我的分數是最高的那個的分數
那如果說我現在這點每一個都求出來了的話
那麼下一下一排的t 加一呢我就只要去算這些東西裡面倒底是誰過來的就好然後一路走往前走這就是viterbi
那你如果這點還記得的話我們這邊講的其實almost 是相同的事情
那麼他這邊的 d 的m n 是個cumulate 我這邊漏寫一個字哦應該是minimum distance up to m n 應該有個minimum 這是minimum distance
那麼換句話說呢我現在如果我走到走到某一點的 n m
m 在這裡n 在這裡的這一點是m n
那有一個分數叫做d
那它是到這裡為止的所有的path 裡面分數最低的
嗯分數因為它現在是在算distance 要找一條minimum distance 所以它現在是算distance 所以是minimum
我們那邊 viterbi 我們因為那邊是在算機率所以是要maximum 這邊是算minimum
也就是說這個時候我我的d m n 呢就是我走到這邊為止的一個minimum distance 的 path 某一條
是我的我的這個分數最低的一條
那如果這點知道的話呢那麼我的這個嗯現在是怎樣呢我現在要算一條新的m n 是對所有的m plum n plum
那我們可以假設這點是m plum 這點是n plum 這個是m plum 這個是n plum 的話
那我現在要算一個新的點
這一點呢是m 跟n 這點是m n
那同樣的這點可以來自很多點
它可以從這點這樣過來也可以從另外一點這樣子過去也可以從另外一點這樣子過來
那每一點呢都相對於它有另外一條optimal path 走過去
那看看是誰加到這邊是minimum distance
那這個精神跟我們那邊講的viterbi 是完全一樣的
所不同的是在viterbi 的時候我們已經變成是一排一排這樣一行一行向前走
在這裡的時候現在講的這裡呢它並沒有規定要一行一行走
你可以從前面的任何一點往前面走就這樣不同
ok 所以呢我現在在這些個m plum n plum
我都有走到那裡為止的minimum distance path 跟它的minimum 的分數就是accumulate minimum distance
那然後呢那我的到這邊怎麼算呢就是看從誰過來分數最低嘛是minimum
那這時候我就要算從m plum n plum 走到m n 的要多加多少就是多加這個
那多加的這個是什麼呢這個東西你可以看它其實就是從m plum n plum 走到m n 的所有的多增加的這個分數
從n plum m plum 走到m不管是從這個還是這個我都可以過來嘛那就看哪一點過來的分數加起來最少
那這裡面又要稍微算複雜一點
那就是說因為我現在呢我現在從這個n 這邊走到這邊我的每一點呢我都要先由剛才那個function 對應回去
所以這個f x 跟f y 的inverse 就是這些mapping 的先對應回去到 i j
因為在i j 可以算它的distance measure 在原來的d t w 的frame work 裡面最重要就是
你任何一個frame 的x 跟任何frame 的y 你可以算他們的distance 所以它就去算這個distance
但這個distance 定義在這個上面呢
所以你現在是要把這個n plum 我現在看看這點是什麼那這點跟這點是什麼
然後你拿這個的x 跟這個的y 去做去求它們的distance
好所以呢那也就是我們這邊所說的
你從這裡m plum n plum 這 plum 這邊呢你先inverse 回去得到在原來的i 跟j 軸上面各是哪個地方
然後拿這個的x 跟這個的y 才能夠算它的distance
那你算distance 的時候你還要算另外一個東西
這個所謂的w 的delta i delta j 是什麼呢是 weight for different types of move
什麼意思呢你從這邊或者這一點或者這一點往那邊走可以有很多種走法
你可以想像的是我可以這樣子我可以這樣子走我可以這樣子走我也可能是這樣子走那我可也等等哦
那你每一種你可以讓它們分數不一樣
你可以定義如果這個是一步的話這個是一步這個可能是一點五步或者這個是什麼你可以定義它們的weight 不同好
那這就是這邊所說的這個w i w j
也就是你這個地方到底向這邊走的是w i 嘛這個走的是w j 嘛
那你看w i w j 你看是走走多少看你是走哪一種move 那你可以有不同的weight
那這樣的話呢你就可以把從你的m plum n plum 走到n m 那中間一路這個走好幾步這樣走到這邊那麼這中間的通通加起來
但是weighted by 這個這個weighting factor 那這樣子之後呢你就可以那這個精神這樣做的話呢你這個精神就跟我們那邊講的viterbi 是完全一樣的
所以呢你可以假設一開始的那一點這點很容易開始
然後呢你每一次要往上走的話呢你就是看倒底哪一個哪從哪一點走過來是這變成一個iteration 嘛你可以看出來這就是一個iteration
那這個式子跟我們viterbi 的那個iteration 是完全一樣的
那你每一次你只要看說這個前面走到這邊了我下一步走到這邊的話應該從哪裡過來分數是最低的
喔跟那邊的情形是完全一樣所不同的是我們剛才講viterbi 的話我是進步到一排一排一排變成一排一排走下來
它這邊沒有它這個你往上面走都可以
那這個的精神你基本上可以看成是就是所謂的dynamic programming
那你如果清楚的話知道它是什麼那麼如果不清楚的話呢我底下幾章在這裡有有一個在講什麼是dynamic programming
它的基本精神就是說replace the problem by a smaller sub problem 然後formulate 一個iterative procedure
你看它就是在做這件事我們講的viterbi 也是這樣子
就是你本身你要viterbi 是想要找一條optimal path 這個很難找啊你怎麼找這個optimal path
那我其實是把它reduce 成為一個簡單的多的一個sub problem 是smaller sub problem
在這個problem 裡面我只是要算這個iteration
假設我已經有optimal path 到這裡了
假設我已經有optimum path 到這裡了那我現在看的是
下一個在這裡的話應該從哪來
我只要做好這一點就行了這是一個smaller sub problem
我有了這個smaller sub problem 之後呢我就變成一個iteration 就把就可以把它做出來
那這個意思就等於是dynamic programming 的基本精神就是這樣
那我們的viterbi 其實就是在做這麼一件事
那同理這邊在做的這個也是這件事
那我的sub problem 也是一樣
如果我已經有了這些個點的minimum distance path 到這裡的話
那下一步怎麼辦我就看誰走的最近我只要做這個這一步就好了那這一步就是所謂的smaller sub problem
好那如果是這樣來看的話呢那其實大概這就是d t w 的基本精神
雖然詳細做不一定要這樣子做啦這只是這裡面這裡寫的只是諸多的方法裡面的一種而已喔
那這裡面呢其實還有很多別的譬如說呢有所謂的local constraint 跟global constraint
什麼叫local constraint 呢就是我們剛才說ok 你可以這樣子走可以這樣子走但是你可能規定說我我一次最多不能走多少步
這邊最多不能走多少步然後遠到多少程度就不能走了對不對我一次只能夠跳多這一類的就是所謂的local constraint 你可以做下各種規定
那global constraint 呢是說我們通常會規定說這邊有一個限制吧在這個範圍之外不能走
你就是說你如果走到這裡的話是有點問題走到這邊的話好像是說這整個的聲音聲音都歸給它一點點
你走到這邊來就它這麼一點點要要涵蓋所有這不太通所以你應該是最底下是不能走
同理最這邊也不能走好你如果在這邊也不通的
所以呢你會有一個global constraint 說我整個的我大概在哪個範圍之內才可以走
好這是所謂的global constraint 等等
那這些就構成所謂的d t w
我們說這個是沒有h m m 之前的古代最成功的方法
那它有很多當然它有它的缺點
一個缺點就是缺好缺少一個training 的方法你這會怎麼train
那麼其實它們怎麼train 的呢它們就是說你今天如果有譬如說san francisco
你念個二十次的話有san francisco 有長有短怎麼辦你就用這個方法把它們都warp 到一個長度
對不對你如果有二十個二十個san francisco 的training data 你就拿其中的一個當reference
其它的通通都都跟它對應成相同長度用這個方法得到一條path
然後就把以那條path 為準把它們全部平均起來變成一個做為reference
他們就當當是這樣做的也因為這樣所以它本身沒有一個統計模型
它本身欠缺統計模型所以它比較這個這個pattern 比較train 不好
那當然還有一個大問題就是它不是它沒有辦法做這個continuous speech
可以做做不太好你怎麼把它連起來你怎麼把一堆pattern 連起來你還要能夠找這條path
那變成有點難做他們當時其實都做了那但是做的不如h m m 做得好就是了
ok 關於這個d w d t w 我們說到這裡
那麼它基本上你可以想像它是一個它是就是我們剛剛講的這個dynamic programming 的一個具體的實現的做法
那麼這樣子的在search 過程其實跟我們講的viterbi 是一樣的
那其實viterbi 就是我們底下要講的我們底下要講我整個的這個problem 怎麼做其實也是用viterbi 哦
所以你再看下去的話我們底下這邊最後後來做的其實都是viterbi 我們都在都在做viterbi
那麼viterbi 的精神也就是這個dynamic programming
那這底下是在說呢其實就是這這個剛才我已經講過了
就是我們在在講四點零的時候講這個basic problem two
那個那個viterbi 我們就可以拿來做isolate work recognition
那其實那也就是一個最基本的search algorithm
那麼我們底下要說的就是這樣子的一個search algorithm
那這個基本的基本的問題就是我們剛才講的
你一開始其實我的每一個我一開始每一個phone 都有可能的
你可以想像我現在進來一個一段聲音的話
這難度在哪裡進來這段聲音的話
這是第一個 phone 第一個phone 假設我有六十個可能的phone 這第一個phone 六十個都可能第二個phone 也六十個都可能這些phone 都是六十個都有可能的
那你要知道倒底是什麼你必需一一都去找的話不得了
然後呢這些phone 可以構成某一個word 這個phone 可以構成某一個word 這個word 六萬個都有可能對不對
你每一個都有可能所以呢你怎麼樣從頭去找這件事情是一件非常難做的事
那麼比起我們之前講的這個這個要難很多就是了我們這個是蠻簡單的
你現在如果這樣想這是非常難的問題那我們底下先來看幾個例子
這都滿簡單的就是呃digit stream recognition
假設我只是辨識digit
辨識digit 的話呢假設就是零到九
我有個digit stream 不過我不知道有幾個所以呢就是譬如說三二一五一八七
那麼你每一個你第一個呢不知道是幾不過就是十個裡面的一個然後
這個呢也是十個裡面的一個你每一次就只有十個裡面的一個就是了
那你三二一五一八七這個也不是那麼容易辨識
那你想想看是怎麼做的呢
那第一個問題就是說我在這個情形裡面因為只有digit 只有digit 所以呢我沒有辭典
呃並沒有規定說哪個number 後面要接哪個 number 對不對
那麼因此呢我們沒有number 之間的constraint 沒有辭典
並沒有誰跟誰連起來是某一個word 沒有這回事兒同樣也沒有language model
所以三二一後面會接什麼是沒有n gram 的所以也沒有language model
那這時候怎麼辦你可以想像情形是這樣
就是我零到九任何一個數字都可能而任何數字講完的時候後面又可以接所有的可能
那這個情形我們如果畫h m m 的話呢這是零有零的model 一有一的model 到九有九的 model
每一個model 走完都可以從頭再走但是你不曉得從頭走幾次因為你不曉得它有幾個digit
那這樣的問題在中文的數字比英文的數字難一點
那麼難不少應該講因此你如果做中文的digit recognition digit stream recognition 正確率都會比英文的來的低一點
為什麼中文比較難呢因為中文很多音是不容易分辨的
譬如說五五五五五五倒底是幾個五我們人耳朵聽得出來是幾個五機器是很難分別是幾個五喔
然後呢一一一一是幾個一那我們還有七七的話呢是七還是七一七一跟七是非常像的對不對
甚至於七一一那麼這倒底是幾個這是很難分辨的
那麼那我們還有這個譬如說這個六跟九六跟九是非常像的
這個二跟五其實也是蠻像的那因此呢你有很多這種問題這個在英文是沒有的
你的英文six five 還是seven 還是nine 這非常清楚它們都不一樣喔
因為它們都是這個前面有子音後面有子音什麼的所以這很清楚
但是中文的話這個地方是是難度比較高的
那這個怎麼做那麼最直接的做法就是像這樣子
你可以想像呢就是就跟我我們就跑 viterbi 然後呢假設零的這個這個其實就是我們在做的viterbi 的那個的那個圖那
就就就跟這邊畫的一樣就這張圖
那你如果是零的話我有零的model 在這裡一有一的model 在這裡這是某一個零的model 或者一的model
那你就在這邊走一個那你在這邊走一個零或者一的model 就是這裡等於一個平面嘛
等於這個平面上走一次走一個零跟一的path 那這樣就是這一個平面
但是呢我一開始是零到九九個都有可能嘛
所以我就有九個平面我等於是有九個model 在這裡我一開始的這九個平面都在這上面走
所不同的是說呢我走到底的時候我立刻可以跳到任何一個
因為你譬如說一走完之後下面又可以從頭走嘛
一走完之後我沒有理由接一後面是零到九都可以接
所以你凡是走到頂的時候你立刻可以跳下來可以接到任何一個零到九的繼續走
所以呢我就變成有有這樣的九個九個平面然後呢嗯十個平面
我這樣一路每一個平面我都向上走凡是走到頂的時候我就可以立刻跳下來我就可以立刻跳下來又可以從這裡面的任何一個接下去走
那就是我們這邊講的switch to the first state of the next model at the end of the previous model
你前面一個走完你就可以switch 又從頭開始走
那這樣呢因為我不知道到底有幾個數字
那在我們而言問題很多就因為你五五五不曉得是三個五還是二個五還是一個五
那因此你並不知道到底有幾個數字你只好一路這樣走
那麼這樣走的話呢我等於是在一個三d 的一個 grid 上面在那裡在那邊走
這張圖跟這張圖意思是完全一樣的喔只是你用不同的方法來畫而已
這是一樣我先在這個平面上走走到頂的時候
我跳下來到任何一個平面又從頭開始走走到頂的時候再跳下來再走這樣子這樣我總共走n 次喔
那這樣的話我可能會造成什麼呢
就是會造成substitution deletion 跟insertion
這是我們通常講continuous speech 最大的問題就是這三個嘛
那所謂substitution 就是說你一當成是七這是substitution
那麼這個deletion 呢是說他明明說是七一但是我以為是只有一個七把那個一丟掉了這就是deletion
那麼insertion 是明明是一個七我當成七一了就是多了一個一出來這就是insertion
那這是我們講的三大type 的error
那這個等於是一個前菜你大概可以想像我們想的問題像這樣一些問題
不過這個算是簡單的因為我們總共只有十個digit 每一次就是十個你選一個而已
那底下的這張呢講的是呃這個其實是稍微容易一點就是假設有幾個digit 我已經知道了
譬如說我已經知道這是四個四個digit 三二一八我知道四個所以我就是走四次嘛
那如果是這樣的話呢我其實就是做四層
第一層也是零到九有九個平面第二層也是零到九有九個平面嗯等等我做四層
然後在第一層走完的時候一樣的我立刻跳到第二層的頭任何一個都可以接
每一層走完我都可以接到下一層的所有的頭這樣子
那所不同因為我我知道它是四個所以我走完四層之後一定要走完
到這邊沒走完的都不算我到這邊一定要走完
那這種東西呢叫做所謂的level building 那這就是一個一個level 就是假設我知道是四個的話我就是四個level
哦所以number of level 呢就是number of digits 那在這個情形之下呢你是
這句話我們剛才講了就是你你當這個last state of the previous model 走完的時候你
就自動跳到first state of the next model
在這個時候你可以從跳到任何一個那這樣走法你走到最後你也可以算算哪條path 分數最高嘛
那這個也是一個比較複雜的一個viterbi 我們這一這一頁跟上一頁這兩個都是viterbi
你一樣跑viterbi 只是說你這個比較稍微多一點稍微複雜一點而已
那在這種情形之下它的好處就是沒有insertion 沒有deletion 為什麼沒有了因為我已經知道有四個了
我知道有四個最後要走完所以辨識出來就是四個
那這個時候只會算錯不會算只會算錯不會算多了一個少了一個這個是這個情形
好那有了這些個基本的想法之後我們底下來看我們真正要講的問題是這一個
這一個就是我們剛剛講的你一開始的話你不知道是哪一個phone
然後我並不知道每一個phone 都有所有的六十個可能
然後哪些phone 可以串成一個word
你基本上可以想成我有六我每一個都有六萬個word
那這樣一路我還不曉得到底有多少個phone 我也不曉得倒底有多少個word 喔
那麼因此呢這個問題是非常複雜然後如何來做其實很不容易
那我們這邊講的是一個最基本的做法是這樣子
那最上面這個回到我們一開始所說的map principal
那這就是我們剛才講的喔就是在二點零裡面就說到
我今天的problem 是什麼problem 進來一個x 一個sequence of sequence of feature vector x
我要找一個w 就是這個word sequence w 使得這個機率最大
那個這個機率就是a posterior probability 所以這個就是m a p 的principal 這就是我們二點零所說的
那然後我們說這個怎麼這麼這個怎麼做呢
這個機率我們不會算我們就把它倒過來這是bayes theorem 把它倒過來
倒過來之後這個可以不要看
因為我們是要在在不同的w 裡面去找機率最大那一個
這個都是相同的對任何w 都一樣所以這個可以不要看我就變成算是maximum 上面這兩個相乘
上面這兩個相乘呢這一個就是這個就是h m m 所算的這個就是n gram 所算的
那基本上我們就要maximize 這個東西
那這個是沒什麼問題這個跟我們之前講的是完全一樣的
那問題只是說這個怎麼做這個幾乎不能做
因為就是我們剛才講假設你這個word 總數有六萬的話
你那個word sequence 有 r 個word 就有這麼多個word sequence
你不可能為每一個word sequence 都去算這件事
所以這個其實這個式子其實是不能不能解的
那麼我們底下就在講怎麼解這個怎麼做這件事
那為什麼它不能解第一個你可以想到這個其實你還不能做這個東西因為這個是什麼
這個是我有無限多個word sequence 我有無限多個state sequence
那麼我們舉例來講假設就就用這個就用這個來看
假設你的那個你的那個word sequence w
有一個state sequence 在這裡
那這個是時間t 你這邊好比我得到的就是x
就是我我的我這邊所輸入的我輸入的這個這個聲音的sequence feature vector sequence 在這裡
就是這個橫軸在時間上面
然後我的word sequence w 可以串成一個大的這個hidden markov model 在這裡
但這裡面呢有無限多個path 都可以走啊有非常多的path 你每一條都可以走啊
那麼你的機率這個機率應該是什麼呢
這個機率應該是這個機率就是
如果是任何一個state sequence 的話它的機率是多少
然後把所有state sequence 全部加起來應該是這樣
所以這個機率呢照說就是這個
那這個式子是什麼其實就是我們只前講的basic problem one 你如果還記得的話
我們當時寫成這樣就是在做這件事這是我們的basic problem one
那麼你要從也就是說呢你所謂的你要在這個model 裡面
你在這個model 那個那個時候我們把這個叫做lambda 叫做這個這個model
你要在這個 model 裡面看到這個o o 就是這個東西我們那時候叫做o
你要在這裡看到這個o 的話你必需要把所有可能的path 統統都加起來那就是要加這個哦
那這個非常非常大的一個加法這個做起來就會累死人
那麼因此呢光是算這個就會這麼就要那麼難算
所以呢怎麼辦那麼我們通常不這樣做我們就做一個 approximation
我們就說呢ok 照說你這個跟這個相乘去maximize
那這個又要等於這個嘛所以你就變成是這個完全一樣這只是把這個代過來
我就變成要這個東西我要用這個來算我要把所有的state sequence q 統統加起來
然後再乘上這個language model 然後再來看maximize
如果這樣的話做死人了
所以怎麼辦呢我們就做個假設
做個 approximation 說這個呢我們就不要那麼做了啦
我們就選一個這裡面看哪一條path 最大我就拿那個就好了
那這個的意思是其實跟我們在講四點零viterbi 我們就說過這件事
我們當時講的是這麼一件事其實跟這個精神是完全一樣的
我們當時說如果你要做isolate word recognition
假設我有這是零的model 這是一的model 二的model k 的model 一直到九的model
進來一個聲音我怎麼知道它是零還是一
那我就是在算某一個聲音o given lambda k 看誰最大
最大的那一個的k 就是我的答案
對不對我這是零的model 一的model 到九的model
進來一個六我怎麼知道它是六呢
我就把這個六放到每一個model 裡面去然後看誰最大最大的那個就是六k 等於六的時候會最大答案就是
六但是呢我們也可以做另外一件事情是
那這個是什麼這個就是我們的basic problem one
要算這個東西就是我們講的basic problem one
這個solution 就是所謂的 forward algorithm
但是呢我們也可以不做這個我們可以做另外一個
就是什麼就是跑viterbi
如果跑viterbi 的話你可以跑完的時候可以得到一個optimum 的probability given 某一個lambda k
然後你看誰最大
那這個東西是跑什麼這是我們的basic problem two
我們的solution 是viterbi algorithm
那這個的意思是說我跟這個有何不同
這個是我對每一個我進來一個六的時候六的所有的path 我都算進去了
也就是我們剛才講的我進來一個六的時候如果這是六的model 這是六的聲音的話呢
它所有的path 我全部都算進去了我的forward algorithm 就是把所有的全部加起來
那這個才是真正的那個機率那麼因此這個機率我得到就是這個那這個就是forward algorithm
basic problem one 得到forward algorithm 那這樣是一個正確的答案
那我如果跑viterbi 的話那我我那是左邊的那個
我如果跑右邊這個的話變成說我沒有真的去加全部的我只去找誰最大
我找到說ok 是它最大
最後走最後走到底的時候呢是它最大
那我就以最大的那個分數為準來算那麼其它我就不看了啦
那些path 因為基本上最大的那個機率大概已經告訴我誰最大了
所以呢我就只看最大的那條那條path 的機率誰最大就好
那麼因此呢我就變成我跑viterbi 找最大的那條path 那個機率然後看誰最大就好
那這兩件事情是完全不一樣的
這兩個機率是不同的
這個是把所有的path 的機率全部加起來
這個是我只找最大的那一條就好了
但是我現在重要的不是在算機率是算誰最大
那turns out 這兩個常常是相同的
就是最大的那個model 常常是同一個
最大的常是同一個
所以呢我只是在找我只是在找誰最大而已我並不是真的要算機率
所以你要算這個也可以算這個也可以
那麼如果那個model 六那個聲音真的是六的話它的最大那條path 大概就是在六裡面最大
跟你這邊把它全部算出來之後它大概也是它最大
所以呢最大的常常是同一個所以呢我就算這個跟算這個是同樣答案通通是相同的
所以當你要做isolate word recognition 的時候你用這個forward algorithm 算這個還是用viterbi 來算這個
其實沒什麼不同答案大概差不多
那你如果是是在當時去看這兩個演算法的話你也會發現
他們也沒什麼這個演算法所需要的計算量大概也差不多
好像沒有理由要選哪一個因為計算起來大概差不多你去看這兩個algorithm 大概也差不多
可是到了這裡的時候就不一樣了
我們這裡講的事情是跟那個一樣的喔這件事情從這個到這個其實就是從這個到這個就是從這個到這個
那這個是把所有的path 都算進去這個我只找最大那一條
那就是這邊講的這個這個事情那麼在這邊的時候對不對
那在這邊的時候我是要把所有的 path 全部加進去
這邊全部這個summation over 所有的state sequence q 的這件事
就相當於那邊的算那個東西就是forward algorithm 要做的事情就是所有的path 全部都要算的
而這邊的話呢就相當於viterbi 我只做一條path 我只做一條path 喔
那麼那麼這個情形就是我們剛剛講的那麼其實是完全不一樣的機率但是因為我只是要找最大的那一個
對最大的那一個而言大概是一樣的通常是差不多的所以我這樣的話省了很多事兒
因此呢我們在講isolate word 的時候從這個變成這個好像沒什麼道理因為這兩個計算量差不多
在isolate word 的時候這個跟這個反正也差不多沒有什麼不同
可是你如果像現在在這裡是一個大字彙有六萬個word 又是continuous speech
在這個情形之下的話這兩個就差別很大了
那麼因此呢我這邊就每一次只要算一條path 這個所有path 都要算那麼
那麼因此呢我們就把它簡化成為只算一條
當你簡化成為只算一條的時候其實就是簡回簡化到回到所謂的viterbi
那麼因此呢我們就回到viterbi 那麼你唯有只算一條只算最大那一條你才可能用viterbi
那才有才有辦法解出來但是因此我們底下講的都是用viterbi
那如果用viterbi 的話呢我們必需了解一點就是我們這邊講的都是一個sub optimum 的approach
什麼是sub optimum 也就是說我其實我是做了這個假設的這個假設其實不見得正確嘛
所以真正的機率是這一個我已經省我已經省掉成為這樣子了
所以這個不見得真的能夠得到我的optimum
所以我這個只是一個sub optimum
因此我這邊做的其實只是一個sub optimum 的做法
好那麼這樣子之下呢那麼我們怎麼做這件事
我現在要跑viterbi 的話呢我還是一樣
那麼這個我們剛剛也講過了就是跟我們剛剛講viterbi 的精神完全相同
我這個呃我就是formulate 一個簡單的sub problem
然後呢變成一個algorithm 變成一個iterative algorithm 這樣一路一路這樣子算過去
那麼在這個時候如果這樣算就會發現我是一個時間一個時間一個一個frame 這樣算過來的
那是為什麼他叫做所謂的time synchronous 或者frame synchronous 的意思也
就是best score at time t update from all states at t 減一
viterbi 精神就是這樣子嘛我在前面一個state 的時候的所有東西去算下一個得到一個optimal path 一個optimal 點在下一個
所以呢這就是viterbi 的做法
就是一個time 或者叫做time synchronous 或者frame synchronous viterbi search
那真正的難題在哪裡難題在底下
那你可以想像其實我現在並不是跑一個model
我不是跑一個model 把我我分別每一個去跑viterbi 那沒什麼問題現在不是的
我現在有六萬個word 而且每一個每一個word 都有六萬種可能它們是連起來的
那怎麼辦呢那最基本的這個想法就是你要有一個tree lexicon 來做為我的working structure
什麼是tree lexicon 呢就是這裡畫一個很小的例子
就是我把它的每一個音拿來建一個tree
譬如說如果是第一個音是斯後面如果是a 的話呢就變成say
如果後面接p 接e e 後面再接咳就變speak
後面這邊如果接c h 就是speech 嗯這個接這變成 spell 等等等等
那我如果有個辭典有六萬個word 我就可以如果每一個word 都告訴我它是哪些音拼起來的
我的lexicon 本來就是這件事嘛lexicon 就是lexicon 就是一個辭典裡面有所有的word 然後每一個word 都告訴我它是哪些音拼起來的
我就寫一個程式把那些建成一個這樣的tree
建成一個這樣的tree之後這裡的每一個arc 是一個hidden markov model
譬如說每一個arc 這裡是一個phone 嘛哦那麼這是一個h m m 這是一個h m m 這是一個h m m 因此我這些 h m m 是連起來的
然後你走走每走到任何一個leaf note 的地方呢就是一個word
ok 那這樣的話我構成一個tree 所謂的tree lexicon 就變成一個這樣子
那我一路走走到底的時候呢就是一個 word
那這個時候我的viterbi 怎麼走從頭開始走
那麼從頭開始走的時候呢跟我們這邊所畫的圖是一樣的唯一不同的是我現在是一個tree 不是只有一條不是只有一條而是一個tree
因此會變成怎樣呢會變成這樣我們舉個例子來講
假設我走到這裡之後分叉成為兩個的話
那其實相當於我這個平面相當於我這個平面走到這邊的時候就拆成兩個平面
對不對所以呢假設我在走viterbi 的時候
我我我這是一個很大的這變成一個tree 的h m m 了對不對
我這裡的每一每一每一個arm 其實是每一個arc 是一個h m m 那這些h m m 全部串起來變成很大的tree 嘛
那所以你如果是這樣子的話呢它就變成說是在這邊的時候它可以走這個也可以走這個平面
那麼因此呢我現在在這裡走的時候呢譬如說我這條path 走到這裡的時候是可以往這邊走
也可以在上面這條走走這邊對不對
我就可以可以這樣走嘛
那同理呢我到這邊的時候我又拆開啦
我又拆成拆成兩個
我這邊又拆開了所以走到這邊的時候呢我可以再拆開來
我這邊又拆成兩個對不對
於是我這邊又得到這一個跟這一個等於是這樣子嘛等等呢
因此你可以想像我們現在在講的事情是一個很大的h m m 的state 所構成的一個很大的一個tree
它我我我每這裡每一個小arc 可以看成一個小的h m m
但是呢我整個變成一個那麼大的我的viterbi 在上面走的話呢在這邊就開始兩邊都我本來這邊都可以走但是現在呢我到這邊就變成又可以拆分開來走這邊又可以分開來走等等等等
所以走到這邊的時候呢我又可以往這邊走跟往後面對不對
喔那就就就這樣子一直往下走的話呢那你現在的這個可以走的這個路就非常非常多了
那在這個情形之下你就可以想像為什麼要用這個viterbi
因為你如果要去算所有的state sequence 不得了
那因此呢我就我就每一次我在每一個時間上我就只算機率最大的那一條嘛對不對
我就只算即使是這樣都已經很累了因為我其實我在在每一條的時候我其實都要把它全部算出來啊對不對
按照我這邊來講我在每一個時間t 我要把全部的都算完才能算下一個嘛
那這邊已經多到不得了了所以你光是這樣走已經很累很累了
那那這是為什麼我們一定一定要用這個approximation 然後做這個viterbi 而沒有辦法再做這件事情
那這樣子當你變成一個tree 有什麼好處
變成一個tree 的最大好處應該就是這句話就是你的search processes for a segment of utterance through some common units 的話呢就可以share
什麼意思我們說我我不知道第一個word 是什麼word 它有六萬個可能
但是呢如果它是它是speak 還是speech 其實只在最後不一樣而已
前面一路走過來都是一樣的
所以呢你不需要去跑一次viterbi 去跑一個speak 的word 的viterbi
再跑一次speech 的不需要跑兩次
其實你從頭一路走你只要跑斯的跑這個音的跑這個音的到這裡為止
它們都可以share 同樣一條path 同樣的分數只有到最後才拆開來ok
那麼因此呢我到前面的這一長段跑的viterbi 就它們就可以全部都share 那就這句話的意思
那麼因此呢我這中間譬如說這個一開始當然你可以想像
假設我有六十個phone 的話這一開始理論上應該有六十個path
那這邊下去又應該有六十個path 但是事實上也沒那麼多
因為這個這個tree lexicon 告訴我說哪些音才可以構成一個word
所以呢要後面有這個word 它才會接它
譬如說s p e 之後不是接所有的音因為只有你有這些字的才會接
所以到這裡的時候呢你到這邊就接k 變成speak 接c h 變成speech 或者再接哪些音會變後面有字才有啊
所以其實有另外的好處我這邊沒有寫在這裡的就是
應該說就是這個search space reduced by the constraint given by the words in the lexicon
也就是說這個這個辭典這個辭典裡面有六萬個word
那其實這六萬個word 已經告訴我說哪些音
如果前面這三個音下來的話它不是接所有的音
只有後面有哪些字的時候它才會接那些音
那你不是所有的音都要都要找了你就只要找這些有字的音就好了
因為我現在每走我走的每一步都是因為後面有字的關係
喔所以完全根據有哪些word 來找
所以呢它把我的不是每一每一段都可以用所有的phone
所以我的search space 是reduce 因為我的辭典裡面的word 給我這些constraint
那當然這裡還有一點就是我們這邊呃沒有說的應該講這樣講比較簡化一點
這是一個phone 這是一個phone 每一個phone 它有一個h m m 然後把它串成這些word
那實際上呢我們說這個可能是一個tri phone
喔這些可能是tri phone 也一樣啊
那你如果說我的我的辭典我是用tri phone 來做的話呢
我的辭典裡面每一個word 告訴我是哪些tri phone 連起來的
那我就變成把這些tri phone 建成一個tree
那這個基本上這個這個是完全一樣那其實如果是tried phone 建成一個tree 的話那它有更多的constrain
因為如果這是哪一個tri phone 已經確定它後面會接什麼對不對
tri phone 是已經知道後面會接什麼的
所以呢這個雖然多了很多但是我後面接什麼是確定的喔
所以你真正用的可能是一個tried phone 的tree 那我們這邊沒有畫就是了我們這邊是假設是一個普通的phone 就是了
那如果是這樣做的話呢那麼會怎樣
底下這句話意思是說same tree copy reproduce at the each leaf note
那這個意思是說我真正跑起來會怎樣呢
我跑起來會變成這樣這麼大一個tree 跑死了才跑一個word 哦
你有一個很大的tree 跑他最後有一堆一堆word 有六萬個word
這是這是一個lexicon tree
你你跑一個這麼大的六萬個word 的串的所有的它的phone unit 串的這麼大的一個tree 的 h m m
你跑完的時候呢我理論上我六萬個word 都跑出來
所以我就知道這六萬個word 裡面是它的分數是多少是它的分數是多少每一個都有一個viterbi 分數就是這一個
不過這才是一個word 哦那這這後面又可以接嘛
譬如說就這個而言它又可以接另外一個tree 後面又有六萬個word
那這個後面也接另外一個tree 後面又有六萬個word 這個後面又有又有另外一個tree 等等等等
那這就是所謂的tree copy
那麼因此呢你可以想像的是這所謂的tree copy 就是你這個又又做同一個tree
那麼也就是說你的我們說第一個word 有六萬種可能第二個word 又有六萬種可能但是其實是什麼是六萬乘六萬了嘛對不對
是接它的也有六萬個可能接它的也有六萬個可能這接這六萬個都有六萬個可能嘛喔
所以呢你這個是第二個word 就變這樣子那第三個word 呢這裡面的每一個都要再接一個tree 一個一個tree 嘛對不對
所以你光是這樣想這個search 仍然是非常龐大
那這個就是我們講基本上你如果要這樣做的話就變成這樣子
那這個仍然是非常複雜的問題所以我們需要把它把它做得比較有效率一點
這就是底下要講的事情ok 我們在這休息十分
ok 我們接下去
我們剛才在講的情形就是
我的第一個word 就是一顆tree
這顆tree 就後面就有六萬個
第一個word 我就就這個這個非常大的tree 的hidden markov model
那麼走到最後有六萬個
然後之後呢那每一個走完之後應該都要再接一個
都要再接一個
所以你就接很多
這是第二個word 就有六第二個word 其實就有六萬個這個tree 在這裡喔等等
那在這個情形之下我整個的viterbi 會變成非常複雜的viterbi
不是我們原來單獨的這一個
不是單不是單獨這這一個這樣子走而已
而變成是我一路這樣子散開來一路散開來變成很大很大
那為了要讓這個比較清楚得來講我們怎麼來做這件事情
所以底下呢用用一些這個符號把它們specify 清楚它們是什麼
這講起來是蠻複雜的
不過事實上其實你如果對於我們原來所講的那個viterbi 了解的話其實是一樣的
好那我們第一個現在現在比較複雜所以我們一堆東西
第一個呢就是我要define 清楚我這是什麼這個是什麼
這個其實就是我們原來viterbi 裡面所說的那個東西
只不過現在變複雜了所以符號變多了而已
我們原來viterbi 不是這樣
當我走到時間t 在state i 的時候
我這邊所define 的一個東西叫做delta t 的i
就是走到這裡為止的一條有最高機率的path
它的機率就是這個東西
那這個東西其實就是我現在的這個東西
是一樣的東西
這個東西其實就是這個
只不過我現在的東西變複雜了所以我現在東西變一堆符號喔
那這個意思還是一樣
那我現在變成是怎樣呢你可以看到是
我在我在時間t 的時候
在state q k 啊 q t of the word w
那當我走過來走到這段是一個word w
譬如說這裡我是一堆state
那麼這一堆這個這是一個word w
那這個word w 是是我們剛才在這個一路這樣跑下來的中間的某一段嘛
譬如說我在這裡的時候從這裡跑到這裡的這條是一個word w 對不對
因為這個我從這個tree 的頭一直走到tree 的尾的時候這是某一個word
如果到這邊發現走到這邊發現這個word 是w 的話就表示說這整個path 是w
那這個w 中間會有一堆 是一堆phone 一堆state 走起來的
那這個呢就是這個
如果這樣的話呢我在時間t 的時候
停在某一個時間t 的時候停在某一個state
這個state 叫做q t
這個q t 是屬於word w 的到這裡為 止的這個分數
那麼我一路走過來
走到這裡的這個分數
就是這邊的這個 d
這個d 的這個什麼t q t w 就是這個東西
ok 其實是一樣的
那為為什麼這個w 那這邊還有什麼因為我從頭走過來嘛
我可能從從前面走走第一個tree 走第二個tree 這樣走走到這邊
這是某一個w 嘛喔
所以這是走到路的這個path 整個path 中間的某一個w 而言
在這個w 裡面的某一個state q t
我在時間t 的時候我剛好停在這裡的這個分數喔
那麼就是所以這個其實是跟跟這個意思是一樣
只是我現在比較複雜而已
那所謂的 object function 其實也就是這個東西
也就是這個一路走過來最高的機率就是所謂object function
那我就是要optimize 這個東西
我要maximize 它嘛
那麼這個呃那是什麼是best partial path any at time t is state q t for the word w
也就是說這邊有有千千萬萬個path 走過來
但是我現在講的是到這裡為止
最好的那一個
那就相當於viterbi 裡面到這裡為止最好的那一個這樣的意思
那我都是以某一個時間來算的
所以呢是時間t 的時候來算的
那你記得viterbi 裡面很重要的一件事情是做什麼back track
因為你每一次得到這個之後下一個的時候呢
我下一個state 當t 加一的時候
它可以從這上面的任何一點跳過來
depends on 誰過來的那條path 最大
那你很可能發現結果是從這一點跳到這邊的時候機率是最大的
所以結果呢
你要把到這裡為止最好的path 就變成是這一條了
對不對這是viterbi 的基本精神嘛
所以呢你你要算t 加一的時候
你得要在算從 t 的時候所有的state 都會過來的
然後看誰最大
最後最大的是這一條的話呢
你到這邊為止最大的就變成這一條了不是它了
那因此呢我我到這邊的時候我一定要記得說
哦剛才是從這裡來的
所以這是所謂的back 這是back track 的那個pointer
我要我要說ok 它是從這兒來的
這樣我一路要記得它的前一個是哪裡
那這個是viterbi 裡面很重要一件事情你要能夠記得
你剛才從你的最佳這點是從哪裡來的
這是所謂back track pointer
那我這邊也一樣也要
那就用這個符號來代表
就是說你如果是在你的這個這個你的partial path
你你的這個best partial path end 在time t in state q t for word w 跟剛才一樣
那你如果現在是在這裡的話
那你要算剛才是哪裡來的
剛才如果是在t 減一的時候
如果是這裡來的話
那你要把這個記下來說哦剛才是從這裡來的
那個那記下來的這件事情就是就是這邊的這個h
好我就記就是就是這個back track pointer
所以這個事情跟我們講的viterbi 是完全一樣的
只是說我現在要這個變得很複雜而已
好當這個沒問題之後呢
底下的這兩件事情其實說穿了也很簡單
也跟viterbi 這邊所想的事情是完全一樣的
那只是呢我現在變得複雜了
那我們要弄清楚
現在有兩種狀況
一個叫做 intra word transition
一個叫inter word transition
什麼是intra word 就是在一個word 裡面
也就是在一個tree 裡面
這是h m m only
沒有language model
當我在這裡面走的時候
當我在這一棵 tree 裡面走的時候
我是在這裡面這條路上走
那這條路其實你可以想像
最後就是一個word
是這六萬個word 裡面的一個word
那在這個word 上面走的話呢
我其實是走一個相當長的h m m 而已嘛
所以呢它是在其實就是在走h m m 的viterbi
因此呢這裡面的東西
跟這裡面這個東西
其實就是我們原來講的viterbi 這件事情是一樣的
這是h m m only
那在這個是後沒有 language model 的事情發生因為它是在算一個word
那底下呢
什麼是inter word transition 呢
是在當你這個走完我走下一個的時候
當你這個走完我要再走下一個的時候
那你你從這個跳到下一個去的時候呢
那這個時候我我是在一個h m m 跳到另外h m m
這時候中間有什麼有language model 分數要加進來
所以呢所以呢底下是所謂inter word
是這個這個時候是language model 分數要加進來
但是我我不是在h m m 裡面是在h m m 的外面
所以呢其實很簡單就是上面是跑h m m 沒有language model
底下是跑language model 沒有h m m
好那我們分別看一下這兩個情形
那就這個而言
它在說的事情其實我們先說 h m m only
也就是在intra word 裡面
也就是在一棵tree 裡面
一棵 tree 裡面的某一條path 上面的h m m
你在上面走的時候其實這個走的事情
就是在這邊走這段word
就是在走這段嘛
那走這段的情形跟這邊是完全一樣的
跟我們從前說的其實是完全一樣的
所以這個式子其實也就是我們從前講的那個式子
只不過現在看起來比較複雜一點而已
你看我要算時間t 在q t
時間t 在q t 怎麼算
我就先算t 減一的嘛
算算這個在t 減一的時候掉在q 的t 減一的時候
在這個word 裡面我現在都在這個word 裡面嘛
我是intra word 在這個word 裡面
所以呢我在t 減一的時候
我是在q 的t 減一state
對不對所以呢我在這個t 減一的時候
還是在word w 裡面的q 的t 減一的state
在這個q 的t t 減一的時候
在q 的t 減一的那個 state
上面我也有一個最高的分數
就是那個分數然後呢再加上跳過來的分數
跳過來分數有兩個
在我們當時講viterbi 的時候呢
那兩個是什麼一個是a i j
一個是b j 的o t
你如果記得我們是有這兩個東西
a i j 告訴我從這個跳到這個機率是多少
那b j 的o t 是我現在要把現在把這個新的vector
放進來放到這個state 來ok
所以就是這兩個分數
那這兩個其實就是這裡的這兩個
就是這個東西就分成這兩個這兩個就是這個
所以呢對不對就也就是說你現在從這裡的時候
我現在要算如果說這個是 t 這個是t 加一的這是t 這是t 減一的話
我現在在t 的分數是要所有的t 減一的
都有可能跳過來看誰最大
所以所有的t 減一跳過來有有兩種
一個是a i j
每一個跳過來都有個a i j
一個呢是我要把這個b 把這個o t
放到這個新的state 裡面去
這是這是state j b j 的o t
我要把這個放進來
那現在這兩個機率其實也就是我們這邊講的這兩個機率
所以你看到譬如說這個是什麼這就是a i j 嘛
這就是在word w 裡面
然後我從q t 減一跳到q t 的機率
所以這個東西其實就是a i j
就是從這邊跳到這邊的機率就是a i j
那這個是什麼呢
這個其實就是b j 的o t
因為你你你現在就是這個啊
它就是我在這個word 裡面那麼我現在是這個o t
我看到的這個第t 個 feature vector
第t 個vector 掉在q t 的機率
那其實就是這個東西喔
那你現在把這兩個它現在是寫log 用加的啦
那意思是一樣啦就是你這兩個加起來的這個
就是說就是這個這一項嘛
就是我從t q t 減一在時間從t 減一到t 的時候
我state 從t 減一到 t 的時候
那麼我要加進去的機率是這個
那然後呢因此呢我現在在算t 的時候
就是把t 減一的所有可能的q t 減一通通加起來嗯通通看起來誰最大
就跟這邊是一樣的嘛
你你現在要算t 的時候
你就把t 減一的看看是從這過來的還是從這過來從這過來看看是誰的最大
你就算那一個最大的那嘛
那那這邊其實也是完全一樣啊我現在就是把這個嗯我現在d 的t
q t 的話呢是什麼呢
就是在t 減一的時候的所有可能的q t 減一
那這邊所有可能的q t 減一在這裡
這邊所有可能在這裡就就是等於這邊的所有可能一樣的
這邊所有可能在這裡
那所有q t 減一都有一個最大的最佳的分數
再加上跳過來的時候可能的
然後加起來之後呢我在所有的q t 減一裡面看誰最大
然後知道ok 我就是從那裡過來
所以跟這個是完全一樣的情形
然後呢我現在就找到那個最大之後我就把我就得到一個下一條path
就最大的path
因此呢這邊所說的事情
跟我們原來所說的 h m m 是完全一樣的喔
只是現在整個整個變成複雜而已
其實精神是完全相同的
那底下這個式子只是在說
那我要做這個back track
我要記得從哪過來
所以剛才的maximum
倒底是誰是q t 減一
剛才是從哪一個過來的我要記得
我就把那個剛才過來那個記下來
這就是我的back track pointer
那麼說明這個我的q bar
就是指我的t 減一的時候如果現在t t 在q t 的話
那麼t 減一是從哪個state 是在哪個state
我把那個記下來
就是剛才那個裡面的maximum 的q t 減一是哪一個
把它記下來
然後然後把它放在那個back track pointer 裡面
於是我就記得剛才是從這樣過來的
好就這樣子而已
所以這個說穿了沒什麼特別符號變複雜而已
那麼其實講的就是這件事情
就是這件事情是完全一樣的
這是intra word transition
所以這個沒什麼不同
跟我們之前講的一樣
不同的是底下這個
因為我現在還會從一棵tree 接掉接到下一棵tree 去
當我從這棵tree 接到下一棵tree 的時候會怎樣呢
那就是做了一個inter word 的transition
從這個word 跳到下一個word
那這個時候呢我們假設說
現在這個word 走完了叫做v
v 是一個word
然後呢q f 的v 是它的final state
畫清楚一點
我現在走完了這一個這個word 叫做v
它有好多state
然後它一直到最後
這是它的最後一個state
這是它的q f 的v
就是 v 的這個word
q f v 是v 的word 的這個final state
然後我現在要從這裡開始接下一棵tree
怎麼接法呢
我先增加一個空的state 叫做q
q 是一個空的state
沒有裡面沒有任何東西只是為了接方便起見
為了是要接底下這棵tree
所以q 底下呢就接底下這棵tree 出來
那這棵tree 底下會有會有這個六萬個word
所以呢我現在的這個那如果後面這個後面這個word 呢
叫做w 好
所以呢我現在這裡的這裡有六萬個word
不過每一個word 我們都叫它w
所以呢這個我這個v
v 現在這個word 走完了
到了最後final state
了那我現在要開始接下一個tree 了
那下個 tree 有六萬個可能我們叫它是w
那麼開始的時候我有一個pseudo initial state
這是一個空的state 只是為了串接方便起見叫做q
那麼如果是這樣的話呢
好我現在就有一個空的 state 在這裡
這是我的 v 的final state 接下去
那麼因此呢我現在這個這個低的這個分數啊我就先給它跳到這裡面
跳到q 裡面來了
是也就是說當我這個t 到這邊
如果我們在這裡我們說是這邊是我的word v
這邊是我的v word v word v
走完的時候呢
是在t 的時候
我我這個這個最後這個state呢就是我的final state
所以這個state呢 就是q f 的v
那麼這個時候呢
我增加一個空的state 是 q
所以這邊我有一個空的state 是q
我仍然在時間t 的時候就給它走過來
ok 我在時間t 的時候我讓它
在這個空的state 裡面然後開始要往下接
那這個時候我就是在在這個呃時間t 從q 開始要走這個word w 了
那麼這個時候呢我這分數怎麼算
我先算我時間同樣的時間t
是我同樣的時間t 走到v 的final state 之後
當時的分數
然後現在要加language model 的分數
那麼這裡其實在這個寫錯了這要改一下喔
這個language model 分數你我們這邊是假設就是只假設language model 我們做bi gram 就好
其實那tri gram 更複雜啦喔
你想一想就知道tri gram 是怎樣的
不過我們現在只講bi gram
bi gram 的話呢這個很直覺的以為是這個
因為我現在v 後面要接w 嘛
所以我現在就是given 這個 v 後面接w 有個bi gram
其實這個寫錯了
我們應該是要看前面的
也就是前面的這一個
這個是u
如果是這樣的話呢那個機率應該是
probability 的這個v given u
是u 後面接v 的bi gram
不是 v 後面接w 的bi gram
這寫錯了喔
為什麼
因為其實你走到這邊為止的時候
你只知道我這條路上到這邊是v 而已
我後面w 還不知道了還沒開始走
所以w 有六萬個可能
這邊有六萬個可能的w
那你不可能把這六萬個bi gram 統統加上來
那就變成有六萬個分數了這不太可能的
所以這裡其實你不太可能知道那個w 是多少
我w 還沒開始走嘛
w 還沒開始走我我沒有這六萬個word 的不知道是誰我我如果真的要這個機率的話我有六萬個是不可能加
的而是應該是走到這邊走完的時候v 知道了
因為我走到最後才知道是哪一個word 嘛
我一路在找
一路在找這個最佳的path 對不對
所以我一路走過來走到最後才知道我這個word 是v
當我知道這個word 是v 的時候我可以把這個u 後面接v 的bi gram 加進來喔
所以你剛才在這邊你v 的bi gram 沒有加進來因為我不知道是什麼我我一路找嘛
我一路找不到最後不知道它是哪一個word嘛
所以呢我找到最後的時候才知道它是v
這個時候我是把u 後面的v 加進來
ok 所以這個地方是應該是這個u 後面接v
是這個的bi gram
不是v 後面接w 的bi gram
ok 那這個意思是什麼
這個意思跟剛才這邊是一樣的
也就是說我們剛才是說我從這邊過來
它可以從前面一個時間可以可以從任何地方過來
那我這邊其實也是一樣
你可以想成我在時間t 的時候其實這邊有六萬個word
我這個一路散開來的時候
在時間t 走完的時候其實有譬如說有三百個word
我這邊有一個v one
這邊有一個v two
那這邊有一個v 三
v one v two v 三都在這個時間t 的時候結束
它們都可以跳到這個q
你可以想是這樣ok
那也就是說我們這個圖現在已經不夠畫了
這個圖現在不夠畫了因為我其實這個不是一個 one d 的
這邊是我們這邊所畫的這個tree 嘛
這邊是一個tree 的結構你長上去的時候很多啦
所以呢當你到這邊的時候
這邊譬如說你在時間t 結束的時間t 所結束的word
其實有v one v 不是只有一個v
有v one v two v 三
都在最後結束分數都在分別是在它那個path 裡面最高的
那它們都在時間t 結束
所以呢我這邊其實有有好幾個
有好幾個
那這個v one v two v 三都在時間t 的時候結束
那因此我現在要跳到這個q 來
準備接下一個word 的時候呢
我可以有好多個可以從這個跳過去也可以從這個跳過去也可以從這個跳過去
那麼因此呢我先要看它是從哪一個v 跳過來的
那就是這件事
那精神跟這邊講的是完全一樣嘛
我現在只是說是要看它是從這個word
還是從v one v two v 三的哪一個的的最後的那個final state
會跳到這個q
來它的分數才是最高的
所以呢我就分別把所有的這些我這邊有六萬個word 在這邊結束
那有的早一點有的晚一點
你可以假設在這個時間t 的時候
有三百個word 在這邊結束
在 t 加一呢又有五百個word 在這邊結束了
t 加二又有一千個word 在這邊結束等等都可能
那麼因此呢你在每一個時間都在做這件事
就是whenever 你的word 走完的時候
你word 走完的時候
你就把那個 language model 那個的word 加進bi gram 加進去之後
然後我要看到底是哪一個word
會跳到那個q 分數是最高的
我就選那一個
那這個精神跟這邊是完全一樣的喔
所以呢我現在就是每一個v 走完的時候的分數
加上那個v 接在那個前面的 u 後面的language model 分數加進去
然後看誰的v 最大
我就從那個跳過去
那麼因此呢我這樣就得到這個那這個是相對於這個
只不過我現在是從是從這個h m m 跳到下一個h m m
或者說從這個tree
跳到下一個tree 的時候的的這個
跟剛才是在裡面走不同的地方在這裡而已
那這樣子我知道是誰最大之後呢我也一樣在這裡
我把那個最大的記下來
所以就把剛才那個maximum
所以這個也寫錯了喔
這個也是應該是這個應該也是這個u 後面接v 的bi gram
這個也是寫錯了
就把剛才這個maximum 誰最大記下來
最大的那個就是我的前一個對不對
所以呢如果是這個v bar 才是最大的
我們現在v one v two v 三
都都在這邊結束後我現在看到的這個
是看到現在是最大是這個
那麼因此我就應該把它的最後最後state 接記記下來
所以我就知道它是從這樣過來的
於是我現在就知道ok 它是從這樣過來的
於是它是從這樣過來的
於是呢我後面開始接下一個tree
那麼因此我現在就把它的這個v bar記下來
做為我的所以我的那個v bar 的最後那個state
就做為我的這個back point
back 這個back track pointer
那麼於是呢那就這這是兩種transition
只要這兩種繼續操作
那我就可以一路走下去
ok 一路走下去是可以
不過這個這個還是大的不得了
所以我們要有一些辦法來簡化它
有很多種方法來簡化它因為現在這個search 你可以想像非常大
這是我們所謂的search
那怎麼簡化它呢一個最簡單的辦法就是所謂的beam search
beam search 意思是說在每一個時間t 我只保留一個sub set
of 最可能的path
其它都丟掉
你可以想像我從一開始走
它很快就長很多很多很多
長那麼多之後你簡直沒辦法處理
所以呢最簡單的辦法就是做beam search
舉例來講define 一個beam width l
我們通當講譬如l 是三百或者六百或者二百
也就是說我我我很快走過來這邊就很多很多了
那我就只保留分數最高的
那二百個還是六百個path
其它全部丟掉
那我一路走的時候呢我一路在算分數最高的那個path
之後我保留六百個譬如說
其它全部丟掉
那這樣我才有辦法往前走
那當然如果這樣走的話就表示這不是已經不是一個optimum 的了
喔這又是一個喔這又是一個是個approximation
因為你可以想像
分數最高的path 不見得從頭到尾一定分數最高嘛喔
這個龜兔賽跑的原理嘛
期中考考最好的人期末考不一定最好嘛
所以你如果一開始就把ok 期中考裡面考最好的十個人留下來其它通通殺掉的話
那到最後其實可能最好的被你殺掉了對不對
這裡也是一樣的
你這個這個一路跑過來的時候
所以你的這個這個beam width 如果保留的越大是比較好
但是你的計計算量立刻就會大很多嘛
所以這個就是怎麼選擇這個問題
通常我們兩種簡單的辦法
一種是保留一個就定義一個beam width
譬如說你就是每在每一個時間點t 上面
我永遠只keep 前六百名或前三百名
等等那這樣的話讓我的計算量不會太大
第二種我就定義一個threshold
凡是的我分數比最高分少那個threshold 之內的我都保留
不管多少個
那有的時候這裡有一百個有的時候這裡有一千個
我反正是是這個在這個threshold 之內的我都保留喔
這兩種基本上這都是我保留一個beam
然後呢我就在beam 裡面走
那我自然就已經把可能的optimum 丟掉是可能的
所以你這樣子得到不見得是最佳的
但是是接近就是了
那這是最簡單最常用的這個reducing search space 的方法
當然還有很多進一步的方法我想我們這邊就不說
你如果有興趣去看讀相關的reference 就會覺得講很多種方法
因為這個其實是一個關鍵性的問題那麼有一堆研究如何做
那麼一個例子就是ok 你也可以從acoustic model 從acoustic 的h m m 的分數裡面去看
哪一些地方h m m 看起來它比較好比較不好把它丟掉
從language model 來看
那麼哪些應該丟掉什麼這這都有
那麼另外一個非常標準的做法就是所謂的multi pass 的search
也就是說我至少分成兩個pass
那這個意思是什麼呢
就是說喔應該是講說我在我先有第一個path
用比較簡單的knowledge
簡單的constraint
我就得到一個比較簡單的比較小的search space
在第二個裡面再做複雜的
嗯這話怎麼講呢
最簡單的想法就是說譬如說tri phone
tri phone 太複雜了
我前面就只做一mono phone
我我我一開始我我我不要做那個
我不要那這裡面我tree我這個lexicon 也可以有兩種嘛
一種是phone 的
一種是tri phone 的
tri phone 數目多很多所以會複雜
我就我先不要用tri phone 我就先用這個單獨的phone 做
那這樣就比較簡單
我就可以做第一個pass
或者譬如說這個language model 那裡呢
你可以想像我們這邊只講bi gram
是因為tri gram 複雜哦
我如果tri gram 的話
我走到這裡的時候
我不但要把這個bi gram 加進來
還要把這個tri gram 加進來
那我每一次都要都要再再算一個bi gram 算一個tri gram 是會複雜
那我也可以說我在我在第一個path 的時候我只做bi gram
後面呢才做tri gram
或者說我甚至於language model 我在前面不做
我我 language model 到後面才做等等
那因此呢我的第一個path
就可以比較簡單一點
那第一個path 的的出來結果呢
我們把它做成一個word graph
或者一個n best list
什麼意思呢
所謂的一個word graph 就是所有可能的word
可能性比較高的分數比較高的word
把它的時間點通通記下來
就構成一個 word graph
這是時間點
所以呢它譬如說譬如說到這個時間為止
從這一點到這一點
是可能是w one 是某一個word
到這一點也可能是 w two 是這個word
那這邊呢可能有另外一個word 是w 三
那這邊可能有另外一個word 是w 四
那這邊可能有另外一個word 是w 五
這可能有另外一個word w 六
那這邊可能有另外一個word 是w 七
ok 所以呢我從這個時間點到這個時間點的話呢
我可能是這樣子
這個可能是w word w one 這個可能是w two
它也許是w two 的前面一半喔
那那它也許到w two 也許不是對也許是w 四的前面一半喔等等
那麼因此我就把所有可能的word 它的時間點的起點終點通通記下來
它就可以構成一個graph
那這個graph 呢其實你給我一句話我可以先把這個graph
找出來當我這個graph 找出來之後呢
那其實它告訴我我現在只要在這上面找就好了
它 either 是一三五
或者是二五
或者是一六
或者是四七等等
那搞不好這邊還有一個
譬如說這可能也是一個
這個w 八
於是也可能是一三八七對不對
那麼因此呢你就在這裡面去看喔
那麼如果這樣的意思是說我的第一個pass
基本上做法還是跟剛才一樣這樣子做
但是呢我我只用比較簡單的東西
譬如說我只用這個這個我不要用tri phone 我只用單獨的一個phone
我不要用tri gram 我只用 bi gram
什麼的話
我也可以這樣走
這個程式稍微簡單一點然後我就取最可能的分數最高的word
哪裡是可能分數最高的word
那你可以想像因為我現在六萬個word
有的早一點結束有的晚一點結束
有的早結束有的晚結束就是我們這邊所畫的就是
譬如說w one 在這邊就結束了
w two 到這邊才結束
w 四要到這兒才結束對不對
我就把這裡面分數最高的word 保留下來
就構成一個 word graph
那麼這個東西我底下就只要在這上面算就好了
那麼因此呢我這個複雜的東西
在後面算
那麼我這個時候我這個再把複雜的
那也等於是說我我這個很複雜的這個這個tree 後面接這麼多tree 後面接這麼那這個東西呢我就把它reduce 成為變成只有那樣子
不但是變成只有那樣子而且它不會發散
而是最後會reduce 到一點
不一定是一點啦你這邊可能也有也有不只一個
但是譬如說這邊還有一個w 九
但是基本上你不不會一直這樣越長越大越長越大
你你你可以限制它就這麼大
ok 於是呢我真正的複雜的tri gram 啦
或者tri phone 啦什麼這個複雜的東西
我只在這上面算
那這個search space
比原來那個要小很多很多那個太大了
那個大到無法算所以我就先我先用一些簡單的就是less knowledge 或者less constraint
用一些簡單的辦法
把那個大的tree
那個太大的那個那個那個 tree
reduce 到變成一個小的graph
然後呢我現在把這個東西
在這上面才做詳細的
那這是我們通常稱為re scoring
你現在再把你的詳細的你的tri gram tri phone
分數詳細去算
那剛才因為只用簡單的所以你那個分數不太對
我現在可以把詳細重算一次分數
所以叫做re scoring
那之後呢你可能會發現這上面雖然有這麼多種可能
其實最可能的是這條
譬如說是w two 接w 八接w 九
可能這條才是你的答案譬如說這樣子
那你就可以在 word graph 上面找出來
那這是所謂的multi pass search 的基本觀念
那當然這樣做的時候基本上你前面的這塊第一個path
所謂的這個word graph generation
其實跟那剛才那個是一樣的
只是簡單一點
我用比較簡單的knowledge 用比較簡單的constraint
譬如說我只用我我不要用tri phone 我不要用tri gram
等等我簡單一點就其實是一樣的
然後我就是保留最重要分數最高的word
譬如說在這個時間點結束是以它最高
或者你也可以再保留一個啦對不對你可以再保留
你保留若干個這個時間點結束的分數最高的
然後你在這個時間點你把它保留你這樣一路這樣你會得到一堆
那你就把它們構成一個graph
那如果是這樣子的話很可能我們可以把它畫成
這樣子這是w 十
那它們都n end 都在同一點
然後後面都可以接這些等等
那這就是所謂的 word graph
那你有了word graph 之後在word graph 上面
再用比較詳細的再重跑一次
re score 這些所有的path 之後
你算哪一條path 分數最高等等
那這是這個所謂用word graph 的方法
那麼n best list 是相同的意思
只是說呢它沒有做成這樣子的word graph
而是直接把前一百名譬如說這個n best 就是n 就是這個這個前n 個名次分數最高的word list
全部把它保留下來
那麼舉例來講在這個case 的話你就可能
就這個case 的話你可能想像的就是譬如說
一三五這是一個
w 一 w 三 w 五
這是一個一三五
那麼二二五也是一個w 二 w 五這也是一個
那麼w 四九也是一個喔等等等等
那你如果沒有把這個word graph 建起來
只是說把分數最高的一些word 的word sequence 把它通通都留下來
譬如保留前一百名或者保留前二百名或者前五十名
那就所謂的這個n 等於一百或者五十或者二百的 n best
那你就把這個list 留下來之後我重新在這上面算分數
那你可以想像這兩種那一個好呢
這個是比較精簡啦
這個可以把它們這個其實包含的東西比這個還豐富
這個只告訴我說一後面接三三後面接五
那這個其實告訴我說
一是在什麼時候結束
三在什麼時候開始
三是在什麼時候結束後面五等等
所以呢我我其實三後面還可以接八接九什麼
它都都在這邊都呈現了
所以這是一個比較精簡的描述的方法
你這樣保留一個這個word graph 的效果
會比這個好
但是這個簡單
你這個這個呢你就是把剛才一路找過來的你第一個path 也是用比較簡單的方法來做
但是我一路走過來之後我就把前一百名留下來
得到一個一百的list 喔
那這就是所謂的n best list
那這兩種方法都可以
我這上面舉的這兩個例子在說明這個這個n best list
是不如這個 word graph 來的有效喔
那像這個例子呢你常常前幾名是只差一點點
i’ll tell you what i think 還是 why i think 還是when i think
只是這個地方不對不曉得是哪一個
其它都一樣
那你如果是做保留這個n best list
就會發現常常譬如說前五名
都一樣只差一個字
那你保留這個呢你全部重算有點浪費嘛
其實你應該把它變成一個word graph
那這邊都一樣只有這個地方不同
對不對只有最後這個地方不同
那這樣子的話你的這個嗯比較有效的使用空間跟這個資訊
那所以呢這個這個這個是
word graph 這是n best list
那不管怎樣你都是這樣
所以呢我的真正的效果呢就是
我de cup 這個de couple 本來的這個複雜的search problem into a simpler process
對不對就是說我我現在就是把我整個的做的話這個太複雜了
所以呢我可以把它拆成兩半
第一半用比較簡單的東西
the first primary by acoustic scores
或者是the second by language 這也是一種辦法
我language 在在第二個做哦等等
或者這個是一個例子
這底下也是一個例子
那基本上我就是把它 de couple 成為兩個stage
或者可以更多
所謂的multi pass 不一定兩個啦你還可以第三個啦
你可以在在這邊之後
我還不做決定
我在這邊之後呢我可以這個弄一個比較複雜的word graph
在這邊再做一次re scoring 把它簡化成一個再簡單一點的再做第三次也可以哦看你要怎麼做
所以你可以分成不只一個path
那這樣的話呢就每一個path 都比較簡單
那我的search space 只有在第一個的時候很大
後面就算動縮小
縮小之後
我再做精緻的
那這是一個常用的方法好
那再下來的這一些呢是是另外一招
這個也是使用很多的
那這一招其實就是所謂的heuristic search
就是我們底下要說的
heuristic 跟這個a star
那heuristic 跟a star 呢這個基本上是a i 裡面搬來的喔
那麼各位之中如果你修a i 的課的話就講一大堆這種東西就很清楚了
那我們這邊呢稍微提一下喔
那等於就是把a i 裡面的heuristic search 搬來
那這個是一個非常有效的方法
那麼也是我們常用的喔
那我們在這裡休息十分鐘好了
ok 我們接下去
我們剛才在講的情形就是
我的第一個word 就是一顆tree
這顆tree 就後面就有六萬個
第一個word 我就就這個這個非常大的tree 的hidden markov model
那麼走到最後有六萬個
然後之後呢那每一個走完之後應該都要再接一個
都要再接一個
所以你就接很多
這是第二個word 就有六第二個word 其實就有六萬個這個tree 在這裡喔等等
那在這個情形之下我整個的viterbi 會變成非常複雜的viterbi
不是我們原來單獨的這一個
不是單不是單獨這這一個這樣子走而已
而變成是我一路這樣子散開來一路散開來變成很大很大
那為了要讓這個比較清楚得來講我們怎麼來做這件事情
所以底下呢用用一些這個符號把它們specify 清楚它們是什麼
這講起來是蠻複雜的
不過事實上其實你如果對於我們原來所講的那個viterbi 了解的話其實是一樣的
好那我們第一個現在現在比較複雜所以我們一堆東西
第一個呢就是我要define 清楚我這是什麼這個是什麼
這個其實就是我們原來viterbi 裡面所說的那個東西
只不過現在變複雜了所以符號變多了而已
我們原來viterbi 不是這樣
當我走到時間t 在state i 的時候
我這邊所define 的一個東西叫做delta t 的i
就是走到這裡為止的一條有最高機率的path
它的機率就是這個東西
那這個東西其實就是我現在的這個東西
是一樣的東西
這個東西其實就是這個
只不過我現在的東西變複雜了所以我現在東西變一堆符號喔
那這個意思還是一樣
那我現在變成是怎樣呢你可以看到是
我在我在時間t 的時候
在state q k 啊 q t of the word w
那當我走過來走到這段是一個word w
譬如說這裡我是一堆state
那麼這一堆這個這是一個word w
那這個word w 是是我們剛才在這個一路這樣跑下來的中間的某一段嘛
譬如說我在這裡的時候從這裡跑到這裡的這條是一個word w 對不對
因為這個我從這個tree 的頭一直走到tree 的尾的時候這是某一個word
如果到這邊發現走到這邊發現這個word 是w 的話就表示說這整個path 是w
那這個w 中間會有一堆 是一堆phone 一堆state 走起來的
那這個呢就是這個
如果這樣的話呢我在時間t 的時候
停在某一個時間t 的時候停在某一個state
這個state 叫做q t
這個q t 是屬於word w 的到這裡為 止的這個分數
那麼我一路走過來
走到這裡的這個分數
就是這邊的這個 d
這個d 的這個什麼t q t w 就是這個東西
ok 其實是一樣的
那為為什麼這個w 那這邊還有什麼因為我從頭走過來嘛
我可能從從前面走走第一個tree 走第二個tree 這樣走走到這邊
這是某一個w 嘛喔
所以這是走到路的這個path 整個path 中間的某一個w 而言
在這個w 裡面的某一個state q t
我在時間t 的時候我剛好停在這裡的這個分數喔
那麼就是所以這個其實是跟跟這個意思是一樣
只是我現在比較複雜而已
那所謂的 object function 其實也就是這個東西
也就是這個一路走過來最高的機率就是所謂object function
那我就是要optimize 這個東西
我要maximize 它嘛
那麼這個呃那是什麼是best partial path any at time t is state q t for the word w
也就是說這邊有有千千萬萬個path 走過來
但是我現在講的是到這裡為止
最好的那一個
那就相當於viterbi 裡面到這裡為止最好的那一個這樣的意思
那我都是以某一個時間來算的
所以呢是時間t 的時候來算的
那你記得viterbi 裡面很重要的一件事情是做什麼back track
因為你每一次得到這個之後下一個的時候呢
我下一個state 當t 加一的時候
它可以從這上面的任何一點跳過來
depends on 誰過來的那條path 最大
那你很可能發現結果是從這一點跳到這邊的時候機率是最大的
所以結果呢
你要把到這裡為止最好的path 就變成是這一條了
對不對這是viterbi 的基本精神嘛
所以呢你你要算t 加一的時候
你得要在算從 t 的時候所有的state 都會過來的
然後看誰最大
最後最大的是這一條的話呢
你到這邊為止最大的就變成這一條了不是它了
那因此呢我我到這邊的時候我一定要記得說
哦剛才是從這裡來的
所以這是所謂的back 這是back track 的那個pointer
我要我要說ok 它是從這兒來的
這樣我一路要記得它的前一個是哪裡
那這個是viterbi 裡面很重要一件事情你要能夠記得
你剛才從你的最佳這點是從哪裡來的
這是所謂back track pointer
那我這邊也一樣也要
那就用這個符號來代表
就是說你如果是在你的這個這個你的partial path
你你的這個best partial path end 在time t in state q t for word w 跟剛才一樣
那你如果現在是在這裡的話
那你要算剛才是哪裡來的
剛才如果是在t 減一的時候
如果是這裡來的話
那你要把這個記下來說哦剛才是從這裡來的
那個那記下來的這件事情就是就是這邊的這個h
好我就記就是就是這個back track pointer
所以這個事情跟我們講的viterbi 是完全一樣的
只是說我現在要這個變得很複雜而已
好當這個沒問題之後呢
底下的這兩件事情其實說穿了也很簡單
也跟viterbi 這邊所想的事情是完全一樣的
那只是呢我現在變得複雜了
那我們要弄清楚
現在有兩種狀況
一個叫做 intra word transition
一個叫inter word transition
什麼是intra word 就是在一個word 裡面
也就是在一個tree 裡面
這是h m m only
沒有language model
當我在這裡面走的時候
當我在這一棵 tree 裡面走的時候
我是在這裡面這條路上走
那這條路其實你可以想像
最後就是一個word
是這六萬個word 裡面的一個word
那在這個word 上面走的話呢
我其實是走一個相當長的h m m 而已嘛
所以呢它是在其實就是在走h m m 的viterbi
因此呢這裡面的東西
跟這裡面這個東西
其實就是我們原來講的viterbi 這件事情是一樣的
這是h m m only
那在這個是後沒有 language model 的事情發生因為它是在算一個word
那底下呢
什麼是inter word transition 呢
是在當你這個走完我走下一個的時候
當你這個走完我要再走下一個的時候
那你你從這個跳到下一個去的時候呢
那這個時候我我是在一個h m m 跳到另外h m m
這時候中間有什麼有language model 分數要加進來
所以呢所以呢底下是所謂inter word
是這個這個時候是language model 分數要加進來
但是我我不是在h m m 裡面是在h m m 的外面
所以呢其實很簡單就是上面是跑h m m 沒有language model
底下是跑language model 沒有h m m
好那我們分別看一下這兩個情形
那就這個而言
它在說的事情其實我們先說 h m m only
也就是在intra word 裡面
也就是在一棵tree 裡面
一棵 tree 裡面的某一條path 上面的h m m
你在上面走的時候其實這個走的事情
就是在這邊走這段word
就是在走這段嘛
那走這段的情形跟這邊是完全一樣的
跟我們從前說的其實是完全一樣的
所以這個式子其實也就是我們從前講的那個式子
只不過現在看起來比較複雜一點而已
你看我要算時間t 在q t
時間t 在q t 怎麼算
我就先算t 減一的嘛
算算這個在t 減一的時候掉在q 的t 減一的時候
在這個word 裡面我現在都在這個word 裡面嘛
我是intra word 在這個word 裡面
所以呢我在t 減一的時候
我是在q 的t 減一state
對不對所以呢我在這個t 減一的時候
還是在word w 裡面的q 的t 減一的state
在這個q 的t t 減一的時候
在q 的t 減一的那個 state
上面我也有一個最高的分數
就是那個分數然後呢再加上跳過來的分數
跳過來分數有兩個
在我們當時講viterbi 的時候呢
那兩個是什麼一個是a i j
一個是b j 的o t
你如果記得我們是有這兩個東西
a i j 告訴我從這個跳到這個機率是多少
那b j 的o t 是我現在要把現在把這個新的vector
放進來放到這個state 來ok
所以就是這兩個分數
那這兩個其實就是這裡的這兩個
就是這個東西就分成這兩個這兩個就是這個
所以呢對不對就也就是說你現在從這裡的時候
我現在要算如果說這個是 t 這個是t 加一的這是t 這是t 減一的話
我現在在t 的分數是要所有的t 減一的
都有可能跳過來看誰最大
所以所有的t 減一跳過來有有兩種
一個是a i j
每一個跳過來都有個a i j
一個呢是我要把這個b 把這個o t
放到這個新的state 裡面去
這是這是state j b j 的o t
我要把這個放進來
那現在這兩個機率其實也就是我們這邊講的這兩個機率
所以你看到譬如說這個是什麼這就是a i j 嘛
這就是在word w 裡面
然後我從q t 減一跳到q t 的機率
所以這個東西其實就是a i j
就是從這邊跳到這邊的機率就是a i j
那這個是什麼呢
這個其實就是b j 的o t
因為你你你現在就是這個啊
它就是我在這個word 裡面那麼我現在是這個o t
我看到的這個第t 個 feature vector
第t 個vector 掉在q t 的機率
那其實就是這個東西喔
那你現在把這兩個它現在是寫log 用加的啦
那意思是一樣啦就是你這兩個加起來的這個
就是說就是這個這一項嘛
就是我從t q t 減一在時間從t 減一到t 的時候
我state 從t 減一到 t 的時候
那麼我要加進去的機率是這個
那然後呢因此呢我現在在算t 的時候
就是把t 減一的所有可能的q t 減一通通加起來嗯通通看起來誰最大
就跟這邊是一樣的嘛
你你現在要算t 的時候
你就把t 減一的看看是從這過來的還是從這過來從這過來看看是誰的最大
你就算那一個最大的那嘛
那那這邊其實也是完全一樣啊我現在就是把這個嗯我現在d 的t
q t 的話呢是什麼呢
就是在t 減一的時候的所有可能的q t 減一
那這邊所有可能的q t 減一在這裡
這邊所有可能在這裡就就是等於這邊的所有可能一樣的
這邊所有可能在這裡
那所有q t 減一都有一個最大的最佳的分數
再加上跳過來的時候可能的
然後加起來之後呢我在所有的q t 減一裡面看誰最大
然後知道ok 我就是從那裡過來
所以跟這個是完全一樣的情形
然後呢我現在就找到那個最大之後我就把我就得到一個下一條path
就最大的path
因此呢這邊所說的事情
跟我們原來所說的 h m m 是完全一樣的喔
只是現在整個整個變成複雜而已
其實精神是完全相同的
那底下這個式子只是在說
那我要做這個back track
我要記得從哪過來
所以剛才的maximum
倒底是誰是q t 減一
剛才是從哪一個過來的我要記得
我就把那個剛才過來那個記下來
這就是我的back track pointer
那麼說明這個我的q bar
就是指我的t 減一的時候如果現在t t 在q t 的話
那麼t 減一是從哪個state 是在哪個state
我把那個記下來
就是剛才那個裡面的maximum 的q t 減一是哪一個
把它記下來
然後然後把它放在那個back track pointer 裡面
於是我就記得剛才是從這樣過來的
好就這樣子而已
所以這個說穿了沒什麼特別符號變複雜而已
那麼其實講的就是這件事情
就是這件事情是完全一樣的
這是intra word transition
所以這個沒什麼不同
跟我們之前講的一樣
不同的是底下這個
因為我現在還會從一棵tree 接掉接到下一棵tree 去
當我從這棵tree 接到下一棵tree 的時候會怎樣呢
那就是做了一個inter word 的transition
從這個word 跳到下一個word
那這個時候呢我們假設說
現在這個word 走完了叫做v
v 是一個word
然後呢q f 的v 是它的final state
畫清楚一點
我現在走完了這一個這個word 叫做v
它有好多state
然後它一直到最後
這是它的最後一個state
這是它的q f 的v
就是 v 的這個word
q f v 是v 的word 的這個final state
然後我現在要從這裡開始接下一棵tree
怎麼接法呢
我先增加一個空的state 叫做q
q 是一個空的state
沒有裡面沒有任何東西只是為了接方便起見
為了是要接底下這棵tree
所以q 底下呢就接底下這棵tree 出來
那這棵tree 底下會有會有這個六萬個word
所以呢我現在的這個那如果後面這個後面這個word 呢
叫做w 好
所以呢我現在這裡的這裡有六萬個word
不過每一個word 我們都叫它w
所以呢這個我這個v
v 現在這個word 走完了
到了最後final state
了那我現在要開始接下一個tree 了
那下個 tree 有六萬個可能我們叫它是w
那麼開始的時候我有一個pseudo initial state
這是一個空的state 只是為了串接方便起見叫做q
那麼如果是這樣的話呢
好我現在就有一個空的 state 在這裡
這是我的 v 的final state 接下去
那麼因此呢我現在這個這個低的這個分數啊我就先給它跳到這裡面
跳到q 裡面來了
是也就是說當我這個t 到這邊
如果我們在這裡我們說是這邊是我的word v
這邊是我的v word v word v
走完的時候呢
是在t 的時候
我我這個這個最後這個state呢就是我的final state
所以這個state呢 就是q f 的v
那麼這個時候呢
我增加一個空的state 是 q
所以這邊我有一個空的state 是q
我仍然在時間t 的時候就給它走過來
ok 我在時間t 的時候我讓它
在這個空的state 裡面然後開始要往下接
那這個時候我就是在在這個呃時間t 從q 開始要走這個word w 了
那麼這個時候呢我這分數怎麼算
我先算我時間同樣的時間t
是我同樣的時間t 走到v 的final state 之後
當時的分數
然後現在要加language model 的分數
那麼這裡其實在這個寫錯了這要改一下喔
這個language model 分數你我們這邊是假設就是只假設language model 我們做bi gram 就好
其實那tri gram 更複雜啦喔
你想一想就知道tri gram 是怎樣的
不過我們現在只講bi gram
bi gram 的話呢這個很直覺的以為是這個
因為我現在v 後面要接w 嘛
所以我現在就是given 這個 v 後面接w 有個bi gram
其實這個寫錯了
我們應該是要看前面的
也就是前面的這一個
這個是u
如果是這樣的話呢那個機率應該是
probability 的這個v given u
是u 後面接v 的bi gram
不是 v 後面接w 的bi gram
這寫錯了喔
為什麼
因為其實你走到這邊為止的時候
你只知道我這條路上到這邊是v 而已
我後面w 還不知道了還沒開始走
所以w 有六萬個可能
這邊有六萬個可能的w
那你不可能把這六萬個bi gram 統統加上來
那就變成有六萬個分數了這不太可能的
所以這裡其實你不太可能知道那個w 是多少
我w 還沒開始走嘛
w 還沒開始走我我沒有這六萬個word 的不知道是誰我我如果真的要這個機率的話我有六萬個是不可能加
的而是應該是走到這邊走完的時候v 知道了
因為我走到最後才知道是哪一個word 嘛
我一路在找
一路在找這個最佳的path 對不對
所以我一路走過來走到最後才知道我這個word 是v
當我知道這個word 是v 的時候我可以把這個u 後面接v 的bi gram 加進來喔
所以你剛才在這邊你v 的bi gram 沒有加進來因為我不知道是什麼我我一路找嘛
我一路找不到最後不知道它是哪一個word嘛
所以呢我找到最後的時候才知道它是v
這個時候我是把u 後面的v 加進來
ok 所以這個地方是應該是這個u 後面接v
是這個的bi gram
不是v 後面接w 的bi gram
ok 那這個意思是什麼
這個意思跟剛才這邊是一樣的
也就是說我們剛才是說我從這邊過來
它可以從前面一個時間可以可以從任何地方過來
那我這邊其實也是一樣
你可以想成我在時間t 的時候其實這邊有六萬個word
我這個一路散開來的時候
在時間t 走完的時候其實有譬如說有三百個word
我這邊有一個v one
這邊有一個v two
那這邊有一個v 三
v one v two v 三都在這個時間t 的時候結束
它們都可以跳到這個q
你可以想是這樣ok
那也就是說我們這個圖現在已經不夠畫了
這個圖現在不夠畫了因為我其實這個不是一個 one d 的
這邊是我們這邊所畫的這個tree 嘛
這邊是一個tree 的結構你長上去的時候很多啦
所以呢當你到這邊的時候
這邊譬如說你在時間t 結束的時間t 所結束的word
其實有v one v 不是只有一個v
有v one v two v 三
都在最後結束分數都在分別是在它那個path 裡面最高的
那它們都在時間t 結束
所以呢我這邊其實有有好幾個
有好幾個
那這個v one v two v 三都在時間t 的時候結束
那因此我現在要跳到這個q 來
準備接下一個word 的時候呢
我可以有好多個可以從這個跳過去也可以從這個跳過去也可以從這個跳過去
那麼因此呢我先要看它是從哪一個v 跳過來的
那就是這件事
那精神跟這邊講的是完全一樣嘛
我現在只是說是要看它是從這個word
還是從v one v two v 三的哪一個的的最後的那個final state
會跳到這個q
來它的分數才是最高的
所以呢我就分別把所有的這些我這邊有六萬個word 在這邊結束
那有的早一點有的晚一點
你可以假設在這個時間t 的時候
有三百個word 在這邊結束
在 t 加一呢又有五百個word 在這邊結束了
t 加二又有一千個word 在這邊結束等等都可能
那麼因此呢你在每一個時間都在做這件事
就是whenever 你的word 走完的時候
你word 走完的時候
你就把那個 language model 那個的word 加進bi gram 加進去之後
然後我要看到底是哪一個word
會跳到那個q 分數是最高的
我就選那一個
那這個精神跟這邊是完全一樣的喔
所以呢我現在就是每一個v 走完的時候的分數
加上那個v 接在那個前面的 u 後面的language model 分數加進去
然後看誰的v 最大
我就從那個跳過去
那麼因此呢我這樣就得到這個那這個是相對於這個
只不過我現在是從是從這個h m m 跳到下一個h m m
或者說從這個tree
跳到下一個tree 的時候的的這個
跟剛才是在裡面走不同的地方在這裡而已
那這樣子我知道是誰最大之後呢我也一樣在這裡
我把那個最大的記下來
所以就把剛才那個maximum
所以這個也寫錯了喔
這個也是應該是這個應該也是這個u 後面接v 的bi gram
這個也是寫錯了
就把剛才這個maximum 誰最大記下來
最大的那個就是我的前一個對不對
所以呢如果是這個v bar 才是最大的
我們現在v one v two v 三
都都在這邊結束後我現在看到的這個
是看到現在是最大是這個
那麼因此我就應該把它的最後最後state 接記記下來
所以我就知道它是從這樣過來的
於是我現在就知道ok 它是從這樣過來的
於是它是從這樣過來的
於是呢我後面開始接下一個tree
那麼因此我現在就把它的這個v bar記下來
做為我的所以我的那個v bar 的最後那個state
就做為我的這個back point
back 這個back track pointer
那麼於是呢那就這這是兩種transition
只要這兩種繼續操作
那我就可以一路走下去
ok 一路走下去是可以
不過這個這個還是大的不得了
所以我們要有一些辦法來簡化它
有很多種方法來簡化它因為現在這個search 你可以想像非常大
這是我們所謂的search
那怎麼簡化它呢一個最簡單的辦法就是所謂的beam search
beam search 意思是說在每一個時間t 我只保留一個sub set
of 最可能的path
其它都丟掉
你可以想像我從一開始走
它很快就長很多很多很多
長那麼多之後你簡直沒辦法處理
所以呢最簡單的辦法就是做beam search
舉例來講define 一個beam width l
我們通當講譬如l 是三百或者六百或者二百
也就是說我我我很快走過來這邊就很多很多了
那我就只保留分數最高的
那二百個還是六百個path
其它全部丟掉
那我一路走的時候呢我一路在算分數最高的那個path
之後我保留六百個譬如說
其它全部丟掉
那這樣我才有辦法往前走
那當然如果這樣走的話就表示這不是已經不是一個optimum 的了
喔這又是一個喔這又是一個是個approximation
因為你可以想像
分數最高的path 不見得從頭到尾一定分數最高嘛喔
這個龜兔賽跑的原理嘛
期中考考最好的人期末考不一定最好嘛
所以你如果一開始就把ok 期中考裡面考最好的十個人留下來其它通通殺掉的話
那到最後其實可能最好的被你殺掉了對不對
這裡也是一樣的
你這個這個一路跑過來的時候
所以你的這個這個beam width 如果保留的越大是比較好
但是你的計計算量立刻就會大很多嘛
所以這個就是怎麼選擇這個問題
通常我們兩種簡單的辦法
一種是保留一個就定義一個beam width
譬如說你就是每在每一個時間點t 上面
我永遠只keep 前六百名或前三百名
等等那這樣的話讓我的計算量不會太大
第二種我就定義一個threshold
凡是的我分數比最高分少那個threshold 之內的我都保留
不管多少個
那有的時候這裡有一百個有的時候這裡有一千個
我反正是是這個在這個threshold 之內的我都保留喔
這兩種基本上這都是我保留一個beam
然後呢我就在beam 裡面走
那我自然就已經把可能的optimum 丟掉是可能的
所以你這樣子得到不見得是最佳的
但是是接近就是了
那這是最簡單最常用的這個reducing search space 的方法
當然還有很多進一步的方法我想我們這邊就不說
你如果有興趣去看讀相關的reference 就會覺得講很多種方法
因為這個其實是一個關鍵性的問題那麼有一堆研究如何做
那麼一個例子就是ok 你也可以從acoustic model 從acoustic 的h m m 的分數裡面去看
哪一些地方h m m 看起來它比較好比較不好把它丟掉
從language model 來看
那麼哪些應該丟掉什麼這這都有
那麼另外一個非常標準的做法就是所謂的multi pass 的search
也就是說我至少分成兩個pass
那這個意思是什麼呢
就是說喔應該是講說我在我先有第一個path
用比較簡單的knowledge
簡單的constraint
我就得到一個比較簡單的比較小的search space
在第二個裡面再做複雜的
嗯這話怎麼講呢
最簡單的想法就是說譬如說tri phone
tri phone 太複雜了
我前面就只做一mono phone
我我我一開始我我我不要做那個
我不要那這裡面我tree我這個lexicon 也可以有兩種嘛
一種是phone 的
一種是tri phone 的
tri phone 數目多很多所以會複雜
我就我先不要用tri phone 我就先用這個單獨的phone 做
那這樣就比較簡單
我就可以做第一個pass
或者譬如說這個language model 那裡呢
你可以想像我們這邊只講bi gram
是因為tri gram 複雜哦
我如果tri gram 的話
我走到這裡的時候
我不但要把這個bi gram 加進來
還要把這個tri gram 加進來
那我每一次都要都要再再算一個bi gram 算一個tri gram 是會複雜
那我也可以說我在我在第一個path 的時候我只做bi gram
後面呢才做tri gram
或者說我甚至於language model 我在前面不做
我我 language model 到後面才做等等
那因此呢我的第一個path
就可以比較簡單一點
那第一個path 的的出來結果呢
我們把它做成一個word graph
或者一個n best list
什麼意思呢
所謂的一個word graph 就是所有可能的word
可能性比較高的分數比較高的word
把它的時間點通通記下來
就構成一個 word graph
這是時間點
所以呢它譬如說譬如說到這個時間為止
從這一點到這一點
是可能是w one 是某一個word
到這一點也可能是 w two 是這個word
那這邊呢可能有另外一個word 是w 三
那這邊可能有另外一個word 是w 四
那這邊可能有另外一個word 是w 五
這可能有另外一個word w 六
那這邊可能有另外一個word 是w 七
ok 所以呢我從這個時間點到這個時間點的話呢
我可能是這樣子
這個可能是w word w one 這個可能是w two
它也許是w two 的前面一半喔
那那它也許到w two 也許不是對也許是w 四的前面一半喔等等
那麼因此我就把所有可能的word 它的時間點的起點終點通通記下來
它就可以構成一個graph
那這個graph 呢其實你給我一句話我可以先把這個graph
找出來當我這個graph 找出來之後呢
那其實它告訴我我現在只要在這上面找就好了
它 either 是一三五
或者是二五
或者是一六
或者是四七等等
那搞不好這邊還有一個
譬如說這可能也是一個
這個w 八
於是也可能是一三八七對不對
那麼因此呢你就在這裡面去看喔
那麼如果這樣的意思是說我的第一個pass
基本上做法還是跟剛才一樣這樣子做
但是呢我我只用比較簡單的東西
譬如說我只用這個這個我不要用tri phone 我只用單獨的一個phone
我不要用tri gram 我只用 bi gram
什麼的話
我也可以這樣走
這個程式稍微簡單一點然後我就取最可能的分數最高的word
哪裡是可能分數最高的word
那你可以想像因為我現在六萬個word
有的早一點結束有的晚一點結束
有的早結束有的晚結束就是我們這邊所畫的就是
譬如說w one 在這邊就結束了
w two 到這邊才結束
w 四要到這兒才結束對不對
我就把這裡面分數最高的word 保留下來
就構成一個 word graph
那麼這個東西我底下就只要在這上面算就好了
那麼因此呢我這個複雜的東西
在後面算
那麼我這個時候我這個再把複雜的
那也等於是說我我這個很複雜的這個這個tree 後面接這麼多tree 後面接這麼那這個東西呢我就把它reduce 成為變成只有那樣子
不但是變成只有那樣子而且它不會發散
而是最後會reduce 到一點
不一定是一點啦你這邊可能也有也有不只一個
但是譬如說這邊還有一個w 九
但是基本上你不不會一直這樣越長越大越長越大
你你你可以限制它就這麼大
ok 於是呢我真正的複雜的tri gram 啦
或者tri phone 啦什麼這個複雜的東西
我只在這上面算
那這個search space
比原來那個要小很多很多那個太大了
那個大到無法算所以我就先我先用一些簡單的就是less knowledge 或者less constraint
用一些簡單的辦法
把那個大的tree
那個太大的那個那個那個 tree
reduce 到變成一個小的graph
然後呢我現在把這個東西
在這上面才做詳細的
那這是我們通常稱為re scoring
你現在再把你的詳細的你的tri gram tri phone
分數詳細去算
那剛才因為只用簡單的所以你那個分數不太對
我現在可以把詳細重算一次分數
所以叫做re scoring
那之後呢你可能會發現這上面雖然有這麼多種可能
其實最可能的是這條
譬如說是w two 接w 八接w 九
可能這條才是你的答案譬如說這樣子
那你就可以在 word graph 上面找出來
那這是所謂的multi pass search 的基本觀念
那當然這樣做的時候基本上你前面的這塊第一個path
所謂的這個word graph generation
其實跟那剛才那個是一樣的
只是簡單一點
我用比較簡單的knowledge 用比較簡單的constraint
譬如說我只用我我不要用tri phone 我不要用tri gram
等等我簡單一點就其實是一樣的
然後我就是保留最重要分數最高的word
譬如說在這個時間點結束是以它最高
或者你也可以再保留一個啦對不對你可以再保留
你保留若干個這個時間點結束的分數最高的
然後你在這個時間點你把它保留你這樣一路這樣你會得到一堆
那你就把它們構成一個graph
那如果是這樣子的話很可能我們可以把它畫成
這樣子這是w 十
那它們都n end 都在同一點
然後後面都可以接這些等等
那這就是所謂的 word graph
那你有了word graph 之後在word graph 上面
再用比較詳細的再重跑一次
re score 這些所有的path 之後
你算哪一條path 分數最高等等
那這是這個所謂用word graph 的方法
那麼n best list 是相同的意思
只是說呢它沒有做成這樣子的word graph
而是直接把前一百名譬如說這個n best 就是n 就是這個這個前n 個名次分數最高的word list
全部把它保留下來
那麼舉例來講在這個case 的話你就可能
就這個case 的話你可能想像的就是譬如說
一三五這是一個
w 一 w 三 w 五
這是一個一三五
那麼二二五也是一個w 二 w 五這也是一個
那麼w 四九也是一個喔等等等等
那你如果沒有把這個word graph 建起來
只是說把分數最高的一些word 的word sequence 把它通通都留下來
譬如保留前一百名或者保留前二百名或者前五十名
那就所謂的這個n 等於一百或者五十或者二百的 n best
那你就把這個list 留下來之後我重新在這上面算分數
那你可以想像這兩種那一個好呢
這個是比較精簡啦
這個可以把它們這個其實包含的東西比這個還豐富
這個只告訴我說一後面接三三後面接五
那這個其實告訴我說
一是在什麼時候結束
三在什麼時候開始
三是在什麼時候結束後面五等等
所以呢我我其實三後面還可以接八接九什麼
它都都在這邊都呈現了
所以這是一個比較精簡的描述的方法
你這樣保留一個這個word graph 的效果
會比這個好
但是這個簡單
你這個這個呢你就是把剛才一路找過來的你第一個path 也是用比較簡單的方法來做
但是我一路走過來之後我就把前一百名留下來
得到一個一百的list 喔
那這就是所謂的n best list
那這兩種方法都可以
我這上面舉的這兩個例子在說明這個這個n best list
是不如這個 word graph 來的有效喔
那像這個例子呢你常常前幾名是只差一點點
i’ll tell you what i think 還是 why i think 還是when i think
只是這個地方不對不曉得是哪一個
其它都一樣
那你如果是做保留這個n best list
就會發現常常譬如說前五名
都一樣只差一個字
那你保留這個呢你全部重算有點浪費嘛
其實你應該把它變成一個word graph
那這邊都一樣只有這個地方不同
對不對只有最後這個地方不同
那這樣子的話你的這個嗯比較有效的使用空間跟這個資訊
那所以呢這個這個這個是
word graph 這是n best list
那不管怎樣你都是這樣
所以呢我的真正的效果呢就是
我de cup 這個de couple 本來的這個複雜的search problem into a simpler process
對不對就是說我我現在就是把我整個的做的話這個太複雜了
所以呢我可以把它拆成兩半
第一半用比較簡單的東西
the first primary by acoustic scores
或者是the second by language 這也是一種辦法
我language 在在第二個做哦等等
或者這個是一個例子
這底下也是一個例子
那基本上我就是把它 de couple 成為兩個stage
或者可以更多
所謂的multi pass 不一定兩個啦你還可以第三個啦
你可以在在這邊之後
我還不做決定
我在這邊之後呢我可以這個弄一個比較複雜的word graph
在這邊再做一次re scoring 把它簡化成一個再簡單一點的再做第三次也可以哦看你要怎麼做
所以你可以分成不只一個path
那這樣的話呢就每一個path 都比較簡單
那我的search space 只有在第一個的時候很大
後面就算動縮小
縮小之後
我再做精緻的
那這是一個常用的方法好
那再下來的這一些呢是是另外一招
這個也是使用很多的
那這一招其實就是所謂的heuristic search
就是我們底下要說的
heuristic 跟這個a star
那heuristic 跟a star 呢這個基本上是a i 裡面搬來的喔
那麼各位之中如果你修a i 的課的話就講一大堆這種東西就很清楚了
那我們這邊呢稍微提一下喔
那等於就是把a i 裡面的heuristic search 搬來
那這個是一個非常有效的方法
那麼也是我們常用的喔
那我們在這裡休息十分鐘好了
ok 我們接下來這段基本上是
a i 或者別的相關課你可能學過喔
如果你沒學過我們很快說一下
如果學過我們就是複習一下喔
那麼這是這個通常人家把這個search problem 看成是一個譬如說像這樣的一個city travel problem
也就是說你如果要從city s 做為你的starting city
g 是你的goal city
你要從s 走到g
那麼你有一個map 你只知道說從s 呢可以走到a 跟b 
它是三公里它是兩公里
等等
你一路走下去到底走哪一條才是minimum distance path 呢
那如果你已經有一張map 你從頭這樣去加你當然可以找得出來
otherwise 這個變成一個很複雜的問題
那你可以想像這個這個minimum path 的這個minimum distance path 這個問題呢
可以看成是一個tree 的結構
當你變成一個tree 的時候就跟我們剛才講的就比較像了喔比較像這樣子
那怎麼變成一個tree 呢就是ok 你s 呢可以走a 可以走b
如果走a 的話呢又可以走 b 跟c
那走b 的話呢又可以走a 跟d 等等
那你就是把所有可能的path 通通接上去
你可以這樣找出來變成一個tree
然後呢你如果這樣走的話呢各是幾公里一路可以加進來
譬如說走a 跟c 的話你就變成六公里了
再走到e 的話就變成九公里等等都可以這樣子做
那即使你變成一個tree 你需要有好的方法才能夠找到比較好的路
舉例來講這幾個方法都不算是好方法
所謂的這個depth first
就是說你一你凡是碰到你可以選擇的時候
你就任意arbitrarily 選擇一條
然後就向前走
你隨便找一條向前走隨便找一條向前走如果這樣子的話呢當然不見得最好
你只是最快走到最深的地方去而已
那反過來呢breath first 這個這個breath first 的話呢是說
我凡是碰到可以選的時候
我就把這層的所有的note 看誰看誰比較譬如說這裡的話二跟三那二比較近我就先走二
那當然不見得走二就是好啦對不對哦等等
所以呢你每一次就是把這same level 的都去看一次到底誰比較好
不過這也不見得比較好所以這是breath first
這都不見得好
那這種呢都有共同的問題就是所謂blind search
就是你沒有真的知道它們那麼誰是怎麼樣才是最好的
或者說你其實沒有一個sense about where the goal is
就是說你並不知道你的goal 在哪裡
你只是在那裡一路找而已
那怎麼樣比較好呢
所謂的heuristic search
所謂heuristic search 的一個簡單的這個這個解釋是說呢
我現在假設說我要從這裡走到這裡
雖然我現在我沒有辦法把整個地圖畫出來看的話呢
我至少呢假設假設這個是台北一零一
它有一個高樓
從遠方就一路都看得到它
因此呢我就可以每到一個地方的時候我就都可以估計一下
用目測的
目測一下說它的直線距離是多少
那我就用那個當做所謂的heuristic
於是呢我就可以做這件事
就是在我我在每一次可以選的時候
我就算一下走這個跟走這個倒底哪一個比較好呢
除了走到這邊的距離是所謂的g n
所以g n 是distance up to note n
所以假設我從這邊是要走a 還是走b 呢我就看這個是g n
是三這個b 是二之外呢
我還在看假設我從a 來看的話
大概估計是多少
那這裡有一個假設我們舉例來講這是一零一的高樓
所以你可以用目測的測它的直線距離
那假設從這邊測過去的是八點五
我這邊用這個顏色寫的就是假設這邊預測是八點五
那這邊的話呢從這邊估計一下呢是十點三
那這就是h n
h n 就是heuristic estimate for the remaining distance up to g
所以呢於是你你如果這個時候選a 跟選b 就有兩種選法啦
如果a 的話呢我是這邊是要三
這邊到這邊呢估計是八點五
所以加起來是十一點五
這邊是要二
這邊到這邊估計是十點三
所以加起來是十二點三
那它比較近啊
所以我就選擇它
那麼因此呢我這個就開始選擇a
那這個時候呢因為這是三這是八點五
所以我的估計是十一點五
所以呢我在每一次可以選的時候
我就把這兩個選的東西我都算一次這個分數
其中一個是我知道的
一個是我不知道的但是我做一個estimate
那麼然後呢我用根據這個estimate 來選
那麼等到到了a 之後我現在再看的話呢可以走b
可以走c
走c 的話這邊是三
這邊是五點七
那這樣子的話呢接下來是八點七再加三是十一點七
可是我如果走b 的話是十點三
還要再加四
還要再加三
是十七點三
那這個遠得多了
所以我就說c
所以呢a 之後我就選c
這個時候我的估計是十一點七
是因為這是三六再加五點七是十一點七
那那當我走了這個的時候我就到了到了這個c 了
c 沒有什麼好選的因為只有這個就走到e
這個就是這個再加三
那到了e 的時候呢我這個時候再來看的話呢
你這邊是九
這邊是二點八
所以就是十一點八
那等等那這樣下去的話呢這樣我一路走下去這條path 就是非常接近optimum 的一條path
那我其實一路估計的其實差不了多少真正的距離是十二
我一路估計的是十一點幾是很接近的
在這個例子而言
那這樣的方式這就是所謂的heuristic 這個heuristic search
那有另外一個名字所謂best first search
我就一路在做估計
然後一路呢找best
那有另外一個名字叫做heuristic pruning
你知道pruning 的意思就是砍樹
就這個tree 太大了我就一路把它砍掉
那我們之前講的這個也叫做pruning喔
所謂的pruning 都是這個樹太大我要把這個樹砍小的意思
那就這個example 而言
我們比較像是在我們比較像是在因為是在走這個travel
所以是要找 minimum distance
跟我們講的不太一樣
那我們講的應該是底下這個case
我們把它另外做個題目
這是算這個最高分數
因為我們其實都是算最高分數
而且呢我們其實最終目標不是一個
而是你只要走到任何一個word 都可以
所以呢比較像這樣
這也可以就是說我現在如果從a 開始走
那每走一步會得到一個分數
每走一步得到一個分數
那麼走到最後走到底就都可以
沒有一個固定的
不像剛才
剛才這是有一個固定的goal 在那裡
我這邊可以是不固定的
然後呢我要看哪一個分數最高
那這個problem 是完全一樣喔
那麼你怎麼做
你這邊a 如果下面可以選b c d 的話我就把 b c d 都列出來
那這個時候呢我的我可以先算走到b c d 各得幾分
四三二
然後呢在 b c d 的時候各去估計一下
我如果走到底的話可以得幾分
這就是h n
就是這個estimate value
然後把它加起來
那這就是我的估計
然後根據我的估計來看的話呢
嗯c 最高
我就選c 等等喔
所以呢這個意思是一樣的
就是我每每一次當我有選有得選的時候
我都去看走到這邊的話我知道得多少分之外呢
後面我不知道但我想辦法估一個值
然後呢我用這二個去相加
然後呢去看我要這個來選
那這樣子的選法呢
這個這在a i 的課本裡面他們他們有一套說法
那麼這個說法就是嗯第一個它有一個叫做所謂的admissibility
所謂的admissibility 是說有某一種search 的方法是admissible
如果保証你找的第一個solution 就是optimum
只要optimum solution 存在
只要optimum solution 存在你照那個algorithm 去做
你找到的那一個第一個就是optimum
這個叫做admissible
那當然很多時候你的你的algorithm 不是 admissible
那當然我們會希望
它是admissible
那麼舉例來講
我們剛剛講的beam search 顯然就不是因為
beam search 我已經很可能把那些個嗯optimum 都已經丟掉了
所以beam search 顯然就不是
它只是一個practically engineering solution
它適合engineering 的方法來得到答案而已
它不見得是
喔它顯然不是
那
嗯
我們比較希望我們的方法是admissible
那麼也就是說只要存在一個optimum solution 我就照這個方法去找的我的第一個solution 就是
有沒有條件呢有的
這個這個這是這個一個定理
在課本裡面有
那a i 課本裡面也有這個到處都有
那我們這邊並沒有打算要去說它如果你有要了解它的 exactly 的意思
或者它怎麼証明的
課本都查的到
不過講起來很簡單就是說
以我們剛才那個case 而言你如果是要最高分數的話
你可以証明它是admissible
你的條件就是我的所有的都是高估
那h n 是什麼
h n 是真正的
我真正的從這裡到那裡
真正從這裡到那裡會得幾分是真正的
那h n 的star 呢是你的估計值
那麼
這個
定理是說呢你如果是一個最高分的problem 你要找最高分我們剛才像剛才這個就是我要找最高分看走到哪裡最高分
你如果是要找最高分的話
你其實就是
你所有的都是高估
你只要所有的都是高估
你就會得到
那麼就就它就是admissible
反過來如果是minimum 的problem 的話
像我們前面那個
是一個 minimum distance 的problem
你要minimum
的話呢那反過來就是要低估
你如果所有的都是低估的話
它就是admissible
那
這個
詳細的証明我們就不講
那凡是符合這個條件的admissible 的話呢我們就說它叫做a star search
喔這個a star 這個是a i 裡面的名詞
那
那麼因此我們會prefer
這種a star
那麼它的基本的做法
那那這樣來的我們這個這樣一來這個heuristic
這些heuristic search 就有道理啦
喔
我們剛才講好像沒什麼道理因為你你憑什麼估記
你憑什麼去估這個東西呢
那現在有一個很簡單的原則就是你要高估或者低估
你如果要minimum distance 就是要每一次都低估
然後你如果有 maximum score 就是你每一次都要高估
你如果就是高估或低估的話就可以保証你是
a star
嗯這叫做所謂的a star search
那這個做法就是我們剛剛已經講過就是每一次
凡是你有得選的時候
你就把所有的可以選的都算一次
算的時候就是包括剛才講的這兩個嘛
就是
已經知道走到這邊會有幾
的分數以及我高估或者低估的那個分數
把它加起來
然後你把它列出來看看誰誰是你要的
你就照那個來選
你如果是這樣子的話
所以呢譬如說這邊講你如果是要最高分數的 problem 的
話
你就是用高估的
然後呢你每一次可以選的時候就選那個最高的
那這樣的話呢你這個就是一個a star
然後你就是可以得到admissible 的的答案
好
那我們大致這樣講那做語音的時候怎麼做
做語音的時候是一樣的情形
你可以想像的情形是說
其實我們是在做一個跟剛才一樣的
這個multi multi pass 的search
但是呢我在第二個pass 的時候我現在可以用a star
因為我在我在第一個pass 走過來的時候譬如說
我已經建好這個tree 了
tree 上已經有很多東西了
我可以用這個tree 上的知識
來來估計
來高估
所以我第二次重走的時候呢
我就可以走a star
我就可以算說
我走這個的話後面估計是多少
如果我走這個的話後面估計是多少
我走這個的話後面估計是多少等等
那就等於說我是在這個
所以呢我的這個第二步的時候呢我可以用 a star 的方式來做
來make sure 我走的是admissible 的
然後我會得到optimum
至少在在那個word graph 裡面是optimum
喔
等等
雖然我一開始建的時候可能我有把最好的丟掉是有可能的
那這個是這我們用a star 來做那一塊基本上是這樣子
那你怎麼用
這個這個tree 這個word graph 上面的東西來估計
做這a這a star 要估計啊
要做高估啊
那
你怎麼怎麼個高估法呢
那我們這邊舉兩個例子很多方法都可以用
那我們舉這兩個兩個例子
那第一個例子呢就是說
你你估計時間
你先估計這個 average score per frame
那舉例來講呢我可以有training data
那麼我可以算這個東西
這是什麼呢
這是o i j 就是我一堆observation
從frame i 到frame j
然後呢
q i j 是它的一個state sequence
然後中間呢有多長
因此呢這個就是
我如果有一段signal
這是從time frame i 到time frame j
那麼這一段呢
它假設走某一個state sequence
走過來
那麼這個呢就是o i j
這個呢就是q i j
那我如果是
假設是given 這一個state sequence 的話呢
它的總分數是多少
那麼總分數是多少呢我除以j 減i 加一就總共多少 frame 嘛
那我大概可以估計這個每一個frame 每一個frame 分數是多少
那這樣的話呢我現在如果有夠多的training data
我可以用這個來統計
我大概可以算出來平均每一個frame
每一個frame 它的分數
平均是多少maximum 是多少minimum 是多少等等
這些都知道了
然後呢
因為我剛才在b第一個pass 已經走過一次
所以我就知道
每一點到最後的時間還有多少
我就用這個時間來算
這就是t 減t
大t 就是
最後的
那你在時間t 的時候
那你可以估計我這邊還剩下多少個frame
對不對因為我前面已經走過一次我的first pass 已經走過一次
我已經知道到到大t 會結束 大t 是多少我已經知道了
因此我現在到這個node 的話呢我其實我已經知道
還剩下多少時間
然後我可以估計平均每一個frame
會有多少時間minimum 是多少時間maximum 多少時間等等
我可以用這個來來算
我每一點估計到最後大概有多少時間
那這樣的話我就可以得到我的這個heuristic 分數
用那個來估
那我現在是要高估
那我就我要make sure 我找的是maximum 我就高估好
等等
那第二個例子是差不多的情形就是你先在現用week constraint
可以得到它的分數
那那個可以拿來當做heuristics 用
就是說你現在我剛才假設我只用
我沒有用tri phone 我只用最簡單的phone model
我也可以得到它們的acoustics 分數
我沒有用tri gram 我只用bi gram 我也可以得到它們的
linguistic language model 分數
這些分數都可以拿來做estimate嘛
所以我可以用第一個pass 所得到的比較粗的
東西
所以呢這個這個first path 得到的那些比較粗的week constraint 的分數
拿來估計我的heuristic
也可以啊
那其實這個估計的會比剛才那個還更準一點啦
喔因為我這個可以把那些分數算進去
那這樣的話呢我這個都是這類的分法
那這樣的話我們的這個之前講的這個這個multi pass 的方法呢
其實我到第二個階段的時候我就可以做a star
那這個也是一個常用的方法
好那到這裡呢我們八點零講完了
那麼嗯或者說是我們的這個所有的basic 到這裡都講完了
那我們來先來說一下期中考
今天是四月今天是四月二十五
下週是五月二號
五月九號五月十六號
期中考範圍到這為止
所以呢這個我上次提過合理的考試時間可能是兩週以後
但是呢我希望排在十六號
原因是那週我出國
那所以那週考試的話我們不影響進度
就可以大概可以至少可以等於是補了一次課一樣啦
那所以我們十六號考試沒問題吧
好 ok 那所以我們期中考是嗯五月十六號
我們的考試時間兩小時
也就是十點
我們十點十分到十二點十分好了好不好
我們就是考最後的考後面的一百二十分鐘
這是所以就是十六號那一週我出國我們就是考期中考
十點十分到十二點十分
考試範圍是到八點零為止
那麼我會在下週把上一次的考古題發給各位
那你就比較容易知道我會怎麼考
那嗯這個期中考不會難
因為並沒有要為難各位喔
那麼期中考的目的其實最簡單的就是兩件事
第一個就是make sure 大家有在念書喔
那我了解這個我們台大同學的這個最主要的問題就是如果不考試的話你就不會念的嘛
所以呢我們就是需要考試來make sure 你有念書就是了
然後當然第二個目的是我們得要有個分數喔
要有分數才能夠才能夠算成績嘛喔
喔只是這樣原因而已
所以期中考不會難考
喔但是你要念喔
要念什麼
除了你不是光是上課講這些東西而已
我每一個每一個八點零七點每一個點零的地方前面都會有它的reference
這些基本上我大概都會說到
喔譬如說我剛才講這個時候我就會說這三個裡面你選一個嘛
有or 就是你選一個
要念其中一個
嗯然後呢那這個我就沒有說你一定要念啊我就說這個是一個很好的reference
那這個我也沒有說你要念啊喔
這我就說這個是屬於那個古代的方法裡面的很好的reference
所以這兩個應該是不會考的譬如說
我都會再講到
所以呢那但是呢我如果有講說什麼地方要念你要念哦不然那個地方會考到
那然後我們這門課因為修課同學background 差異很大
我們從大三一直到博士班
從資工的到電子的到什麼都有
所以呢我們基本上哦你可以假設就是這我之前也講過就是
你如果會覺得沒有辦法看下去你就跳過去
你繼續往下看
但是當你看了很多之後你可以回過去再看
你可能發現你原來你看不下去的地方你再看就看懂了因為你看了後面的東西
喔所以你always 可以再跳回去看前面的
如果你跳回去看仍然看不懂的話
那那個地方不會考啦喔
那基本上就是這樣喔
所以我想我會考的部分應該是不會depends on 你的某個專業的background 的
所以呢我想這個是這個期中考的部份
然後呢我們的習題還有第二題會在下週給你
但是交習題時間會在期中考以後哦
所以第二題習題是做language model
就是train n gram
哦等等
因為你的第一題是acoustic model 嘛哦
第二題是language model train n gram 嘛
會在下週給你
然後在考後交
那之後的話你現在就等到期中考考完之後
你只要把第二題習題交完你只剩下一件事
就是期末報告
喔那所以呢
那這個我們從下週以後開始我們就在講後面的了
我們這個fundamental 到這裡結束
那那我下週開始我用跳的
所以呢喔我下週會先直接講這個這個十一點零跟十二點零
然後會往下走
那麼我九點零跟十點零
屬於另外一些個理論比較多的東西
我覺得留到比較晚一點再說
那從十從這個十一點零以後都是各個研究領域的相關的適合給你做報告的題材
喔所以呢我儘可能提早先講這些個各個研究領域的部分
那麼讓你提早接觸這些東西
這樣你期中考一考完你就可以開始思考你期這個期末報告可以做什麼
那在這個裡這裡以後我講的方法就會跟這邊都不一樣了
因為到這裡八點零為止我們是在講basic 所以我每樣東西講得很慢
那從十一點零開始
那其實我每一個都只講它是什麼然後觀念是什麼
我就跳下去了
哦就不斷用跳的
所以從這後面開始會用比較快的跳的方式來進行
那麼跟這邊的我們講的basic 是不太一樣的喔
好這個是講這個我們之後的進行的情形
那麼所以呢底下我們可以稍微開始一點點十一點零
那麼十一點零我們在講的是speaker
從這裡開始我剛才講我們現在就講一樣一樣東西
我們每一樣東西就會給你reference
然後這個嗯我每一樣都只講一下它的基本精神就往下跳了喔
那我們十一點零是講不同的speaker 聲音不一樣的問題
那你可以想像我們到目前為止沒有考慮不同speaker 會怎樣
但其實每一個speaker 聲音是不一樣的
我們說ok
這堆是嗚
這堆是啊
這是一個很粗的說法
如果單獨一個speaker 的話
一個人它的嗚是會這樣子
有一個distribution 啊會有一個distribution
但是你如果一群人的話它的嗚顯然會比較大
那它的啊也顯然會比較大
於是就會overlap喔
那因此呢你你這個不同的人顯然就會有很多不同的問題跑出來喔
那麼那麼我們要解決這個不同的speaker 的問題
那我們底下會說一些重要的方法
那我每每一樣重要的東西我們就會列個reference 給你喔
那這些都是我們後面你如果要做期末報告的很好的起點
那我們先說一下這裡的主要的problem 是什麼
那麼基本上來講最好的當然是speaker dependent
也就是說我只為一個speaker 來train
只用你的聲音train 你的系統
這個顯然是正確率最高的
只是說呢我們需要大量的data
那你的聲音需要train 所有的tri phone
對不對需要train 所有東西
所以呢你這個這個本身就已經很大了嘛
那麼在早年做語音研究的時候人家都以為這樣是最好的方法這樣正確率最高
不過到後來就知道這個是不可行
為什麼不可行
就是因為需要的training data 太大了
而天下的user 是天下最懶惰的一群人
如果你要叫他先發夠多的聲音去train 的話就沒有人要發
所以呢天下最懶惰的一群人就是user
所以就不要靠user 的話是不可能的
所以最後我們就知道這個solution 是不通的
那後來就有人想說那這樣子嘛我們做multi speaker
譬如說同一個家庭的人聲音都比較像
這個兄弟姐妹都很像
所以呢你就可以把他們train 成一個model
那是不錯啦喔
你這樣的話是因此呢你如果本來要用十個小時的data
你兄弟姐妹四個人所以呢
你每個人用二點五小時嘛哦
那這個是稍微有點好處但是幫助不大
這個沒有太多用
所以這個後來也放棄
那最理想是什麼 speaker independent 嘛
那就是我用譬如說找一千個人
來五百個男生五百個女生
那我這樣的話我就可以train 所有的聲音都在裡面了
那這樣子的話呢就這個這個每一個人其實這一千個每一個人可能只要三十分鐘的data
我就可以那這樣我希望可以good for all speakers
但是這個turns out 也不那麼成功就是因為
我的accuracy 一定會比較低
就是我們剛才講這個情形嘛
你如果一個人的時候我可能啊跟嗚可以分得開來
可是當你可能有一千個人的時候呢每一個人的嗚不太一樣
所以它的distribution 就會變大
每一個人啊都不一樣就會變大所以顯然就會overlap
於是就不容易分得開來嘛喔
所以呢這個時候呢speaker independent
顯然是我們都希望的
但是它的缺點就是accuracy 一定會低一點
那麼今天其實我們所有的語音系統
幾乎都是這個
但是呢它都是會差一點喔
就是我們幾乎已經沒有沒有這個speaker dependent 的case
或者這個幾乎是沒有了
但是呢大概都是朝這個在做
但是都不會太好
那怎麼辦
那最好的solution 我們今天所知道的最好是這個
就是speaker adaptation
就是用這個speaker independent 開始
但是呢它去學speaker 的聲音
你儘量學
讓你說最少的話我就學會你
所以譬如說呢你一開始是一個speaker independent model
所以我一開始的時候我我輸入的時候它的正確率稍微低一點
這個也有也有技術你怎麼樣做得比較好
但是不會特別好
那這個時候呢
當你開始對它講三句話講五句話之後
它就馬上學進去
它就開始知道你的聲音是怎樣的
它就會開始知道說喔你的嗚其實是這一群
你的啊其實是這一群
如果它有辦法幫你把根據你講的那少數的幾句話
就幫你把你的嗚由這堆收縮到這兒來
啊由這堆收縮到這兒來的話呢
欸你就分開來了嘛
所以呢你就可以這個用這個limit quantity 的data 用少量的data
我們稱之為adaptation 的data
於是呢你就可以學你的聲音
於是你就可以正確率可以馬上提高
那這個觀念是技術上是可以做得到
實際上也是可行
也就是說我們今天所看到所有的系統幾乎是這一種
也就是基本上它是speaker independent
但是你如果跟它講話講的講的話他就越學越快喔
那就是所謂的speaker adaptation
那我們這個十一點零主要就是講這個speaker adaptation
那怎麼樣來讓系統讓這些model 學你的聲音學得最快
那這裡面可我們可以分成supervised 跟unsupervised 兩種
那所謂的這個 supervised 是說你所輸入的東西假設是知道的
譬如說這個一開始系統先先跟你講幾句話
先叫你說個什麼
那其實你就照它說
所以你說的話是它已經知道的
因此呢它完全知道你的這些你輸入的這個聲音裡面
這是什麼phone 這是什麼它完全知道
因此它就可以用這個phone 去train 這個
用這個phone 去 train 這個等等
它都知道那這是所謂supervised
但是這個比較不太好的地方就是在於你user 一開始得要回答系統一堆問題
喔user 可能不喜歡
那user 喜歡可能我就開始我就跟他講我要講的嘛
如果是那樣的話呢
就表示user 一開始說的我系統已經不知道他在講什麼了
所以呢怎麼辦呢
也就是我的 text 你輸入的聲音的文字它其實不知道
那這個時候呢
你知道一開始就是用用speaker independent model 去做辨識
所以我一開始不知道這你在說什麼但是我就用我這個model 去辨識
它可能是這個它可能是這個
那我就用它去train 它跟它去train 它
那這樣當然會有錯
有錯所以呢可能是你要你如果可以iteratively
perform 會比較好
也就是說
我現在你講的第一句話我雖然不知道是什麼但是我就用我的原來的speaker independent model 去辨識一下
說這個可能是這個
這個可能是這個
所以我用它來調它用它來調它
當我把這些都調過之後這個比較好了
我再來辨識一次
我可能會辨識比較準嘛
等等我可以用 iterative 方式
來做那這是所謂unsupervised
那我們今天比較prefer 是這一種
這樣你系統 user 可以跟系統直接講話
然後呢你就可以進步
但是這個就是說這個有技術嘛
那再來呢我們可以分成batch 跟 incremental online 的區別
所謂batch 就是你輸入一堆然後它一起幫你幫你調
incremental 就是你一步一步跟它講它一路調
也就是說舉例來講譬如說你你當你講了前三句話的時候
它就根據你的前三句話調一次
你又講了三句話的話呢它會再調一次
或者根據你的前六句話重調一次
就你不斷的講它不斷的學喔
這樣子它的正確率不斷的提高
這是我們今天大部分是這種
就是incremental 的或者是online 的
那當然 batch 的話是會其實會效果更好
你給它一堆
它一次喔
因為你一步一步的時候可能都 step by step 的時候很可能每一步都不是optimum
因為你再加東西之後呢你的前面那個就已經不是optimum
喔所以呢但是反過來呢我們在user 來講是這個比較比較attractive的喔
所以大概這是adaptive adaptation 的這些東西
那我們底下就會講adaptation 幾個重要的基本的方法跟觀念
然後我就會給你這些reference 等等喔
那我會告訴你哪個方法的reference 哪一個等等等等
那這是後面的從下週以後我們的上課的方式會變成這樣
那不像我前面會把它講得那麼清楚
好那我們今天上到這裡
