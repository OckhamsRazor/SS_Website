那麼這這樣講有點抽象我們底下舉一些例子來看喔
對了還有漏掉的我們要講一下
這邊講到說取log
取log 當然很重要的是你到底log 的基底是取什麼對不對
你這個是log 是什麼呢
那麼在課本裡面它會說你depends on 你取什麼
最常用的是基底是取二
如果是log 是二的話
那這個單位就叫做bits 
那麼這個有它的道理的我們底下就會解釋為什麼是這樣
但是呢這個是最常用的一個一個基底
當你不一定要要用二你用其他也可以
譬如說那個用自然對數的e 也可以
你用log 十也可以等等
但是最常用的是二
當你用二的時候呢
算出來那個數字的單位叫做bits 
那麼這裡有點有點confuse 
因為我們平常也常常用bit 來講另外一件事情
我們常常講的bit 就是講這一個這一個零或者一叫做一個bit 
那麼這個bit 是binary digit 的簡稱
binary digit 叫做bit
那麼這是一個零或者一叫做一個bit 
但是這裡不是
這裡的bit 是information 的量
是取log 出來的值
它可以是任何的real number
這裡一定是整數
你如果講這一個bit 就是指一個bit 沒有半個bit 嘛阿
所以呢bit 一定是一個整數嘛
可是這裡不是這裡bit 是一個real number 阿
是一個real number
那麼為了區別起見我這邊凡是後面寫個括號of information 
就是指這個bit 是information 的量
而這個我如果沒有寫of information 的話呢
就是指它是我們所熟悉的bit 的意思
那為什麼會這樣子搞得一個bit 有兩個意思呢
這個其實都都是有原因的這個我們底下就會解釋
好那麼底下我們就舉一些例子來看這些東西這樣會比較清楚一點
那麼最簡單的例子就是我們這邊所熟悉的零跟一
如果說我我送出來的如果說我的這個就是零跟一
而且它們機率各是二分之一
這是個最容易想像的case 
就是那麼零的機率是二分之一
一的機率也是二分之一
就是我們這邊講的這個case 
在這個情形之下你很容易算
你怎麼算它的information 你就是把這個機率各取log 嗎對不對
因為我們剛才講我就是把那個機率把那個機率取log 嘛
取log 就是我的這個information 的量
所以我就取log 你就會發現那個零給我的是一個bit 的information 
一也給我一個bit information 
平均起來它還是給我一個bit information 
所以呢也就是說呢
譬如說這一個這一個binary digit 
我們把它寫清楚這樣比較不會混淆
這一個binary digit 給我多少information
一個bit of information 
對不對
這一個binary digit 給我一個bit 的information 
同樣呢這一個零也是一樣的
這個零也是一個binary digit
也給我一個bit information
所以呢這些既然每一個bit 都是給我一個bit information 
所以呢我就是每一個bit 就是帶一個bit information
ok 所以呢平均起來我每一個symbol 
或者每一個bit
每一個symbol 或每一個給我的就是一個bit 的information 
那這裡你比較容易可以想像為什麼它要用information 量也要要這樣樣取然後也叫做bit 
因為它就是一個bit 
這一個bit 就給我一個bit information
那頭第二個例子呢是我如果有四種的話
那麼就變成我有四種
那麼我有這四種四種的時候呢我們說它各是四分之一的機率
m 一m 二m 三等等等等
那他呢可以有四種
也就是說有x one x two x 三x 四有四種
如果這四種的機率各是四分之一的話
那你可以想像這四種既然有四種不同東西
我可以其實它就可以分別代表零零零一一一一零
代表這四個兩個bit pattern 
那麼因此呢如果它們各是四分之一的話
我也很容易算它們每一個給我的information 量呢就是四分之一的log 嘛
我一樣的就用剛才的那個式子
去算四分就算那個機率的log 嘛
我取log 之後我得到的
就是兩個bit 嘛
所以呢我每一個symbol 
這個symbol 呢
給我的是什麼
它帶的是two bits of information
它這一個帶的就是兩個bit information
剛才根據我們來算就是算它是四分之一嘛
它的機率各是它它機率各是四分之一的話它就是帶了兩個bit information 
那麼那麼因此呢那我我現在每一個都是都是兩個每一個都是兩個bit 所以平均還是兩個
於是呢平均起來呢每一個都是帶了兩個bit information
那麼因為它帶了兩個bit information
所以呢你也可以很容易想像
欸我就是其實就是每一個相當於一個two bit pattern
那它就是等於是這個是零零這這是零一
它每一個就是帶了兩個bit 
那所以從上面這兩個例子來看的話
是比較容易想像它為什麼要取bit 的這個名字
然後它為什麼要用log 二
為什麼用二來做這個log 的基底哦
為什麼要用二來做這個log 的基底然後叫做bit
這樣比較容易想像
因為這樣就跟我們平常的binary 平常的這個binary 的這些哦想法是一致的
不過當然剛才這兩個case 都很簡單
因為他們都是是機率都是一樣的
就是二分之一或者四分之一是完全一樣的
如果不一樣會怎樣呢
我們底下的第三個例子就是如果不一樣的話
如果不一樣的話我們舉例來講
像這個例子就是不一樣的
那麼如果說一個是四分之一一個是四分之三
那這裡應該會多一點零
這樣子
那麼我零的機率是四分之一
一的機率是四分之三
如果是這樣的話呢你也可以算算看那麼零給我幾多少的information 呢
因為零是四分之一呀
這就如我們剛才所說的因為零比較少
所以我如果看到一個零的
話我的information 給我看到information 是比較多的
那麼因此呢你這樣一算的結果你發現那個零給我就是兩個bit information
所以呢譬如說這樣應該講說這一個binary digit
也就這一個bit 
零是一個bit 阿
那這一個不過這個是一個binary digit
它呢它給我多少
兩個bit of information 
因為這個比較少看到難得看到一下
它其實給我的是兩個bit information
那反過來呢一比較多呀
所以呢你如果看到一的話呢
一的information 就少啦
譬如說這個一的話呢
它只給我多少呢
這一個binary digit 
這一個bit 這是一個bit 這也是一個這個bit 呢只給我多少呢
你用四分之三去算的話呢
發現取四分之三的log 之後只有零點四二個bits of information 
ok 所以呢這個呢是只有它的information 就很它只有零點四二bit 
它是兩個bit 
它有零點四二
它是兩個bit 
零點四二兩個bit 零點四二等等等等
這樣子
那這樣子有的帶的多有的帶的少
那平均帶多少呢
那你就可以算它的平均就是這個h of s 
那這個h of s 其實就是
零是兩個bit
可是呢它的的機率是四分之一
一是零點四二個bit
但是呢它的機率是四分之三
對不對
你這樣算起來它平均是多少呢
算起來是零點八一
ok
我這個這個零的話呢它有兩個bit information
不過呢它的機率只有四分之一
一的話呢只只有零點四二bit information
但是呢它有四分之三
平均起來是零點八一
那什麼意思呢是變成說呢
現在的in average 那麼每一個every bit 這個bit 我們寫清楚是binary digit
每一個binary digit 呢只給我多少
零點八一個bit of information
換句話說平均起來每一個bit 只給我零點八一個bit information 
跟剛才每一個bit 給我一個bit information 是不一樣的
在這邊的話
二分之二分之一的時候呢每一個bit 就給我一個bit information
我現在呢因為一個是多一個是少
結果每一個bit 呢只給我零點八一的information
就差了阿
那這個就說明它為什麼這樣define 
它其實是在說明你的每一個bit 
每一個binary digit 這個bit
它到底給我的information 是不是充足
還是是不是efficient 
那顯然呢當你是各二分之一個時候
一個bit 就是是一個bit information
可是你如果不是各二分之一而是一個多一個少的話呢
一個bit 帶了info information 就不到一個bit 了
那麼從這個引申下來
其實就有一個非常有名的在information theory 裡面有所謂的叫做binary entropy function
我把這個擦掉了哦
這個應該知道了沒什麼難的
所謂的這個binary entropy function
這個意思我們待會也會再解釋不過簡單的講就是說假設我現在這個假設我現在的這個送出來還是只有零跟一
然後呢譬如說一的機率是p 
零的機率呢是一減p
p 現在是一個random variable 哦
p 是一個variable 從零到一之間
一個是一一個是一減p 
那麼這個時候它的這個h of s
就是我們這邊講的這個東西
平均每一個symbol 給我多少information 這個東西呢
你就會發現它就是我們講的p log p 
加上一減p log 一減p
就是這個
這個東西p log p 加上一減p log 一減p 
也就是這個log p 是一的機一所帶的information
乘上一的機率
這是零所帶的information
乘上零的機率對不對
那其實這個就是我們剛才在算就是在算這個式子
那你如你如果把這個整個都畫出來的話呢
就得到一個這樣子的圖
一個這樣子的一個對稱的圖
畫的有點不對稱不過應該是對稱的
零到一
橫軸是p
縱軸呢就是在零點五的地方
是一點零
那這個縱軸就是這個bits 
就是information 的量阿of information 
縱軸又是bits of information
那這個就是我們剛才的剛才講的所有的example 都是這裡面的圖這裡面你一個點
譬如說我們剛才擦掉的就是零跟一各是零點五
就是這一點
當p 等於零點五的時候
一減p 也是零點五
所以呢也就是零跟一各是二分之一的情形
當零跟一各是二分之一的時候呢
每一個binary digit
每一個bit 帶的量
就是一個bit 的information
就是一個bit information
那我現在剛才這邊的這個例子是四分之一跟四分之三
一是四分之三零是四分之一
所以呢是相當於p 是四分之三
零是四分之一
那就是這一點
零點七五
那這樣子算上來的話呢
你得到的是多少呢就是零點八一
就是我們剛才那個零點八一的那一點
是不是零點八一
就是這個零點八一的這個這個例子
你如果一個是p 是四分之p 跟一減p 一個是四分之一一個是四分之三的話就是零點零點七五的地方
或者是這邊也可以
零點二五也可以
都一樣都是零點八一
那同樣呢你也可以猜得到
這點是哪一點
這點是零
p 等於零也就是說呢根本就沒有一
全部都是零嘛
也就是根本就是零零零零零一整串都是零
一整串都是是零的話就是我們說它不帶information
因為我都猜的出來後面一定是零
每一個我都猜得到它是零
根本不要看了都是零
所以你給我一個零我沒有給我任何東西
所以都是零都是沒有information 的
所以呢它的總共的info information 量就是零
所以在在縱縱軸上就是零的位置
同理呢這一點呢是p 等於一
就是整串都是一整串
都是一的話
這個任何一個一也不給我任何information
所以它給我information 量是零
ok
那這樣我就不知道一個一個整個的一個一個function
變成是一個function of  p
那麼於是呢這個時候剛才的這些這個例子跟這個例子
都是它的一個special case
對不對這個例子是指各二分之一的時候
每一個就是一個bit information
那這個例子呢是說一個四分之一一個四分之三的時候呢
給我的是零點八一個bit information 等等
那這樣就可以發現呢在這個case 而言
那麼只有有零跟一各是零點五的時候
是最efficient
因為每一個bit 就給我一個bit information
所以呢這個是在是在它的頂端是最高的那一點
你如果不是各二分之一而是一個多一點一個少一點的話呢
in average 你所帶的information 是減是降低的
是比較不efficient
所以呢只有在最只有在零跟一各是二分之一的時候它給給你帶的information 是最efficient
然後一個bit 就是一個bit 阿
這是為什麼它叫做bit 的的意思
它取了這個取了這個這個量我用bit 來做information 的量的原因
那就是這樣的意思
那麼也因此呢我們這邊剛才看的這個例子如果有四個的話呢
你如果各是四分之一的話就相當於兩個bit 阿等等
好那如果這點可以了解的話我們可以再進一步衍申出更多的東西出來
那麼一個最容易想像的情形就是這樣
假設我有三個好了
我有三個會怎樣
譬如說我的m j 我會有三種
x one x two x 三
如果我有三種可能的話
那我說x one 的機率是p
x two 的機率是q 
然後呢x 三的機率呢就是一減p 減q 
如果是這樣的話我我也一樣可以像剛才一樣算這個算這個function 
只不過現在剛才是一個是p 一個是一減p 
所以是一個function of p 是比較好算的
現在我有一個p 有一個q 有兩個變數怎麼辦呢有
兩個變數我沒有不不太容易畫那樣子
所以我可以想個辦法就是把p 固定
p 固定的話呢就變成q 變成variable
所以q 呢我就讓它從一減p 哦從零到一最小可以是零
最大是到一減p 
如果這樣的話我可以畫一樣的可以畫一個q 的圖
我仍然可以算這三個三種symbol
這三種可能的symbol 所造成的平均每一次看到一個東西的的時候它的information 量是多少呢
這個答案我們不去算了不過答案你可以猜的出來也是這樣
跟剛才那個圖是一樣的
所不同的是minimum 那點不是零而已
這個就是零跟一減p
也就是說我的q 的值
是在從零到一減p 的中間
那麼q 的值在零到一減p 的中間
那麼這個那麼在什麼時候最大呢
是在二分之一一減p 的時候最大
那什麼意思呢跟剛才那個完全一樣
譬如說這點呢就是x one 的機率是p
x two 的機率呢是二分之一減p 
x 三的機率也是二分之一減p 
那就是這一點
那如果這一點是什麼呢
這一點就是q 等於一減p 啦
那麼因此呢這個x 三就沒有了
所以呢就變成一個是p 
一個是一減p 零
這個根本就沒有了給我個x 三根本不會發生
只有這個x one 跟跟x two 會發生x 三根本不會發生了
這就是在這裡
那反過來呢如果在這裡的話呢
那是p 零一減p 是x 二根本不會發生
x 二根本不會發生
x 二根本不會發生就就是x 三是一減p
那在這裡面你也可以發現呢這個簡單的現象就是說
我如果把x one 的機率固定的話
這個最可以給我in average 每一個symbol 給我最多information 是在二跟三機率相同的時候
在二跟三機率相同的時候給我最多的information 
那如果說是什麼時候最少呢
根本就沒有三
或者根本就沒有二
這時候就只有兩種東西了
這裡根本沒有三的話只有兩種
這裡根本沒有二也只有兩種
如果只有兩種的話我的information 就少了嘛阿
所以它就會減減少就變成這樣
那這個圖的情形呢畫的呢
其實跟那個是完全一樣
只不過我們這邊因為有三個變數
所以呢有兩個變數所以我們只固定其中一個來畫
好如果這個圖可以想像的話
那我們可以再進一步衍生
你就可以想像哦如果說是我這邊是假設x one 機率是固定
這個時候機率最大是發生在二跟三機率相同
都是二分之一減p 
同理我也可以把q 固定
q 固定的話呢它跟什麼時候這這這個information 量最大呢
是它跟它相同都是一減q 的時候
我也可以把它固定
那麼什麼時候最大呢是它跟它機率相同的時候等等
以此類推你就可以猜的出來
真正information 最大的時候
就是這三個各三分之一的時候
對不對
也就是應該要這這三個機率各是三分之一的時候
如果各是三分之一的時候它們機率是最大
那如果可以這樣子衍生的話
我們就可以這些都是可以證明的
不過在information theory 裡面都有證明
不過我們這邊就不證
我就這麼用嘴巴講一下
那麼我們就可以得到一個重要的結果
那就是這邊所說的底下這個這個講的就是這就是這件事情
也就是說
你如果總共有m m 是什麼
m 就是我所總共的總
我這邊x 一x 二x 三總共有大m 個嘛
大m 是這裡symbol 的總數
所以呢剛才這邊的這邊m 就是二
我們那邊舉例的時候m 就變成三
這m 就是這個東西
那麼如果m 是這個的話呢
那麼你什麼時候這個就是h of s
就是我們這邊所在算的這個所謂的這個source 的entropy 的這個東西
的這個東西這個平就是平均也就是h of s 就是這個entropy
也就是我們所說的平均每一個symbol 到底給我多少information 的這個東西
那麼它的值呢應該是它的maximum 發生在什麼情形之下
發生在我們每一個機率都是m 分之一的時候
那麼剛才在這邊看到的時候呢就是什麼時候它的它的peak 在哪裡
peak 在我這兩個都是二分之一的時候
因為我大m 這裡在我這個case 大m 等於二嘛
在我這裡大m 等於二所以呢
就是它的peak 發生在各是二分之一的時候
那我們這邊講如果這個這邊舉舉的case 是大m 等於三
這邊講的例子是大m 等於三
於是這個peak 發生在什麼時候
我去同時調p 跟q 的話
會在p 跟q 各是三分之一的時候
那個時候會最大
那如果是四的話呢
那我我最大的應該就它們各是四分之一的時候
那就是等等
那麼因此也就是不管是大m 等於幾
它的這個上限都它的peak 就是這一點
這一點
或者說是這一點
都發生在它們equally probable 的時候
或者說呢就是h of s 
is maximum s when all symbols are equally probable
也就就是說我的p 的x i 
都等於大m 分之一
那麼我的每一個symbol 它們的機率都一樣
就是這邊的的這m 個symbol 呢它們機率要完全一樣都是m 分之一
當它們的機率有都完全一樣都是二m 分之一的時候
它的這個它的這個這個entropy 呢是maximum
那個時候呢就是各是m 分之一
那如果是m 分之一的話呢那這很容易易算
我們帶回去算帶回去這個式子的話
你就知道
每一個的機率是log 的m 分之一
然後呢它們的然後平均的話呢
這個這個然後平均我我我一樣都是都是乘以個m 分乘以m 分之一嘛
然後加起來加m 個嘛
所以平均起來就是這個值
平均起來就是這個值
因為每一個每一個都是m 分之一嘛阿
因為每一個就是log 就是這個就是log m 嘛
負的log m 嘛
每一個就是log m
然後再平均起來還是log m 嘛
所以呢我所得到的就是log m 
所以它的上限就是log m
就剛才而言就是log 二就是一點零
那如果是三個的話呢
那這上面就是log 三
就是log 三
如果四個的話呢就是剛才這個例子的話呢
那這個例子的話呢就是log 四就是二嘛等等
所以呢我的上限就就是log m
發生在什麼時候發生在每一個symbol 都equally probable 的時候
就是上限就是log m 
那它的下限呢就是零
你只要有其中有一個的機率是一別的都是零的話
像這一點
它全部都是這就是這個全部都是零的情形
它根本就沒有information
這點就全部都是一的時候它也沒有information 等等
那其實你這個m 個裡面只要存在某一個x j 
存在某一個x j 它的機率是一
而其他的所有的j 都是零的話
那它就會變成零
那麼因此它的上限下限就是這樣子
那這其實是information theory 裡面非常重要的定理
是該證明的不過我們不證它了
我們就這樣講一下
那這個在這個情形之下的話
那你會說ㄟ那這邊不是零呀
這邊有有一個不是零的值
是因為我我永遠有p 嘛
我這邊已經讓p p 等於這個有一個p 在這裡
所以這個其實就是你加起來不不是零就是因為有個p 的關係
那你所以像這個情形的話
一個是p 一個是一減p 一個是零
所以在這裡並沒有任何這三個裡面沒沒有任何一個機率是一的
所以它不會在零的地方
這裡也是一樣
沒有任何一個機率是一的
所以呢並不符合剛才的這個下限的條件
所以它們都不是零
它是一個有一個非零的值
有一個非零的值因為它沒有一個是機率是一的
那麼剛才在這裡是零是因為它真的有一個是機率是一的的關係阿等等
好那這個是一個非常基本的
關於information 量的一個這個簡單的這個這個這個說法
那有了這個之後我們底下就可以用它來推我們底下要做的事情了
好我們先在這裡休息十分鐘
ok 
我們剛才在解釋這個這個式子
就是這個entropy 這個東西
那麼它的上限是log m 
下限是零
上限發生在這個狀況
它們是equally probable 
下限發生在只有一個是一其它都是零的情形
這是什麼意思我們如果再衍生一下的的話
就可以這樣子來看
我現在如果把這個看成是一個機率p of x i 
這是x one x two 等等到x 的m
那麼這個就是我的我的這這m 個我們之前講的這m 個可能的值它們的機率就是p 的x i
那麼這樣子畫是什麼case 呢
這樣子畫的case 就是我們剛才講講的發生上限的情形
也就是也就是在嗯這裡
那麼如果他們各是m 分之一的話
那這個時候我的我的這個所以每一個機率各是m 分之一
那這個時候我的entropy 呢其實就是log 的大m 
那這是它的上限
下限是什麼呢下限是只有其中的一個是一
對不對其中那個是一
其它全部都是零
其它根本不會發生那
如果只有那個是一的話呢其實它不帶任何information 因為你猜都猜得出來這樣子
所以這是x one 這是x m 
那只有其中的某一個x j 是一
那這個呢就是它的的entropy 就是零
那就是我們這邊所看到的下限
那在這個上限跟下限的的中間那當然還有很多個case
你可以想像的是哪些case 呢我們舉例來講
它不再是完全一樣m 分之一而是有多有少
但是都有
有多有少
這樣我總共這是x m 這是x 一
它有多一點有少一點這樣子的話呢
那它是在比它那這是什麼情形
就好比我們剛才看到的
從這點向下移動嘛對不對
這點pick 的這點就是剛才的上限
就每個都是m 分之一的話等於是在這點上面
那你現在有有有有多有少它就在這邊來在這邊來
所以以呢平均起來就在這些地方
所以那就是這個情形
那再來的話呢
它可能慢慢集中到譬如說說若干個這附近的
譬如說這裡有幾個或者這邊有幾個
其它的很少很少
這個是x m 
這個是x one
如果它集中在少數幾個而多數都變成零了的話
那就更少一點
那如果再過來的話呢
它可能就只集中在在這附近有一點
其它都沒有了
這是x m
這個x one 
ok那你可以想像差不多是一個這樣的情形
就是由完全的uniform distribute 的
簡單講就是uniform distribute 的
或者是equally probable 的時候
這是最大的
然後慢慢的它開始有大有小了
然後慢慢集中在一個比集中在少數的地方了
然後別的地方會變成零了
最後集中在一個了
這就變成零
所以這就是它的這個分佈的情形
那你如果是這樣子看那你就可以猜的出來
它這個東西其實有非常豐富的意思
它可以拿來做很多用途
其實我們講了半天我們底下已經也就是要用這個
用這個的意思你可以看成是在最底下的是什麼是純度最大
純度最高
什麼叫純度最高就是說它的distribution 集中在一點上面
其它都沒有
這非常純的
這純度最高的
那當然然在上面就是純度最低的就不純了嘛
就完全的是完全沒有有純度可言
那在這上面呢這什麼我們可以說是亂度最大
所以這個可以描述一個distribution 純度或者亂度
那麼這邊是最亂的嘛
最完全最混亂的
你每一個都有一樣的機率
而這邊是最不最不混亂最純的
完全就是一種情形
那你也可以說這個是是這個不確定性最高
因為它的它的每一種都都一樣的可能所以最不確定嘛
這邊是最確定嘛
這個這個底下呢這是確定性最高
那我也可以說是這個它的distribution 的這個分這個嗯等等
那所謂的這個就是uncertainty 
那這所謂的純度就是purity 
對不對
這個那所謂的純度就是最高就是指highest purity 
那亂度最大就是highest randomness
這是random 的情況最高
所謂不不確定性就是uncertainty 最高對不對
那然後我也可以用這個來說就是一個這個the spread of a probability distribution 
我也可以說它就是代表the spread of a probability distribution 也就是說它的分散集中的的程度
一個機率的分的分分佈的分散集中的程度
分散或集中的程度
ok
那麼因此呢我們我們後面會一再的用到這個東西
其實是就是在用這件事
就是我只要算出這個東西來的話
其實就是給我一個distribution
給我一個p of x i 這個東西的話
那我來算這個東西
其實算出來就可以告訴我either 是算它的randomness
或者是算它的uncertainty
或者是算它的purity
或者是算它的spread
那這個是最不spread 嘛這是最集中在一點上
那這是最spread 的情形
所以算它的這個這個分散集中的程度等等
那我都用這個來做
好有了這個之後
底下這一頁倒是沒有太多複雜的東西
這個看起來很複雜其實倒只是一些簡單的數學
它在講兩件事情
這都是後我們後面要用的現在利用這個機會就講的
那第一個所謂的jensen's inequality
這個沒有什麼特別只是我們在算的那個東西其實就是p log p 嘛
你現在如果再回去看剛才這個式子
我算的這個東西哦ok 我們還漏了那麼為什麼這個東東西叫做entropy 
那我們現在取取這個東西的名字叫做entropy 的意思就很清楚
entropy 原來是熱力學裡面的一個名詞
那你如果去看熱力學裡面的講它也就是在講那些分子它的這個混亂的亂度
所以entropy 其實就是在講亂度
那麼因此呢
那麼在熱力學裡面的那個entropy 它它計算公式也就是這個
就是這是一個機率
p of x i 這是一個機率的distribution 就是這個東西
這個東西就是p of x i 
那麼你如果把p 的x i 乘取了log 之後再乘以p x i
然後去加起來的話
就是p log p 嘛
p log p 然後summation over i 的這個式子
在熱力學裡面
也就是用這個式子來算
的這個entropy
所以這個就把熱力學那個公那個名詞借過來
那麼這邊的意義其實跟熱力學裡面一樣
也一樣是在描述亂度跟純度
描述它的uncertainty 等等
都一樣的的意思
那麼因此
我們這邊才會有這個說法
就是說呢
它也叫做degree of uncertainty
它也可以說是quantity of  information
也就叫做entropy
那對一個random variable 而言呢
有一個distribution 我都可以這樣算
然後都可以得到這個東西來算它的這些東西等等
好那麼有了這個的話呢
