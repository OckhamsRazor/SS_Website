這個我們底下還會再說到不過你可以想到就是
那我現在如果知道這一百個
我分成一百群做成一百個 language model 的時候
它們每一百每一個有各自的 topic 
有各自的 concept 
因此 depends on 你現在講什麼我就用哪一個
那這個就是 language model 的 adaptation 
同樣我也可以做 information retrieval 
我今天我要找什麼東西的時候
我如果把這一百萬篇文章先分成一百群
你輸入一個一個 instruction 
我先看你要找找的是哪一群
再從那一群裡去找比較好找嘛等等
所以你文章分群的話也可以很有用的
那這些都是我們的 linguist processing 
那文章分群在這邊的這邊的這個剛才已經講過了
那我的做法我就是找文章之間的關係
這也就是剛才一跟這個完全一樣的
求 cosine theta 就是做內積
然後除以它的長度
只不過我現在每一個 v 呢是這個 v 
這個 v 是什麼v 是這個 s 乘上這個 v j 
所以呢就變成這樣子
嗯那這就是文章分群
那同樣呢我現在要做搜尋
我要做 information retrieval 的話
你說我要找什麼東西的時候
我們原來像 google 現在的基本上現在是一個 lexical matching 
就是在 match 它的 word 
你說我要找九一一恐怖攻擊它就去找所有的九一一恐怖攻擊的
但是如果那篇文章裡面沒有講九一一恐怖攻擊
可是裡面講了賓拉登講了阿富汗
它不知道
那你如果有這個的話你就知道了因為你現在是 concept matching 
所以你可以去 match 它的 concept 
所以呢你的相關的文件你是可以找得到的
只要有相類似的 concept 
它們不需要有 exactly same words 
也可以找得到
那這個做法怎麼做
簡單的解釋就是說你把你的 query 當成是一個新的 document 
放進去
然後去量它的 similarity 這是一個簡單的例子
什麼意思就是說你的我現在要找我要找賓拉登恐怖攻擊什麼你就把你那個文章
把你這句這個 query 你把它輸入這個 query 
當成是一個 document 
你就去看那個 document 跟其它所有的 document 那個相似度
就放在那個空間那個那個八百維的空間裡面去算
它跟誰像那就是誰嘛
喔等等這就是做這個 information retrieval 
好那這時候有一個重要的問題我們要解決的要說明的就是所謂的 fold in 
什麼叫做 fold in 
fold in 是說我們這這整套是假設
我上網找到十萬篇文章
我有一個辭典是兩萬詞我都已經做好了之後
我整個這樣做做完了
做好之後我得到這一套
但是呢我網路上不斷有新的文章出來啊
今天每天多了一萬篇新的文章我這新的文章怎麼辦
我每次新的文章來我要重新 train 一次
那這樣不是累死了嗎
所以最好是我 train 好之後新文章可以放進來
我把新的文章塞進這個 model 把它放進來就好了
我 as long as 新的文章所描述的這些 concept 沒有新的
如果有新的 concept 的話那當然你得要重新把它弄進來了
但是如果說你描述的新的文章沒有新的 concept 
只是原來的話
你就把它塞進來就好了
怎麼塞就是這邊所謂的 fill fold in 
就是把新的文章只要沒有新的 concept 的話
就是 assuming 它們沒有新的 concept 所以這些 u 跟 s 都不變 
u 就是左邊的這個 matrix 
左邊這個 singular matrix 
v 就是右邊singular 這個左右都不變
那麼中間這個都不變
如果這樣子的話呢我新的怎麼做
我只要讓這個再多增加一維
這個叫做 d d p 
我剛才如果 d one 到 d n 是十萬篇文章的話
我新的文章進來就是 d 十萬零一篇
那個 p 就是在後面我多加一維就好了
如果這個多加一維的話呢
那這邊其實發現就是這邊多加一維
所以這就是 v p 
這邊多加一維意思就是這邊多加一維那這個不動
這個不動那就是我們這邊所講的這件事情
ok 就是說你現在如果一篇新的文章進來 
outside outside of 原來那個 training corpus t 的話
我們只要假設如果它的整個 language pattern 
跟它的 concept 都是不變的話
那麼我其實只要把新的放到這來
因為新的那些文章我馬上可以數一數它裡它裡面有哪些 word 
我已經把它裡面哪些 word 數一數我就得到一個新的 d p 在這裡
所以 d p p 大於 n 就是指在原來的十萬之外的
譬如說十萬零一
我就排在這裡
排進來之後呢那我整個可以不要動
其實原來的這個 relation 不動只是這邊再多一行
這一行是 v p 就是了
就是這邊的這一行跟這邊這行是一樣
就好像這邊這一行跟這邊這一行是一樣的嘛
所以你這邊多一行就是了
或者說這邊也多一行
這邊也多一行
那麼所以這個乘以這個多的這一行就是這一行
那這一行就是原來的這一行就這樣子
你把這行塞這邊多一行這邊多一行就好了
那麼它們的關係是什麼呢就是這個式子
這個式子其實就是原來這個式子是一樣的
原來的這個式子是說你這個整個的 w 
是 u 乘上 s 乘以 v transpose 
這 u 乘上 s 乘上 v transpose 
那這個意思你可以看成是 
u 乘上 s 乘上這裡面的每一個 column 
就是這裡的那個 column 
就是這個意思 ok 
所以你原來的 w 是它乘以它乘上它
跟它裡面的這個 column 
相當於它乘以它乘上這個 column 
是一樣的意思
那我現在只要把這個 column 換作這個 column 
這個 column 換成這個 column 就一樣了
所以就變成這個式子
所以這個 d p 呢就是 u 乘上 s 乘上 v p 的 transpose 
就是這個東西
它這邊有一個這個 t 還是一樣的
因為在在那篇 paper 的 notation 裡面
這個 v 是一個 row 
你要現在是一個 column 所以就是加一個 t 就是了
所以這個 column 其實就是這個 u 乘上 s 乘上這個
就是這個式子
那如果是這樣的話呢這個很容易求啊
那 u 跟 s 你已經知道啦
所以這個你也就知道啦
那就底下這個式子的意思 
ok 這個 u 跟 s 是我原來 train 好的
新的文章進來這個就數一數有幾個 word 就出來了
所以這個也很容易求
如果這也很容易求的話呢那這兩個都已知那這個很容易求嘛
就是你拿這個式子求一求變成這個式子
底下這個式子跟上面這個式子是完全一樣的
只是說你現在如果 u 是已知 s 是已知
 d p 是很容易求的也是已知
那你 v p 怎麼求就是這樣求
只是在解這 equation 而已
那這個也沒什麼特別其實就是你要的這個v 這個是 v p 的
這個是這個式子是 v p 的 underline 
這個 v p 的 underline 呢就是這個 v p 乘上 s 
就是這個 v p 就是這個乘上跟 s 相乘
那其實是什麼呢
你可以看其實就是這個式子重新解一解而已
我只要把它的這個 transpose 這個 transpose 
然後呢這個這個 transpose 就變成 v p s 
那因此這個 transpose 搬過來就變成 d t b 五
就變成這樣的意思
因此我就得到一個這個或者說這個
就是一個 r dimension representation of the new document 
所以一個新的文章把它塞進來這樣塞就可以了
直接帶到原來的關係塞進來就可以 work 
那這個意思其實也可以說就是把你的 d p 投影到這個跟 u 去相乘
就是投影到新的空間
你可以想像我這個 d p 這個 vector d p 是什麼
是在這裡的一個新的點
它在一個新的點進來
不過這點我其實可以對應到這邊來
看這點對應到哪裡來就是這點
就把這點對應過來就是了
那怎麼對應
其實就是在每一個上面做投影對不對
我拿它對每一個 e i 去做投影
那這件事情其實就是這件事情
你看這個 u 其實就是 u 其實就是一堆 e i 嘛
所以你去跟它去做所以你這個 d p 去跟這個去做去相乘
其實就是它分別去做內積
它跟它一個個分別去做內積它就分別在做投影
所以投出來就是那裡面那一點
所以就得到
所以它跟這八百個去做內積就是這八百個投影
就得到八百個維就是這裡的八百個或這裡的八百個
就這樣的意思
所以這樣我就 fold in 進去了
