第三部份就是 language  model
我們在第一週的時候曾經說過一下
也就是說並不是我們所想的那麼容易
而是很多聲音都很像
然後很多聲音都會搞不清楚
我們舉例來講
這個假設這是時間你這個一個位置的聲音進來的時候
我們可以把裡面的很多的基本的單位音都變成一個 Hidden  Markov  Model
記得我們那時候說譬如說
this  is 你可以想成是這個這是一個基本單位音
這是一個這三個拼成這個
然後呢再來恩這個可能是這個
this  is 等等
那也就是把每一個基本的單位音都做成我們剛才講的 Hidden  Markov  Model
於是呢你可以判斷說那裡有一個這個音那裡有一個這個音這裡有一個這個音
然後呢這些音呢那那於是呢這些個東西呢拼起來呢是這個字這些個音拼起來呢是這個字等等
但事實上不是這麼簡單因為這個音也跟這個音很像這個音也跟這個音很像
於是呢你這三個可能都很像而這個呢這個跟這個音也很像這個跟這個音也很像
譬如說那麼你都可以拼來拼去變成很多不同的字出來
那我們舉例來講譬如說這個我們說一句話譬如說 the  computer  is  listening
基本上你講的是這句話
但是呢你很可能因為變成這裡面是很多小的單位音在裡面拼嘛
你可能會拼成譬如說這個是有一個 they 這裡有一個是 come 這邊有一個音是 tutor 這邊有一個這個這個 is 這裡有一個 list 這裡有一個字是 sunny
有很多這種字啊
當你有辨識出一堆這種基本單位音的時候因為都有很多 confuse 的音嘛
所以可以拼出很多奇奇怪怪的字出來
變成說they  come  tutor  is  list  sunny 這也是一句話
那憑什麼不是這句話而是這句話呢
那我們就希望知道到底哪些個字連起來哪些個 word 連起來比較像一句話
那這個方法就是我們算他的機率
我希望我算出來機率是這個 the  computer  is  listening 的這個機率跟另外一個 probability  of  they  come  tutor  is  list  sunny
那我要這個機率要比他大很多才行
因此我到時候就知道呢這個應該不對吧應該是這個吧
那這個就是 language  model 的功能
換句話說光靠前面的 hidden  markov  model 我知道它是這些基本的音
我把它拼出字來可能拼出來完全不對的字
所以呢我要靠這些這些這些字串起來到底通不通來來來算他到底合理不合理然後得到一個比較合理的答案那這就是 language  model 在做的事
所以我們就是要算假設我的一個 word  sequence 一個 sequence  of  words  w  one  w  two 都是一個 word 總共有 r 個 word 構成一個 r 的 sequence 叫做大 W 的話
我就是要算這個大 W 的機率
這個機率就好比是這個這個句子的機率或者這個 word  sequence 的機率
我要他算機率比它大
那怎麼算呢
那基本上你很容易想像這個算法
其實我後面上面的那個數學式子其實就是我們最基本的機率的算法
這個是 word  one  word  two  word 三 word 四 word 五等等等到 word  r
我先算他的機率
有了他的機率之後呢
在 w  one 之後會接 w  two 的機率
然後 w  one  w  two 之後會接 w 三的機率
然後呢一二三後面會接四的機率
一二三四後面會接五的機率以此類推一直到最後會接最後一個機率
這把他這樣一路乘起來那就是上面的這個式子的意思 ok
所以你是先把第一個 w  one 的機率算出來
先把它它的機率算出來
然後呢 given 它之後有下面的一個機率
那 given 這兩個之後有下面第三個機率
given 這三個機率有第四個等等
因此呢就是 given 一到 i 減一之後到第 i 個的機率
然後呢我 i 從二開始一直算到 r
所以後面這一堆乘起來就是我剛才講的這一些乘起來
基本上要算這個東西那這是一個標準的算法問題
只是這個無法算
為什麼無法算
因為我們的 word 很多我們以英文為例
英文日常用語我們每天日常用語的英文大約三萬是免不了的
少一點我們說兩萬五多的話要六萬才夠
depend  on 你要 cover 多少東西
我們以三萬為例
假設你有三萬個 word 的話第一個 word 有三萬種
第二個 word 也有三萬種
這都有三萬種
所以我 r 個 word  sequence 有三萬的 r 次方種
那我隨便舉裡面的一個
總共有 i 個嘛所以就是有三萬的 i 次方種對不對
因為這個有三萬種
有這麼多你要把每一個機率都求的出來會求死人的
而且也沒辦法做
所以這個是一個這是一個很直接的答案
但是不好做那我們怎麼辦
我們就做一個簡單的假設假設說你出現一個 word
只跟前面的 n 減一個 word 有關這是一啊只跟前面的 n 減一個 word 有關
也就是說我不要算那麼多啦
我不要每一次都是從一到 i 減一之後
given 從一開始到 i 減一再去看下一個 i 的機率
這個太多了
我每一次只看前面的 n 減一個因此呢就會變成怎樣呢
我們舉例來講呢我這個時候呢我要看的這個五的機率不是一二三四 given 一二三四之後五的機率
而是 given 譬如說前面的 n 減一個之後五的機率
那六的時候怎麼辦呢我要看六的機率的話呢也是前面的 n 減一個然後看這個的機率
也就是我每次都只看 n 減一個在 n 減一
再前面的我就不看了
好這是基本假設是這樣子的
這個是一個 assumption 沒有理由說他一定是這樣
那我們也知道從我們日常對語言的了解我們也知道不是這樣
那這只是為了所以這個 assumption 其實是不通的
但是為了讓它可以數學可以做
那這是一個這個 approximation 不是真的
那麼呢因此呢我每一次只用前面的 n 減一個來看下一個
因此你看我這邊的不同就是我把這個機率從 i 一二到 i 減一之後看到下一個 i 的機率
我簡化成為不是全部這麼多而是只有從 i 減 n 加一 i 減 n 加二到 i 減一
也就是我只算這是 i 的話這個是 i 減一 i 減二到一直到 i 減 n 加一
到 i 減一為止的這 n 減一個我只看前面的 n 減一個就好了
所以這個我這個機率就用這個機率來替代
那這個等號其實是不成立的
我們只是一個假設或是 approximation
如果這樣假設的話我只算前面的 n 減一個 word
那於是呢其實我每一次其實就是多少個
這邊的 n 減一個再加下一個就是 n 個嘛
所以我總共就是算這樣子嘛
對不對總共就是 n 個這 n 個 word 之間這 n 個 word 會連在一起的
given 前面這 n 減一個下面會有 n 的機率
那其實總共有多少種呢
就是三萬的 n 次方種
那這個數字至少在我的 control 之內我 n 可以小一點嘛
n 可以是比較小的數
那這種東西呢我們就叫做 n  gram  language  model
這個 gram 這個字當初他們用的時候是
grammer 的簡寫grammer 就是文法
所謂的文法的意思解讀成為就是前面什麼字後面要接什麼字
這樣叫做文法所以這個叫做 grammer
所以它叫做 n  grammer 就是我這個 n 個 word 之間的關係叫做 n  gram
那麼如果是這樣的話呢n 等於二的時候呢叫做 bi  gram 就是 given 前面一個 word 會出現下面一個 word 的機率這叫做 bi  gram
n 等於三的叫做 tri  gram 就是 given 前面兩個 n 等於四叫做 four  gram  given 前面三個等等
那那當然也可以有 uniform 當 n 等於一的時候就只算一個 word 的機率
所以我們就有這些這所謂的 n  gram
那通常我們在做的時候絕大多數簡單一點的情形我們做到 tri  gram
就是這三個的那麼比較複雜一點的就做到 four  gram 就是做四個的
你可以想像 bi  gram 是兩兩相連的機率有了這個會出現這個的機率
tri  gram 是三三相連有了前面這兩個會出現下一個的機率
那 four  gram 呢就是有前面四個等等我們大多數做到 tri  gram 複雜一點做到 four  gram 再多大概不做了因為再多太多了
那麼於是這些機率怎麼來我們用一個 training  text  code  database
也就是說你去上你最簡單的去上網嘛他們今天最常用的辦法是上網
你上網可以抓上千千萬萬個網頁每個網頁上都有一大堆文字就用那些文字去算
就可以算前面有哪幾個 word 會出現下一個的機率就可以算的出來
那麼我們以這個 tri  gram 為例那麼這樣的一個機這樣的一個 word  sequence 的機率怎麼算就是底下這個算法
那這個算法其實很容易看就是這樣
比如說這是 w  one  w  two  word 三 word 四 word 五 word 六一直到 word  r
你第一個機率是他單獨出現的機率第二個呢是有了他之後出現他的機率
從第三個開始呢就是三三相連了就是這個有了他之後出現
有了一二之後出現三然後呢有了二三之後出現四
有了三四之後出現五有了四五之後出現六等等
我每次都只看兩個不是這個都全部從頭看這裡全部從頭看我現在不是了我只看前面的兩個的下一個
這就是用 tri  gram 來算的方法就變成這樣所以呢這個是第一個這個 w  one  word  one 的機率
然後我這個呢是 word  one 之後會出現 word  two 的機率
之後呢我就是有出現 i 減一 i 減二之後的 i 的機率
所以就是兩兩兩個出現第三個等等這樣出來的
那這樣子我就用這個方法算就可以算到一個 word  FREQUENCY 的機率
也就是我這邊講的是 they  come  tutor  is  list  sunny 的機率呢還是 the  computer  is  listening 的機率等等
好那這個是這個我們用 n  gram 來做 language  model 最簡單的解釋是這樣子
那怎麼來算這些東西呢我們在下一頁有說那基本上這個算法很簡單就是這幾個式子
