那後來這個方法有再進一步的版本就是我們把每一個詞的意思拆開來
因為我們很多詞是有好多意思
譬如說就這句話而言你知道這是一個單字詞
這個單字詞在這裡的意思跟這裡的意思是不一樣的
那麼因此到底哪一個詞歸哪裡
那麼在剛才的方法裡面的話ok 
這個會這個哦這個是一個非常有非常豐富的詞類的詞
它可以是這個意思可以是這種詞類可以是這個詞類可以是這個詞類
有很多種
那它必須要有另外一個詞跟它完全一樣有那麼多的才跟它變成同一個class 
這樣太複雜了
所以一個辦法是說
你就讓一個詞你如果它有不同的詞類的話呢就給它more than one class 
那你乾脆就是就是這兩種通常是一樣的是
那這兩種是通常是在一起的
這些詞類是同樣的這些詞類都是同樣的詞會有的那我就乾脆那同時是它是它的話我乾脆把
這個詞在這裡也放在這裡讓它出現在兩邊
啊這是第二種方法
就是我的詞呢我之前的剛才那個法是每個詞只放在一個一個class 裡面
我現在可以讓它我現在可以讓它放在兩個以上的class 裡面所以同一個word 可以放在兩個以上的class 裡面
然後就那樣去做
可是這個時候做你的language model 要很難就很難train 了
為什麼
你要知道它是屬於那一個詞類
我們剛才是剛才是凡是有這個詞我就是就是照樣去train 了根本不管因為那個詞就是那個詞類
現在不是了
因為這個詞有的時候是這些詞類有的時候是這些詞類
那你怎樣train 它的language model 
你要train n gram 的時候
對不對
這個後面這後面這兩個後面接這個
那你要你你現在變成要知道
你要train 你要知道說它是什麼詞類
它是什麼詞類你就知道這兩個詞後面會接什麼詞類
於是你的training data 裡面要知道每一個word 是什麼詞類才行
那就是所謂的tag corpus 
這個tag 這個tag 這個字的意思是把這個詞類掛上去
那啊語言學界他們很早就做這件事
就是你要有一個自動掛詞類的方的程式
你在一篇文章裡面每一個詞屬於什麼詞類自動把它掛上去
那麼你要用這種掛好詞類的的文章去train 
那他就會知道他什麼詞類
所以你就知道什麼詞類後面什麼詞類會接什麼詞類
那這樣子的話呢可以做
不過這個是工程比較浩大
因為你你的這個要把詞類掛進去才能train 
但是這樣出來的language model 會好嘛
因為它把哪個詞類後面會接什麼詞類都己經算進去了
那這樣子的這個嗯
那我們說一下你怎樣掛詞類
掛詞類的辦法其實也是一樣
你第一個呢就是
所所謂的pos 的tagging 就是掛詞類
你就是為句子裡面的每一個詞
你就是為句子裡面的每一個詞都把它的詞類標上去那麼你標詞類的方法呢基本上就是有一個詞典
都把它的詞類標上去
那麼你標詞類的方法呢基本上就是有一個詞典
每一個詞會有那些詞類你都有了
有一個詞類的n gram 
就是說你有一個詞類的n gram 
什麼詞類跟什麼詞類後面會接什麼詞類
這是詞類的n gram 
那麼通常因為詞類的總數有限
你的n gram n 可以比較大
我的word 有有六萬個所以我的trigram 沒辦法太大
可是如果我的詞類才兩百個嘛
我可以變成four gram five gram 
那麼它們通常詞類的n gram 是做到
至少做five gram 
因為你總共只有只有幾十個到上百個
所以不難做
你做到five gram 的話呢
你就有這個詞類的之間的關係
全部都有那就是所謂的pos n gram 
你如果有這個pos n gram 的話
那麼這pos n gram 怎麼來是要用人先用人去標一次嘛
譬如說你拿一個database 
是用人標好的詞類
你就可以train 詞類的n gram 
然後呢你現在就可以做這個自動標詞類的工作
那你可以想像是這樣子
就是你現在有個詞典
這個詞
它有這個詞類這個詞類這個詞類
這個詞它有這個詞類這個詞類
這個詞它有這個詞類這個詞類這個詞類
你有一個這樣的詞典
這個詞典就是我這邊所謂的lexicon of words with possible pos 
就是我都有這個詞典
我有這個詞典之後呢
我今天任何一個句子出來
這個詞可以是這三種詞類
這個是可以是這兩種詞類
這個是可以這三種詞類等等
那你可以想像我是一個這樣子的net work 
那到底是哪一個呢
我用我的n gram 來算
我現在有的詞類的n gram 
你就知道這個後面應該是
如果是它的話它後面接這個接這個
的機率是多少對不對
那如果是它的話呢它後面接這個
後面接這個後面接這個機率是多少
你這個都可以算嘛
你就就可以根據這個詞類的n gram 來算出來
這個是自動掛詞類的方去
就是根據這兩樣東西
就是一個是有標詞類的詞典
一個是這個啊詞類的n gram 
你根據這個就可以把自動標詞類
你自動標詞類標好之後你就有一個很大的database 
來可以做自動標詞類的的的標好詞類的語料庫
你就可以train 這種
好以上大概這個這個也是另外一篇碩士論文你如果有興趣的話可以查得到的
那我們要講一下就是說你
其實這個class based 跟word based language model 其實是可以整合的
那什麼意思呢
你基本上來講
這個word 以word 來做的n gram 
跟以class 來做的n gram 哪一種比較好
其實是如果是真的是word 的你train 得好的話word 比較好啊
因為它比較精緻
那class base 是比較粗的
因為它把一大堆class 搞在一起嘛對不對
你可以想像
class 是比較粗的language model 
因為你把一堆東西搞在一起
你想這個比較細緻還是這個比較細緻
當然是這個比較細緻啦
你如果這是哪一個word 跟哪一個word 之後會碰見
你如果這個train 得好的話
這個鐵定比這個好啊
這個是把一大堆word 跟一大堆word 跟一大堆word 放在一起了
所以這個是比較粗的
只是說這個的data 可能不夠你可能train 不好對不對
是data 不夠你train 不好的問題
所以這個其實這個class base 是有點像smoothing 的效果
train 不好的話不如用這個嘛
可是你如果train 得好的話這個顯然比這個好嘛
這是我們必須了解這一點
所以呢class 的不一定表示說一點會比word 好
應該是說word base 的是more precise 
如果是常用詞
你的頻率夠高的話
你count 的數目對的話
其實是比較好的
只是說當你data 不夠的時候
你可能需要用class 
所以呢比較好的辦法應該是把這兩者整合
你back off 到class 
如果count 不夠的話
就是說你其實是可以做就是只要count 的數目夠高
我就用word 的的的n gram 
數目不夠高我才用class 
那這是一個比較最好的辦法
數目不夠高就用class 
數目高我就用word 
然後呢只要它的夠高夠高頻的word 就單獨一個class 
夠高頻的那有很多很特你你知道在中文裡面有很多很特別的字
舉例來講把
這是一個非常有意思的一個字
在中文裡面
把什麼幹什麼這個由
那那那這是一非常高頻的詞它你你你要它跟誰在一起呢
不如它自己一個人在一起比較好嗯
那凡是這種很高頻的這個
那當然最高頻的是這個字嘛
那
這個可以用來做各種各樣的目的
啊
這個很漂亮的
坐在那裡的同學啊
