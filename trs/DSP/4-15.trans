好那有了這個之後那再下來呢我們再大的一個問題是要算這個 mean 跟這個 covariance 我每一個 Gaussian 都有一個 mean 要算怎麼算
它有一大堆 covariance  matrix 怎麼算
那 again 我用這堆東西就是剛才算的 gamma  t 的 J K 就是這個東西
那這個式子是什麼呢這個式子我們可以這樣子看你如果回想 one  dimension 的one  dimension 的 random  variable  x 
那如果它有一個 distribution 是 p  of  x 
是一個 p  of  x 的話它的 mean 怎麼求是不是這樣求 x 乘上 p  of  x 然後積分
就是它的 mean 
這個沒問題吧這個是非常簡單的一個求 mean 的假設是一個 random  variable 只有一個 one  dimension 的 random  variable 
它的 probability  density  function 它的 probability  density  function 是 p  of  x 的話我怎麼求 mean 就是拿這個 x 的值乘上它的 distribution 然後全部去積分
等於說每一個值都可能是 mean 每一個值都有可能然後把它的機率都乘進去然後全部去平均起來這就是它的 mean 
如果你可以了解想像這個式子的話那上面那個式子意思是完全一樣的
它就是這個式子那何以見得呢
我們寫一下你就知道了它是分子是 summation 的 gamma  t 的 J  K 然後乘上 ot 
是從 t 等於一到大 t 然後分母的話呢就是這堆 t 等於一到大 t 的 gamma  t 的 J K 
對不對這是那個式子那你如果仔細看這個式子的話呢
那我說這堆東西其實就是 p  of  x 相當於那邊的 p  of  x 那這個東西呢就是我相當於那邊的 x 
然後這堆東西呢就是積分那這樣是不是就是一樣了那這個就是這個意思
我其實只是在求平均而已換句話說我現在看到從一到 t 這兩百五十個 o 
我要知道這兩百五十個 o 裡面的平均但是呢 well 這兩百五十個 o 是可以在每一個裡面啊
所以我得第一個 identifies 在這裡的不要把這些東西再算進去嘛所以我先要把這個東西算出來嘛對不對
那所以怎麼辦呢我就要去根據那我要這些 gamma 就是就是要 gamma 就是要在 t 上面的嘛
對不對所以這個 gamma 的定義就是它在它在這個 t 上這個 gamma 就是我要在 state  j 上面的
然後呢我的這個後面的 k 就是我要在第 k 個 Gaussian 的位置
所以把這個都算進去之後呢我得到的其實其實就是
我把這些東西去求平均但是我只要算它在這裡面的這部分不要算到這些地方去
只要算這部分我只要算它在這裡的 j 是說我只要算它是在 state  j 裡面的機率
然後呢這個 k 的意思是說這個 k 的意思是說我現在只要算這個第 k 個 Gaussian 的東西所以呢我是以這個 k 跟這個 j 來算
那這個其實就是一個 distribution 那麼那這個等於是說我每一個時間的時候它這是除以 total 嘛在每一個時間它除以全部佔多少
然後呢這個把它的這個 mean 乘過去然後全部積分全部加起來那這樣呢等於是在算這個式子
所以這樣的結果我就會得到這一個 Gaussian 的這個 mean  OK 那如果這個你可以想像的話
那底下這個式子的意思是完全一樣的現在求 covariance 了求 covariance 怎麼求
那就跟你平常所想像的 variance 怎麼求是一樣的如果對單一的 random  variable 跟剛才一樣哦
如果這是單一的 random  variable 的話我的 variance 怎麼求 variance 是這樣求的對不對
是我的一個 x 值減掉它的 mean 
然後平方然後然後乘上它的 probability  density  function 
積分嗯 D  X 積分
這樣的話我得到的是那個 x 的 variance 對不對這個式子應該是很容易講跟那個意思是完全對等的
就是我要求 variance 是這樣求
那你如果 variance 這樣求你可以想像的話那我這邊也是一樣啊那我現在這個式子跟剛才一樣我不再寫一次了
跟剛才這邊的情形是完全一樣的你可以想像中間這個就是 p  of  x 就是這一個這個除以這堆就是那個相當於那個 probability  density  function 
那後面這個呢就是相當於這個 x 減 mean 的平方
只不過因為我現在是 vector 現在都是都是 n  dimension 的 vector 所以呢我變成
這個要剪掉之後這個是什麼這是 transpose 
這兩個這個剪掉之後再乘上一個 transpose 之後的話就變成是這個每一個 component 相乘嘛
對不對就是說你現在因為是我這邊講的都是 vector 所以呢這一個是這樣子的
它減掉 mean 是一個這樣子的 vector 然後呢它的 transpose 是一個這樣子的 vector 
嗯這樣寫有點反了它很可能它把它當成是當成是 row 來寫的話
哦沒有錯沒有錯這樣對因為我這樣子的話就是每一個乘一個每一個乘一個每一個乘一個所以就乘出來會變成一個 covariance  matrix 
對的沒錯這樣子也就是說我這邊是減一個一個值減掉它的 mean 
但是我現在變成有一把值分別減掉它的 mean 然後呢那它的 transpose 的話是一把值分在這裡
那這個跟這個在做 matrix 的相乘的話這後面是它的 transpose 那這個一乘的話這每各自都可以乘一個 component 出來就變成一個 matrix 
然後我現在在求平均的時候就求出所有東西出來那其中的對角線上的這些東西呢就是你原來的每一個 dimension 的這個 variance
但是我還有這些對角線以外的這堆東西呢是它們的 cross 的 covariance 的參數
那因此我就可以得到這整個不過這裡面所有的意思都跟這個一樣就是你分別去某一個對不對
我們如果要寫得詳細一點的話就是把它寫成一個譬如說這個 xi 減掉它的 xi 的 mean 
乘上這個 X J 減掉 X J 的 mean 的這種東西去做平均嘛
這個是 covariance 的這裡面的每一個 element 的意思是這樣子嘛那這個跟這個的意思是一樣的
只是我把這個平方變成一個是 i 一個是 j 然後變成這樣子去做就是了
如果照這樣子來看的話呢當 i 等於 j 的時候就是這些東西對角線上的那就是求出它的它的這些 variance 的值
就是每一個各自的 variance 的值當 i 不等於 j 的時候就得到這些對角線以外的點就是它們之間的 cross  correlation 
它們的就它們的 covariance 的參數那這樣我就得到這個 matrix 那麼如果這樣看的話你如果可以想像那麼這個式子其實就是這個意思
所以你可以看成中間的這個一堆其實就是這個分子除以這個分母就是我這個 probability  density  function 
然後呢我右邊乘上這堆東西呢那就是相當於這個嘛或者相當於這種東西只是我現在做出來就是一個 matrix 
那然後這邊來積分嘛積分就是在求平圴嘛求 expectation 或者就是對機率來平均的意思
因此我這樣我就得到我的 covariance  matrix  OK 　好那麼這樣一來的話這才是我們今天真正用的最多的forward  backward  algorithm 是這一塊哦
也就是我們剛才講的當你算出 gamma  t 的 J K 以後
那麼我現在來算它每一個 Gaussian 的 weight 然後我來算每一個 Gaussian 的 mean 每一個 Gaussian 的 covariance 
那麼於是呢我這些東西全部都可以調了全部都可以調了以後那當然那還有的東西是前面的嗯譬如說這個
 a  i  j 的話就用前面這個嘛 a  i  j 是不受影響這個 pi 就這兩個所以這個 pi 呢就用這個式子就可以求
 a  i  j 就用這個式子就可以求那只有這個 B K 呢是這個式子是對 discrete  model 這樣求
對 continuous 的要有 Gaussian 就是這樣子求
 OK 所以 continuous 的話就是這樣求
於是呢我就可以都可以求出來了但是真正求不是這樣求的是一個 iteration 的 procedure 怎麼講呢
嗯應該這樣說我們舉例來講假設你是辨識零到九的十個聲音那你有零的一個 model 
一的一個 model 有二的一個 model 你有一個九的 model 假設這個是你的九的 model 你已經用了一堆人說的九的聲音所 train 好的一個九的 model 在這裡
現在有一個新的聲音進來你知道它是九我要把這個聲音再 train 進去
 OK 是這個 problem 是這樣子假設我已經有一個九的 problem 九的 model 已經在那裡了我用了一堆聲音 train 好一個九的 model 在這裡
現在一個新的聲音九進來的話那怎麼辦那我現在要把這個聲音 train 進去怎麼 train 呢那就用我們剛才講的這一招
那這一招是怎樣呢其實是一個蠻複雜的過程因為你要先求這一堆 alpha  beta  gamma  epsilon 
所以呢你看譬說說這個 epsilon 裡面要有 alpha  beta 所以你 alpha  beta 都要求嘛
所以你要一面要求 forward 的 alpha 一面要求 backward 的 beta 通通都要求然後你要求 gamma 然後你要求 epsilon 
當你這些東西都求出來之後等於是說你把那個 o 放到現在這個 model 裡面去了那個 model 是用別人 train 好的聲音
 train 好的一個 model 你現在把這個新的 o 放進去之後我得到所有的機率就是 alpha  beta  gamma  epsilon 等等都有了之後呢我現在用它來
估計重新估計這堆東西得到一個新的model 
 於是我的這些個 a  i  j 啦或者是這裡面的 mean 啦 covariance 全部都換新的我得到一組新的 model 
可是你不能相信這個是好的為什麼
因為這新的 model 怎麼來的我們剛才說我是把這個新的聲音放到舊的 model 裡面去然後呢得到那堆機率 alpha  beta  gamma 這些東西機率之後
我用那些東西來算出這些所有的新的值
嗯你很難說它會它會好為什麼因為原來這裡面的 model 是有我另外找了一把人
另外找了一群人的聲音 train 出來了現在就為了這個聲音你就把它全部都動掉了你動成這樣子了
一定對嗎這是有點懷疑因為你可以看到我這邊其實是在把這個聲音我把這個新的聲音去算它的 mean 
對不對去算它的 covariance 是這個意思那我把原來這麼多人都動掉了只為了去配合這個人的聲音
所以呢不見得這個真的是好的我們不敢說它是好的但是它只是說呢我已經把這個聲音又放到這個 model 裡面去求機率
又然後全部重新調了就是了那麼因此呢它考慮了這個東西也考慮了這個那這個沒有理由它會比較好所以呢我要把這個拿來重新放回來
再做一次再拿回來再放一次這樣經過一個一堆 iteration 當它會收斂的時候就表示它比較好
 OK 就這個意思哦也就是說你
我新的聲音來的時候我用它來得到一個新的但是我沒有理由說這個一定會有多好因為它顯然很參考我現在這個新的聲音
而對於舊的聲音好像比較不重視了那麼所以不曉得會怎樣所以我就把這個呢再拿來再來一次再拿來再來一次那麼等於說是
這個讓這個新的聲音跟舊的 model 之間的所有的差異性儘量地慢慢在新的 model 裡面越來越模糊越來越模糊那最後收斂的時候這個應該就好了
那麼在我們剛才講這裡面整套是用 E M  theory 來證明的我們現在沒有講在 E M  theory 是可以證明底下這個 can  be  shown 這個是用 E M  theory 證明就是說你其實
每做一次每做一個 iteration 的話這個機率是會提高的
那其實我們講的就是這個我就希望在這個 model 裡面看到這個的聲音的機率要大我要這個東西儘可能最大
所以呢但是同樣的我也要看到原來的每個聲音都要機率都要大我不能為了光把這個聲音大光把這個聲音的機率變大別的都變小也不行
所以呢我要我要一面要有原來所有的這些統計特性在裡面我一面要把新的聲音的考慮進來
那我那在 E M  theory 可以證明當你這樣做的時候你每做一個 iteration 其實這個機率是會變大的
因此大到最後他會收斂收斂的那個時候呢就是我們要的結果那這就是我們一開始講說這一個 problem 三是沒有 close  form  solution 的
我只能夠借助這個 forward  backward  algorithm 那麼經過 iterative 的方式讓它趨進我要的值
 OK 那麼到這裡呢那我們也可以解釋這裡還有一些重要的問題那第一個問題就是說我需要一個 initial