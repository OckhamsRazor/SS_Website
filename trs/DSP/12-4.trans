那下一頁跟這個是完全平行的
我下一頁我圖都完全一樣就是上面圖是完全co 過來的
就上面這個圖也就是下面這個圖
是完全co 過來的
那我現在是反過來來看後面這一半
我剛才是講這兩個相乘是這個
那我現在可以看另外一件事情是這兩個相乘
這兩個相乘了還是這個
還是一個這樣的東西
這個是r 乘以r 
這個是r 乘以m 
所以這兩個相乘之後還是r 乘以m 
就是這個東西
那同樣的情形我現在在這上面看的
這裡的每一個譬如說第j 個document 這個
這也是八百維
其實相對的是這邊的八百維
這是e one e two 這邊的八百維
e m 
那如果你剛才那個觀念可以了解的話現在是完全一樣對稱過來
我剛才是這兩個變成一個r 乘m 乘r 的
這寫錯了
這個是r 乘n 啦
我現在是這兩個相乘的仍然是一個r 乘n 這寫錯了
變成一個仍然是一個r 乘n 的
但是現在這裡的每一個column 其實只有八百維
就代表剛才這裡的column 
是有二萬維
我剛才的每一個column 
這邊的兩萬維
也就是這邊的兩萬個嘛
也就是說在這裡的每一維它這邊有多少個詞對不對
它每一個維代表它跟這個詞之間的關係
每一個維代表它跟一個詞之間的關係
這樣我也總共有兩萬個詞
所以有兩萬維
來代表這個document 
那我現在這個document 也不再需要兩萬維了
我只需要八百維了
為什麼只需要八百維這意思是一樣的
你可以想一想這八百這八百個
變成我的b i 
如果這個叫做b one 
這個叫做b two 的話
這個b one 其實乘上這個e i 
summation 的b i e i 
這個b one 乘上這個e one 
b two 乘上這個e two 
全部乘起來加起來就是這一個
這個詳細的數學我想你自己去去figure out 
這個這個觀念跟剛剛是相同只是反過來
所以呢這邊的這個兩萬維的這個vector 
其實你可以看成是這八百維的e i 
分別weighted by 這個八百個b i 加起來的結果
所以b i e i 就是這個
就像剛才的a i e i prime 就像這個是一樣的
那麼因此呢我這邊的也有相同的情形
就是我本來的一個document 
是要兩萬維的
這是另外一個space 
我的documents 
我這邊有一個兩萬維的空間
這裡面的每一個點代表一篇document 
代表一篇文章
那我也是一樣這兩萬維裡面我重新找一個八百維的子空間
之後我把這些所有的點都投影到這個這個八百維上面來
因此我其實reduce 成為一個八百維的一個新的只有八百維的subspace 
這裡面的每一個dimension 
其實就是e one e two e 三
當然我只能畫三個但是你只能想像有八百個
這個e one e two e 三就是剛才那些個那些個eigen vector 
那於是我把這些點呢重新點到這上面來只有八百維了
那之後呢那就是我們這邊所講的這件事情
那這裡面的所有的話都跟剛才是平行的
所以這個意思你只要剛才的了解的話其實就是反過來就一樣了
所以這個第一句話的意思
跟剛才的那第一句話是一樣的
我的每一個剛才說我每一個row 代表一個word 
本來說要八百個我現在只要本來要十萬個
現在只要八百個了
就是這個u i bar 
u i 的underline 就是u i 乘上s 
s 就是這個u i 乘上這個s 
這個乘上s 就是這個i 
那我現在也一樣就是這個v j 的bar 是什麼
就是這一條
真正講應該是右邊這個啦
就是我們真正講它是一個column 
它是一個column 
所以呢我現在的這一個
這一個就是我現在講的這一條
就是我這邊說的v j 的bar transpose 
v j bar 的transpose 
嗯這樣子寫是因為我完全follow 剛才reference 裡面第一篇的reference 它的寫法
它的寫法裡面凡是寫一個v j 這種東西都是一個row 
所以現在這個明明是一個column 
是一個column 所以它就必必須要寫一個transpose 
所以我這邊講它其實是一個column 
但是你要把它寫成就把它寫成transpose 所以就是就是這個東西
就是這個v j 的bar 這個東西v j 的bar 的transpose 的這個東西
那它是什麼是s 乘上v j 的transpose 
它是什麼
它就是這個s 乘上這裡的這一個
這裡的這一個第j 個
這個s 乘上這個就是這個嘛就這樣看對不對
這個乘上這個就是這個
所以也就是s 乘上v j 的t 
就是這裡的就變成加一個bar 
所以加一個bar 是表示這兩個相乘的結果
剛才再加一個這兩個相乘的結果是一樣的
那這個是一個這是寫成一個column 
你如果要寫成row 的話就變成這樣子了
這個是因為那篇我第一個reference 它的寫法
把這個寫成這個也是可以的那只是把它transpose 一下
所以這個的transpose 變成這個嘛
那這個transpose 變成這個嘛
把它transpose 過來的話就變成這個是寫成row 的寫法
所以我說這個是row 的寫法
這是column 的寫法就是了
但這個意思是完全一樣的
就是我現在本來是有這裡本來是有兩萬個dimension 
代表兩萬個詞的這個vector row vector 
現在變成只有八百百維了就是這個意思
那麼因此呢我這本來是有兩萬個word 所代表的這兩萬個dimension 
也就reduce 到只有八百維了
那這每一個維是什麼呢
每一個維是這邊的eigen vector 
這個eigen vector 是什麼其實你也可以看
譬如說你如果看這個eigen vector 的話呢
這個呢其實相當於這個
那你看這上面是什麼譬如說e one 是什麼是這個
那你看它是它是這個這個嗯這個就是零點三五這零點多少這零點多少
那你看它哪些是零
它代表哪些詞你把那些詞的觀念加起來其實就是代表它的那個concept 等等
那我們剛才的的詞的時候的那個dimension 呢
你應該我可能講錯一點
你這個零點零三這個其實是這個對應的是這邊的document 
所以你應該是說譬如說這個零點四五這個零點三一
這篇document 是在講九二一恐怖攻擊
這個零點三一是在講阿富汗跟達凱組織什麼什麼
你把它加起來的話它們就是所以這裡的每一個是代表這邊的一篇文章 
ok 所以呢我們剛才在詞的那裡的那八百維
在詞的那裡的那八百維每一維
是這個e i prime 
那這個e i prime 這裡面每一個component 
其實是代表哪一篇文章
你可以去看這些文章加起來是什麼意思
就是那個concept 
那我現在的這裡的這個八百維就底下這個八百維呢
變成是我是這個
那每一維是這裡的每一個component 
它代表的是它這邊的的詞
你把這些詞的意思加起來就是那個觀念
就是那個concept ok 
好那麼如果是這樣的話
那我們也同樣的嗯就是這邊所講的
就是說我現在就是把這個嗯這些eigen vector 
就做成變成normal 這是orthonormal basis 
來展開一個space
那它dimension 就是底下這個space 
那在這裡面也同樣情形我們在下一頁的這一句話
是在講這一件事情
就是說呢每一個component 在這裡面呢
就是代表association with 這個concept 
也就是我這就是就是在講這件事
這裡的每一個component b one b two 分別代表這邊的e one e two 的weight 
就是這個意思喔
所以呢每一個component 在這個裡面的
就是這個component 的就是這裡每一個e i 的association 或者它的weight 
所以就代表這個document 其實是哪些concept 
那因此呢這樣你大概可以想像就是說
我每一篇我每一篇document 
其實是裡面有好些個concept 
而每一個word 也有好些好些個concept 
那麼因此呢
這裡的每一個document 是是一堆這種東西
有好些個concept 加起來
這裡的每一個word 也是這些東西
也是好好些個concept 加起來喔這樣子
好那如果是這樣子的話
那麼我們上面這句話也有類跟剛才是這句話是對應到剛才這句話了
那麼上面這句話是對應到上面這句話
就是說我現在在這個空間裡面
如果兩篇文章講的東西很像
它們都在講紐約恐怖攻擊的話
那兩篇文章在這邊就會很接近
就會在這裡
那因為它們相對的那些dimension 會在一起
所以它們就會在這裡接近
而它們接近的時候呢
它們不需要有exactly same words 
就是說我原來在這個我原來在這個裡面
你如果要說這篇文章跟這篇文章像的話
除非它們的word 都一樣
同樣的word 它們都多
同樣的word 它們都少
這樣才我才知道它們兩個是相像的
我現在不用了
我現在是在那個八百維的空間裡面
不見得需要有完全相同的word 才知道它們像
我只要知道那八百維裡面那空間上距離近就像喔
所以呢我現在就是說它們不需要再有完全相同的word 
只要它們有有這個類似的type of word 
同一類的word 在一起的話
就表示它們是同一個了
所以呢它們只要是concept 接近的話就會在那裡就會接近
好那如果這個個concept 你可以了解這個這個想法你可以了解的話
那我們這兩頁的底下這句話你大概就可以想像了
就是說我把原來的所謂的association structure between words and words and document 
就是這個matrix 
這個matrix 所描述的就是word 跟document 之間所有的relation 
我現在呢可以可以完全保留
幾乎是完全保留
而且我可以把 noise information 拿掉
但是我的 dimension reduce 到變成一個只有 r 個 dimension 
也就是說原來的這一堆詞
這些 word 之間的關係 
reduce 到這邊來
那麼所有關係都在
原來這些 document 之間的關係 
reduce 到這邊來原來都還在
而且我還可以把 noise information 拿掉
什麼叫 noise information 拿掉
你可以想像其實就是在這個過程之中我把這些東西拿掉
我把這些東西拿掉我把這些東西拿掉
就這些東西其實是很可能造成 noise 的部分
我保留了最乾淨的部分這是 eigen value 的意思 
eigen vector 的意思
你記得 eigen vector 就是在做它不是隨便找一個八百維
它是找最有意義的八百維
你記得我們在講 p c a 的時候講過一個 case 
就是你如果這些點在這裡的話
你最後找的是這個這個軸
因為在這個軸裡面它的分得最開
而不會你不會找到這一軸
因為這軸它們比較緊
你不會找這軸這軸它們比較緊
一樣的意思
那我其實是找一個真正能夠描述它的 distribution 
最清楚那些 dimension 
我現在八百維都是這樣來的
都是找到它最能夠描述它 distribution 的那些 dimension 
所以呢我是把一堆我丟掉的是那些
所以我丟掉的這些東西基本上是比較 noisy 的
這些東西或者這些東西是比較 noisy 
我可以得到比較乾淨的
那麼於是我可以得到一個但是我的這個 association structure 幾乎是維持不變
然後呢還有還有一個有趣的地方是重要的地方是這樣
我由這個十萬維成八百維的時候
這是一個詞的空間
我這兩萬維變成這八百這是一個文件的空間
可是你發現這兩個空間其實它的每一個 dimension 是對應的
這個 e one 就是 e one prime 
每一個 dimension 就是對應的
為什麼
因為它們都是對應到同一個 eigen value ok 
也就是說這裡的 e one 的那個 concept 
其實對應到這個 eigen value 
這個 e one prime 的那個 concept 
也是這個 e one prime 其實是同一個
所以雖然說它們是兩個你真正講起來是兩個 space 
這個八百維這個也八百維
可是其實如果 e one 是描述恐怖攻擊的話
這個 e one prime 也是的
是同一個 concept 
如果 e 三是描述對美外交的話 e 三 prime 也是的
它們其實是同一件事
因此呢你也可以想像成我真的需要畫兩個嗎不用
我可以畫成一個行不行可以
所以呢在有的人的說法裡面它就說
其實我只有一個就夠了
這個 dimension 是 e one 
同是也是 e one prime 
因為是同一個 concept 
這個 dimension 是 e two 
同時也是 e two prime 
是同一個 concept 
你如果這樣看的話呢
我的詞也在這裡
我的文件也在這裡
它們通通都你可以看的是都在同一起
這樣也可以
但是你可以想的其實是這兩個啦
其實是這兩個啦只不過它們的每一個 dimension 其實是指同一件事
那畢竟這裡的每一個是一個兩萬維的代表 document 的一個 concept 
這裡每一個是十萬維的代表 word 的一個 concept 
但是其實是講同一件事
所以你也可以是想的是同一個
你如果想成是 concept 的話呢
那就是同一個了
這是另外一個說法
這是 concept one 
這是 concept two 
如果這樣的話這就是同一個了
ok 那這個就是我們這邊在做這個 s v d 的意思
那有了這個之後
我們再下來的如果這點都能夠想像的話再下來就比較容易了
譬如說我們可以拿來做什麼事
這個剛才講的這套就是所謂的剛才講的這套就是所謂的這個嗯 latent semantic analysis 
所謂的 lsa 
