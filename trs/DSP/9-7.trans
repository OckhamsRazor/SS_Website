好我們現在要講下一個m c e  
這裡面的下一個東西是m c e  
m c e 也是另外一個非常重要的觀念 
那麼 
帶到後來來就是所謂的discriminative  
discriminative training  
的觀念是由它開始的 
那麼 
所謂m c e 的就是minimal classification error 的training  
這個觀念的基本精神是說 
我們到目前為止講的所有的方法的training 都是只是為了要maximize 這個機率 
舉例來講我如果要train 零到九這十個音的model  
就是零的model  
一的model  
二的model  
等等 
到九的model  
那我的目的都是讓每一個model 裡面的這個likelihood function 最大 
譬如說如果是一的話 
我就是希望一的聲音放進一的裡面來是最大的 
這個我們到目前為止所有的觀念都是這樣子做 
那這樣是不是最好呢其實這個不見不盡然是最好 
為什麼呢 
因為呢它不見得讓因為我們的目的是error rate 要最低而不是這個機率最高 
這個機率最高是一定表示這個error rate 是最低嗎不一定 
那麼我們舉例來講的話呢 
譬如說在零到九的這個case 而言 
一個很大的問題是一跟七很像 
所以呢我用一大堆一的聲音train 成一個一的model  
我用一大堆七的聲音來train 出七的的model 
雖然這些個一讓這這個model 最大 
這些個七讓這個model 機率最大 
可是一跟七本來就很像 
那麼 
你現你現在如果去算這些個所有的位置一個未知的聲音進來我去算它lambda k 的這個model 的話呢 
這個值你可能如果這個是一的話 
是 
假設這個假設這個聲音是一的話一最大 
那麼六在這裡用六算出來在這裡用用這個八算出來在這裡 
可是七呢呢很接很接近 
對不對因為一跟七很像 
它們兩個很像會很接近啊 
那今天如果說我的training data 跟testing data 不太一樣我換另外一個人講的話
換另外
譬如說本來是這個人講train 出來我換另外一個人講的話
搞不不好它的七會比會比這個一大 
因為這邊已經很像了這邊非常接近 
也就是說 
當我在用傳統的方法在train model 的時候 
我常常只是要讓那個model 本身的機率最大 
但是呢並不表示我的error rate 會最低 
因為它並沒有考慮它們model 之間的相互關係 
在這個case 而言呢這個有所謂的相互關係有所謂的competing class  
這個一跟七就是competing class  
非常近的 
那即使我這堆training data 讓這個一在這裡讓七在這裡 
它們有一些差別 
那換了另外一個人來來講 
或者在另外一個noise 的環境之下搞不好那個七就比算出來就就比一大 
它很可能贏了它之後 
我一就變成七了 
那麼因此呢 
我如何解決這個competing class 的問題 
否則的話呢你很可能換了一個data 之後 
喔 
你你到了test data 之後呢 
在你的test data 裡面搞不好你的competing class 就會得到比較高的分數 
那麼因此呢 
那麼就有人想說 
既然這這樣的話我們應該要想的是除了要讓這個東西最大之外 
我還要想辦法讓它們凡是competing 的model 我把它拉開 
要想辦法讓它們會compete 的部分把它們拉的比較開一點讓它們不會compete  
那拉開的辦法呢就是我用一個新的criteria  
就是minimum error  
那因為我真的目的是要error minimum 才是我的目的嘛 
這個maximum likelihood 並不表示error 會minimum 嘛 
所以我要用一個新的criteria  
就是這這個minimum 的error 來做它 
那這就是這一堆所謂m c e 的原始觀念的由來 
這個在九零年代出來之後是一個非常重要的方法 
那麼很多重要的問題都是由它來解決的 
那麼在英文裡面有一個很有名的問題叫做e set 
你知道什麼是e set  
就是英文字母裡面的b c d e v t z 等等 
那麼這些個就像像我們的一跟七一樣 
是很像很像的 
它們只有最前面一個子音不一樣後面都一樣 
所以呢這個b c d e v t z 的時候 
你你在講一個一個spelling 的時候是很容易辨錯的 
那當然在中文裡裡面有更多這種set  
譬如說八搭它啦嗯這些 
嗯這種都是屬於一種非常confusing set  
那所有的這些set 都會發生這個問題就是它們會非常像 
那麼你即使train 的時候 
用maximum likelihood 把它們train 得很像 
但是呢你test 的時候環環境不一樣speaker 不一樣noise 的環境不一樣之後 
很可能會互相confuse  
那麼後來就是有了這一招之後 
就可以把它們都拉開來 
所以這一招是一個非常重要的 
那在絕大多數情形之下它們都非常有效可以把正確率提高 
