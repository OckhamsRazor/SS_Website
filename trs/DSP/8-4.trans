那麼因此呢這個問題是非常複雜然後如何來做其實很不容易
那我們這邊講的是一個最基本的做法是這樣子
那最上面這個回到我們一開始所說的map principal
那這就是我們剛才講的喔就是在二點零裡面就說到
我今天的problem 是什麼problem 進來一個x 一個sequence of sequence of feature vector x
我要找一個w 就是這個word sequence w 使得這個機率最大
那個這個機率就是a posterior probability 所以這個就是m a p 的principal 這就是我們二點零所說的
那然後我們說這個怎麼這麼這個怎麼做呢
這個機率我們不會算我們就把它倒過來這是bayes theorem 把它倒過來
倒過來之後這個可以不要看
因為我們是要在在不同的w 裡面去找機率最大那一個
這個都是相同的對任何w 都一樣所以這個可以不要看我就變成算是maximum 上面這兩個相乘
上面這兩個相乘呢這一個就是這個就是h m m 所算的這個就是n gram 所算的
那基本上我們就要maximize 這個東西
那這個是沒什麼問題這個跟我們之前講的是完全一樣的
那問題只是說這個怎麼做這個幾乎不能做
因為就是我們剛才講假設你這個word 總數有六萬的話
你那個word sequence 有 r 個word 就有這麼多個word sequence
你不可能為每一個word sequence 都去算這件事
所以這個其實這個式子其實是不能不能解的
那麼我們底下就在講怎麼解這個怎麼做這件事
那為什麼它不能解第一個你可以想到這個其實你還不能做這個東西因為這個是什麼
這個是我有無限多個word sequence 我有無限多個state sequence
那麼我們舉例來講假設就就用這個就用這個來看
假設你的那個你的那個word sequence w
有一個state sequence 在這裡
那這個是時間t 你這邊好比我得到的就是x
就是我我的我這邊所輸入的我輸入的這個這個聲音的sequence feature vector sequence 在這裡
就是這個橫軸在時間上面
然後我的word sequence w 可以串成一個大的這個hidden markov model 在這裡
但這裡面呢有無限多個path 都可以走啊有非常多的path 你每一條都可以走啊
那麼你的機率這個機率應該是什麼呢
這個機率應該是這個機率就是
如果是任何一個state sequence 的話它的機率是多少
然後把所有state sequence 全部加起來應該是這樣
所以這個機率呢照說就是這個
那這個式子是什麼其實就是我們只前講的basic problem one 你如果還記得的話
我們當時寫成這樣就是在做這件事這是我們的basic problem one
那麼你要從也就是說呢你所謂的你要在這個model 裡面
你在這個model 那個那個時候我們把這個叫做lambda 叫做這個這個model
你要在這個 model 裡面看到這個o o 就是這個東西我們那時候叫做o
你要在這裡看到這個o 的話你必需要把所有可能的path 統統都加起來那就是要加這個哦
那這個非常非常大的一個加法這個做起來就會累死人
那麼因此呢光是算這個就會這麼就要那麼難算
所以呢怎麼辦那麼我們通常不這樣做我們就做一個 approximation
我們就說呢ok 照說你這個跟這個相乘去maximize
那這個又要等於這個嘛所以你就變成是這個完全一樣這只是把這個代過來
我就變成要這個東西我要用這個來算我要把所有的state sequence q 統統加起來
然後再乘上這個language model 然後再來看maximize
如果這樣的話做死人了
所以怎麼辦呢我們就做個假設
做個 approximation 說這個呢我們就不要那麼做了啦
我們就選一個這裡面看哪一條path 最大我就拿那個就好了
那這個的意思是其實跟我們在講四點零viterbi 我們就說過這件事
我們當時講的是這麼一件事其實跟這個精神是完全一樣的
我們當時說如果你要做isolate word recognition
假設我有這是零的model 這是一的model 二的model k 的model 一直到九的model
進來一個聲音我怎麼知道它是零還是一
那我就是在算某一個聲音o given lambda k 看誰最大
最大的那一個的k 就是我的答案
對不對我這是零的model 一的model 到九的model
進來一個六我怎麼知道它是六呢
我就把這個六放到每一個model 裡面去然後看誰最大最大的那個就是六k 等於六的時候會最大答案就是
六但是呢我們也可以做另外一件事情是
那這個是什麼這個就是我們的basic problem one
要算這個東西就是我們講的basic problem one
這個solution 就是所謂的 forward algorithm
但是呢我們也可以不做這個我們可以做另外一個
就是什麼就是跑viterbi
如果跑viterbi 的話你可以跑完的時候可以得到一個optimum 的probability given 某一個lambda k
然後你看誰最大
那這個東西是跑什麼這是我們的basic problem two
我們的solution 是viterbi algorithm
那這個的意思是說我跟這個有何不同
這個是我對每一個我進來一個六的時候六的所有的path 我都算進去了
也就是我們剛才講的我進來一個六的時候如果這是六的model 這是六的聲音的話呢
它所有的path 我全部都算進去了我的forward algorithm 就是把所有的全部加起來
那這個才是真正的那個機率那麼因此這個機率我得到就是這個那這個就是forward algorithm
basic problem one 得到forward algorithm 那這樣是一個正確的答案
那我如果跑viterbi 的話那我我那是左邊的那個
我如果跑右邊這個的話變成說我沒有真的去加全部的我只去找誰最大
我找到說ok 是它最大
最後走最後走到底的時候呢是它最大
那我就以最大的那個分數為準來算那麼其它我就不看了啦
那些path 因為基本上最大的那個機率大概已經告訴我誰最大了
所以呢我就只看最大的那條那條path 的機率誰最大就好
那麼因此呢我就變成我跑viterbi 找最大的那條path 那個機率然後看誰最大就好
那這兩件事情是完全不一樣的
這兩個機率是不同的
這個是把所有的path 的機率全部加起來
這個是我只找最大的那一條就好了
但是我現在重要的不是在算機率是算誰最大
那turns out 這兩個常常是相同的
就是最大的那個model 常常是同一個
最大的常是同一個
所以呢我只是在找我只是在找誰最大而已我並不是真的要算機率
所以你要算這個也可以算這個也可以
那麼如果那個model 六那個聲音真的是六的話它的最大那條path 大概就是在六裡面最大
跟你這邊把它全部算出來之後它大概也是它最大
所以呢最大的常常是同一個所以呢我就算這個跟算這個是同樣答案通通是相同的
所以當你要做isolate word recognition 的時候你用這個forward algorithm 算這個還是用viterbi 來算這個
其實沒什麼不同答案大概差不多
那你如果是是在當時去看這兩個演算法的話你也會發現
他們也沒什麼這個演算法所需要的計算量大概也差不多
好像沒有理由要選哪一個因為計算起來大概差不多你去看這兩個algorithm 大概也差不多
可是到了這裡的時候就不一樣了
我們這裡講的事情是跟那個一樣的喔這件事情從這個到這個其實就是從這個到這個就是從這個到這個
那這個是把所有的path 都算進去這個我只找最大那一條
那就是這邊講的這個這個事情那麼在這邊的時候對不對
那在這邊的時候我是要把所有的 path 全部加進去
這邊全部這個summation over 所有的state sequence q 的這件事
就相當於那邊的算那個東西就是forward algorithm 要做的事情就是所有的path 全部都要算的
而這邊的話呢就相當於viterbi 我只做一條path 我只做一條path 喔
那麼那麼這個情形就是我們剛剛講的那麼其實是完全不一樣的機率但是因為我只是要找最大的那一個
對最大的那一個而言大概是一樣的通常是差不多的所以我這樣的話省了很多事兒
因此呢我們在講isolate word 的時候從這個變成這個好像沒什麼道理因為這兩個計算量差不多
在isolate word 的時候這個跟這個反正也差不多沒有什麼不同
可是你如果像現在在這裡是一個大字彙有六萬個word 又是continuous speech
在這個情形之下的話這兩個就差別很大了
那麼因此呢我這邊就每一次只要算一條path 這個所有path 都要算那麼
那麼因此呢我們就把它簡化成為只算一條
當你簡化成為只算一條的時候其實就是簡回簡化到回到所謂的viterbi
那麼因此呢我們就回到viterbi 那麼你唯有只算一條只算最大那一條你才可能用viterbi
那才有才有辦法解出來但是因此我們底下講的都是用viterbi
那如果用viterbi 的話呢我們必需了解一點就是我們這邊講的都是一個sub optimum 的approach
什麼是sub optimum 也就是說我其實我是做了這個假設的這個假設其實不見得正確嘛
所以真正的機率是這一個我已經省我已經省掉成為這樣子了
所以這個不見得真的能夠得到我的optimum
所以我這個只是一個sub optimum
因此我這邊做的其實只是一個sub optimum 的做法
好那麼這樣子之下呢那麼我們怎麼做這件事
我現在要跑viterbi 的話呢我還是一樣
那麼這個我們剛剛也講過了就是跟我們剛剛講viterbi 的精神完全相同
我這個呃我就是formulate 一個簡單的sub problem
然後呢變成一個algorithm 變成一個iterative algorithm 這樣一路一路這樣子算過去
那麼在這個時候如果這樣算就會發現我是一個時間一個時間一個一個frame 這樣算過來的
那是為什麼他叫做所謂的time synchronous 或者frame synchronous 的意思也
就是best score at time t update from all states at t 減一
viterbi 精神就是這樣子嘛我在前面一個state 的時候的所有東西去算下一個得到一個optimal path 一個optimal 點在下一個
所以呢這就是viterbi 的做法
就是一個time 或者叫做time synchronous 或者frame synchronous viterbi search
那真正的難題在哪裡難題在底下
那你可以想像其實我現在並不是跑一個model
我不是跑一個model 把我我分別每一個去跑viterbi 那沒什麼問題現在不是的
我現在有六萬個word 而且每一個每一個word 都有六萬種可能它們是連起來的
那怎麼辦呢那最基本的這個想法就是你要有一個tree lexicon 來做為我的working structure
什麼是tree lexicon 呢就是這裡畫一個很小的例子
就是我把它的每一個音拿來建一個tree
譬如說如果是第一個音是斯後面如果是a 的話呢就變成say
如果後面接p 接e e 後面再接咳就變speak
後面這邊如果接c h 就是speech 嗯這個接這變成 spell 等等等等
那我如果有個辭典有六萬個word 我就可以如果每一個word 都告訴我它是哪些音拼起來的
我的lexicon 本來就是這件事嘛lexicon 就是lexicon 就是一個辭典裡面有所有的word 然後每一個word 都告訴我它是哪些音拼起來的
我就寫一個程式把那些建成一個這樣的tree
建成一個這樣的tree之後這裡的每一個arc 是一個hidden markov model
譬如說每一個arc 這裡是一個phone 嘛哦那麼這是一個h m m 這是一個h m m 這是一個h m m 因此我這些 h m m 是連起來的
然後你走走每走到任何一個leaf note 的地方呢就是一個word
ok 那這樣的話我構成一個tree 所謂的tree lexicon 就變成一個這樣子
那我一路走走到底的時候呢就是一個 word
那這個時候我的viterbi 怎麼走從頭開始走
那麼從頭開始走的時候呢跟我們這邊所畫的圖是一樣的唯一不同的是我現在是一個tree 不是只有一條不是只有一條而是一個tree
因此會變成怎樣呢會變成這樣我們舉個例子來講
假設我走到這裡之後分叉成為兩個的話
那其實相當於我這個平面相當於我這個平面走到這邊的時候就拆成兩個平面
對不對所以呢假設我在走viterbi 的時候
我我我這是一個很大的這變成一個tree 的h m m 了對不對
我這裡的每一每一每一個arm 其實是每一個arc 是一個h m m 那這些h m m 全部串起來變成很大的tree 嘛
那所以你如果是這樣子的話呢它就變成說是在這邊的時候它可以走這個也可以走這個平面
那麼因此呢我現在在這裡走的時候呢譬如說我這條path 走到這裡的時候是可以往這邊走
也可以在上面這條走走這邊對不對
我就可以可以這樣走嘛
那同理呢我到這邊的時候我又拆開啦
我又拆成拆成兩個
我這邊又拆開了所以走到這邊的時候呢我可以再拆開來
我這邊又拆成兩個對不對
於是我這邊又得到這一個跟這一個等於是這樣子嘛等等呢
因此你可以想像我們現在在講的事情是一個很大的h m m 的state 所構成的一個很大的一個tree
它我我我每這裡每一個小arc 可以看成一個小的h m m
但是呢我整個變成一個那麼大的我的viterbi 在上面走的話呢在這邊就開始兩邊都我本來這邊都可以走但是現在呢我到這邊就變成又可以拆分開來走這邊又可以分開來走等等等等
所以走到這邊的時候呢我又可以往這邊走跟往後面對不對
喔那就就就這樣子一直往下走的話呢那你現在的這個可以走的這個路就非常非常多了
那在這個情形之下你就可以想像為什麼要用這個viterbi
因為你如果要去算所有的state sequence 不得了
那因此呢我就我就每一次我在每一個時間上我就只算機率最大的那一條嘛對不對
我就只算即使是這樣都已經很累了因為我其實我在在每一條的時候我其實都要把它全部算出來啊對不對
按照我這邊來講我在每一個時間t 我要把全部的都算完才能算下一個嘛
那這邊已經多到不得了了所以你光是這樣走已經很累很累了
那那這是為什麼我們一定一定要用這個approximation 然後做這個viterbi 而沒有辦法再做這件事情
那這樣子當你變成一個tree 有什麼好處
變成一個tree 的最大好處應該就是這句話就是你的search processes for a segment of utterance through some common units 的話呢就可以share
什麼意思我們說我我不知道第一個word 是什麼word 它有六萬個可能
但是呢如果它是它是speak 還是speech 其實只在最後不一樣而已
前面一路走過來都是一樣的
所以呢你不需要去跑一次viterbi 去跑一個speak 的word 的viterbi
再跑一次speech 的不需要跑兩次
其實你從頭一路走你只要跑斯的跑這個音的跑這個音的到這裡為止
它們都可以share 同樣一條path 同樣的分數只有到最後才拆開來ok
那麼因此呢我到前面的這一長段跑的viterbi 就它們就可以全部都share 那就這句話的意思
那麼因此呢我這中間譬如說這個一開始當然你可以想像
假設我有六十個phone 的話這一開始理論上應該有六十個path
那這邊下去又應該有六十個path 但是事實上也沒那麼多
因為這個這個tree lexicon 告訴我說哪些音才可以構成一個word
所以呢要後面有這個word 它才會接它
譬如說s p e 之後不是接所有的音因為只有你有這些字的才會接
所以到這裡的時候呢你到這邊就接k 變成speak 接c h 變成speech 或者再接哪些音會變後面有字才有啊
所以其實有另外的好處我這邊沒有寫在這裡的就是
應該說就是這個search space reduced by the constraint given by the words in the lexicon
也就是說這個這個辭典這個辭典裡面有六萬個word
那其實這六萬個word 已經告訴我說哪些音
如果前面這三個音下來的話它不是接所有的音
只有後面有哪些字的時候它才會接那些音
那你不是所有的音都要都要找了你就只要找這些有字的音就好了
因為我現在每走我走的每一步都是因為後面有字的關係
喔所以完全根據有哪些word 來找
所以呢它把我的不是每一每一段都可以用所有的phone
所以我的search space 是reduce 因為我的辭典裡面的word 給我這些constraint
那當然這裡還有一點就是我們這邊呃沒有說的應該講這樣講比較簡化一點
這是一個phone 這是一個phone 每一個phone 它有一個h m m 然後把它串成這些word
那實際上呢我們說這個可能是一個tri phone
喔這些可能是tri phone 也一樣啊
那你如果說我的我的辭典我是用tri phone 來做的話呢
我的辭典裡面每一個word 告訴我是哪些tri phone 連起來的
那我就變成把這些tri phone 建成一個tree
那這個基本上這個這個是完全一樣那其實如果是tried phone 建成一個tree 的話那它有更多的constrain
因為如果這是哪一個tri phone 已經確定它後面會接什麼對不對
tri phone 是已經知道後面會接什麼的
所以呢這個雖然多了很多但是我後面接什麼是確定的喔
所以你真正用的可能是一個tried phone 的tree 那我們這邊沒有畫就是了我們這邊是假設是一個普通的phone 就是了
那如果是這樣做的話呢那麼會怎樣
底下這句話意思是說same tree copy reproduce at the each leaf note
那這個意思是說我真正跑起來會怎樣呢
我跑起來會變成這樣這麼大一個tree 跑死了才跑一個word 哦
你有一個很大的tree 跑他最後有一堆一堆word 有六萬個word
這是這是一個lexicon tree
你你跑一個這麼大的六萬個word 的串的所有的它的phone unit 串的這麼大的一個tree 的 h m m
你跑完的時候呢我理論上我六萬個word 都跑出來
所以我就知道這六萬個word 裡面是它的分數是多少是它的分數是多少每一個都有一個viterbi 分數就是這一個
不過這才是一個word 哦那這這後面又可以接嘛
譬如說就這個而言它又可以接另外一個tree 後面又有六萬個word
那這個後面也接另外一個tree 後面又有六萬個word 這個後面又有又有另外一個tree 等等等等
那這就是所謂的tree copy
那麼因此呢你可以想像的是這所謂的tree copy 就是你這個又又做同一個tree
那麼也就是說你的我們說第一個word 有六萬種可能第二個word 又有六萬種可能但是其實是什麼是六萬乘六萬了嘛對不對
是接它的也有六萬個可能接它的也有六萬個可能這接這六萬個都有六萬個可能嘛喔
所以呢你這個是第二個word 就變這樣子那第三個word 呢這裡面的每一個都要再接一個tree 一個一個tree 嘛對不對
所以你光是這樣想這個search 仍然是非常龐大
那這個就是我們講基本上你如果要這樣做的話就變成這樣子
那這個仍然是非常複雜的問題所以我們需要把它把它做得比較有效率一點
這就是底下要講的事情ok 我們在這休息十分
ok 我們接下去
我們剛才在講的情形就是
我的第一個word 就是一顆tree
這顆tree 就後面就有六萬個
第一個word 我就就這個這個非常大的tree 的hidden markov model
那麼走到最後有六萬個
然後之後呢那每一個走完之後應該都要再接一個
都要再接一個
所以你就接很多
這是第二個word 就有六第二個word 其實就有六萬個這個tree 在這裡喔等等
那在這個情形之下我整個的viterbi 會變成非常複雜的viterbi
不是我們原來單獨的這一個
不是單不是單獨這這一個這樣子走而已
而變成是我一路這樣子散開來一路散開來變成很大很大
那為了要讓這個比較清楚得來講我們怎麼來做這件事情
所以底下呢用用一些這個符號把它們specify 清楚它們是什麼
這講起來是蠻複雜的
不過事實上其實你如果對於我們原來所講的那個viterbi 了解的話其實是一樣的
好那我們第一個現在現在比較複雜所以我們一堆東西
第一個呢就是我要define 清楚我這是什麼這個是什麼
這個其實就是我們原來viterbi 裡面所說的那個東西
只不過現在變複雜了所以符號變多了而已
我們原來viterbi 不是這樣
當我走到時間t 在state i 的時候
我這邊所define 的一個東西叫做delta t 的i
就是走到這裡為止的一條有最高機率的path
它的機率就是這個東西
那這個東西其實就是我現在的這個東西
是一樣的東西
這個東西其實就是這個
只不過我現在的東西變複雜了所以我現在東西變一堆符號喔
那這個意思還是一樣
那我現在變成是怎樣呢你可以看到是
我在我在時間t 的時候
在state q k 啊 q t of the word w
那當我走過來走到這段是一個word w
譬如說這裡我是一堆state
那麼這一堆這個這是一個word w
那這個word w 是是我們剛才在這個一路這樣跑下來的中間的某一段嘛
譬如說我在這裡的時候從這裡跑到這裡的這條是一個word w 對不對
因為這個我從這個tree 的頭一直走到tree 的尾的時候這是某一個word
如果到這邊發現走到這邊發現這個word 是w 的話就表示說這整個path 是w
那這個w 中間會有一堆 是一堆phone 一堆state 走起來的
那這個呢就是這個
如果這樣的話呢我在時間t 的時候
停在某一個時間t 的時候停在某一個state
這個state 叫做q t
這個q t 是屬於word w 的到這裡為 止的這個分數
那麼我一路走過來
走到這裡的這個分數
就是這邊的這個 d
這個d 的這個什麼t q t w 就是這個東西
ok 其實是一樣的
那為為什麼這個w 那這邊還有什麼因為我從頭走過來嘛
我可能從從前面走走第一個tree 走第二個tree 這樣走走到這邊
這是某一個w 嘛喔
所以這是走到路的這個path 整個path 中間的某一個w 而言
在這個w 裡面的某一個state q t
我在時間t 的時候我剛好停在這裡的這個分數喔
那麼就是所以這個其實是跟跟這個意思是一樣
只是我現在比較複雜而已
那所謂的 object function 其實也就是這個東西
也就是這個一路走過來最高的機率就是所謂object function
那我就是要optimize 這個東西
我要maximize 它嘛
那麼這個呃那是什麼是best partial path any at time t is state q t for the word w
也就是說這邊有有千千萬萬個path 走過來
但是我現在講的是到這裡為止
最好的那一個
那就相當於viterbi 裡面到這裡為止最好的那一個這樣的意思
那我都是以某一個時間來算的
所以呢是時間t 的時候來算的
那你記得viterbi 裡面很重要的一件事情是做什麼back track
因為你每一次得到這個之後下一個的時候呢
我下一個state 當t 加一的時候
它可以從這上面的任何一點跳過來
depends on 誰過來的那條path 最大
那你很可能發現結果是從這一點跳到這邊的時候機率是最大的
所以結果呢
你要把到這裡為止最好的path 就變成是這一條了
對不對這是viterbi 的基本精神嘛
所以呢你你要算t 加一的時候
你得要在算從 t 的時候所有的state 都會過來的
然後看誰最大
最後最大的是這一條的話呢
你到這邊為止最大的就變成這一條了不是它了
那因此呢我我到這邊的時候我一定要記得說
哦剛才是從這裡來的
所以這是所謂的back 這是back track 的那個pointer
我要我要說ok 它是從這兒來的
這樣我一路要記得它的前一個是哪裡
那這個是viterbi 裡面很重要一件事情你要能夠記得
你剛才從你的最佳這點是從哪裡來的
這是所謂back track pointer
那我這邊也一樣也要
那就用這個符號來代表
就是說你如果是在你的這個這個你的partial path
你你的這個best partial path end 在time t in state q t for word w 跟剛才一樣
那你如果現在是在這裡的話
那你要算剛才是哪裡來的
剛才如果是在t 減一的時候
如果是這裡來的話
那你要把這個記下來說哦剛才是從這裡來的
那個那記下來的這件事情就是就是這邊的這個h
好我就記就是就是這個back track pointer
所以這個事情跟我們講的viterbi 是完全一樣的
只是說我現在要這個變得很複雜而已
好當這個沒問題之後呢
底下的這兩件事情其實說穿了也很簡單
也跟viterbi 這邊所想的事情是完全一樣的
那只是呢我現在變得複雜了
那我們要弄清楚
現在有兩種狀況
一個叫做 intra word transition
一個叫inter word transition
什麼是intra word 就是在一個word 裡面
也就是在一個tree 裡面
這是h m m only
沒有language model
當我在這裡面走的時候
當我在這一棵 tree 裡面走的時候
我是在這裡面這條路上走
那這條路其實你可以想像
最後就是一個word
是這六萬個word 裡面的一個word
那在這個word 上面走的話呢
我其實是走一個相當長的h m m 而已嘛
所以呢它是在其實就是在走h m m 的viterbi
因此呢這裡面的東西
跟這裡面這個東西
其實就是我們原來講的viterbi 這件事情是一樣的
這是h m m only
那在這個是後沒有 language model 的事情發生因為它是在算一個word
那底下呢
什麼是inter word transition 呢
是在當你這個走完我走下一個的時候
當你這個走完我要再走下一個的時候
那你你從這個跳到下一個去的時候呢
那這個時候我我是在一個h m m 跳到另外h m m
這時候中間有什麼有language model 分數要加進來
所以呢所以呢底下是所謂inter word
是這個這個時候是language model 分數要加進來
但是我我不是在h m m 裡面是在h m m 的外面
所以呢其實很簡單就是上面是跑h m m 沒有language model
底下是跑language model 沒有h m m
好那我們分別看一下這兩個情形
那就這個而言
它在說的事情其實我們先說 h m m only
也就是在intra word 裡面
也就是在一棵tree 裡面
一棵 tree 裡面的某一條path 上面的h m m
你在上面走的時候其實這個走的事情
就是在這邊走這段word
就是在走這段嘛
那走這段的情形跟這邊是完全一樣的
跟我們從前說的其實是完全一樣的
所以這個式子其實也就是我們從前講的那個式子
只不過現在看起來比較複雜一點而已
你看我要算時間t 在q t
時間t 在q t 怎麼算
我就先算t 減一的嘛
算算這個在t 減一的時候掉在q 的t 減一的時候
在這個word 裡面我現在都在這個word 裡面嘛
我是intra word 在這個word 裡面
所以呢我在t 減一的時候
我是在q 的t 減一state
對不對所以呢我在這個t 減一的時候
還是在word w 裡面的q 的t 減一的state
在這個q 的t t 減一的時候
在q 的t 減一的那個 state
上面我也有一個最高的分數
就是那個分數然後呢再加上跳過來的分數
跳過來分數有兩個
在我們當時講viterbi 的時候呢
那兩個是什麼一個是a i j
一個是b j 的o t
你如果記得我們是有這兩個東西
a i j 告訴我從這個跳到這個機率是多少
那b j 的o t 是我現在要把現在把這個新的vector
放進來放到這個state 來ok
所以就是這兩個分數
那這兩個其實就是這裡的這兩個
就是這個東西就分成這兩個這兩個就是這個
所以呢對不對就也就是說你現在從這裡的時候
我現在要算如果說這個是 t 這個是t 加一的這是t 這是t 減一的話
我現在在t 的分數是要所有的t 減一的
都有可能跳過來看誰最大
所以所有的t 減一跳過來有有兩種
一個是a i j
每一個跳過來都有個a i j
一個呢是我要把這個b 把這個o t
放到這個新的state 裡面去
這是這是state j b j 的o t
我要把這個放進來
那現在這兩個機率其實也就是我們這邊講的這兩個機率
所以你看到譬如說這個是什麼這就是a i j 嘛
這就是在word w 裡面
然後我從q t 減一跳到q t 的機率
所以這個東西其實就是a i j
就是從這邊跳到這邊的機率就是a i j
那這個是什麼呢
這個其實就是b j 的o t
因為你你你現在就是這個啊
它就是我在這個word 裡面那麼我現在是這個o t
我看到的這個第t 個 feature vector
第t 個vector 掉在q t 的機率
那其實就是這個東西喔
那你現在把這兩個它現在是寫log 用加的啦
那意思是一樣啦就是你這兩個加起來的這個
就是說就是這個這一項嘛
就是我從t q t 減一在時間從t 減一到t 的時候
我state 從t 減一到 t 的時候
那麼我要加進去的機率是這個
那然後呢因此呢我現在在算t 的時候
就是把t 減一的所有可能的q t 減一通通加起來嗯通通看起來誰最大
就跟這邊是一樣的嘛
你你現在要算t 的時候
你就把t 減一的看看是從這過來的還是從這過來從這過來看看是誰的最大
你就算那一個最大的那嘛
那那這邊其實也是完全一樣啊我現在就是把這個嗯我現在d 的t
q t 的話呢是什麼呢
就是在t 減一的時候的所有可能的q t 減一
那這邊所有可能的q t 減一在這裡
這邊所有可能在這裡就就是等於這邊的所有可能一樣的
這邊所有可能在這裡
那所有q t 減一都有一個最大的最佳的分數
再加上跳過來的時候可能的
然後加起來之後呢我在所有的q t 減一裡面看誰最大
然後知道ok 我就是從那裡過來
所以跟這個是完全一樣的情形
然後呢我現在就找到那個最大之後我就把我就得到一個下一條path
就最大的path
因此呢這邊所說的事情
跟我們原來所說的 h m m 是完全一樣的喔
只是現在整個整個變成複雜而已
其實精神是完全相同的
那底下這個式子只是在說
那我要做這個back track
我要記得從哪過來
所以剛才的maximum
倒底是誰是q t 減一
剛才是從哪一個過來的我要記得
我就把那個剛才過來那個記下來
這就是我的back track pointer
那麼說明這個我的q bar
就是指我的t 減一的時候如果現在t t 在q t 的話
那麼t 減一是從哪個state 是在哪個state
我把那個記下來
就是剛才那個裡面的maximum 的q t 減一是哪一個
把它記下來
然後然後把它放在那個back track pointer 裡面
於是我就記得剛才是從這樣過來的
好就這樣子而已
所以這個說穿了沒什麼特別符號變複雜而已
那麼其實講的就是這件事情
就是這件事情是完全一樣的
這是intra word transition
所以這個沒什麼不同
跟我們之前講的一樣
不同的是底下這個
因為我現在還會從一棵tree 接掉接到下一棵tree 去
當我從這棵tree 接到下一棵tree 的時候會怎樣呢
那就是做了一個inter word 的transition
從這個word 跳到下一個word
那這個時候呢我們假設說
現在這個word 走完了叫做v
v 是一個word
然後呢q f 的v 是它的final state
畫清楚一點
我現在走完了這一個這個word 叫做v
它有好多state
然後它一直到最後
這是它的最後一個state
這是它的q f 的v
就是 v 的這個word
q f v 是v 的word 的這個final state
然後我現在要從這裡開始接下一棵tree
怎麼接法呢
我先增加一個空的state 叫做q
q 是一個空的state
沒有裡面沒有任何東西只是為了接方便起見
為了是要接底下這棵tree
所以q 底下呢就接底下這棵tree 出來
那這棵tree 底下會有會有這個六萬個word
所以呢我現在的這個那如果後面這個後面這個word 呢
叫做w 好
所以呢我現在這裡的這裡有六萬個word
不過每一個word 我們都叫它w
所以呢這個我這個v
v 現在這個word 走完了
到了最後final state
了那我現在要開始接下一個tree 了
那下個 tree 有六萬個可能我們叫它是w
那麼開始的時候我有一個pseudo initial state
這是一個空的state 只是為了串接方便起見叫做q
那麼如果是這樣的話呢
好我現在就有一個空的 state 在這裡
這是我的 v 的final state 接下去
那麼因此呢我現在這個這個低的這個分數啊我就先給它跳到這裡面
跳到q 裡面來了
是也就是說當我這個t 到這邊
如果我們在這裡我們說是這邊是我的word v
這邊是我的v word v word v
走完的時候呢
是在t 的時候
我我這個這個最後這個state呢就是我的final state
所以這個state呢 就是q f 的v
那麼這個時候呢
我增加一個空的state 是 q
所以這邊我有一個空的state 是q
我仍然在時間t 的時候就給它走過來
ok 我在時間t 的時候我讓它
在這個空的state 裡面然後開始要往下接
那這個時候我就是在在這個呃時間t 從q 開始要走這個word w 了
那麼這個時候呢我這分數怎麼算
我先算我時間同樣的時間t
是我同樣的時間t 走到v 的final state 之後
當時的分數
然後現在要加language model 的分數
那麼這裡其實在這個寫錯了這要改一下喔
這個language model 分數你我們這邊是假設就是只假設language model 我們做bi gram 就好
其實那tri gram 更複雜啦喔
你想一想就知道tri gram 是怎樣的
不過我們現在只講bi gram
bi gram 的話呢這個很直覺的以為是這個
因為我現在v 後面要接w 嘛
所以我現在就是given 這個 v 後面接w 有個bi gram
其實這個寫錯了
我們應該是要看前面的
也就是前面的這一個
這個是u
如果是這樣的話呢那個機率應該是
probability 的這個v given u
是u 後面接v 的bi gram
不是 v 後面接w 的bi gram
這寫錯了喔
為什麼
因為其實你走到這邊為止的時候
你只知道我這條路上到這邊是v 而已
我後面w 還不知道了還沒開始走
所以w 有六萬個可能
這邊有六萬個可能的w
那你不可能把這六萬個bi gram 統統加上來
那就變成有六萬個分數了這不太可能的
所以這裡其實你不太可能知道那個w 是多少
我w 還沒開始走嘛
w 還沒開始走我我沒有這六萬個word 的不知道是誰我我如果真的要這個機率的話我有六萬個是不可能加
的而是應該是走到這邊走完的時候v 知道了
因為我走到最後才知道是哪一個word 嘛
我一路在找
一路在找這個最佳的path 對不對
所以我一路走過來走到最後才知道我這個word 是v
當我知道這個word 是v 的時候我可以把這個u 後面接v 的bi gram 加進來喔
所以你剛才在這邊你v 的bi gram 沒有加進來因為我不知道是什麼我我一路找嘛
我一路找不到最後不知道它是哪一個word嘛
所以呢我找到最後的時候才知道它是v
這個時候我是把u 後面的v 加進來
ok 所以這個地方是應該是這個u 後面接v
是這個的bi gram
不是v 後面接w 的bi gram
ok 那這個意思是什麼
這個意思跟剛才這邊是一樣的
也就是說我們剛才是說我從這邊過來
它可以從前面一個時間可以可以從任何地方過來
那我這邊其實也是一樣
你可以想成我在時間t 的時候其實這邊有六萬個word
我這個一路散開來的時候
在時間t 走完的時候其實有譬如說有三百個word
我這邊有一個v one
這邊有一個v two
那這邊有一個v 三
v one v two v 三都在這個時間t 的時候結束
它們都可以跳到這個q
你可以想是這樣ok
那也就是說我們這個圖現在已經不夠畫了
這個圖現在不夠畫了因為我其實這個不是一個 one d 的
這邊是我們這邊所畫的這個tree 嘛
這邊是一個tree 的結構你長上去的時候很多啦
所以呢當你到這邊的時候
這邊譬如說你在時間t 結束的時間t 所結束的word
其實有v one v 不是只有一個v
有v one v two v 三
都在最後結束分數都在分別是在它那個path 裡面最高的
那它們都在時間t 結束
所以呢我這邊其實有有好幾個
有好幾個
那這個v one v two v 三都在時間t 的時候結束
那因此我現在要跳到這個q 來
準備接下一個word 的時候呢
我可以有好多個可以從這個跳過去也可以從這個跳過去也可以從這個跳過去
那麼因此呢我先要看它是從哪一個v 跳過來的
那就是這件事
那精神跟這邊講的是完全一樣嘛
我現在只是說是要看它是從這個word
還是從v one v two v 三的哪一個的的最後的那個final state
會跳到這個q
來它的分數才是最高的
所以呢我就分別把所有的這些我這邊有六萬個word 在這邊結束
那有的早一點有的晚一點
你可以假設在這個時間t 的時候
有三百個word 在這邊結束
在 t 加一呢又有五百個word 在這邊結束了
t 加二又有一千個word 在這邊結束等等都可能
那麼因此呢你在每一個時間都在做這件事
就是whenever 你的word 走完的時候
你word 走完的時候
你就把那個 language model 那個的word 加進bi gram 加進去之後
然後我要看到底是哪一個word
會跳到那個q 分數是最高的
我就選那一個
那這個精神跟這邊是完全一樣的喔
所以呢我現在就是每一個v 走完的時候的分數
加上那個v 接在那個前面的 u 後面的language model 分數加進去
然後看誰的v 最大
我就從那個跳過去
那麼因此呢我這樣就得到這個那這個是相對於這個
只不過我現在是從是從這個h m m 跳到下一個h m m
或者說從這個tree
跳到下一個tree 的時候的的這個
跟剛才是在裡面走不同的地方在這裡而已
那這樣子我知道是誰最大之後呢我也一樣在這裡
我把那個最大的記下來
所以就把剛才那個maximum
所以這個也寫錯了喔
這個也是應該是這個應該也是這個u 後面接v 的bi gram
這個也是寫錯了
就把剛才這個maximum 誰最大記下來
最大的那個就是我的前一個對不對
所以呢如果是這個v bar 才是最大的
我們現在v one v two v 三
都都在這邊結束後我現在看到的這個
是看到現在是最大是這個
那麼因此我就應該把它的最後最後state 接記記下來
所以我就知道它是從這樣過來的
於是我現在就知道ok 它是從這樣過來的
於是它是從這樣過來的
於是呢我後面開始接下一個tree
那麼因此我現在就把它的這個v bar記下來
做為我的所以我的那個v bar 的最後那個state
就做為我的這個back point
back 這個back track pointer
那麼於是呢那就這這是兩種transition
只要這兩種繼續操作
那我就可以一路走下去
ok 一路走下去是可以
不過這個這個還是大的不得了
所以我們要有一些辦法來簡化它
有很多種方法來簡化它因為現在這個search 你可以想像非常大
這是我們所謂的search
那怎麼簡化它呢一個最簡單的辦法就是所謂的beam search
beam search 意思是說在每一個時間t 我只保留一個sub set
of 最可能的path
其它都丟掉
你可以想像我從一開始走
它很快就長很多很多很多
長那麼多之後你簡直沒辦法處理
所以呢最簡單的辦法就是做beam search
舉例來講define 一個beam width l
我們通當講譬如l 是三百或者六百或者二百
也就是說我我我很快走過來這邊就很多很多了
那我就只保留分數最高的
那二百個還是六百個path
其它全部丟掉
那我一路走的時候呢我一路在算分數最高的那個path
之後我保留六百個譬如說
其它全部丟掉
那這樣我才有辦法往前走
那當然如果這樣走的話就表示這不是已經不是一個optimum 的了
喔這又是一個喔這又是一個是個approximation
因為你可以想像
分數最高的path 不見得從頭到尾一定分數最高嘛喔
這個龜兔賽跑的原理嘛
期中考考最好的人期末考不一定最好嘛
所以你如果一開始就把ok 期中考裡面考最好的十個人留下來其它通通殺掉的話
那到最後其實可能最好的被你殺掉了對不對
這裡也是一樣的
你這個這個一路跑過來的時候
所以你的這個這個beam width 如果保留的越大是比較好
但是你的計計算量立刻就會大很多嘛
所以這個就是怎麼選擇這個問題
通常我們兩種簡單的辦法
一種是保留一個就定義一個beam width
譬如說你就是每在每一個時間點t 上面
我永遠只keep 前六百名或前三百名
等等那這樣的話讓我的計算量不會太大
第二種我就定義一個threshold
凡是的我分數比最高分少那個threshold 之內的我都保留
不管多少個
那有的時候這裡有一百個有的時候這裡有一千個
我反正是是這個在這個threshold 之內的我都保留喔
這兩種基本上這都是我保留一個beam
然後呢我就在beam 裡面走
那我自然就已經把可能的optimum 丟掉是可能的
所以你這樣子得到不見得是最佳的
但是是接近就是了
那這是最簡單最常用的這個reducing search space 的方法
當然還有很多進一步的方法我想我們這邊就不說
你如果有興趣去看讀相關的reference 就會覺得講很多種方法
因為這個其實是一個關鍵性的問題那麼有一堆研究如何做
那麼一個例子就是ok 你也可以從acoustic model 從acoustic 的h m m 的分數裡面去看
哪一些地方h m m 看起來它比較好比較不好把它丟掉
從language model 來看
那麼哪些應該丟掉什麼這這都有
那麼另外一個非常標準的做法就是所謂的multi pass 的search
也就是說我至少分成兩個pass
那這個意思是什麼呢
就是說喔應該是講說我在我先有第一個path
用比較簡單的knowledge
簡單的constraint
我就得到一個比較簡單的比較小的search space
在第二個裡面再做複雜的
嗯這話怎麼講呢
最簡單的想法就是說譬如說tri phone
tri phone 太複雜了
我前面就只做一mono phone
我我我一開始我我我不要做那個
我不要那這裡面我tree我這個lexicon 也可以有兩種嘛
一種是phone 的
一種是tri phone 的
tri phone 數目多很多所以會複雜
我就我先不要用tri phone 我就先用這個單獨的phone 做
那這樣就比較簡單
我就可以做第一個pass
或者譬如說這個language model 那裡呢
你可以想像我們這邊只講bi gram
是因為tri gram 複雜哦
我如果tri gram 的話
我走到這裡的時候
我不但要把這個bi gram 加進來
還要把這個tri gram 加進來
那我每一次都要都要再再算一個bi gram 算一個tri gram 是會複雜
那我也可以說我在我在第一個path 的時候我只做bi gram
後面呢才做tri gram
或者說我甚至於language model 我在前面不做
我我 language model 到後面才做等等
那因此呢我的第一個path
就可以比較簡單一點
那第一個path 的的出來結果呢
我們把它做成一個word graph
或者一個n best list
什麼意思呢
所謂的一個word graph 就是所有可能的word
可能性比較高的分數比較高的word
把它的時間點通通記下來
就構成一個 word graph
這是時間點
所以呢它譬如說譬如說到這個時間為止
從這一點到這一點
是可能是w one 是某一個word
到這一點也可能是 w two 是這個word
那這邊呢可能有另外一個word 是w 三
那這邊可能有另外一個word 是w 四
那這邊可能有另外一個word 是w 五
這可能有另外一個word w 六
那這邊可能有另外一個word 是w 七
ok 所以呢我從這個時間點到這個時間點的話呢
我可能是這樣子
這個可能是w word w one 這個可能是w two
它也許是w two 的前面一半喔
那那它也許到w two 也許不是對也許是w 四的前面一半喔等等
那麼因此我就把所有可能的word 它的時間點的起點終點通通記下來
它就可以構成一個graph
那這個graph 呢其實你給我一句話我可以先把這個graph
找出來當我這個graph 找出來之後呢
那其實它告訴我我現在只要在這上面找就好了
它 either 是一三五
或者是二五
或者是一六
或者是四七等等
那搞不好這邊還有一個
譬如說這可能也是一個
這個w 八
於是也可能是一三八七對不對
那麼因此呢你就在這裡面去看喔
那麼如果這樣的意思是說我的第一個pass
基本上做法還是跟剛才一樣這樣子做
但是呢我我只用比較簡單的東西
譬如說我只用這個這個我不要用tri phone 我只用單獨的一個phone
我不要用tri gram 我只用 bi gram
什麼的話
我也可以這樣走
這個程式稍微簡單一點然後我就取最可能的分數最高的word
哪裡是可能分數最高的word
那你可以想像因為我現在六萬個word
有的早一點結束有的晚一點結束
有的早結束有的晚結束就是我們這邊所畫的就是
譬如說w one 在這邊就結束了
w two 到這邊才結束
w 四要到這兒才結束對不對
我就把這裡面分數最高的word 保留下來
就構成一個 word graph
那麼這個東西我底下就只要在這上面算就好了
那麼因此呢我這個複雜的東西
在後面算
那麼我這個時候我這個再把複雜的
那也等於是說我我這個很複雜的這個這個tree 後面接這麼多tree 後面接這麼那這個東西呢我就把它reduce 成為變成只有那樣子
不但是變成只有那樣子而且它不會發散
而是最後會reduce 到一點
不一定是一點啦你這邊可能也有也有不只一個
但是譬如說這邊還有一個w 九
但是基本上你不不會一直這樣越長越大越長越大
你你你可以限制它就這麼大
ok 於是呢我真正的複雜的tri gram 啦
或者tri phone 啦什麼這個複雜的東西
我只在這上面算
那這個search space
比原來那個要小很多很多那個太大了
那個大到無法算所以我就先我先用一些簡單的就是less knowledge 或者less constraint
用一些簡單的辦法
把那個大的tree
那個太大的那個那個那個 tree
reduce 到變成一個小的graph
然後呢我現在把這個東西
在這上面才做詳細的
那這是我們通常稱為re scoring
你現在再把你的詳細的你的tri gram tri phone
分數詳細去算
那剛才因為只用簡單的所以你那個分數不太對
我現在可以把詳細重算一次分數
所以叫做re scoring
那之後呢你可能會發現這上面雖然有這麼多種可能
其實最可能的是這條
譬如說是w two 接w 八接w 九
可能這條才是你的答案譬如說這樣子
那你就可以在 word graph 上面找出來
那這是所謂的multi pass search 的基本觀念
那當然這樣做的時候基本上你前面的這塊第一個path
所謂的這個word graph generation
其實跟那剛才那個是一樣的
只是簡單一點
我用比較簡單的knowledge 用比較簡單的constraint
譬如說我只用我我不要用tri phone 我不要用tri gram
等等我簡單一點就其實是一樣的
然後我就是保留最重要分數最高的word
譬如說在這個時間點結束是以它最高
或者你也可以再保留一個啦對不對你可以再保留
你保留若干個這個時間點結束的分數最高的
然後你在這個時間點你把它保留你這樣一路這樣你會得到一堆
那你就把它們構成一個graph
那如果是這樣子的話很可能我們可以把它畫成
這樣子這是w 十
那它們都n end 都在同一點
然後後面都可以接這些等等
那這就是所謂的 word graph
那你有了word graph 之後在word graph 上面
再用比較詳細的再重跑一次
re score 這些所有的path 之後
你算哪一條path 分數最高等等
那這是這個所謂用word graph 的方法
那麼n best list 是相同的意思
只是說呢它沒有做成這樣子的word graph
而是直接把前一百名譬如說這個n best 就是n 就是這個這個前n 個名次分數最高的word list
全部把它保留下來
那麼舉例來講在這個case 的話你就可能
就這個case 的話你可能想像的就是譬如說
一三五這是一個
w 一 w 三 w 五
這是一個一三五
那麼二二五也是一個w 二 w 五這也是一個
那麼w 四九也是一個喔等等等等
那你如果沒有把這個word graph 建起來
只是說把分數最高的一些word 的word sequence 把它通通都留下來
譬如保留前一百名或者保留前二百名或者前五十名
那就所謂的這個n 等於一百或者五十或者二百的 n best
那你就把這個list 留下來之後我重新在這上面算分數
那你可以想像這兩種那一個好呢
這個是比較精簡啦
這個可以把它們這個其實包含的東西比這個還豐富
這個只告訴我說一後面接三三後面接五
那這個其實告訴我說
一是在什麼時候結束
三在什麼時候開始
三是在什麼時候結束後面五等等
所以呢我我其實三後面還可以接八接九什麼
它都都在這邊都呈現了
所以這是一個比較精簡的描述的方法
你這樣保留一個這個word graph 的效果
會比這個好
但是這個簡單
你這個這個呢你就是把剛才一路找過來的你第一個path 也是用比較簡單的方法來做
但是我一路走過來之後我就把前一百名留下來
得到一個一百的list 喔
那這就是所謂的n best list
那這兩種方法都可以
我這上面舉的這兩個例子在說明這個這個n best list
是不如這個 word graph 來的有效喔
那像這個例子呢你常常前幾名是只差一點點
i’ll tell you what i think 還是 why i think 還是when i think
只是這個地方不對不曉得是哪一個
其它都一樣
那你如果是做保留這個n best list
就會發現常常譬如說前五名
都一樣只差一個字
那你保留這個呢你全部重算有點浪費嘛
其實你應該把它變成一個word graph
那這邊都一樣只有這個地方不同
對不對只有最後這個地方不同
那這樣子的話你的這個嗯比較有效的使用空間跟這個資訊
那所以呢這個這個這個是
word graph 這是n best list
那不管怎樣你都是這樣
所以呢我的真正的效果呢就是
我de cup 這個de couple 本來的這個複雜的search problem into a simpler process
對不對就是說我我現在就是把我整個的做的話這個太複雜了
所以呢我可以把它拆成兩半
第一半用比較簡單的東西
the first primary by acoustic scores
或者是the second by language 這也是一種辦法
我language 在在第二個做哦等等
或者這個是一個例子
這底下也是一個例子
那基本上我就是把它 de couple 成為兩個stage
或者可以更多
所謂的multi pass 不一定兩個啦你還可以第三個啦
你可以在在這邊之後
我還不做決定
我在這邊之後呢我可以這個弄一個比較複雜的word graph
在這邊再做一次re scoring 把它簡化成一個再簡單一點的再做第三次也可以哦看你要怎麼做
所以你可以分成不只一個path
那這樣的話呢就每一個path 都比較簡單
那我的search space 只有在第一個的時候很大
後面就算動縮小
縮小之後
我再做精緻的
那這是一個常用的方法好
那再下來的這一些呢是是另外一招
這個也是使用很多的
那這一招其實就是所謂的heuristic search
就是我們底下要說的
heuristic 跟這個a star
那heuristic 跟a star 呢這個基本上是a i 裡面搬來的喔
那麼各位之中如果你修a i 的課的話就講一大堆這種東西就很清楚了
那我們這邊呢稍微提一下喔
那等於就是把a i 裡面的heuristic search 搬來
那這個是一個非常有效的方法
那麼也是我們常用的喔
那我們在這裡休息十分鐘好了
