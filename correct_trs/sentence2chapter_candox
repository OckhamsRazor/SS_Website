我們從這裡開始四點零開始我們進入這個比較詳細的然後比較深入的部分	4-1
那四點零我們先講 Hidden  Markov  Model	4-1
這個 H  M  M 畢竟是我們最核心的東西	4-1
所以我們現在開始再深入的來講 H  M  M	4-1
我們雖然在上週已經說過一下不過我們現在再仔細的來說它	4-1
那這一部分最好的 reference 應該是這一本課本的第六章	4-1
這一本課本是比較古老的是在一九九三年的課本已經十二年了十三年了	4-1
那麼這本課本大概別的章節大概不見得還需要看它	4-1
但只有這一章的 H  M  M 我認為是它講的最好的因為當初他們都是發展這裡面東西的人	4-1
然後他們所寫的這段是寫的最清楚的	4-1
那麼之後的所有課本都會講它不過看起來他們都是照著它寫	4-1
然後希望寫的有點不一樣之後都沒有原來的好	4-1
所以呢我比較喜歡用原來的	4-1
所以我底下講的這一段基本上都是 based  on 他們這本書的這一章第六章講的	4-1
那麼我們講的 Hidden  Markov  Model 這個 hidden 是個多加的字比較多的是所謂的 Markov  Model	4-1
那 Markov  Model 並不是什麼專門為語音想的事情 Markov  Model 是一個非常普通的 stochastic  process	4-1
你如果修隨機程序應該就會學到所謂的 Markov  Model	4-1
所以我們先從它來講	4-1
那麼 Markov  Model 其實是簡單的多 in  general 是一個這樣的東西	4-1
它並沒有像我們所規定的知道我們我們已經說 ok 我們是一個 one  d 的一個系列的 state	4-1
我們把它想成這樣這是針對語音而言因為聲音是這樣有順序過來的	4-1
你說了什麼聲音一路是這樣有先後的所以我們就讓它是這樣做	4-1
但是 in  general 你如果去看隨機程序的課本的話一個 Markov  Model 也就是所謂一個 Markov  chain	4-1
它不需要是這樣子 one  d 的而是可以這樣子的任何一個 state 可以跳到任何一個 state 去	4-2
也就是說我們沒有讓它可以跳回去但其實 in  general 是可以跳的	4-2
在那樣的情形下我們看到的 Markov  Model 它是說是一個 triplet	4-2
包括哪三樣東西呢	4-2
第一個就是所有 n 個 state 每一個 state 構成所謂的大 S	4-2
然後呢 state 之間可以有 transition 我們叫做大 A	4-2
那 in  general 的話呢這個 transition 應該是 given  on 它的 history	4-2
我在 t 減一的時候是哪一個 state  t 減二的時候是哪一個 state 我 given 前面的這一堆 state 那麼我下一個會到哪一個 state 機率是多少	4-2
所以 in  general 的話這個 Markov  Model 是說 given 前面這些 state 的 history 下一個機率下一個是哪一個 state 的機率是多少是這一個	4-2
但是呢這個太複雜了所以我們通常把它簡化變成只 depend  on 一個	4-2
這就是我們在上週所說的 A  I  J	4-2
我只看 t 減一個時候是在 i 那麼 t 的時候是 j 的機率就是 A  I  J 我變成只只 depend  on 一個了	4-2
那這種情形我們叫做 first  order  Markov  Chain	4-2
所謂 first  order 就是我只 depend  on 一個雖然 in  general 我也可以 second  order  third  order 等等都可以	4-2
然後再來我開始是哪一個 state 就是在哪裡這是 initial  state  probabilities	4-2
那麼在傳統的 Markov  Chain 就是這樣跟我們上週所說的 H  M  M 還有一個最大的差別是什麼呢	4-2
在這裡在這裡的話呢就是說每一個 state 的 output 都是確定的每一個 state 的 output 是確定的	4-2
也就是說如果它在 state  one 它就是什麼 output state  two 就是什麼 output 所以呢你其實很容易解	4-2
那我們上週所說的不一樣我們上週所說的是說在它在某一個 state 的時候它出來的東西不確定只有一個 distribution 對不對	4-2
所以即使在這個 state 我不知道它長怎樣我只知道它會是有這樣的 distribution	4-2
那跟這個 state 是不同的如果是這個 state 的話呢它有另外一個 distribution	4-2
那麼這個情形之下呢我看到的 output 在這裡	4-2
但是我並不知道它是那個 output 是在哪個 state 那那個才是 H  M  M 真正不同的地方	4-2
所以我們這邊講的還是很簡單的是說呢對某一個 state 而言那它其實是是有一個固定的 deterministic  output	4-2
所以呢其實是不難做的	4-2
我們舉個例子來講這個簡單的 case 那就是假設這很容易就是假設只有三個 state	4-2
那這三個 state 我讓它可以這個有很簡單 output	4-2
所有 state  A 那個 S  one 的 state 的 output 都是 A	4-2
 S  two 的 output 都是 B	4-2
 S  three 的 output 都是 C	4-2
這個跟剛才不一這個跟我們這邊講的不一樣	4-2
我們這邊講的是這個 output 的話呢有很多很多千千萬萬只有一個 distribution	4-2
對不對它呢有很多種只是有一個 distribution 而已	4-2
但是這邊的話是說我都是確定的所以這是比較傳統的 Markov  Model	4-2
它的 output 就是 A 它的 output 就是 B 它的 output 就是 C	4-2
然後呢它們有它們的 state  transition  probabilities 在這裡就是這個 matrix	4-2
 initial  probability 在這裡	4-2
那這個這樣其實很容易啦	4-2
我們舉例來講你如果有一個 ob  sequence 是你 observe 到一個 sequence 是 C  A  B  B  C  A  B  C	4-2
那你馬上就知道因為如果第一個是 C 的話 C 是來自哪裡呢就是 S 三嘛	4-2
第二個是 A  A 來自哪裡呢就是 S  one 嘛	4-2
 B 呢就是 S  two 嘛等等	4-2
你馬上得它的 state  sequence 就就是這個嘛	4-2
既然是這個 state  sequence 是這個的話譬如說你要解我們剛剛我們上週說過你要解這個問題就很容易解啊	4-2
你要你要看到這個東西的機率是多少	4-2
很容易解啊那就是什麼你看	4-2
這個第一個第一個是 C 嘛是 S 三嘛	4-2
所以表示說我的 initial 要從三開始	4-2
所以我先從這個這個從這個 initial 是從三開始的這個機率就是這個零點一嘛就在這裡	4-2
然後再來呢就是從三跳到一嘛一跳到二嘛	4-2
所以我就從三跳到一嘛一跳到二嘛這就是一個一個的 state  transition  prob 把它走過去	4-2
那每一個都在這裡都有嘛就把它乘起來就好	4-2
所以這是一個非常容易做的情形這個就是傳統的 Markov  Model	4-2
那我們現在講的不是這樣現在講的是怎樣呢就是我現在講的是	4-3
每一個 state 我不是有確定的 output 而是它的 output 只是一個 distribution	4-3
它是千千萬萬的	4-3
那麼因此我當我看到這一堆 distribution 的時候我不知道它到底我看到一個 output 它可以是在這裡也可以是在這裡只是機率不一樣而已	4-3
因此我不知道它到底是在哪一個 state 裡面	4-3
等於說我這個 state 這個 state 的 sequence 是躲在後面的看不到的是 hidden	4-3
因此呢那就是我們所謂的 Hidden  Markov  Model 就是底下講的這一個	4-3
當我前面加了這個字加了 hidden 這個字之後有何不同	4-3
那就是說我的 observation 呢變成是一個機率	4-3
那這個機率呢那麼因此呢是 depend  on 一個 state 的	4-3
那麼在哪一個 state 會有不同的機率在哪一個 state 會有怎不同的機率	4-3
因此呢我就不再是一個剛才那麼簡單的 case	4-3
那麼因此呢什麼東西是 hidden	4-3
就是 state 是 state  sequence 是 hidden	4-3
當我 observe 到一堆 output 的時候我只能猜這一堆 output	4-3
我們上週說譬如說是這個這個 x  one  O  one  O  two  O  one  O  two  O 三的時候	4-3
我只能猜說這個在這裡的話有一個機率在這裡也有一個機率	4-3
不過呢在這個機率比較大所以呢我就認為它是它	4-3
那這個呢到底是在哪裡都有可能只是機率有大有小而已	4-3
但是我永遠不能確定到底它是在哪一個 state 裡面	4-3
所以這個 state  sequence 是 hidden	4-3
那麼因此呢也就是這邊講的喔我根據這個 observation  sequence	4-3
就是這些 O  one  O  two 的話根據這 observation  sequence 這些 O  one  O  two 的話呢我	4-3
 never  know 到底我的 state 是哪一個	4-3
我其實是永遠不知道我都是用猜的	4-3
因此呢這就是我們上週說過是個 double 是個雙重的 stochastic  process	4-3
我有兩個兩層的 random	4-3
第一層是這個 state 會跳來跳去這個也是 random 的有一個機率	4-3
然後我並不能確定它在哪裡	4-3
第二層是 given 每一個 state 它是哪一種也是不確定的	4-3
所以是雙層的 stochastic  process	4-3
這個我們大概簡單的複習一下因為這個我們上週都已經講過了	4-3
因此呢它跟剛才有何不同就是多了一個 B 嘛	4-3
我現在會有一個 B 也就是這一堆 prob 這一堆 distribution	4-3
 given 每一個 state 它的 distribution 會長怎樣呢那就是我們講的 B 嘛	4-3
就是上週說的那個 B	4-3
那那個 B 長怎樣呢	4-3
那麼一般你去看那個課本的話通常它們會說這個 B 有長兩個樣子	4-3
就是我們這邊講的就是我現在因為有了這個 random 的 given 那個 state 我還我只有一個 distribution 嘛	4-3
那麼每一個 state  distribution 我們叫做 B  j 的這個 O  t 嘛	4-3
譬如說這個是 B  one 的話呢 j 等於一就是這個 state 它會有這樣 distribution	4-3
 B  two j 等於二的話呢在這樣 state 它會有這樣的 distribution 等等	4-3
那這是所謂的這個 B  j	4-3
那麼這些東西就構成我們講的 B	4-3
這一堆的就是我們講的 B 所有的 j	4-3
那就多了這個 B	4-3
它是所有的這個 probability  function	4-3
那麼每一個 describe 它的對某一個 state 而言它是怎樣的機率	4-3
就是多了這個 B	4-3
那麼這個 B 長怎樣呢	4-3
我們上週說我們 B 就是把它看成什麼呢看成一堆 Gaussian	4-3
所以呢這邊可以你可以看成這是一個 Gaussian 這是一個把它看成 model 成為一堆 Gaussian 兜起來的等等看了一堆 Gaussian	4-3
那就是我們這邊所說的底下這個	4-3
那這個其實沒什麼特別跟我們只是跟我們上週符號有一點不一樣而已	4-3
那其實是一樣的那這個就是 gaussian 嘛	4-3
我們上週所講的那個 multi  variate 就是有 n 個 vec  n 個 variable 兜在一起的那個大 gaussian	4-3
那這個就是那個 vector 減掉 mean 然後呢 transpose 相乘	4-3
跟它的 inverse 這就是 co  variance  matrix  inverse 等等	4-3
所以這個就是我們上週所說的那個那這是一個大 gaussian 就是 B  j  k 的	4-3
這個就是我在 state  j	4-3
 j 是那個 state  j 就是這個 state 的的 index	4-3
 k 呢就是第 k 個 gaussian	4-3
所以呢 B  j  k 就是第 k 個 gaussian	4-3
然後呢我的這個有一個 weight 加起來等等	4-3
這應該是小 b 啊寫大 B 寫的應該是這個小 b	4-3
那所以這就是我們上週所說的這個的東西	4-3
那這邊是寫的比較嚴謹一點	4-3
它說呢你可以想像成我的每一個 observation	4-3
這個每一個 observation 相當於是我們這邊的 O  O  t 啦這種東西啦	4-3
這個每一個 observation 你可以看成是 d 的 dimension 的 vector	4-3
每一個 component 是一個 real  number	4-3
對不對寫這樣的意思就是說我這裡的每一個呢是一個是一個 vector	4-3
總共有幾個呢總共有 d 個	4-3
 d 就是它的 DIMENSION 的數目	4-3
總共有 d 個	4-3
然後每一個呢可以是一個任何的 real  number 是一個 arbitrary  real  number	4-3
所以呢這個寫法就這樣的意思	4-3
表示它是一個 d  DIMENSION 的的這個 real  number 所構成的 vector	4-3
那麼因此它的機率呢就長成這樣等等	4-3
那這個就是指我在第 j 個 state 的時候	4-3
那它的 OBSERVATION 的長相就是一個這樣的 gaussian	4-3
這是這所以這一塊就是我們這邊所說的喔喔上週所說的一堆 gaussian 這樣的 distribution	4-3
那麼其實我們今天用的都是這一個	4-3
那你如果去看課本的話它還會講上面那一個	4-3
那這個是在九零年代的初期或者到八零年代的末期的時候人家用的是這一個	4-3
在當時的 computer 還很還很破	4-3
在那個年代這麼多的 gaussian 很難算	4-3
因此當時呢都用這個方法	4-3
所以這個是大概在九零年代的初期以及八零年代用的方法	4-3
那是我們所謂的 discrete 那所謂 discrete 是什麼呢	4-4
我們底下這是叫 continuous	4-4
我們今天其實都是用這個啦	4-4
我們簡單說一下 discrete 其實只是一個 discrete 的 approximation	4-4
那麼換句話說呢我如果是有一個 distribution 是這樣子	4-4
我們叫做 f  of  x 的時候是說呢它可以它是在會有這樣的機率對不對	4-4
如果這個太難算因為如果動不動都要積分什麼很難算的話呢	4-4
我換一個辦法就是把它變成一個 discrete	4-4
這也可以我用這個來 approximate 它	4-4
那麼我說呢譬如說這個機率只有零點零三這個有零點零一這個有零點零零五	4-4
我給它每一個我總共就是本來這邊是有無限多個值嘛	4-4
我這邊只剩下譬如說一百個值	4-4
那每一個值是多少我都有	4-4
那如果用這個的話呢也代表這個東西	4-4
只不過呢我現在呢我可能只有 x  k	4-4
這個 k 呢就是從這邊到這邊對不對	4-4
這邊只有 x  one 到 x  m	4-4
我就這 m 個值各有一個值其他就都沒有了	4-4
我這個呢是做為它的 approximation 這個比較容易算	4-4
同樣的情形那我們現在在這邊講的跟相對於這個關係就跟這個一樣	4-4
那在早年沒有辦法算這麼複雜的 gaussian 的時候呢它就是這樣算的它就是在這個在這個 n 維的這個 d 維的大空間裡面我們 define 一堆點	4-4
舉例來講五百一十二的點或是兩百一十二個點或者是多少個點	4-4
每一個點給它一個數字就是它	4-4
每一個點給它一個數字就是它	4-4
所以呢如果在 state  one 的話呢	4-4
你說 ok 這個是零點零零三這個是零點零零五這個是零點零二什麼什麼什麼你都給它一個值	4-4
那如果是在這個 state 的話呢它等於這堆值的話呢那就是這個 state	4-4
那如果是在這個 state 的話呢那就是數字換了而已這就換另外一堆數字	4-4
那就是這個 state 等等	4-4
那跟這個來 approximate 來 approximate 它是完全一樣的	4-4
那那這個情形呢用這個來 approximate 它	4-4
那這個呢就是所謂的 discrete  time 就是 discrete  finite  observation 的做法	4-4
那所以你看這樣你就可以了解這邊講的就是這麼回事兒	4-4
這時候我就 define 大 M 個 r 的 d 次方的 real  number	4-4
這大 M 個是什麼呢就是這邊的大 M 個點	4-4
我總共有這這這個大 M 我總共有大 M 個點在這裡	4-4
那然後呢我就是為每一個 state 看它是在哪一個 state	4-4
那這裡的每一個點上面都有一個機率	4-4
就像這邊給他零點零零三這邊給他零點零零五等等等等	4-4
那這樣我得到這個東西用這個來取代它	4-4
那這就是所謂的 discrete	4-4
所以呢你如果看比較早年的文章或者書裡面它會講有 discrete 跟 continuous 兩種 hidden  Markov  Model	4-4
那就是這樣的來的	4-4
雖然我們今天都用底下的這一種不會再用上面那個	4-4
因為這個顯然比那個好嘛	4-4
好這是一個簡單的解釋	4-5
有了以上的呢我們現在來看一個最簡單的例子	4-5
那就是喔這個跟剛才那個很像跟剛才的那張圖很像	4-5
唯一不同的是剛才的那張圖是普通的傳統的 Markov  Model	4-5
你如果是 state  s  one 的話呢 output 就是 A  state  s  two 的話呢 output 就是 B	4-5
那我現在不是了那我現在呢變成了 hidden 的 Markov  Model	4-5
多了 hidden 這個字之後呢你會發現呢不管是 S  one 也好 S  two 也好 S 三也好它的 output 都是 A  B  C 三種都有	4-5
假設只有三的話呢它都是 A  B 三種都有	4-5
只是呢機率不同而已	4-5
所以呢在 s  one 的話機率是零點三零點二零點五	4-5
到 s  two 的話機率是這樣不同了就是了	4-5
但是呢它都會出現的	4-5
那麼於是呢	4-5
我今天如果 observe 到一個 sequence 是 A  B  C 的話	4-5
你不能確定這個 A 是從哪兒來的	4-5
雖然在這裡機率最大是零點七	4-5
你不能說它不會從這裡出來雖然它是零點三	4-5
對不對它也會從這兒出來因為它也是零點三	4-5
你這個 B 的話呢這邊是零點六好像是機率最大	4-5
不過呢零點一也會出來啊這個零點二也有可能啊	4-5
所以呢你這個都有可能	4-5
所以呢 A 會有三種可能的 state 來源	4-5
 B 也有三種 C 也有三種	4-5
所以總共有二十七種可能的機率	4-5
那這就是我們講的 hidden  Markov  Model 這就是有這個 hidden 的現象	4-5
那這個情形等於是一個簡化的這個 model	4-5
那也有另外一個畫法就是這個我們上週也說過就是你可以把它想像成是三個桶子放在幕後	4-5
就是桶一桶二桶三	4-5
那每一個桶子裡面都有一堆 A  B  C 球	4-5
只是說它們的數目不一樣多	4-5
所以呢你在每一個桶子裡面你撈出球來它是 A 的機率在這裡是零點七	4-5
在這裡面撈出一個球它是 A 的機率只有零點三等等	4-5
因此呢就有一個人躲在幕後他不斷的 randomly 從一個 state 一個桶裡抽一個球出來	4-5
然後他唸給你聽說它是 A 它是 B 等等	4-5
你得到這個 A  B  C 是一樣的意思	4-5
但是那個人躲在幕後	4-5
你看不到所以呢你永遠不知道他是從這個 A 是從哪個 state 哪個桶出來的	4-5
所以你就有二十七種可能	4-5
好那麼在這個情形之下怎麼辦	4-5
那麼我們舉例來講其實也不難我們舉例來講	4-5
譬如說我現在要在這個 model 裡面這個 model 就是所謂的 Lambda	4-5
 given 這個 model  Lambda 我要 observe 到這個 o 這個 o 是 A  B  C 的機率是怎麼算呢	4-5
你要把這個 o 看成是所有的這二十七種	4-5
這個 Q  i 呢就是一個 state  sequence	4-5
譬如說是 s 二 s 三 s 一什麼之類的我有這麼多 sequence 我有二十七種	4-5
 Q  i 呢就從這個 i 等於一加到二十七	4-5
對每一個 case 的話呢如果 given 這個 model 而它是這個 state  sequence 的時候我看到這個的機率是多少	4-5
然後把這二十七種全部加起來	4-5
把這個全部加完不就是它	4-5
那你如果是這樣子看的話呢那這個怎麼算呢	4-5
這個再進一步拆開成兩個	4-5
這個其實很容易看就是只是一個條件機率	4-5
那麼你可以簡單的看法就是你可以先不看這個 Lambda	4-5
因為它們都是 given  Lambda 都在 Lambda 都在這個槓槓的右邊都是屬於 given 的條件	4-5
所以你可以先不要看	4-5
如果不看那個就很清楚 o 跟 Q  i 的機率是 o  given  Q  i 再乘上 Q  i	4-5
那這兩個機率一乘就是它嘛	4-5
那我現在通通都加上 Lambda 機率的條件還是一樣就是了	4-5
所以呢我的每一個呢都拆成兩個機率	4-5
一個是如果在這個 model 之下而且是這個 state  sequence 的話那麼我看到這個機率	4-5
乘上第二個是說在這個 model 之下會有這個 state  sequence 的機率	4-5
對不對所以呢我在這個 model 之下會有這個 state 這會有這個 state  sequence 的機率乘上	4-5
如果 given 是這個 state  sequence 的話我的看到這個的機率	4-5
那這樣一乘就是這個	4-5
然後我把二十七個全部加起來就可以了	4-5
我們舉一個例子來看這二十七個裡面的一個假設說它是 s 二 s 二 s 三	4-5
也就是假設這個 A 來自 s 二這個 B 也是來自 s 二 C 來自 s 三的話	4-5
那怎麼算我們舉例來講那麼這個機率這個機率其實很容易算	4-5
就是我已經知道第一個是來自 A 是來自 s  two  B 是來自 s  two  C 是來自 s 三	4-5
那我就把這三個裡面這個這個機率分別乘進去就可以了	4-5
所以呢這個只是這個這是第一個機率	4-5
我就分別把它的每一個 state 得到這個 observation 的機率是拿出來算就可以了	4-5
那第二個呢 given 那個 model 會有這個 state  sequence 的機率是什麼呢那也很容易	4-5
我的第一個 state 是 s  two	4-5
所以我就從 initial 是 s  two 開始那就是這個	4-5
然後呢開始從 s  two 跳到 s  two  s  two 再跳到 s 三就一路跳	4-5
對不對就是它跳到它它跳到它	4-5
那就是這個 A  I  J	4-5
二跳到二二跳到三把它們乘進來那就是了	4-5
那等等你就把它加起來那就是我們要的	4-5
所以這個是我們簡單的解釋	4-5
如果我是一個這麼簡單的 hidden  Markov  Model 的話是可以這樣算的	4-5
好那我想這個都不難所以我們只是簡單的這個 overview	4-5
底下我們要進入比較難的就是這三個 basic  problem	4-6
那這個我們其實上週已經說過一次	4-6
我們真正要解要用 H  M  M 來做我們要做的事情的時候	4-6
最核心的三大問題就是這三大問題	4-6
那麼我們底下要做的事情就是講這三個問題的 solution	4-6
那我們今天會講 problem  one 跟 two	4-6
大概今天的時間是講這兩個	4-6
然後呢 problem 三留到下週	4-6
我們從這個 problem  one 開始講起	4-6
喔不在這兒了我要跳到另外一個去	4-6
 ok 在這裡	4-6
那麼這是我們來看 problem  one 來解這個問題	4-6
那麼 problem  one 其實是這裡面最容易解的一個	4-6
這個因為當初這個不在一不在同一個時間做的所以這個不是 powerpoint	4-6
那麼喔 problem  one 是這裡面最容易解的一個問題	4-6
那麼它的問題就是算我們剛才講的那個機率	4-6
這就是我們剛才講的那個機率啊	4-6
 given 一個 model 我會看到一個 observation 的機率	4-6
我就是要算這個	4-6
那這所以 given 一個 model 呢就是 given 某一個 n 個 state 的 model	4-6
它有 n 個 state 有 a 有 b 有 Pi 都已經在那裡了	4-6
所謂的 Lambda 就是 a  b  Pi 各是一堆參數它們的集合叫做 Lambda	4-6
 given 這個 model 然後呢我 given 某一個 o	4-6
 o 是什麼就是 o  one  o  two 到 o 的大 T	4-6
這個這段聲音總共的長度是大 T	4-6
有大 T 個 observation  vector	4-6
那麼構成一個大 O 我 given 了這個	4-6
那麼我想知道它們這個會看到它的機率到底是多少	4-6
那麼痾我們講這樣這個 problem 本身的這個問題其實就是在做 recognition	4-6
我們舉例來講如果我要辨識零到九的十個聲音	4-6
我就是有十個這樣子的 model	4-6
今天進來一個聲音是八	4-6
我就把這個八放進來在每一個 model 去算一個機率	4-6
那照說會在八的 model 的時候機率會最大	4-6
因為我放在我把八放在一的 model 放在二的 model 裡面它進去會很小	4-6
等等我所以這就是我們基基本上在做 recognition 的時候要算的機率	4-6
那我們要來看怎麼算	4-6
那麼要看這個之前呢當然我們要為每一個 o 呢 define 一個它是在哪個 state 裡面	4-6
就是 q  one  q  two 就是指它的	4-6
如果 q  one 是一就是它的第一個 state 等等	4-6
就是有它的 state  sequence	4-6
好那就是我們這邊要講的問題就是這個問題	4-6
我這邊的所有的符號所有的 notation 都是 follow 那本課本裡面的第六章	4-6
那原因是說這樣子的話你去看那本課本的時候比較容易對	4-6
所以呢我這邊的 notation 都是照那邊的課本的	4-6
也因此跟我的其他地方也許稍微有點不一樣不過大致是很接近的就是	4-6
那麼這個 problem 怎麼求呢	4-6
基本上就可以用剛才的那個方法	4-6
這邊講的這個式子就是我們剛才的那個方法	4-6
那這個的意思其實講起來很簡單	4-6
就是說我現在要算這個 probability 就是	4-6
我要算這個 probability 的 o  given 這個 Lambda	4-6
根據我們剛才的講法我就是要算什麼東西呢	4-6
每一個 state 每一個 state  sequence	4-6
不過它這邊的 state  sequence 叫做 q  bar	4-6
我每一個 state  sequence 叫做 q  bar	4-6
就是我上面這裡嘛就是這個這個 q  bar 嘛就是我的 state  sequence	4-6
 given 每一個 q  bar 然後呢 Lambda	4-6
然後我 summation  over 所有的 q  bar	4-6
你可以先把它寫成這樣子	4-6
這跟剛才那個是完全相同的	4-6
我就是我現在不知道哪個 state  sequence 我就假設某一個 state  sequence 那這有一個機率	4-6
然後把所有可能的 state  sequence 全部加起來這就是我的答案	4-6
那然後這東西怎麼辦呢這東西把它拆成兩個	4-6
那就是第一個呢是	4-6
如果是 given 某一個 state  sequence 的話	4-6
然後呢第二個是	4-6
那一個 state  sequence 的機率還是一樣	4-6
所有的 q  bar	4-6
我就是把這個機率呢把它拆成兩個	4-6
跟我們剛才那個簡單的例子是完全相同只是我現在比較複雜而已	4-6
因此呢我如果是在某一個 state  sequence 看到這個的機率的話	4-6
我可以拆成我先 given 這個 model 會有那個 state  sequence 的機率乘上 given 那個 state  sequence 看到它的機率這兩個相乘	4-6
當我看拆成這兩個之後呢那就是我們這邊所寫的再下一步	4-6
那其中的這裡的第一式再拆出來就是上面的這一個	4-6
所以你看到我上面的這一個呢就是上面這個嘛	4-6
上面這一大堆東西這一大堆東西就是上面這個	4-6
上面這個就是這裡的第一式	4-6
然後乘上底下這一這一大堆東西呢	4-6
底下這一大堆東西就是就是它的第二個就下面這個	4-6
這個就是這邊的第二式	4-6
所以呢我這個就會這個這個再出來就會變成那一大堆	4-6
然後那個再出來就會變成這一大堆	4-6
那就是我這邊的式子	4-6
那這一這兩大堆其實也都很容易看	4-6
那麼譬如說如果它是這個 state  sequence 如果它是這個 state  sequence  given 是這個 state  sequence 的話	4-6
我看到這個機率是什麼呢那就是	4-6
把 o  one 放在第一個 state 去把 o  two 放在第二個 state 等等全部乘起來	4-6
對不對那就跟剛才我們那個簡單的例子是一樣的對不對	4-6
如果說我已經知道了是這個 state  sequence 的話	4-6
它的第一個 state 是 q  one 第二個 state 是 q  two 就是這個嘛	4-6
第一個 state 是 q  one 第二個 state 是 q  two 嘛	4-6
那我就把 o  one 放在第一個 state 的那個 B 裡面去	4-6
把 o  two 放在第二個 state 的那個 B 裡面去等等	4-6
這樣乘起來不就是這個嗎	4-6
就是這個	4-6
那至於說第二個呢	4-6
你如果要看到這個 state  sequence 機率是什麼呢	4-6
那你就從 initial  probability 開始啊	4-6
我的第一個 state 會是 q  one 的機率就是 Pi 的 q  one	4-6
然後就開始用跳的嘛	4-6
從 q  one 跳到 q  two 的是 A 這是 a  i  j	4-6
 q  two 跳到 q 三	4-6
一路 q 一路 q 這樣跳過來跳到最後一個 q  t	4-6
那因此我就得到以下那個機率	4-6
所以這兩個就這樣可以算的出來	4-6
那這個式子其實跟我們剛才的那個	4-6
這個跟我們剛才那個簡單的那個 case 是完全相同的	4-6
你如果再看一次的話	4-6
我們來看剛才的那個就是在	4-6
這裡面的這個	4-6
 yeah 就是這張	4-6
我們剛才講這一張其實是完全一樣的	4-6
那麼在這個簡單的例子裡面很容易看嘛	4-6
就是這樣子	4-6
然後這個拆成這個這個拆成這兩個	4-6
就是我們剛才講的就是這個拆成這個這個拆成這兩個	4-6
那這兩個就是這兩個嘛	4-6
就是兩個就是就是這邊這兩個	4-6
然後這裡面的第一個怎麼算就是用這些東西來算	4-6
那麼第一個怎麼算用這些東西來算就是我們剛才看到的這一堆東西	4-6
然後呢第二個怎麼算第二個用這些東西來算	4-6
那就是我們剛才看到的這一堆東西	4-6
所以呢我們看到的這一堆只是這個簡單的問題的一個比較複雜的寫法而已	4-6
好如果這個沒問題的話那我們回過頭來看	4-6
它的問題在哪裡問題在這個計算量太大了	4-6
因為我我這邊要對每一個 state  sequence 都去算這些東西	4-6
然後我的 state  sequence 呢有多少個	4-6
有 n 的 t 次方個 in  general	4-6
其實會比這個少一點啦不過也夠大了	4-6
這個 in  general 的意思是說我現在等於是假設所有 state 都可以跳來跳去	4-6
我假設是我有 t 個 observation	4-6
我有 t 個 observation	4-6
第一個呢假設是有 n 個 state 都有可能	4-6
第二個也是 n 個 state 都有可能	4-6
第三個也是 n 個 state 都有可能	4-6
所以有我總共有多少呢有 n 的 t 次方個	4-6
但是我這個 t 可能很大喔	4-6
譬如說我一段聲音你記得我們這些是 o  o 是怎麼來的	4-6
它是它是它是這個我這個訊號我這樣子取取第一個 o 嘛	4-6
然後這 shift 過來我再取第二個 o 嘛對不對	4-6
因此我這樣一弄的話呢我可以這個	4-6
平常你一個 utterance 你講一句話	4-6
我這出來是幾百幾千個幾百幾千個的這個 o	4-6
那如果你這個 n 在幾百幾千次方就很大很大了	4-6
這個是這是 order 非常高的一個計算量	4-6
雖然我可以用 computer 來算	4-6
這個還是會非常非常大	4-6
那麼即使說 ok 我們簡化到讓它一定從這裡開始	4-6
所以譬如說我開始不會從中間開始不會從後面開始一定會從這邊開始	4-6
然後呢我規定它不能跳回去	4-6
使得我的這個 state 狀況少一點	4-6
但是也這個數目還是非常大	4-6
所以呢這個計算量是非常大的	4-6
所以事實上呢這個我們這個問題我們不是用這個方式來解的	4-6
那我們有一個更有效的方法	4-7
那就是我們底下要講的這一個	4-7
所謂的這個在那本課本裡面在十多年前的課本裡面它叫做 forward  procedure	4-7
它是靠 define 一個叫做 forward  variable  Alpha  t 的 i 來解	4-7
那這個就完全用 iteration 的方式	4-7
就很方便就可以把它答案找出來	4-7
那在今天的話通常呢我們有一個名字它就叫做 forward  algorithm	4-7
所以一般我們講的所謂的這個東西我們就說它是所謂的 forward  algorithm	4-7
那麼我就用它來解就可以了	4-7
那借助一堆 iteration 就馬上就可以算出來	4-7
這個計算量大為簡單	4-7
 ok 我們這個留到下一堂課	4-7
我們在這裡休息十分鐘	4-7
 ok	4-7
 ok 我們接下去	4-7
接下來講這個 forward  problem	4-7
這個 forward  algorithm 那麼	4-7
講這個東西的時候從這裡開始我都用這麼一張圖所以我們來先來畫一張圖	4-7
我這裡的三個 basic  problem 都用這個圖來解釋	4-7
這個圖的橫軸是時間 t	4-7
所以呢	4-7
有 t 時間 t 等於一等於二等於三	4-7
到時間等於 t 到最後到大 t	4-7
這個是我的最後的	4-7
這個大 t 是我的最後的時間	4-7
因此 t 等於一的時候呢相當於我有一個	4-7
 o  one 在這裡	4-7
 t 等於小 t 的時候我相當於有一個 o  t 在這裡	4-7
等等這是我的橫軸	4-7
我的縱軸呢是 state  number	4-7
橫軸是 time  index	4-7
縱軸呢是 state  number	4-7
譬如說呢這是 state 一 state 二 state 三	4-7
一直到 state  n 喔這樣子	4-7
那麼舉例來講呢這個其實等於是說我把那個 hidden  markov 排到這邊來	4-7
這是第一個 state 這是第二個 state 第三個 state	4-7
一直到最後第 n 個 state 在這裡	4-7
那麼它會這樣跳過去他也可以跳回他自己他可以跳到下一個	4-7
等等喔那這是我的這個	4-7
縱軸是 state 是就那個 model 橫軸是我的時間	4-7
因此我會怎樣呢舉例來講	4-7
我的第一個如果是相當於第一個 state 的話呢等於是這一點	4-7
那麼下一瞬間在二的時候呢	4-7
我會有一個 o  two 了 o  two 呢可能還是回到原來的那我就表示這樣子	4-7
他二的時候仍然在一的地方	4-7
但是呢我也可能跳到下一個於是呢我就到這裡	4-7
這我只有兩種可能它可以到這裡也可以到這裡	4-7
他其實也可以變成三如果那樣的話呢我這邊還有一個他也可以跳到這裡來	4-7
等等那然後呢如果二的時候還是在	4-7
時間二的時候還在 state 一的話那三的話呢仍然可能還在一	4-7
因為我還是有個機率回到原來的嘛這還是可能是一	4-7
但是呢我也可能由這裡呢到三的時候會跳到二那就是跳到這裡	4-7
我也可能三的時候呢跳到三就到這裡	4-7
同理呢我在二的時候如果是二的話	4-7
剛才二就跳到這裡來的話呢二還是可以回到二	4-7
可以在這裡也可以跳到三也可以跳到四	4-7
等等那三呢如果一是二跳到三的話我也可以到這裡來也可以過來等等	4-7
那麼以此類推我就可以在這上面畫出這張圖出來就是我的所有的可能	4-7
那麼因此呢我的每一瞬間他是在	4-7
可以在哪一個 state 上然後我可以怎麼跳怎麼都在這邊畫出來	4-7
那然後我每一個我我要找的某一個 state  sequence 其實就是某一條 path	4-7
上面的某一條 path 走走走走到哪裡	4-7
那那一條 path 就是我的 state  sequence	4-7
那我們在之前說我有那麼多個 state  sequence 那其實所有的 state  sequence 都在這裡	4-7
我們剛才說我的 state  sequence 那麼多那麼多那麼多我每個都要算	4-7
那其實呢我可以不用算那麼複雜因為他們我都在這張圖上	4-7
 ok 好有了這個之後我們現在來看我們怎麼 define 這個	4-7
喔	4-7
做這個 forward  algorithm	4-7
那麼我們先 define 叫做這個	4-7
 forward  variable 就 alpha  t 的 i	4-7
這個定義是什麼呢	4-7
就是我從頭看看過來看到 t 為止	4-7
看到 t 為止其中呢在 t 的時候是在 state  i	4-7
至於前面是在哪裡呢我無所謂的	4-7
我沒有規定但是我就規定在 t 的時候要在在 i 上舉例來講譬如說這個是 state  i	4-7
那麼在 t 的時候要在 i 上	4-7
我只有規定在 t 的時候要在 i 上	4-7
我從頭看到 t 為止	4-7
那麼從一到 t 減一呢我沒有規定他要在哪裡	4-7
所以呢我可以隨便來設一個都行我們舉例來講	4-7
我們舉例來說我的	4-7
我可以看成是這樣	4-7
這個意思是說呢我從頭看到	4-7
從 t  o  one 一直到 o  t 我看到 o  t 為止	4-7
其中在 t 的時候呢規定一定要在 i 上面	4-7
除了 t 之外前面的到 t 減一為止沒有規定所以我在這整塊裡面	4-7
不管怎麼走都沒有關係	4-7
怎麼走都可以我只是都可以但是只有只有這一點一定要在這裡	4-7
那這個機率呢叫做 alpha  t 的 i	4-7
 ok 所以你看到呢 given 這個 model 之下呢	4-7
我要看到的就是這一堆一到 t 其中 t 的時候是 i 其它的沒有規定	4-7
這個叫做 alpha  t 的 i 那然後呢你看到他是兩個參數就是一個 t 一個 i 嘛	4-7
所以 alpha  t 的 i 他有兩個參數其實就是這個圖的橫軸跟縱軸 t 就是這個橫軸	4-7
 i 就是 state  number 就是這個縱軸所以呢這個意思是說這個 alpha  t 的 i 其實是 define  for	4-7
這上面的每一點這上面的每一點都可以有一個 alpha  t 的 i	4-7
因此你也可以想像成這個 alpha  t 的 i 呢	4-7
這個所謂的這個 forward  variable 這個東西呢	4-7
是一個數字可以放到這裡的每一點上去	4-7
每一點我都可以填上一個這個東西	4-7
那這整個就是一張表因此這整個 algorithm 在幹麻他就是在填這一張表	4-7
他的填法就是一行一行的填	4-7
我的前面 initialization 這個 initialization 就是	4-7
是如何填第一行第一行填完之後呢我就有了第一行就可以算下一行	4-7
就有了每一行就可以算下一行就是這邊講的這件事情其實就是一個 iteration	4-7
當我有了這一行我就填下一行	4-7
那麼這樣於是可以一路填過來等到這一行填完的時候	4-7
我的答案就出來了	4-7
那就是最後這就是所謂的 forward  algorithm	4-7
那為什麼叫 forward  algorithm 你也可以想的出來因為它就一行一行向前走	4-7
每一次就是多看一行每一次多看一行這樣子走	4-7
走到最後答案就出來了所以他叫做 forward  algorithm	4-7
那這些 alpha 叫做 forward  variable	4-7
好那我們現在看我如何填第一行	4-7
第一行其實很容易	4-7
因為第一行的話根本沒有前面的東西嘛	4-7
你看我的定義是如果是 alpha  t 的話呢是指	4-7
 t 一定要在 i 上面	4-7
至於 t 減一之前呢沒有規定嘛	4-7
可是如果是 t 等於一的話我現在如何填第一行	4-7
填第一行是 t 等於一當 t 等於一的時候	4-7
我其實根本沒有前面的東西	4-7
這個前面前面這些根本沒有嘛所以呢根本就是把它的這個會在這些點上面的機率算出來而已	4-7
那很簡單就是把 o  one 放在第一個 state 的機率是多少	4-7
把 o  one 放在第二個 state 的機率是多少乘上 initially 一開始他有個機率	4-7
就是 pi  one  pi  two 那麼因此就是這個對不對	4-7
就是 t 時間等於一的時候根本沒有比 t 減一之前這些問題都沒有這一這一點就在這裡嘛	4-7
這一點就在這裡嘛所以根本沒有這個前面的問題所以我只要看這個會在那個的機率是多少	4-7
所以呢就是要從這個 initial  probability 就是就是第一個要會在 state  i 的機率	4-7
就是 pi  i 然後呢我現在把第一個機率放進去	4-7
第一我再把第一個 vector 放在第 i 個 state	4-7
就是 b  i 的 o  one 那這樣子的話呢那我現在把 i 從一算到 n	4-7
我全部算出來那第一行就排出來了嘛	4-7
所以呢這是第一行其實很容易算這就 initialization	4-7
那這個 iteration 的核心就是第二這是第二個就是你如何有了前面一行如何算下一行	4-7
那這個其實也很容易你可以想像	4-7
根據這邊的式子是說如果我有了如果我有了第 i 行哦	4-7
如果我有了第 t 行要如何算 t 加一行	4-7
如果有了第 t 行我現在要算第 t 加一行怎麼算	4-7
我現在要算 t 加一行如果是在 j	4-7
我們舉例來講如果這個是 j 的話	4-7
我要算這一個	4-7
那這個是什麼呢	4-7
這個是跟剛才一樣	4-7
是我要是這個這樣的	4-7
對不對	4-7
也就是說呢我要把整個向前推一步	4-7
我現在是 t 加一要在 j 上面到 t 為止呢沒有規定通通都可以	4-7
那麼於是呢於是呢到 t 為止當然囉我可以從這個 i 過來	4-7
但是我也可以從另外的其他的東西過來	4-7
都可以那於是呢我們舉例來講	4-7
如果是在如果在 t 的時候是 i 的話那就是我們剛才的 alpha  t 的 i	4-7
就是這個東西然後我現在要從 i 跳到 j	4-7
所以要乘上一個 a  i  j 對不對這個 alpha  t 的 i 就是我們前面前一行已經算好的	4-7
如果前一行是 t 的時候是在 i	4-7
前面都有了前面都都算了在 t 的時候是 i 的這個	4-7
然後乘上這個 a  i  j 就是這個跳到這邊的機率	4-7
那就是這個東西但是現在這個 i 呢是每一個都可以阿	4-7
因為我也可能是從這個過來的	4-7
從這個過來的話應該有另外一個 alpha  t 的 i 也可以用的等等	4-7
舉例來說	4-7
我也可以是這一個那他有一個是這個跟這個也可以阿對不對	4-7
那那那表示說我在 t 的時間也許是在這個上面	4-7
那之前的所有的從這邊跳過來跳到這裡的那這個也可以有個 a  i  j 也可以到那兒去阿	4-7
等等那我可能還有另外一個是在這裡阿	4-7
 t 的時候可能是在這裡阿我也有另外一個在這裡呀那他也是前面的都算了	4-7
只有這個在這裡了那他也可以跳過去阿	4-7
那我如果把所有這個全部都加起來的話不表示把這一行也都所有都都算進去了嘛	4-7
那於是就得到下一個嘛	4-7
對不對那麼因此呢我就是把這邊的 alpha  t 的 i 的這個東西乘上 a  i  j	4-7
就在這裡乘上這個 a  i  j	4-7
那也可以是這個 i 乘上這個 a  i  j 也可以這個 alpha  t 的 i 乘上這個 a  i  j 那如果全部通通加起來的話	4-7
其實就是我把 t 的這一行也讓他每一點都可以了	4-7
那個加起來不就是我要的這個嗎對不對	4-7
所以呢我就加起來就是這個但是呢你加完之後還要記得這件事	4-7
我現在要把 t 加一的 vector	4-7
這是 t 加一的這個 vector 也放進	4-7
把這個 vector 要放進那個 j 裡面去得到這個機率	4-7
所以我最後乘上一個這個那這樣我就變成 t 加一的 j	4-7
那這就是說明我怎麼算下一行我只要這一行的通通都有了	4-7
我下一行的一個都可以那樣算那每一個都可以這樣算	4-7
那麼我這邊只要 j 等於一到 n 的話我的所有的都可以算出來	4-7
雖然我只要這一行算完之後下一行我就都可以照算	4-7
然後以此類推的話呢我就每一行每一行都可以算	4-7
所以 t 呢一直算到 t 減一於是我就把大 t 都算出來	4-7
因此呢我變成是	4-7
那麼從 initialization 到這個 iteration 我等於是在填這個表	4-7
一路填過來等到這邊都填完的時候這邊都有了	4-7
都填完了那麼這個時候呢這些就什麼呢就是	4-7
 alpha  t 的 i	4-7
這個小 t 已經變成大 t 了這是最後這一行	4-7
當我最後這一行算出來的時候呢我的答案很簡單我的我要的那個機率就是	4-7
所有的最後那一行加起來一加 i	4-7
這邊全部加起來就對了是不是這樣	4-7
因為就這個 case 而言他是講我的最後一個 state 在這裡	4-7
前面的所有的機率我都算進去了	4-7
等等我都已經算進去了	4-7
那是最後停在這裡的那我把這邊這個的話就這邊停在這裡全部算進去了	4-7
這個最後停在這裡全部算進去了那這樣一完全部加起來的話我是把全部所有的 path 都算完了	4-7
所以呢這麼一來的話呢我所有 path 全部算完他機率都算進去了	4-7
那麼因此就是我要的這個值 ok 那麼因此呢	4-7
我現在只要把這個 iteration 這樣一走的話	4-7
我的計算量大為減少只是填這張表	4-7
這張表不過是 t 乘以 n 而已 t 乘以 n	4-7
那麼那麼我算完就完了	4-7
那麼它的這個這就所謂的 forward  algorithm 講起來是蠻容易的	4-7
這是我們三個 problem 裡面最容易的一個 problem 那麼那麼他為什麼可以把那麼複雜的	4-7
本來是 n 的 t 次方變成 t 乘以 n 那主要就是說他所有的 state  sequence 呢	4-7
就底下這句話講的意思你雖然他的好像有那麼多 state  sequence 其實他們永遠在這張表上面而已	4-7
這張表已經把所有的那麼多都都算進去了	4-7
不管他前面怎麼走他每一個時間永遠只有 n 個點	4-7
他在每一瞬間永遠只有 n 種 state 因此我就是	4-7
把前面的 n 個算出來之後就得到下一個 n	4-7
那就是就像填表一樣我這一行填完填下一行我每一次算出這 n 個出來	4-7
這樣一路下去答案就都出來了	4-7
那就底下這句話講的不管你前面用多少個 state	4-7
怎怎麼走喔你但是你最後的話最後的話都是 merge 到這 n 個 state	4-7
所以呢你只要填每一個每一個時間只要填這 n 個數字就可以了	4-7
那這就是所謂的 forward  algorithm 這也就是我們第一個 problem 的 solution	4-7
這是最容易的一個 problem	4-7
我們來看 problem  two  problem  two 就比較麻煩了	4-8
他我現在想希望得到的是 state  sequence	4-8
我我希望得到這個 state  sequence	4-8
我剛才並沒有真正的去求 state  sequence 我剛才是把全部的全部加起來了	4-8
這裡面有無限非常多個 state  sequence 就 n 的 t 次方種 state  sequence 都在這裡面	4-8
我可以這樣我可以這樣我我全部都算在裡面了	4-8
但是到底哪一個才是最可能的 state  sequence 我們沒有講	4-8
那我們 problem  two 是要來解說	4-8
到底哪一個是最可能的哪一個是最可能的 state  sequence	4-8
求這個 q	4-8
那也就是說呢我的 sequence 總共是有 n 的 t 次方個	4-8
但是其實裡面應該有個機率最高的那一個	4-8
機率最最高的那一個可能是譬如說這樣走這樣走這樣走這樣走	4-8
最後走到這來	4-8
那那一條才是機率最最高的那一個我要求那一個	4-8
這是 state  two 這個 problem  two 的問題	4-8
那 problem  one 的話我沒有算這個問題 problem  one 我是把全部通通加在一起算他的機率	4-8
那我 problem  two 現在是要找機率最高的那一條 path	4-8
在這個圖而言就是要找機率最高的那一條 path	4-8
那在這邊來講的話就是要找那個 state  sequence	4-8
那這裡我們要解釋一下就是	4-8
這邊講的這三個 problem 我們都做 general 的假設並沒有簡化的假設	4-8
我們講過簡化的假設是	4-8
我只是 one  d 的每一個 state 跳到下一個	4-8
他不可以跳回來的	4-8
那麼然後呢他一定從第一個 state 開始走的	4-8
把這些假設放進去之後在這個圖上有一點不一樣哪一些地方不一樣呢就是	4-8
譬如說這邊一開始是零這一堆有一堆是零	4-8
因為你要從這邊開始走	4-8
然後開始慢慢慢慢 spread 開來	4-8
一開始這上面是是沒有的一開始一定從這開始	4-8
如果有了那個假設的話而且我的 path 只能向上走不能向下走對不對	4-8
他那邊就是只能向右走不能回去的意思就是說只能向上走不能向下走	4-8
所以在在那樣的假設之下這個問題會稍微簡化一點	4-8
但是我們現在講的這三個 problem 我們現在講法是沒有做那個假設	4-8
沒有做那個假設所以我並沒有假設說這邊會是零	4-8
然後也沒有假設說他一定只能向上走他也可以向下走	4-8
好	4-8
那我們現在再來看這個 problem  two 怎麼解法呢	4-8
那麼一個簡單的辦法是	4-8
再定義第二個 beta 叫做 backward  variable	4-8
 beta 是一個 backward  variable	4-8
然後呢我們也有一套演算法來算這個 beta	4-8
那這個呢叫做 backward  algorithm 就是往回走的那麼就有這個所謂的	4-8
那麼這個是什麼呢我們底下來解釋不過他的意思是跟剛才剛好反過來	4-8
就是說呢我是從先算最後這一行	4-8
我也是一樣 define 另外一套東西叫做 beta  t 的 i	4-8
我 define 這堆東西	4-8
也是一樣他是對每一個 t 每一個 i 就跟這個完全相同	4-8
每一個 t 每一個 i 上面都有一點都有一個數字所以呢等於這每一點都有一個數字	4-8
不過他是這個東西然後呢我我反過來	4-8
我是先從先算最後一行	4-8
然後你只要有前面一行就可有後面一行就可以算前面一行有後面一行就可以算前面一行這樣一路倒回去	4-8
把它全部填滿這個叫做 backward  algorithm	4-8
然後呢我們求出來的這個東西叫做 backward  variable	4-8
 backward  variable  beta  t 的 i	4-8
那麼底下我們講的就是這件事	4-8
那我們後面後面會看到怎麼用這個來來用就是了	4-8
好那麼我現在先來看什麼是這個 backward  variable  beta  t 的 i	4-8
他的定義是 given 這個 model	4-8
然後呢我要 given 說我 state  i 在 state 這個在在在 time  t 的時候是在 state  i 上面	4-8
 ok  time  t 在 state  i 上面然後呢我是從 t 加一開始看到大 t	4-8
好那我們現在把剛才這個都擦掉	4-8
我們看這個 case 是什麼	4-8
如果我的時間 t 要在 i 上面的話	4-8
我還是一樣是時間 t 要在 i 上面	4-8
但是呢	4-8
有點不太一樣的是說我看到的是從 t 加一到大 t	4-8
所以我看到的是從 t 加一開始的	4-8
到大 t 但是我完全都沒有規定這樣子我這邊都看到了	4-8
從 t 加一開始都看到了我都完全沒有規定但是我要 t 的時候要在這點上	4-8
這個叫做 beta  t 的 i	4-8
 ok	4-8
那麼因此呢你看到就是這樣子	4-8
我要在時間 t 的時候是在 i 上面	4-8
然後我看到的是從 t 加一開始看到大 t	4-8
這個定義跟剛才有一點有一點對稱但是不完全像喔什麼地方	4-8
不完全像呢我們可以仔細看一下	4-8
剛才的這個時間 t 在 i 的時候是放在這條槓的左邊是要算機率的	4-8
我現在是放在這條槓的右邊是一個條件	4-8
你看一下	4-8
我剛才的 alpha  t 的 i 這個這個在在 time  t 是在 state  i 這件事情是在槓的左邊	4-8
是算機率要算進去的那我現在呢	4-8
如果在 beta 這裡的話呢是在槓的右邊是一個 condition 而不是算機率的是一個條件	4-8
那為什麼會這樣我們待會會解釋	4-8
還有一點不同的是	4-8
我現在如果是算 t 的 beta  t 的話這個如果是 t 的話	4-8
我這邊裡面沒有 t 我是從 t 加一開始往後看 t 是沒有的	4-8
對不對這裡沒有 o  t	4-8
但是如果剛才的 alpha 的話呢	4-8
這個如果 t 的話這邊 t 是在這裡的	4-8
所以呢我是如果算 alpha 的話是我是把 t 算進去的	4-8
而 beta 的話呢 t 是不算進去的	4-8
 ok 那就是為什麼我剛才算你記得我剛才算畫 alpha 這張圖的時候我是這樣畫的	4-8
我說如果時間 i	4-8
時間 t 在 state  i 的話呢	4-8
我是這樣子看的	4-8
也就是這個機率要算進去	4-8
另外前面的呢都可以算但是我這點是要算進去的	4-8
但是我現在的話這點是沒有算進去喔時間 t 在 i 的這點沒有算進去	4-8
所以我是這樣子這邊並沒有這樣包進來	4-8
那也就是說在這個 t 的時候	4-8
在這個 t 的時候在這邊是沒有 t 的	4-8
這為什麼也是有原因的我後面會解釋	4-8
哦那這是兩點不同的地方其他的看起來是很像	4-8
就一個是這個從一個是 given 前面一個是 given 後面	4-8
一個是看到前面一個是看到後面	4-8
好那我們現在先來看他的這個怎麼算這個這個所謂的 backward  algorithm	4-8
那這個講起來其實也一樣不難	4-8
那反過來意思就是說我先從最後這一行開始填起	4-8
我如果在 t 的部分都可以填起來的話呢我就可以填前面一行我的這個這個	4-8
 iteration 就是有了後面一行就算前面一行有了後面一行就算前面一行有了後面一行就算前面一行就這樣一路算下去	4-8
那麼一開始我後面一行最後一行怎麼算他說全部都給他一	4-8
這個條件其實有點怪怪的	4-8
但是呢哦為什麼怪怪的呢其實講起來很簡單因為其實在最後這一行是 undefined	4-8
為什麼呢你看我的 beta  t 呢是指看到 t 加一以後的	4-8
當我如果 beta  t 是這一點的話呢是指我看到 t 加一以後的	4-8
所以如果這個 t 變成大 t 的話在這邊的 beta 什麼是看到大 t 以後的阿	4-8
應該是講大 t 加一以後的機率	4-8
可是大 t 加一以後根本就沒有	4-8
所以那個算什麼其實是根本在不在上面的定義裡面 ok	4-8
也就是說你這裡的這個大 t 如果是大 t 的話其實不在上面這個定義裡面	4-8
因為上面定義是要講大 t 加一以後的東西根本就沒有嘛	4-8
所以這裡根本沒有上面的定義所以我們就隨便隨便 define	4-8
目的是說我 define 之後後面要通	4-8
也就是說像他這邊他就說我全部讓他是一	4-8
這當然不是隨便亂設的他讓他隨便都是一之後	4-8
你根據他的這個演算法	4-8
給我這一行去算前面一行給我這一行去算前面一行對這個演算法我也從這個通通都是一來算前面一個看看對不對	4-8
那它 turns  out 就是當我都設成一的時候把它再算前面一個的話呢用那個演算法來算它發現是對的	4-8
所以它可以這樣子做 ok 所以呢我們要先來講這個這個呃 algorithm 也就是說它的 iteration 的過程	4-8
如果我有了 t 加一的話要算 t 怎麼算	4-8
就是向前算嗯如果我 t 加一這一行如果我 t 加一這一行都有了的話	4-8
我如何算 t 的那一行	4-8
如果這個 iteration 沒問題了	4-8
那我們現在拿這個來做在我最後一行假設它都是一的情形之下看看通不通	4-8
欸也通於是我就假設它後面都是一是這樣來的	4-8
好那我們現在來看這個怎麼算這個其實跟剛才很像只不過反過來	4-8
如果我有了 t 加一怎麼算 t 呢	4-8
你看我要的 t 是這樣是指 t 的時候在 i 上面但是要前面全部的機率	4-8
那我如果有 t 加一的 j 的話 beta  t 加一加上 j 的時候是這個	4-8
那這個時候會是怎樣呢譬如說我們說這個是 j	4-8
 t 加一的 j 是這一個 beta 的 t 加一的 j 呢是說	4-8
我 t 加一的時候是在 j 上面但是我看到的是 t 加二以後的	4-8
這是 t 加二我看到的是 t 加二以後對不對根據我們剛才的定義對不對	4-8
也就是說你現在 t 加二以後我都有了然後 t 加一在 j 上面	4-8
不過我沒有算他的機率對不對就是那這這就是這個東西	4-8
那如果是這樣的話呢那我顯然應該還要再把 t 加一的那個機率算進來	4-8
因為我們剛才講過 beta 的定義都沒有把它算進去 beta 定義所以 t 加一的 beta 我沒有把 t 加一算進去	4-8
所以我現在要把 t 加一放到那裡去把它的機率算進去那這樣的話得到就是我黃色這個	4-8
這邊後面都有了但是這邊是在這裡	4-8
可是當然我也可以這邊在這裡阿	4-8
也可以這邊在這裡阿對不對譬如說在這邊在這裡這個 case 是	4-8
是 t 加一在這裡但是這邊是後面都有的這顯然是一個	4-8
然後呢那這裡顯然也是一個 t 加一在這裡那這後面都有的	4-8
也是一個對不對那你把這些都全部加起來那就是這個	4-8
然後你還要一個什麼呢還要一個 a  i  j	4-8
那這時候呢它可以這個可以這樣過來可以這樣過來可以這樣過來	4-8
我每一個機率都要算進去	4-8
那全部算進去之後呢我現在就得到現在在這裡而這以前都可以的	4-8
那就是這個就是 beta  t 的 i	4-8
 ok 所以你仔細看這個式子其實不難啦它的意思其實就是這樣子嘛	4-8
我的 beta 的 t 加一的 j 是指說我 t 加一的時候在 j 上面	4-8
然後前之後的通通都算但是這點呢沒有算	4-8
但是 t 加一可以是所有的 j 阿	4-8
那我就每一個 j 都要算都要嘛但是呢都是後面通通都算了	4-8
但是呢這個是在所有的 j 上面都要	4-8
那所以呢我就所有的 j 都要然後呢我都要 a  i  j 他們都要從 i 跳過來所以都要 a  i  j	4-8
然後呢我要把這個 t 加一的這個東西放在每一個 j 裡面去都有機率嘛那就是中間這一項	4-8
我這個通通都乘起來然後通通加起來於是我就把剛才的這一個	4-8
跳到向前剛才的這一個向前跳到這來於是我就把多把中間這一行的機率都算進去了	4-8
對不對我等於把中間這一行機率都算進去之後向前跳跳到這一行	4-8
那麼由此的話呢於是我這一行有了前面一行我就可以算後面一行	4-8
因此呢這就是我的這個 backward  algorithm	4-8
我就可以向前算	4-8
所以我從 t 減一 t 減二一路往前算一樣的填這張表我就可以把這個表全部填完	4-8
那每一點都是一個 beta  t 的 i	4-8
當這個 iteration 的這個式子看起來沒問題的時候我們再回來看剛才這個 initialization	4-8
我們說一開始把它設成一好像沒什麼道理我就我就說它在每一點都是一	4-8
有點沒道理但是我現在來看如果這個都是一的話我再算前面一個 t 減一怎麼算也是一樣阿	4-8
我這裡的某一個這裡 t 減一的某一點是什麼呢就是前面的所有的機率	4-8
那麼因此呢我現在如果把	4-8
如果那樣的話呢我現在是大 T 加一叫做大這個小 t 加一叫做大 T 嘛	4-8
這個叫做大 T 嘛那這個變成大 T 減一嘛	4-8
那如果這是這是這這個是大 T 的話我剛才已經設它都是一了	4-8
都不要看了就是這兩個乘起來加起來是不是等於這個呢對不對	4-8
這是我最後最後兩行的意思	4-8
假設這個都是一然後來算這一行那怎麼算呢這邊都是一嘛所以我就是把這個小 t 加一叫做大 T	4-8
於是於是這邊就都是一不用看了就是這兩個乘起來	4-8
然後呢這個 t 呢就變成是這個大 T 減一那你看這樣對不對呢	4-8
這是對的因為我就是把這是大 T 嘛我就把最後一個最後這一個 vector 放到每一個 state 去得到一個機率	4-8
然後分別乘上它的 a  i  j 就得到我的 i	4-8
所以呢譬如說把把這裡也有一個 o  t 阿	4-8
我這裡也有一個 o  t 阿我把這個 o  t 放到這個 state 去放到這個 state 去我都有一個機率	4-8
然後呢它呢可以跳過來可以跳過來可以跳過來阿	4-8
我就通通算進去阿把這些通通算起去所以我就把他們的機率	4-8
把這個 o  t 放到這些 state 裡面機率然後分別乘上這個 a  i  j 就是 given 這個 i 之後所有的可能	4-8
就是這樣子阿對阿嗯所以呢把這當成一就對了所以我們講說這個可以這樣設	4-8
嗯那麼這麼一來呢這兩個都合理於是我 beta 可以算了好當 beta 可以算之後	4-8
我們還沒說這個跟我們的 problem 有什麼關係	4-8
所以我們現在回來看當我的 forward 和 backward 都有了之後	4-8
我的第一件事情是可以把 forward 跟 backward  combine 起來	4-8
也就是 alpha  t 的 i 乘上 beta  t 的 i 兩個相乘會怎樣	4-8
我們剛才是用 forward 的話是在這張表上填出一個 alpha  t 的 i 來	4-8
我有一張表每一點都有一個數值就是 alpha  t 的 i	4-8
我現在做 backward 的話我又可以填一張表	4-8
上面每一點是 beta  t 的 i 我現在可以把這兩個相乘是第三張表 ok	4-8
這是第一張表這是第一張表這是第二張表第三張表是兩個的相乘 alpha  t 的 i  beta  t 的 i	4-8
這也是一張表那麼我們來看看這一張表是什麼意思	4-8
那這張表的意思其實很簡單	4-8
就是你看到全部的一到 t 全部的東西而時間 t 的時候是在 i 上面	4-8
 given 這個 Lambda 這個是 given  Lambda 那為什麼其實其實是很容易想像了	4-8
我仍然用剛才那一個圖那邊那個圖我如果這是這是 state  i 這是時間 t	4-8
我剛剛才 alpha  t 的 i 是這樣把這個機率算進來之後前面全部都有	4-8
對不對前面隨便怎麼樣都有這邊呢是在這個算進去這個是 alpha  t 的 i	4-8
那我 beta  t 的 i 是怎樣呢是我也是這個要在這裡但是我是算這後面的從 t 加一以後的	4-8
這是 beta  t 的 i	4-8
所以我這個再乘這個的話不就是把這兩個機率通通都算進來了嗎	4-8
於是我就是從從這個 o  one 一直算到 o  T	4-8
所有機率都都算進去了	4-8
除了在 t 的那點只有放在 i 上不可以 t 的那一點只可以在 i 上這邊沒有對不對	4-8
也就是說這一堆是不可以的這一堆是不可以的	4-8
除了這堆不可以這堆不可以之外其他全部都可以	4-8
然後我全部看到對不對那就是我這邊講的我看到全部的 o  one 到 o  T	4-8
除了在 T 的那點只有一個 i 以外其他我全部都看到了	4-8
那就是這兩個相乘的那就是我如果把這個 alpha 的表跟 beta 的表相乘得到第三張表的話	4-8
那個表上也是對每一個 t 每一個 i 都有一個值那個值就是這個意思	4-8
那個值就是這邊的意思就是我看到全部的	4-8
然後呢在 T 的時候等於 i 的	4-8
那這個說法你如果	4-8
我我想是很直覺的你可以想像就是這樣子因為我們已經講清楚它的意思就是這樣子	4-8
你如果覺得說我不太感覺它是這樣的話我們可以推一推	4-8
我們可以推一個簡單的假設我有一個我從 o  one  o  two 一直看到 o  t 的時候	4-8
 given 這個 Lambda 這是我的一個 event 叫做 a 我我我現在要推一下這個東西	4-8
就是推一下這個這個東西假設這個 event  a 就是在這個在這個 model 裡面我看到零到 T 的	4-8
然後還有一個呢我是看到 t 加一 t 加二一直看到大 T 的	4-8
這個叫做 b 也就是說我一個是從零看到 t 一個是從 t 加一開始看到大 T	4-8
這是 b 然後還有一個是 event 是 c 就是我在時間 t 要在 state  i 上面	4-8
這個是 c 我如果 given 這三個 event 的話那麼我要說是這樣子	4-8
就是 probability 的 a  b  c 的機率是什麼	4-8
就是 probability  of  a 跟 c 乘上 b  given  a  c	4-8
這個沒問題吧嗯我這個只是一個最簡單的條件機率的拆解	4-8
 a  b  c 要同時看到 a  b  c 三件事的話呢	4-8
我可以看到 a 跟 c 的然後乘上 given  a 跟 c 看到 b 的	4-8
那這樣的話呢這乘起來就是這個那如果是這樣的話那麼	4-8
當 b 跟 a 是 independent 的時候我們加一個條件	4-8
就是咳咳當 b 跟 a 是 independent 的話我這個 a 的條件可以拿掉	4-8
就變成 b  given  c	4-8
這個應該也是個很簡單的條件機率的的關係你應該記得就是	4-8
你學機率學過 given  x  given  y 如果它們兩個 independent 的話會怎樣	4-8
就是 x	4-8
這個 independent 的意思就是你給我另外一個東西的條件等於不給一樣	4-8
所以呢你根本就可以不要這個是你回去如果不記得回去查你的機率課本一定學過這個	4-8
所以呢如果是 independent 的話你可以把它拿掉那我這裡呢我說我們可以先假設 a 跟 b 是 independent	4-8
為什麼因為 a 是看到 t 的 b 是看到 t 加一以後的那麼我們現在並沒有假設	4-8
不同的時間上他們 o 有什麼關係我沒有講他們有什麼關係所以我們先假設他們是沒有關係的	4-8
讓他們是 independent 如果是這樣的話呢我這個就沒有了就跟剛才這個情形是一樣的	4-8
於是我就變成這樣當我變成這樣的時後你現在再來看這個是什麼	4-8
這個就是 alpha  t 的 i 那這個是什麼這就是 beta 　 t 的 i	4-8
何以見得你看看 alpha  t 的 i  alpha  t 的 i 的定義	4-8
就是這樣子那是不是 a 跟 c 就是 given  Lambda 裡面我看到一到 t 而且 t 在 i 上面對不對	4-8
就是 given 這個 lambda 我看到一到 t 而且 t 在 i 裡面所以呢就是 a 跟 c 這兩個在這裡的機率就是 alpha  t	4-8
那麼這個為什麼是 beta  t 呢那你再看 beta  t	4-8
就是我看到 t 加一以後的 given 這個東西	4-8
那就是我看到這堆東西 given 這個	4-8
所以是 b  given  c  ok 所以這個 b  given  c 就是這個	4-8
所以 alpha  t 的 i 跟 beta  t 的 i 相乘就是在這裡	4-8
而這個是什麼這個 a 這個 a  b  c 的機率就是我看到全部	4-8
也就是我底下的這個這個的這個或著這個就是我看到全部	4-8
看到全部裡面呢只有 t 的時候等於 i 別的沒有規定	4-8
我看到全部只有 t 的時候在 i 別的都沒有規定別的都可以那這就是 a  b  c	4-8
 ok 所以呢那我我用這個來講的話這個其實是在說推說剛才講的這個這個東西等於這個的意思	4-8
所以呢也就是這邊講有看到全部但是 t 要等於 i 或是說看到全部但是 t 要等於 i 的這個條件	4-8
那我等於是在講這個東西你如果是要推的話你如果不相信要用推的可以推就是用這個	4-8
那有一個條件就是 a 跟 b 要 independent	4-8
那麼你其實不要推也可以我們很直覺來看的話直接用這個圖來看幾乎就可以看出來是這樣子	4-8
好那麼到這裡我們可以回過頭來講我們現在再來看一下我剛才說 alpha 跟 beta 定義不太一樣是有原因的	4-8
第一個不太一樣的地方我們是說這個 t 在 i 的這個條件在 beta 是在這個槓的右邊	4-8
而在 alpha 的話呢是在這個槓的左邊為什麼會這樣你在這邊看就很清楚啦	4-8
這就是 c 嘛是 c 的這個東西在這裡的話是在算全部的機率是算進去的而這邊的話是算在 condition 這裡的	4-8
那麼這樣的這兩個相乘才會條件機率一乘才會乘到這邊來嘛	4-8
對不對嗯這條件機率兩個相乘會乘出乘出這樣來就是要讓一個在這個左邊一個在右邊這樣才會嘛	4-8
那就是 c 嘛所以當然要一個在左邊一個在右邊才可以那然後我們還有說一點就是還有一點不同是什麼呢	4-8
 alpha 的時候這個是 t 的話這個 t 有看到這個 o  t 在這個裡面	4-8
可是 beta 的時候呢如果這個 t 的話這個 t 不在裡面為什麼這樣	4-8
你這邊看也很清楚因為你這個只能算一次不能算兩次	4-8
所以那你這個 t 的時候在 i 的這個機率我歸到 alpha 來了我就不能再歸到 beta 去	4-8
只能算一次所以呢這個東西就只有在這邊不在那邊 ok	4-8
那這樣大概可以解釋我們剛才講的所有東西好於是我就得到這個了當我得到這個之後呢我其實可以做下一件事情就是	4-8
我剛才這個是 problem 　 one 要做的東西你如果看的話這是我的 problem 　 one 的要解的	4-8
我 problem  one 就是解這個嘛嗯就是算整個的 o 也就是所謂算整個的 o 就是我讓它所有的 path 所有 path 全部算進去	4-8
的機率或者所有的 state  sequence 全部算進去全不管怎麼走全部算進去那是什麼呢	4-8
我其實就是這邊講的我只要把這個東西 summation  over 所有的 n 就可以了	4-8
因為你現在這個東西已經是看到所有的我看到所有的 o 了但是呢我只有 i	4-8
時間 t 只有在 i 上面那我就把它 i 全部加起來嘛就是說我們剛才講我已經看到所有東西了	4-8
除了在 t 的時候呢這邊不可以這邊不可以只可以在這裡	4-8
那同理我也可以算另外一個 i 譬如這是 i  prime 我的是在這裡	4-8
我是在這裡那麼那我會得到一個 alpha  t 的 i 是這個 beta  t 的 i 是這個	4-8
那不可以的是什麼不不可以的是這個對不對	4-8
不可以的是這個那是另外一個那我把這個所有的 i  prime 全部加起來的時候就把這一行也都	4-8
每一個都可以了那不就全部都可以了嗎	4-8
對不對所以呢我現在如果把這個我現在如果把不管是這個也好或者這個也好那就是這個跟這個兩個式子	4-8
我都把它從 i 等於一到 n 全部加起來的話我把這個地方全部加起來的話那不就是全部了	4-8
就所有的都可能就所有 path 都算進去了所以我就得到這個那這個其實就是我們剛才 problem  one 的要解的東西	4-8
換句話說我現在如果有了 alpha  t 的 i 乘上 beta  t 的 i 這兩個都有了的話	4-8
我只要在任何一行去加都得到這個值你你回去看我們剛才 problem  one 的	4-8
的答案是怎麼做的 problem  one 是這麼做的我們剛才是	4-8
我如果算這個是 alpha  t 的 i 的話我是這樣一行一行算過來	4-8
等到最後一行最後一行算過來的時候我把它全部加起來	4-8
最後一行算回來是全部加起來就是我的答案	4-8
這是我們剛才 problem  one 的 forward  problem 呃 forward  algorithm 是這樣算的	4-8
那我現在說不見得要這樣因為我只要得到如果我現在這個表得到是 r 我我只要把 beta 算出來第二張表之後	4-8
把 alpha 跟 beta 相乘得到第三張表的話	4-8
那第三張表其實就是我們現在的底下的這個	4-8
當我有了這個第三張表的時候呢	4-8
我其實就任何一行 i 等於一到 n 去加 t 是那一個 t 是多少那一個 t 都沒有關係	4-8
任何一行的 i 一去加的話都得到它	4-8
所以呢我只要得到第三張表的時候	4-8
我隨便拿一行去加都可以假設說我的第三張表像這樣的話我在這一行全部加起來就是那個答案	4-8
那我也可以加這一行我也可以加這一行加起來都是那個答案	4-8
那就是我們這邊講的加起來都是這個東西因為如果是加這一行的話呢其實是說 ok	4-8
這一點要在這裡其他的都可以	4-8
對不對	4-8
那我只要把所有的通通加起來不就是全部嗎	4-8
那如果在這裡的話呢這裡的任何一點的話意思也是說這一點在這裡其他的都可以	4-8
對不對那我只要把它全部加起來就是嘛	4-8
就是全部所以我在任何一個 t 在 any  t 這個 t 隨便那一個都可以我只要把這個 i 通通加起來	4-8
就是我要這個東西	4-8
那也可以說就是只要我只要有 alpha  i  beta 這個 alpha  beta 相乘的這個表的話我任何一行去加	4-8
不管 t 是多少任何一樣去加都是這個答案	4-8
那如果是這樣的話你現在再回過頭來看我們剛才的 solution 其實就是什麼呢	4-8
我們剛才的 solution 就是當這個大這個小 t 等於大 T 的時候	4-8
當這個小 t 等於大 T 的時候我也可以嘛我們說任何一個 t 都可以嘛	4-8
所以小 t 等於大 T 也可以嘛	4-8
當小 t 等於大 T 的時候呢這個 beta 一律等於一阿對不對	4-8
這個 beta 等於一阿所以這邊都沒有啦就剩下這個	4-8
那就是我們剛才的那一式嘛	4-8
所以你看到我們 forward 的最後這個式子就剩下這個嘛因為那還有個 beta 大 T 等於一嘛	4-8
就沒有了嘛就剩下這樣所以這個就是我們剛才的那個	4-8
所以我們在 problem  one 的這個 forward  algorithm 的這個 solution 其實就是在這裡的我就是算最後那一行	4-8
等於是這個意思	4-8
那我們現在得到的是 general 的	4-8
呃得到是這個那其實每一行都可以	4-8
 ok 好那我們到此說了半天我們還沒有解 problem  two	4-8
我們現在只是在說這個這個 backward  beta  backward 的的這個 variable  beta	4-8
以及它怎麼算然後它跟 alpha 合起來有這樣的意思	4-8
那有了這些之後我們現在過來看我怎麼求我要的這個 path	4-9
那我要求的這個這個東西其實是很直接	4-9
你不見得要像這裡講這麼麻煩我們可以回過頭來再看一次	4-9
這裡重畫一下嗯	4-9
我的這是 t 這是 i  alpha  t 的 i 是說	4-9
我這個在這裡	4-9
 beta  t 的 i 是說我這個在這裡	4-9
因此呢你 alpha  t 的 i 乘上 beta  t 的 i 就這邊底下所剛才那剛才所說的這個	4-9
那其實就是把這個連起來了嘛等於說是這樣子嘛	4-9
我全部都有了除了這兩塊沒有對不對除了這邊沒有以外我全部都算進去了	4-9
那這是 alpha  t 在跟 beta  t 的 i 那如果是這樣的話我在時間 t 到底是那一個 state 呢	4-9
我就看它嘛我每一個 i 都可以算我也可以算一個這個舉例來講我這個也可以算	4-9
我在就這一點這個 i  prime 而言我可以得到一個 t 的時候在 i  prime 的其他的都可以	4-9
其他的都可以可不可以這樣可以阿	4-9
那這個就是我看到了全部之後時間 t 在 i  prime 的機率那剛才這個呢是我看到全部之後呢時間 t 在 i 的機率	4-9
那我就在這上面看誰最大嘛誰最大的話我 t 就應該在那裡	4-9
那麼因此呢有一個很簡單的做法就是這樣子	4-9
就是我這邊講的就是 alpha  t 的 i 乘上 beta  t 的 i 也就是我剛才第三張表	4-9
那我現在就在這上面去看誰最大	4-9
在 i 上面 i 從一到 n 看誰最大最大的那一個 i 就是我的 q  t	4-9
就是我的最佳的 q  t 對不對那這個其實就是我們這邊講的這個 approach  one	4-9
嗯這個 problem  two 有兩個 approach 它第一個 approach 其實可就可以這樣解釋	4-9
就是我每一個 individually 去選擇最佳的 state  at  the  most  likely  state  at  time  t	4-9
在時間 t 上我的這一堆東西就是這個機率就是看到就是 probability  of 看到全部的 o	4-9
然後這個嗯就是剛才這個式子	4-9
就是我看到全部的 o 而且 t 在 i	4-9
而且 t 在 i 的機率嘛	4-9
那麼既然是這樣就是指我看到全部不過 t 在 i 上面	4-9
那我同樣我可以看到全部 t 在 i  prime 上面	4-9
我看到全部我 t 在這裡面每一個我就在這一行裡面去看誰最大	4-9
最大的那一個如果現在是 i 最大我就說 t 的時候應該是 i	4-9
然後呢 t 減一我也一樣去看這一行裡面誰最大	4-9
喔發現是這個最大就是這個在這行裡我又看誰最大嗯發現呢是它最大	4-9
就是這個那我把這些連起來就是我的 state 就是我的 path	4-9
我又我現在要這個 problem 就是要找它的我要找這個這個 state  sequence 所謂的 state  sequence 就就是這條 path 嘛	4-9
那我這條 path 怎麼找我就是這樣子找阿我就每一個時間 individual  time 我去算它是這個那個機率最大	4-9
那就是這個那然後呢這個在這個瞬間是它最大這個瞬間是它最大這個瞬間是它最大	4-9
我就把它連起來就是我要的機率那基本上就是我這邊講的的這一個 approach  one 在做的事	4-9
唯一不同的是	4-9
唯一不同的是在課本上它不只是這樣子而是它還除了一個東西	4-9
那它它做了一個 normalization 的過程然後 define 它叫做 Gamma 嗯	4-9
那這個其實是有道理的那麼因為在後面	4-9
我們後面還要再看一大堆就有這個 gamma 很有用 alpha  beta  gamma 都很有用了喔	4-9
後面還有用所以呢他就先這樣 define 了	4-9
那你看我這邊說我其實只要 alpha  t  beta  t 之後看就是在這一行裡面看	4-9
那一個 i 最大看那個 i 最大那個最大就是那個 state	4-9
就那個最大就就是在那個瞬間的那個最佳的 state 然後把它連起來就可以了	4-9
那麼在這裡課本上的寫法呢是不是這樣它這邊還多了一個除法	4-9
就是這邊呢再除一下除以 summation 的 alpha  t 的 i  beta  t 的 i 然後 i 等於一到 n	4-9
它是這樣子多除一個東西所以呢它 define 叫做這個	4-9
它把這個 alpha  t  beta  t 呢都除一個東西之後叫做 gamma 然後呢拿 gamma 再來看誰最大	4-9
那其實跟我剛才講的是一樣的因為你除的這個東西是個 constant 我在所有的 i 裡面	4-9
去算去看誰最大的時候他們都除了這個東西他們除的東西是一樣的這只是一個 normalization 的過程	4-9
它除了一個共同的東西所以所以我們剛才如果看白白色粉筆的部分	4-9
誰最大的話我現在用紅色來看還是誰最大所以這個並沒有改變任何事情	4-9
所以在課本上講的話呢它是用這個嗯用 gamma 來算誰最大那個最大那個就是那個 state	4-9
還有你如果去 check 課本的話他這邊寫成 MINIMUM 是錯的啦啊	4-9
在課本上這個寫 MINIMUM 是錯的這個是 MAXIMUM	4-9
那麼在課本上它是用 gamma 來算誰最大那其實這個 gamma 就是除上做這個 normalization	4-9
其實不除也一樣因為這個對所有的 i 都是相同的	4-9
那麼它為什麼要這樣做其實除一除也是有道理的因為除了有意義的	4-9
因為你看我把它全部加起來就是這個嘛就是我們剛才講的對不對	4-9
我們剛才就是說你把它如果把它全部加起來的話	4-9
把它全部加起來的話就是這個嘛就是我我看到整個 o 的機率	4-9
所以呢你這個把它全部加起來就看到整個 o 的機率所以分母呢	4-9
就是除以看到整個 o 的機率那分子呢是除了看到整個 o 之外我還看到 t 是 i 的機率	4-9
那這兩個一除就是把這個 o 放到條件來嘛那就變成是 given  o 的時候我 t 是 i 的機率	4-9
那有何不同這只是只是做一個 normalization	4-9
我們現在要來把這個viterbi algorithm 今天要講完	4-10
我們來說一下怎麼樣解這個problem two 呢	4-10
最好的的solution 是這個viterbi algorithm	4-10
那它怎麼解的呢它就是從頭開始	4-10
就是找一條最佳的best sequence 或者最佳的path	4-10
那它的解它的解法是再define 一個新的東西叫做delta	4-10
嗯這一段不斷的有新的的數學符號跑出來	4-10
那麼你如果已經搞得很頭大的話回去要看一下那個reference	4-10
回去要弄清楚一下否則我們再下一堂課就聽不下去了	4-10
因為我們下一堂課講problem 三的時候會alpha beta delta gamma 全部全部用還有epsilon	4-10
那麼所以你如果回去要看一下才會弄清楚的話就要回去看一下	4-10
先define一個新的variable 叫做叫做這個delta	4-10
那它是怎樣的呢	4-10
它是說	4-10
我現在如果	4-10
它還是一樣是delta t 的i 嘛	4-10
所以是在任何一個時間t 任何一個state i 上面都有一個delta t 的i 這個東西	4-10
那它是什麼呢	4-10
我還是一樣我要given 這個lambda 我在t 的時候要在i 上面	4-10
我t 的時候要在i 上面	4-10
然後也是一樣我看到從一看到t	4-10
但是我還有一個條件	4-10
就是到i 為止我有一個state sequence	4-10
而這個state 這個state sequence 是從一到t 為止的	4-10
而是所有的一到t t 減一為止裡面最大的那一個	4-10
也就是說在t 的時候是到了i 是沒有問題	4-10
從t 減一之前的呢	4-10
從t 減一之前到一呢	4-10
我都不是隨便的囉我現在是一定要有某一條	4-10
那一條的機率最大的那一條才是	4-10
別的都不算喔	4-10
因此我現在只有這一個	4-10
跟這一條	4-10
其他的都沒有這些都沒有	4-10
不像剛才我都是全部都算進來這邊都不算只算最大那一條	4-10
好這是不同的地方	4-10
也就是說你看我我也是一樣在t 的時候要等於i	4-10
t 的時候要等於i	4-10
而且我是看到從零看到t	4-10
這個都一樣	4-10
所以這邊跟剛才的alpha 是非常像的	4-10
可是我現在不再是像剛才alpha 說剛才alpha 說喔這邊前面前面都可以現在不是	4-10
我現在呢是有一個固定的一到t 減一的一條path	4-10
一到t 減一的這一條path 呢是所有的一到t 減一裡面的最大的那一個	4-10
其他我都不要了	4-10
我就把highest probability along a certain single path ending at state at time t state i at time t	4-10
那也就是說我就變成是這個之前我要先找好那一條是機率最大那一條就是那一條了	4-10
在這裡	4-10
這叫做delta	4-10
那除了這個不同的delta 之外其他是一樣的	4-10
那我還是一樣從頭開始	4-10
先想辦法填d 我就是現在這個這個表叫做delta 的	4-10
這個叫做delta t 的i 嘛	4-10
然後那我就是填這張表	4-10
是delta t 的i 變成一張表	4-10
我每一點去填	4-10
怎麼填也是一樣第一行	4-10
填了第一行之後呢就有下一行每一行就有下一行	4-10
每一行怎麼算下一行呢	4-10
這個基本的原理就是在這裡這個式子	4-10
那也就是說如果我這一行有的的話呢下一行其實不難算	4-10
我們舉例來講	4-10
如果這一行是就t 而言	4-10
我看我這邊是用ok 有了t 要算t 加一的話	4-10
我的這一行的t	4-10
我這一行在t 的時候我得到這一條path	4-10
那我在譬如說在這個上面的我有另外一條path	4-10
那麼在這個上面呢我有另外一條path	4-10
等	4-10
如果都有的話我這一行都有了	4-10
那我下一行呢	4-10
t 加一的時候呢的某一個j	4-10
t 加一的某一個j	4-10
它可能是哪來的	4-10
它的最佳path 呢可以是從這來也可以是從這來	4-10
也可以是從這來	4-10
depends on 誰最大	4-10
因為我只能找一條最大的	4-10
所以我就要看我我如果在t 加一的這個話呢	4-10
它可以從這過去也可以從這過去	4-10
要看那一條最大最大那個才是別的都不是了	4-10
所以呢我就要 del 這個delta t 的i 乘上a i j	4-10
去看誰最大	4-10
我在這裡也有一個delta 是這個機率乘上這個a i j	4-10
在這裡也有一個這個delta 是這個機率乘上這個a i j	4-10
在這裡也有一個是這個機率乘上a i j	4-10
我看誰最大	4-10
最大的那個才是	4-10
所以呢我要看誰最大	4-10
最大的那一個然後我再把後面的t 加一放進去	4-10
所以呢我們舉例來講	4-10
我算到最後之後發現是	4-10
這個才是最大的	4-10
發現這個才是最大的	4-10
所以呢我就要用這條路來算	4-10
ok 我如果是我把這些	4-10
它的delta 乘上這個a i j	4-10
它的這個delta 乘上這個a i j	4-10
它的這個delta 乘上 a i j 看誰最大之後發現是這個綠色的最大	4-10
就把它畫成紫色	4-10
這個最大	4-10
於是呢我就應該是把這一個的最大那一個乘上a i j	4-10
然後我再把t 加一的這個o 的t 加一放到j 的機率裡面去	4-10
那就得到我下一個了	4-10
ok 就這個意思	4-10
所以呢看是是誰的delta 乘上a i j 最大	4-10
然後呢我再把我的最後那個t 加一放進去	4-10
那這樣的話呢我就得到我所要的	4-10
就是我下一個j 下一個delta	4-10
但是我這樣算之後呢我得要記得我是從那來的	4-10
這叫做backtracking	4-10
也就是說當我得到這個的時候	4-10
我得記得說它是從這來的	4-10
我得記得否則的話我後面就忘掉啦	4-10
我忘掉了這個是從那來的了	4-10
所以我要記得說現在這個得到這個delta 的時候	4-10
它是從這來的	4-10
就表示說我的前面的path 是從這邊過來然後這邊走它前面的等等	4-10
ok	4-10
那麼於是呢你可以想像如果我這一行t 加一都填滿的話呢	4-10
t 加二會怎樣	4-10
t 加二的這個的話呢我又會看它可能是從這來的	4-10
可能是從這來的可能是從這來的	4-10
看誰最大	4-10
不一定delta 大就會大因為還有a i j 對不對	4-10
我這邊是要把不是delta 大就會大	4-10
而是要你乘上a i j 之後再看誰大	4-10
是要這兩個所以你是它可能是任何地方過來	4-10
如果這個地方是從這地方過來的話呢我這回就要變成是我的最佳path 從這過去了	4-10
那這地方從這來的等等是這樣子的	4-10
對不對	4-10
所以呢你每一次向前推的時候呢	4-10
我又要重新去記得說我就把看它剛才從那過來的	4-10
那它是從那來的	4-10
它是從那來的它是那來的	4-10
我要這樣才知道我這個時候的最佳path 是那一條	4-10
ok	4-10
那這個就是這個viterbi 的基本精神就是這樣	4-10
那這樣的話等我等我走到最後走完的的時候	4-10
最大最大機率就出來了那條path 就是最大機率的path	4-10
恩	4-11
那詳細的在底下這一章其實我們仔細看看底下這一章就會很清楚	4-11
底下這一章是講我完整的怎麼算的	4-11
也是一樣先in initialize 就算第一行	4-11
然後呢就是我每一行怎麼算	4-11
第一行很簡單	4-11
我就是就是把每一個delta 的第一行	4-11
第一行因為根本沒有前面的path	4-11
根本沒有前面的path	4-11
所以第一行其實就是把這些機率放進去	4-11
所以這個跟剛才的那個alpha 是一樣的嘛	4-11
就是我第一行我就是先把這個它會從第i 個state 開始的機率放進去就是pi i	4-11
然後呢我再把第一個o one 呢我把第一個o one 呢也放進state i 裡面去	4-11
那就是b i 的o one	4-11
然後我就乘起來就對了嘛	4-11
所以這第一行其實沒有什麼學問很簡單因為前面根本沒有path	4-11
所以第一行就直接就算這個在這機率在這機率就是了這是第一行	4-11
有了第一行之後	4-11
我們每一行怎麼算	4-11
given 前一行算下一行就是我們剛才講的了	4-11
那這個式子跟剛才是一樣的	4-11
所不同的只是這個寫起來有點不一樣這邊變成從t 減一到t	4-11
也就是說呢	4-11
如果是t 減一的話	4-11
t 減一的話	4-11
它如果是在這裡有一個	4-11
在這裡有一個	4-11
在這裡有一個	4-11
在這裡有一個	4-11
在這裡有一個	4-11
的話	4-11
那麼到t 的時候的這一個從哪來呢	4-11
它可以從這來	4-11
可以從這來	4-11
可以從這來	4-11
所以呢我在t  的時候	4-11
我要從t 減一	4-11
就t 減一這一行我都算完了要再算t 的	4-11
i怎麼算	4-11
given t 減一的整行	4-11
要算t 在i 的那一點呢	4-11
啊t 在j 的那一點喔	4-11
一樣的t 在j 的那就一樣	4-11
就是	4-11
我現在	4-11
重畫一下好了	4-11
如果我t 減一的這一行都有了	4-11
我現在要算t 的	4-11
然後在j 的那一行	4-11
所以我現在要算t  的在j 的那一行	4-11
那是怎樣呢	4-11
那就是	4-11
我可以從這裡的每一個跳過去	4-11
但是要看它前面是多少機率	4-11
它是哪一個path	4-11
它是哪一個path	4-11
然後它們各的機率是多少	4-11
所以呢	4-11
我就是	4-11
把t 減一的那一行裡面的每一個i	4-11
它就帶了	4-11
每一個i 它就代它前面的所有的path 的機率	4-11
那個最佳path 的的機率已經代好了	4-11
那麼	4-11
乘上a i j	4-11
乘上a i  j 看誰最大	4-11
那最大的那個a i j 就告訴我說它是從哪來的	4-11
於是呢你就知道說ok	4-11
現在發現它這個過來最大	4-11
於是呢	4-11
我就是	4-11
算這樣子的	4-11
這些就沒有了	4-11
這些都不	4-11
它最大的話	4-11
這些就不用算了	4-11
那這時候再來看它是從哪條路來它是從這條路來的	4-11
那我就這樣連起來就到這邊的路	4-11
就這樣子	4-11
所以呢	4-11
我就是只要把前面一行t 減一分別乘上a i j	4-11
看誰最大的那一個	4-11
那個機率	4-11
再這時候我再把最後那個	4-11
o t 放進去那個j	4-11
把這個放進去的機率	4-11
再乘起來就是我新的了	4-11
那於是呢我這個就可以這一行就可以算了	4-11
這一行算就下面就可以算等等一行一行算下去	4-11
所以呢我這個是算那一行	4-11
然後每一行一直算到最後那一行	4-11
那你算的中間呢	4-11
你都要不斷的記得	4-11
我剛才那個最大的是哪一個最大的	4-11
那麼因此是從哪裡過來的	4-11
換句話說你在這個過程裡面	4-11
你等於是在做這件事	4-11
我如果這一行都算出來之後	4-11
我的下一行的這一個	4-11
我必須去看是哪裡過來的	4-11
如果發現它是從這兒過來的話	4-11
那它呢又是從哪過來的它又是從這樣過來的	4-11
它又是從這樣過來的	4-11
這樣的話你才得到它最佳path 就是這條	4-11
所以這樣我一路走的話呢我一路都知道	4-11
我最佳的機率是哪一個	4-11
那到這個時候呢我又得到譬如說這點我發現是從這來的	4-11
而到這點我發現它最佳是從這來的話呢那我要走這條等等	4-11
所以你不斷往前走的時候你會發現每一點它的前面我都要	4-11
重新看看它最佳路徑是哪一條我都要重新算	4-11
那因此呢我就一路都記得我所有的最佳path	4-11
等於是這樣的意思	4-11
所以呢這個phi t 的j 這個符號只是說	4-11
在t 的j 算出來的時候你要記得一下我剛從哪來的	4-11
是從剛才的那個是哪來的	4-11
那個就是這個最大的最大是誰	4-11
嗯的那個	4-11
i 是哪一個i	4-11
所以呢我就把那個i 記得	4-11
就這個意思哦	4-11
當那個i 記得之後	4-11
那我就	4-11
我就可以這樣一路它都記得它的最佳從這來的	4-11
它的最佳是從這來的我都記得一路都記得的話我就一路向一路向後走	4-11
當我走完的時候	4-11
最後看誰最大	4-11
當我一路走到底	4-11
這是大t	4-11
譬如說這個是delta t 的i	4-11
如果這個最大的話最後這邊我就看誰最大	4-11
所以在最後最後在大t 的時候呢我就所有的i 裡面看誰最大那個叫做p star	4-11
也就是說呢	4-11
這個得到的是	4-11
這樣走過來最大的那個機率是這裡	4-11
這邊所得到的是這樣走過來最大的那個機率是這裡等等	4-11
那因此我最後看誰最大	4-11
在我只要在這裡比誰最大就好了	4-11
如果發現是這個最大的話這個就叫做我的機率	4-11
這就最大那一個	4-11
所以呢我就所以我就在這裡去找最大那一個	4-11
然後呢	4-11
最大那一個找到之後	4-11
會最大的那一個就是我的q t	4-11
所以顯然呢這就是我的最機率最大的path就是這一條	4-11
所以我的最後一個state 就是這個	4-11
就是這個	4-11
就是這個	4-11
所以呢就是最大的那一個機率的	4-11
它的誰是誰最大那一個i 就是我的最後一個state	4-11
有了最後這一個state 之後呢我就一路向前推	4-11
這個式子看起來有點頭大其實只是一路向前推的意思	4-11
剛才這個前面是從這來的	4-11
這個前面是從這來的	4-11
這個前面是從這來的	4-11
這個前面是從這來的	4-11
等等	4-11
你就得到哦這樣子這一條就是我的path	4-11
那這就是我的機率最大的那一條	4-11
state sequence就是這樣子	4-11
來 ok 所以呢這個講穿了就是這麼一回事	4-11
那麼數學式子有點頭大不過意思是很簡單的哦	4-11
那我等於就是第一行先填	4-11
然後呢given 這一行就填下一行其實是很容易的事	4-11
因為每前面這一行的每一點都已經告訴我走到這邊為止的最機率最大那條path 的機率了	4-11
我再乘上a i j 看誰最大就是我下一個	4-11
當我找到下一個之後呢我看剛才那個最大是從哪來是從這來的	4-11
那	4-11
像這樣的話我就一路填了這個之後	4-11
我就填下面的每一個我都一路填每一個再一路填下去	4-11
填到最後的時候	4-11
看誰最大	4-11
因為它填到最後的這裡的每一點都是走到最後的	4-11
最大的那的那條機率	4-11
只不過是ending 在這裡還是ending在那裡還是ending 在那裡而已	4-11
看誰最大	4-11
如果是這個最大的話我們就知道這個就是我最大的path 的機率	4-11
然後呢那它是從這來的	4-11
它我一路倒回去	4-11
就知道這一條path 就是我最大的path	4-11
那所以這個呢就一路倒回去我先看到	4-11
最大的那一個的機率最大的	4-11
然後最大的是哪一個i 呢	4-11
那個i 就是我的最後一個state	4-11
於是呢	4-11
從最後這個state 開始	4-11
一路去做剛才這個function	4-11
就是去看前一個state 是誰	4-11
看它是從哪來的	4-11
那我就一路向前推	4-11
就是前一個	4-11
所以呢q 的t  q star 的t 加一	4-11
可以得到q star 的t 啊	4-11
這樣子	4-11
這樣我就於是一路往回推	4-11
我就得到我要的path	4-11
那這樣子的話呢那這一條path 就是我的答案	4-11
ok	4-11
那這樣呢我的這個viterbi 就解出來了	4-11
那麼我要的path	4-11
就得到那條path 也就是我的state sequence	4-11
那麼viterbi	4-11
原來發明這個演算法的時候並不是做這件事的	4-11
那你在別的課可能也學過	4-11
viterbi它原來是做	4-11
這個convolution 的code 就是做數位通訊的那個error control code	4-11
在那裡面要解那個decoding 的時候有一個跟這個很像的問題	4-11
它當初的viterbi algorithm  就是解那個的	4-11
只不過後來做hidden markov model 的人發現其實可以拿來做這個	4-11
所以他就來拿做這個	4-11
所以你在別的課可能也會學到這個	4-11
那我們這邊是講把它用在hidden markov model 的時候的情形	4-11
就是	4-11
假設我現在要做isolated word problem	4-11
現在就isolated word recognition 的話	4-11
你現在已經可以做了	4-11
譬如說我要辨識零到九的十個數字	4-11
零我有一個model 是零	4-11
一我有一個model 是一	4-11
k 我有一個model 是k	4-11
然後呢九我有一個model 是九	4-11
假設我要辨識零到九的十個音	4-11
每一個音有一個model 在那裡的話	4-11
我怎麼做	4-11
我可以算	4-11
這個	4-11
然後看誰最大	4-11
所有的k 裡誰最大	4-11
最大的那個	4-11
就是答案	4-11
ok	4-11
我就我就算這個嘛	4-11
算這個的時候就是我看到了一堆聲我看到我現在	4-11
給我零到九的十個model	4-11
我給我一個聲音	4-11
假如說這個聲音是七	4-11
我就把這個聲音七放到零到到九這十個model 裡面去	4-11
都可以算這個機率	4-11
那麼這個機率之後我看在哪一個model 裡面機率最大的	4-11
那個k	4-11
就是我的答案	4-11
如果這樣講的話這個我是用我們剛才講的basic problem one	4-11
這是problem one	4-11
所以我可以利用什麼用forward algorithm 來解	4-11
但是呢	4-11
我也可以把它看成是for 這個problem two	4-11
為什麼呢我可以算另外一個東西	4-11
就是算這個	4-11
這個這個p star 的這個probability	4-11
這是表示最大那一條path 的	4-11
given 我的每一個model 我都可以算	4-11
看誰最大	4-11
那個最大的那一個	4-11
也是答案	4-11
ok	4-11
我現在我剛才是解這個problem one	4-11
我用forward algorithm 去算這個東西	4-11
我現在是把它看成這是什麼呢	4-11
這是problem two	4-11
我的solution 是解viterbi algorithm	4-11
就是我們這邊講這個	4-11
嗯為什麼呢你仔細看	4-11
因為我如果是我對每一個model 我都找到一條機率最高的那條path	4-11
它有一個機率是這個	4-11
p star 是這個東西	4-11
我看這個東西到底對哪一個model 最大	4-11
如果你那個是七的話	4-11
如果這是七的話	4-11
照說會在七的model 這個機率最大	4-11
你這個七放到什麼六放到五裡面那顯然會比較低嘛	4-11
所以呢它在在七裡面最大	4-11
因此我可以用我可以用這個用這個機率看誰大	4-11
也可以	4-11
那這就變成變成problem two 的問題	4-11
變成解viterbi algorithm	4-11
那你注意到這兩個機率是完全不同的東西	4-11
不同在哪裡	4-11
這是算所有的path	4-11
或者說就是所有的state sequence	4-11
但是這個呢是一個單一的path	4-11
這是單一的path	4-11
或者是單一的state sequence	4-11
但是呢with 最高的機率	4-11
ok	4-11
也就是說	4-11
你給我一個聲音的時候	4-11
七	4-11
在那個任何一個六的model 的裡面的話我有無數非常多的path 都可以走的	4-11
那麼剛才左邊那個機率是把所有的path 全部機率通通加起來了得到一個	4-11
所有機率通通算進去得到一個機率	4-11
右邊呢是我只算機率最大那一條path	4-11
所以這個只算那一條path不過是算機率最高的	4-11
ok	4-11
這個只算那一條path 機率最高的	4-11
而這個呢是算所有的path	4-11
那這兩件事情顯然是不同的	4-11
所以呢你必須要知道這兩件事情是完全不同的	4-11
可是呢你如果去找他們最大的	4-11
去找這個arc	4-11
常常是一樣的	4-11
也就是說最可能的那個model 通常是同一個	4-11
擦掉了	4-11
最可能的model 常是同一個	4-11
所以雖然這個機率不等於這個機率	4-11
但是我現在不是要算這個機率	4-11
我是要算看這個機率誰最大	4-11
那最大的那一個呢常常這個機率也是最大的	4-11
也就是說雖然這個把所有的path 都算進去這個只算一個	4-11
其實這個dominate 嘛	4-11
這是最大的那一個	4-11
最大的那一個基本上dominate 那裡面哦	4-11
所以呢我只要算這個最大的通常也就是那個最大的嘛	4-11
所以你如果是算哪一個model 最大的話呢我是在求誰最大	4-11
那通常是同一個	4-11
雖然這兩個機率是完全不同的	4-11
ok	4-11
所以呢我們現在要講剛才講這個解這個isolated word recognition 的話呢我們可以這兩種方法都可以解	4-11
你如果去看這兩個演算法就是forward algorithm 跟viterbi algorithm 演算法好像也複雜度也差不多所以好像是都可以	4-11
是沒有錯這兩個都可以做	4-11
那麼但是我們到後面會講到八點零的時候我們會看到	4-11
畢竟其實二比較容易做	4-11
為什麼當你有無限多個詞有六萬個詞	4-11
每一個詞後面可以接無限多個六萬個詞的時候	4-11
你六萬詞後面可以接六萬個詞每一個詞後面都有六萬詞的時候你這個變非常複雜	4-11
你如果所有path 都要算的話不得了	4-11
我不如只算一個最大的path 哦	4-11
所以我們後面會看到你真正的problem 你如果是連續語音	4-11
不只是辨識十個音而是連續的有非常多的變化的話	4-11
你其實是每一次永遠只算最大的比較容易	4-11
你把全部都算進進來是很複雜的	4-11
所以我們後面用得多是viterbi	4-11
那雖然在現在來看如果是算這個isolated word problem 的話呢	4-11
這兩個是一樣的	4-11
ok	4-11
好我們viterbi 說到這裡	4-11
我們保留剩下最後一件事情要說的是補課	4-11
那麼下週停課一天啊抱歉	4-11
下週停課因為我出國	4-11
那包括我們上次二二八也停過一次課	4-11
已經停了兩次課我們進度已經落後很多所以我們現在	4-11
應該要找到兩次補課的時間	4-11
才可以補得回來哦	4-11
那我想我們今天至少要確定一個	4-11
如果確定兩個更好	4-11
我們上次說過	4-11
可以補課的時間是禮拜六下午	4-11
似乎是一個大家都可以的時間	4-11
所以我們就以禮拜六下午為目標	4-11
然後我們讓時間可以flexible就是說	4-11
所謂禮拜六下午	4-11
最早是十二點半到三點半	4-11
最晚是三點半到六點半	4-11
ok 那我們就儘可能調到大家都可以的時間	4-11
那另外呢我們這個錄影錄音的同仁告訴我們其實他們現在呢可以提供錄影錄音哦	4-11
因為這間教室的設備都做好了	4-11
所以萬一我們補課的時間你不行的話	4-11
那麼這個	4-11
應該可以有別的辦法看到那一段哦	4-11
只是說這個怎麼怎麼做我們後面技術問題再來解決不過應該是做得到	4-11
嗯不論是用什麼方式嗯	4-11
所以應該也可以解決哦	4-11
那ok	4-11
那我現在看最可能的時間	4-11
三月十八號就是本週六	4-11
二十五號下週六	4-11
四月一號	4-11
四月八號	4-11
四月十五號	4-11
這都是禮拜六	4-11
再下來這週就是期中考週了	4-11
所以從這個觀念來看我們儘量往前面是比較好你到後面比較接近期中考了	4-11
最快的一次補課是三月十八號是本週六	4-11
如果是本週六的話因為我是下午六點的飛機	4-11
所以只能最早	4-11
就是十二點半	4-11
到三點半	4-11
如果是這個的話我們可以本週六補課	4-11
ok	4-11
那如果是本週六這個不行的話我們就是下週六	4-11
下週六就比較flexible 一點	4-11
我想最早是十二點半到	4-11
三點半	4-11
最晚可以從三點半到	4-11
六點半	4-11
我們中間可以調	4-11
讓所有的人都方便的人都可以最好	4-11
對不對	4-11
這個是那個溫書假所以這個其實不太好	4-11
如果這個不太好到這邊已經接近期中考了可能也不好	4-11
所以我會prefer 是前面這兩個	4-11
ok 有沒有問題	4-11
我們說最快的補課是本週六的十二點半到三點半	4-11
春假的這個禮拜二我們基本上假設它是放假	4-11
ok 我現在講的補課補兩次是指一次補下週一次補二二八	4-11
對不對我還沒有補春假	4-11
恩	4-11
春假是有可能會造成另外一個問題因為我後面我還會出國一次	4-11
所以後面可能還要補	4-11
但是我想我們現在不要講太多嘛對不對我們現在如果是講兩次其實一次是補二二八一次是補下週	4-11
我們至少先找出這個補課的的時間	4-11
所以呢最快的補課時間是本週六有沒有問題	4-11
沒問題ok	4-11
所以呢本週六的這個嗯十二點半到三點半好不好我們這個最早的這個時間	4-11
這個只有這個時間	4-11
然後呢我們要不要訂第二次第二次最快就是下週六	4-11
下週六的話可以早可以晚	4-11
我個人會主張早一點因為這樣三點半你下午還有整個半天嘛對不對	4-11
你對不對你如果是三點到六點這個你就把整個週末都搞掉了	4-11
前面也沒什麼時間後面也沒什麼時間對不對	4-11
所以我會prefer 就是早一點是比較好的	4-11
好不好	4-11
那我們就先訂這兩次好不好本週六跟下週六	4-11
把這兩次補下來我們的至少進度我們可以追上來	4-11
這樣子後面就比較好辦事	4-11
我們春假還要不要再補課我們再看這樣子我們至少先補這兩次	4-11
ok	4-11
好那就這樣子喔那我們基本上地點在這裡	4-11
如果不是的話另外公佈	4-11
ok	4-11
我們基本上我們來借兩個這個地方但是如果不是另外公佈	4-11
ok	4-11
好	4-11
今天上到這	4-11
 OK 我們上次把 problem  two 講完	4-12
今天講進入這個 problem 三	4-12
那麼我們來看一下 basic  problem 三是什麼問題	4-12
那基本上是一個嗯假設我已經有一個 model 我知道這個 model 是	4-12
這有一點被切掉了這是那個完整的 model 參數這個 a  b  pi 這個 pi 掉了那麼我 given 這個 model 之後	4-12
我知道這個 model 是某一個音譬如說九那麼我也知道這個聲音是現在是一個新的九的聲音	4-12
我要把這個新的九的聲音 train 到這個 model 裡面去讓這個 model 呢裡面可以學到這個新的九的聲音	4-12
那麼使得呢那麼這個新的聲音的話呢那麼它可以這個的	4-12
當我把這個新的聲音放進去算的時候它的機率可以最大但是呢它不能不能這個	4-12
干擾到原來的那些訓練就是說原來這個 model 也是九的聲音訓練出來的啊	4-12
那麼它們的的這個所描述的那個變化呢不能夠被破壞但是我要把這個新的聲音也訓練進去	4-12
那這就是就是這個 problem 的意思那這個 problem 比較複雜一點這個比前面的一跟二都還要複雜那麼	4-12
它基本上是一個沒有 close  form 的的問題所以呢我們只能用 iterative 的方法來解它	4-12
所以它是一個沒有 close  form	4-12
 solution 然後呢但是呢 can  be  solved  iteratively	4-12
那麼這個解的方法呢就是所謂的有一個名字叫做 forward  backward  algorithm	4-12
也有另外一個名字叫做 Baum  Welch  algorithm	4-12
所以一般你在文獻或者書籍裡面看到所謂的 forward  backward  algorithm 或者 Baum  Welch  algorithm 都是指這一個	4-12
那麼 forward  backward 是比較講它的意思你可以猜得出來是因為用了我們前面的 forward 的 alpha 跟 backward 的 beta 然後它兩邊兩邊都在跑一邊 forward 一邊 backward 所以叫 forward  backward  algorithm	4-12
那麼它也有一個名字叫做 Baum  Welch 這是兩個人的名字	4-12
那麼那它的這個詳細的基本的原理呢是有相當複雜的它的原理基本上是 base  on  E M 所謂的 E M  algorithm	4-12
或者叫做 E M  theory	4-12
也就是說我們底下講的東西它是從根據這個 E M 的推導來的	4-12
那麼 E M 本身是一個有一堆複雜的 theory 所以有的人稱為 E M  theory	4-12
那麼它本身這個也是一個不能解的只能夠 iterative 的解	4-12
所以呢需要一套 algorithm 來解它所以有一堆人稱之為 E M  algorithm	4-12
那麼我們現在講的這個方法是根據 E M 來解的至於 E M 是什麼呢 E M 比較複雜一點我們會留到學期中以後再講 E M	4-12
所以我今天大概並不真的講 E M	4-12
我只是大概講一下這個 forward  backward 的流程跟它大概的意思	4-12
我們了解怎麼樣就好了那有一堆詳細的理論我就留到學期中講 E M 的時候再來講它	4-12
OK 好那麼要解這個問題我們剛才講	4-12
我現在知道某一個聲音它的 model 已經有了但是那個 model 是用一堆別的聲音 train 一堆那個聲音 train 出來我現在有一個新的聲音進來	4-12
我知道它是它的那個聲音我要把它的學進去使得那麼這個新的 model 呢可以算這個機率的時候可以最大	4-12
那麼怎麼辦呢我現在 define 一個新的參數叫做 epsilon 又來了那麼我們之前已經有了 alpha  beta  gamma  delta 現在再來是 epsilon	4-12
那麼你看這個 epsilon 的定義裡面就把我們之前講過的 alpha  beta 這些東西都用進來	4-12
那麼它是什麼呢我們看一下它的意思大概就能夠了解我們仍然用之前的那張圖來畫	4-12
這個橫軸是時間	4-12
T 從一二到大 T 縱軸是 state 一二三到大 N	4-12
我們用上次那個圖那麼它現在講的是什麼呢我要在時間 t 的時候在 i 時間 t 加一在 j 的機率	4-12
所以呢譬如說時間 t 的時候是在 i	4-12
在 t 加一的時候呢在 j	4-12
 OK 所以呢它的要求是時間 t 的時候在 i  t 加一的時候在 j  given 整個的全段的聲音我 given 從零到 t 一到 t 然後呢 given 這個 lambda	4-12
那這是什麼呢怎麼算呢你看它這個分母分子這項其實是很容易看的	4-12
 alpha  t 的 i 是什麼就是我們講過就是在這裡的時候 t 的時候是 i 之前呢	4-12
通通都看到了但是沒有規定它要是什麼 state 這個是 alpha  t 的 i 後面到後面 beta  t 加一的 j 是什麼呢	4-12
 beta  t 加一的 j 呢我們說我是 t 加一要在 j 這裡但是我呢沒有看到它我是看到這個以後的全部	4-12
而且沒有規定它在哪裡所以呢這個是 beta 的 t 加一的嗯 j 那前面這個呢是 alpha  t 的 i	4-12
那那你就馬上會發現會怎樣呢其實 t 加一我沒有真的看到這兩個算進來的時候	4-12
那麼我已經規定好 t 要在 i 了 t 加一要在 j 了這兩個條件已經我所規定的條件已經在這裡了	4-12
譬如說這兩個條件可是呢我現在沒有看到 t 加一因為 beta 的 t 加一的話呢只是看到 t 加二以後的	4-12
這點我們上週前一次說過了 beta 的定義是如果這邊寫 t 加一的話呢這 t 加一沒有看到是看到以後的	4-12
所以它是從 t 加二以後看所以這個沒有看到啊所以呢我現在要把這個 o 的 t 加一這個東西呢	4-12
放到這個 B J 來這樣子我才看到這個所以那就是這一項	4-12
 OK 所以呢我現在要把這個一旦放進來之後呢 OK 那現在我都看到那這個呢就是 B J 的嗯 b 的 t 加一的 j	4-12
那當這個放進來之後呢那這裡有了那還差一個什麼還差一個這個的 transition 就是 a  i  j	4-12
對不對所以我還要有一個 transition 就是這個這個就是 a  i  j	4-12
於是呢這麼一來的話呢我現在就等於是從頭到尾我都看到了	4-12
而且呢知道 t 在 i  t 加一在 j 而且呢一路這樣 trans 過來全部都有了	4-12
那麼因此呢這個分子這個東西呢就是我們這邊講的這個	4-12
對不對就是這個嘛那你看就是我看到全部的 o 了而且呢 t 在 i  t 加一在 j 對不對所以呢這是分子這樣做其實就是在做這件事	4-12
OK 那麼分母的話呢分母我現在其實我要除的就是這個東西這個東西我們現在已經有很多種求法了	4-12
你記得我們在 problem  one 的時候的 forward  algorithm 也可以求它那 problem  two 的時候我們解第一種方法的解法的時候也求的出來	4-12
這個我們都講過了那它這裡的話你其實用任何一個都可以都可以做	4-12
但是它這裡的辦法就是把完全把上面這個照抄下來然後把所有的 i 跟 j 全部加起來也是一樣的意思	4-12
那你看因為我你看這個式子就知道它的分子長這樣它的分母呢長得完全一樣	4-12
只是我把全部的 j 全部加起來全部的 i 全部加起來那也就是說	4-12
我分子是規定它在這裡跟它在這裡分母的話我把這個的所有的通通都加進去嘛	4-12
那麼把這個也全部都加進去那就變成全部都是了嘛對不對那麼因此我就不再規定 t 加一在 j	4-12
 t 在 i 這不規定了因為這邊我全部都加滿了所以呢我只要把這兩個這個 j 跟 i 從一到 n 全部加滿的話那這回這兩個條件就沒有了	4-12
於是呢就是就是這個式子所以呢這個式子又有一個新的新的求法是這樣求的那它的意思是完全一樣的 OK	4-12
那我如果是把這個分子拿來 j 全部加起來 i 全部加起來就跟這個一樣就變成這樣子	4-12
那當我這個有了之後呢分子跟分母一除會怎樣就把這個 observation 放到條件這邊來就變成這樣子	4-12
那這個做法我們之前也已經看過求 gamma 的時候就是這麼做的我們之前求 gamma 就是這麼做的	4-12
所以呢你就把它這個一除之後呢就把這個 o 放到右邊來就邊成這樣子那這個就是 absolute  t 的 i  j	4-12
也就是說呢你給我整個的 given 全部的聲音我都看到了然後呢我又 given 那個 model 的話	4-12
那其中 i 在 t 在 i  t 加一在 j 的機率呢那就是這個 absolute  t 的 i  j	4-12
 OK 好有了這個之後呢我們再回想一下我們之前講的 gamma 是什麼	4-12
 gamma 跟這個很像只是少了一個 t 加一的條件而已	4-12
 OK 這個 gamma 是我們在這個二 problem 二的時候解過就是	4-12
這裡的 gamma 那 gamma 跟剛才那個其實是蠻像的我只是只是這個這邊 alpha 跟 beta 都是 t 然後都是 i 這個分子這堆東西呢分母也是把它全部加起來	4-12
等於是 normalize 一次那這個就是 gamma 那它的它的這個意義就是這個	4-12
我看到這個整個的 lambda 看到整個的聲音之後 t 在 i 的時間叫做 t 的 gamma  i 這個我們在上上次已經講過了	4-12
好那我現在就把這個帶過來看回到我們剛才說的地方就是在這裡	4-12
那麼我現在 gamma  t 是這樣的我已經有了	4-12
好那麼我現在要做的底下這兩件事情就是我把 gamma  t 的 i 從一到 t 減一全部加起來	4-12
以及我把這個 absolute  t 的 i  j 也是 t 到一等於 i 全部加起來	4-12
這兩個一起都加起來那這個是在幹嘛呢	4-12
我們再畫一次剛才的那個圖	4-12
這個橫軸是時間 t	4-12
縱軸是 state  n	4-12
這是一二三到大 t 這個是一二三等等這個是 i 這個是 j 這個是 n	4-12
好我們先說 gamma  t 的 i 是我看到全部的聲音之後在 state 在 t 的時候在 i 的時候譬如說在 t 的時候在 i 的時候	4-12
這是這個是 gamma  i 的這是 gamma  t 的 i 的機率那我現在如果要把它從小 t 等於一到 t 減一全部加起來會怎樣你可以看這個是 t 這個是 t 減一	4-12
這是 t 減一那麼你要把這個 gamma 從第一個開始第一個第二個第三個一路加起來加到 t 減一為止	4-12
把這個全部加起來那是什麼意思呢我們舉個例子來講	4-12
我假設說這個 t 等於兩百五十也就是說我這一句話的聲音總共有兩百五十個 vector 在這裡	4-12
那 t 減一呢就是兩百四十九	4-12
那麼於是呢在這中間會怎樣呢這個每一個都有一個機率	4-12
每一個 gamma  t 我們現在的這裡的每一個 gamma  t 的 i 都有一個機率那麼我們舉例來講譬如說這個呢	4-12
譬如說這個是零點嗯我們說這個是零點零零一	4-12
這個是零點零零三那麼到中間的時候呢譬如說呢這個變成零點一二這個變成零點二三	4-12
那再來呢變成零點一五到最後我又變成譬如說零點零零二等等等等	4-12
這個變成零點零零一	4-12
我舉例來講假設是這樣這什麼意思呢因為它是 t 在 i 的機率嘛	4-12
 t 在 i 的機率的話那你可以想像因為我們看到大部分的狀況我們這個 state 從這邊往上走對不對從這邊往上走走到上面來	4-12
然後所以呢在時間早的時候是在底下時間長的時候是在後面嘛那假設這個 i 在中間的話呢	4-12
那大部分的狀況其實是在中間的時候我這個會在這裡通過這邊就會通過的機會很少	4-12
這邊就會通過的機會很少那是為什麼你看我這邊寫的數字前面只有零點零零一很小很小	4-12
到後面多起來零點一二零點二三這邊是比較高的這邊比較會多一點然後到這邊的話呢那就又少了	4-12
嗯等等那意思就是說大部分的狀況它應該在中間通過到那邊去	4-12
好假設我把這些東西就是我的 gamma  t 的 i 的話那我現在把它全部加起來對不對這邊是講是加起來	4-12
加起來的話我們舉例來講我把它全部加起來的結果呢	4-12
那麼假設這些東西全部加起來	4-12
加起來呢是等於譬如說十點五假設說是十點五	4-12
那是什麼意思呢你那個十點五意思就是說你你可以想像在這兩百四十九次裡面	4-12
in  average 那麼你有多少次這個 state  i 是 visit 也就是說你可以想像我現在總共有兩百四十九個 vector 嘛	4-12
那麼這兩百四十九次裡面平均有十點五次的時候它是在這個上面的等於是這樣的意思	4-12
對不對你再想一次因為我現在 gamma  t 的 i 是這個意思啊就是我現在已經看到這整個的我看到這整個的 o 了然後呢 t 在 i 的機率	4-12
所以呢 t 在這個一在 i 的機率只有零點零零一因為它一開始不太可能在這裡二在 i 的機率有零點零零三稍微多了一點	4-12
到了這個地方呢比較多了零點一二零點二那是比較多了那這樣到後面又會少了因為它不是在後面	4-12
那這樣最後加起來是十點五的話那等於是說在這兩百四十九次裡面我們可以說有十點五次	4-12
平均總共有十點五次譬如說這個在零點二三等於說有零點二三次的時候它會在這裡因為在這裡的話它會有零點二三次在這裡	4-12
那這裡有零點一二就表示在這個時候它會有零點一二次在這裡對不對那我把它全部加起來就十點五次在這裡	4-12
那麼因此呢你就可以想像成這個是我的 state  i 總共被 visit 的次數是譬如說十點五次	4-12
好那如果是這樣的話那還有一個 epsilon  i  j 呢我也可以照樣來做這件事	4-12
那 epsilon  i  j 是什麼呢你再回想一下我們剛才講的是這個啊那意思就是說跟剛才很像我現在看到全部了	4-12
我看到全部了但是呢 t 也是在 i 的但是我規定了 t 加一是等於 j 的所以呢就 j 而言呢是這個	4-12
這些東西但是呢是 t 的時候要在 i 那就跟底下這個條件完全一樣但是我多了個條件 t 加一要在 j	4-12
所以呢就 t 等於一而言它講的是二多了一個二那麼它呢是譬如說我這邊是零點零零一	4-12
那麼 t 等於二的時候呢我們講的 t 加一是三那它呢是零點嗯我們說零點零零零一好了這零點零零零三等等	4-12
那到這邊的時候它可能也會多起來因為它這個會往這邊跳	4-12
那這個譬如說我們說零點零零點一一嗯譬如說等等	4-12
那麼等到 t 等於 t 減一的時候呢那麼 t 加一就是大 t 了那這個時候我有一個機率譬如說是零點零零零一	4-12
那我這邊寫的大小也是一樣譬如說讓它在中間比較大就像我這邊剛才畫的中間比較大的原因我剛才解釋過就是說這個	4-12
假設它這個是在中間的位置的話中間比較會通過它嘛機率比較高一點	4-12
那麼什麼意思呢意思是說這個零點零零一是這邊有零點零零一這邊有零點零零零一	4-12
表示說我從這邊會跳到這邊來的情形是這樣我有零點零零一的機會它會在這裡	4-12
那在這裡面的還有十分之一的話下一個會跳到這來另外十分之九可能沒有跳過去跳到別的地方去了或者怎樣	4-12
對不對你看這個我這邊底下是 gamma 這上面是 epsilon  gamma 的話呢只有講 t	4-12
 t 的時候在 i  epsilon 的話就多了一個 t 加一在 j 所以呢你可以想像假設	4-12
零點零零一的時候是在這裡的話有這麼多機率它會在這裡那這裡面只有十分之一的機率它會下一個跳到這來	4-12
那就零點零零零一那另外十分之九呢它可能也許是停在原來的地方或者跳到別的 state 去或者怎樣它不在這裡	4-12
那同理呢那麼零點零零三在這裡那但是呢我這邊也應該多加一個零	4-12
我零點零零三的時候在這裡有這個機率在這裡可是呢它除了這個之外我同時再下一個 t 會跳到	4-12
 t 等於三的時候它會跳到這裡來的機率呢是零點零零零三那這樣又表示我有十分之一的可能在這裡等等	4-12
那如以此類推的話呢我現在把這些東西通通加起來	4-12
於是呢我這些東西我也加一次這些東西加起來譬如說總共等於一點二三	4-12
那是什麼意思呢那就是說	4-12
我平均我可以猜說我在這整個時間裡面那麼我會從 i 跳到 j 總共有幾次	4-12
總共一點二三次那你可以看到說我在這兩百四十九次裡面平均有十點五次它是在 i 上面	4-12
有十點五個 vector 有十點五個 vector 它在 i 上面但這裡面呢會跳到這個 j 的呢是一點二三個	4-12
那麼因此呢我們就有底下的這個式子就是下一頁的這個式子我們怎麼估計它的 a  i  j 就是這個除這個	4-12
也就是說那麼我的 a  i  j 呢	4-12
怎麼算就是這邊的十點五分之一點二三	4-12
那你說這合理不合理呢	4-12
意思就是我剛才講的這個意思那這個怎麼推出來的這個其實是用 E M  theory 推的我剛才	4-12
剛才寫在這裡 E M  theory 這個並不是這樣隨口亂講的那事實上是有 E M  theory 整個可以導出來最後證明它就是這樣子	4-12
那我們現在不去導那個東西所以我只是把答案寫出來而已所以這個是 results 我只有把答案寫出來	4-12
那那我們現在再解釋說這個答案這個式子其實是有意義的這意義就是我剛才講的意義	4-13
那我們可以再看一次這意義再講一次就是說	4-13
我的分母是這個東西那也就是我假設這個 t 是兩百五十的話我現在是從一看到兩百四十九	4-13
我有兩百四十九個 vector 這兩百四十九個 vector 裡面它都有機會掉在 state  i 裡面不過呢有的時候機率非常小零點零零一啊零點零零三啊什麼	4-13
那你把這些機率全部加起來是十點五的話	4-13
你可以假設說就是這兩百四十九次裡面有十點五次它是在 i 上面	4-13
那這個是 gamma  t 的 i 它就是算 q  t 在 i 的時候那另外我再加這個 absolute  i  j	4-13
那這個呢就多了一個 t 加一呢在 j 的條件比剛才就多一個比剛才多一個嘛所以呢這個跟這個相比的話你就知道它就是	4-13
除了這個要在 i 呢下一個還要在 j 的意思嘛	4-13
這個在 i 下個還要在 j 嘛那這個東西也就是我們剛才講的這個意思	4-13
就是你要用這個來算就是這樣算出來也就是我們剛才講的上面這個算法那麼所以呢你現在如果說是	4-13
把這個也算進去看的話於是呢我現在在這個這個嗯譬如說呢	4-13
我還是 t 等於 absolute  i  j 的時候我還是 t 等於一開始做但是 t 等於一的時候是指	4-13
 t 的時候在 i  t 加一就是二嘛就表示二在 j 所以說 t 等於一而這個這個一的時候等於 i 二的時候等於 j 的時候	4-13
二的時候跳到 j 的機率是多少呢我們說如果是零點零零零一的話是它的十分之一就表示說呢這裡面呢	4-13
我有我有十分之一的機率當我在這裡之後有十分之一的機率會到這來另外十分之九呢它可能回到原來的或者是到別的地方去了	4-13
那同理這個是 t 等於二的時候這邊是零點零零三那這個是零點零零零三又是有十分之一的機率跳到這來	4-13
另外十分之九呢到別的地方去了等等那我每一個都可以加那我這邊還是一樣從一加到兩百四十九	4-13
但是對 t 加一的 j 而言呢是從二加到大 t 這是我能夠觀察到的是 t 嘛也就是從二加到兩百五十	4-13
 OK 那麼於是我把這些東西全部加起來的話你就可以想像總共有一點二三次是會從總共有一點二三次在這個兩百四十九次裡面	4-13
總共有一點二三次它會從 i 跳到 j 去那如果是這樣的話那就可以做我們底下的這個式子你怎麼估計這個 a  i  j 呢	4-13
就是這樣估計你就是把這些東西找出來然後呢我總共有十點五次是在 i 上面	4-13
但是有一點二三次是從 i 跳到 j 那麼一除的話我的 a  i  j 這就是我的 a  i  j 那也就是我們剛才講的這後面講的解釋	4-13
那麼你譬如說這個加起來這個這個分子分母的話呢就是平均有多少次在 state  i 上面	4-13
也就表示平均會有多少個 transition 從 i 出去	4-13
總共有十點五個 transition 從 i 出去那這十點五個 transition 裡面有一點二三個到了 j	4-13
所以呢這個東西呢就是那一點二三就是從 i 到 j 的總共的 transition 是一點二三次	4-13
於是你這兩個一除那我就得到這邊的 a  i  j 的這個公式	4-13
那麼 pi 怎麼求 pi 不難求因為 pi 只是一開始的	4-13
那就是說這個東西會在這裡的機率是多少這是 pi 的意思嘛那其實就是 gamma 的 t 等於一的時候的 i 嘛	4-13
所以呢就是這個嘛所以呢那這個我剛才 gamma 已經求出來所以我會有 gamma 嘛我就把這個東西放進來就是這堆 pi	4-13
好這個是所以我有新的 pi 了我有新的 a  i  j 了	4-13
那麼 b 怎麼求呢這個其實不太需要了因為這是指 discrete 的時候用的	4-13
那麼我們姑且看一看這個很容易看所以姑且看一看其實我們要講的是底下那個 Gaussian 的 case	4-13
那這個東西其實不難看它的意思跟剛才你從剛才看就很了解是一樣的你看我的分母也是這個嘛	4-13
分母幾乎是一樣把 gamma 加起來分子也是 gamma 看它掉在哪一個上面因為這裡講的這個 discrete 的 B K 的 j 的意思是這種東西就是	4-13
我那個 distribution 它不是用一堆 Gaussian	4-13
這是 discrete 不是用一堆 Gaussian 來來描述而是什麼呢用一堆點它說每一個點各是多少	4-13
它說這個點是零點零零零六這個點是零點零零一三等等它用這個點零點零零七五	4-13
它就把一堆點用把一堆點的數值通通標出來的這種這是所謂的 discrete  Hidden  Markov  model	4-13
那這裡的每一個點呢就是所謂的 v  k 這些點就是 v  k	4-13
所以那它這個機率怎麼來的這種零點零零零六這個機率怎麼來的呢就是這樣子跟剛才一樣	4-13
譬如說你現在如果 state  j 的話就是指這個 j 嘛對不對所以就是這個 state  j 嘛那在這個 j 裡面	4-13
我就看這個 j 會有一個對這個 j 而言會有一個 distribution 就是這個 distribution	4-13
這個 distribution 我就 specify 這個 v  k 的值是多少就好了那怎麼看呢那我就是看說	4-13
我的這個我我掉到 v  k 上面的有多少個那麼這是 total 對不對	4-13
譬如說我現在的分母還是跟剛才一樣那麼唯一不同的是我現在可以把第兩百五十個也算進來	4-13
唯一不同是這樣因為我這邊現在不要做 transition 了剛才是剛才為什麼是都是到 t 減一呢	4-13
為什麼是到 t 減一因為還要有一個 transition 我是在算 transition 所以最後一個 transition 是從 t 減一到 t 的	4-13
對不對這個所以這樣的話我只能算兩百四十九個所以剛才這邊都是到 t 減一我現在只算是這個的話	4-13
沒有這個問題了所以我就算到兩百五十所以我如果把這個我如果把最後的這個嗯	4-13
我如果把最後的這一點也加進來之後呢我得到一個呢是變成十點六三	4-13
 OK 那這樣子的話呢我的分子就是分母就是十點六三然後在這些裡面我看它們的 o	4-13
有多少個可以應該可以歸到是算是這一點的	4-13
那其實它這個方法是要算 distance 然後根據這個 distance 來說 ok 它可以算是這一點等等看有多少個所以這就是分子就是這個全部裡面	4-13
你看這些這些裡面是哪些可以歸到這個的然後把它加起來那譬如說這裡面加起來如果是	4-13
這個總共是一點二三可是但是一點二三裡面很多那些 o 是在不同地方	4-13
那麼哪些的 o 是可以距離跟這個最近可以歸到這邊來的呢假設加起來只剩下你把那些個加起來剩下這些譬如說是零點零六於是這個就是零點零六	4-13
那這個呢就是我們的這個嗯我的在 v  k 的那一點那這個一除就是零點零零六那就是這一點嗯譬如說這個這樣子	4-13
所以呢這個沒有什麼特別只是跟剛才一樣我們其實這個它就這樣算出來	4-13
那這個是 discrete 的部分那我剛才講這些其實不是像剛才那樣用嘴巴講的它其實都是用 E M  theory 推出來的	4-13
不過 E M  theory 的推的過程我們留到學期中以後再說就是了那我們現在先只是講你看這個式子其實都可以了解它的意思	4-13
那我們真正 concern 的不是這一個而是哪一個是 GAUSSIAN 的時候也就是說呢我們真正 concern 的是這樣子	4-13
在 j 的時候不是這樣一個點的而是一堆 Gaussian	4-13
因此呢我可能有一堆 Gaussian	4-13
那我要算這堆 Gaussian 的 mean 跟 covariance 那才是我們要 concern 所以底下這個是 continuous  density  H  hidden  Markov  model	4-13
那這個時候我的 B J 呢變成一堆 Gaussian 相加所以呢我要我要求的是 Gaussian 的 mean 跟 Gaussian 的 covariance	4-13
以及那個 Gaussian 的 weight 這三組參數那這三組參數該怎麼求所以我要求的第一個是 Gaussian 的 mean	4-13
譬如說這個 Gaussian 有一個 mean 這個 mean 是一個三十九維的 vector 這個我要求得出來第二個這個 Gaussian 的分散的程度	4-13
怎麼分的那就是它的 covariance  matrix	4-13
然後第三個每一個 Gaussian 有一個 weight 加起來要等於一嘛對不對所以我要求這三樣東西	4-13
那這三樣東西怎麼求呢在下一頁我又要 define 一個新的東西叫做 gamma  t 的 J K 嗯	4-14
這個東西有一點複雜不過呢其實蠻簡單你如果記得 gamma  t 的 j 是什麼的話 gamma  t 的 j 是什麼呢	4-14
就是我們剛才講的這一個	4-14
我們剛才一直在加這個東西嘛就是這一個	4-14
那它就是這個嘛就是我剛才加的這些東西就是 gamma  t 的 j	4-14
那麼就是說我看到整個的聲音而我時間 t 的時候掉在 j 的掉在 i 的機率叫做 gamma  t 的 i 或者 j	4-14
那我現在要再 define 一個新的東西叫做 gamma  t 的 j 跟 k 再多一個 k 那 k 是什麼呢	4-14
你如果仔細看一下的話這個 k 呢它是說就是包括我在第 k 個 Gaussian 的機率	4-14
那其實呢你只要看這裡就好了看這一項其中前面這一項其實就是我們原來的 gamma  t 的 j 前面這一項就是我們原來的 gamma  t 的 j	4-14
那就是多了後面這一項那後面多了這一項就是把這個 k 算進來的意思	4-14
OK 那麼前面這一項就是我們我們這個之前講的 gamma  t 的 j 那基本上就是 alpha  beta 相乘 normalize	4-14
那這個東西呢也就是我們之前在 problem  two 的時候上次上課在講 problem  two 的時候	4-14
講的這個 gamma  t 的 i 這個是 problem  two 的時候第一個 approach 所講過的東西	4-14
那這個你看我們當時說過這個 gamma  t 的 i 就是這個就是我們這邊講的這個啦就是這個啦	4-14
那它的式子就是這樣算的嘛 alpha  t 的 i  beta  t 的 i 除以這個嘛那這個也就是我現在的這個式子	4-14
就是這個式子 OK 所以我就是就是這個 gamma  t 的 j 但是呢我後面多了一個 k 我剛才是只有一個 index  j 的	4-14
現在變成兩個 j 跟 k 那 k 是什麼呢 k 就是後面這個後面這個是什麼你仔細看看就是 Gaussian	4-14
那麼上面是一個 Gaussian 底下是全部 Gaussian 的和什麼意思呢我們如果用 one 一個 dimension 是比較容易想像它的意思	4-14
就是說假設我有很多個 Gaussian	4-14
那麼這是一個 Gaussian 這是一個 Gaussian 這是一個 Gaussian 這是一個 Gaussian 這是一個 Gaussian	4-14
那麼今天呢假設第 k 個 Gaussian 在這裡	4-14
譬如說這是第 k 個 Gaussian	4-14
這是第 k 個 Gaussian	4-14
或者叫做第 k 個 mixture 這邊常常寫成 mixture 因為它們都稱為 mixture 所以一個 Gaussian 就是一個 mixture	4-14
如果這是第 k 個 Gaussian 的話呢那麼所以呢分子就是第 k 個 Gaussian 所以這是第 k 個 Gaussian 的 mean 跟 covariance 等等等等	4-14
而分母呢是全部加起來的然後我他們都拿那個時間 t 的那個 vector 放進去	4-14
我現在講的是 t 嘛這個是 t 嘛所以我是把時間是 t 的那個 vector 放進去所以呢譬如說時間 t 的那個 vector 呢是這一個	4-14
這個是這個是 ot 的話放到這來的話呢在第 k 個 Gaussian 的時候在這裡	4-14
這樣我們就用這個好了這個是在 t 的時候在這裡可是如果放在全部的話呢它是一個它是一個	4-14
有這麼多 Gaussian 加起來它是一個可能是一個這樣子的	4-14
是一個這樣的東西所以呢那個呢是這個	4-14
所以呢我的分母就是這個 Gaussian 的這個全部的 Gaussian 的值分子是這個 Gaussian 的值 OK 所以後面這項是這個意思嘛	4-14
就是說我現在把我那個這是一大堆 Gaussian 的組合嘛對不對一大堆 Gaussian 的組合那你看你是要看一個單一的 Gaussian 還是看全部	4-14
那我如果是單一的 Gaussian 的話我把我的 ot 我時間 t 的 vector 放在那裡得到一個值	4-14
但是呢我如果全部加起來的話我也可以放進去也得到另外一個值看看這兩個的 ratio 是多少	4-14
那這兩個的 ratio 顯然告訴我的意思是說對這個 vector 而言我現在在算這個 ot 我現在是時間是這個 t 嘛	4-14
我對這個 ot 而言對這個 vector 而言那麼它在這一個 Gaussian 裡面	4-14
會得到多少分數跟在全部裡面的比例是多少那那個比例其實就是我們到時候就是要算這個	4-14
就是算我的這個 weight 就是在 state  j 在的時候呢第 k 個 Gaussian 的 weight 多少嘛	4-14
也就是我要算就這個東西就是	4-14
當我在變成這個的時候一大堆 Gaussian 加起來每一個 Gaussian 要有一個 weight 嘛那這個 weight 怎麼算這個 weight 就這麼算	4-14
所以呢我是要跟這個有關 OK 好那我們現在再看一次我現在算這個 gamma  t 的 J K 的目的是為了要算這個 weight 嘛哦	4-14
那我怎麼算呢我就是把 gamma  t 的 j 先算出這就是剛才講的那個	4-14
就是在時間 t 的時候它平均會掉在 i 上的機率是多少這個時候我是只要掉在 state  i 就好了	4-14
我沒有管它掉在哪一個 Gaussian 裡面對不對所以呢 Gamma  t 的 j 呢只是說在時間 t 它在 j 上面就好了	4-14
那麼沒有管它在它在哪一個 Gaussian 上面那所以在後面我再多乘一個東西就是要講它如果在哪一個 Gaussian 上面的話會怎樣	4-14
所以呢我就要算它在這個 Gaussian 上面除以它在全部的這個 ot 放進去它的 ratio 差多少 OK	4-14
好那如果這個了解的話那這個就是我們講的 gamma  t 的 J K 的意思那如果有了這個的話那我現在就可以把它這個算出來就變這樣	4-14
那這個式子你再看看我其實就是把 gamma  t 就是這邊算了半天這個 gamma  t 的 J K 就拿來就是了	4-14
那唯一不同的是你看我分子就是我這邊算的東西從 t 等於一到大 t 我就是把這個 gamma  t 的 J K 從第一加到第兩百五十	4-14
我就全部加起來這就是分子那就表示說我在這個時候	4-14
時間是在 j 的時候那麼它在第 k 個 Gaussian 的 weight 是多少然後我全部加起來這樣總共是多少	4-14
那除以分母呢也是一樣的東西只是呢我每一個 t 的時候我把所有的 Gaussian 全部加起來了	4-14
對不對所以等於是做個 normalization 對不對你如果不看這邊如果不做這個這邊也不做這個的話這個除以這個就是 normalization	4-14
就把所有的這個 k 等於一到 m 就是所有的 Gaussian 全部加起來所以呢我的分母跟分子是一樣的東西我只是把所有 Gaussian 全部加起來	4-14
所以一 normalize 之後就是它的 weight  OK 所以這樣的話我就這個 gamma 算了半天之後我的目的呢	4-14
第一個目的達到了就是我可以估計這個這個嗯在 state  j 的時候在 state  j 的裡面這一大堆 Gaussian 的裡面	4-14
那麼第 k 個 Gaussian 的 weight 怎麼算可以用這麼算那這樣講其實也是嗯跟剛才講的一樣就是這個式子其實也是用也是用 E M  theory 推出來的	4-14
那我們後面會講現在只是說他的物理意義這其實是有物理意義的這這樣的意思	4-14
好那有了這個之後那再下來呢我們再大的一個問題是要算這個 mean 跟這個 covariance 我每一個 Gaussian 都有一個 mean 要算怎麼算	4-15
它有一大堆 covariance  matrix 怎麼算	4-15
那 again 我用這堆東西就是剛才算的 gamma  t 的 J K 就是這個東西	4-15
那這個式子是什麼呢這個式子我們可以這樣子看你如果回想 one  dimension 的one  dimension 的 random  variable  x	4-15
那如果它有一個 distribution 是 p  of  x	4-15
是一個 p  of  x 的話它的 mean 怎麼求是不是這樣求 x 乘上 p  of  x 然後積分	4-15
就是它的 mean	4-15
這個沒問題吧這個是非常簡單的一個求 mean 的假設是一個 random  variable 只有一個 one  dimension 的 random  variable	4-15
它的 probability  density  function 它的 probability  density  function 是 p  of  x 的話我怎麼求 mean 就是拿這個 x 的值乘上它的 distribution 然後全部去積分	4-15
等於說每一個值都可能是 mean 每一個值都有可能然後把它的機率都乘進去然後全部去平均起來這就是它的 mean	4-15
如果你可以了解想像這個式子的話那上面那個式子意思是完全一樣的	4-15
它就是這個式子那何以見得呢	4-15
我們寫一下你就知道了它是分子是 summation 的 gamma  t 的 J  K 然後乘上 ot	4-15
是從 t 等於一到大 t 然後分母的話呢就是這堆 t 等於一到大 t 的 gamma  t 的 J K	4-15
對不對這是那個式子那你如果仔細看這個式子的話呢	4-15
那我說這堆東西其實就是 p  of  x 相當於那邊的 p  of  x 那這個東西呢就是我相當於那邊的 x	4-15
然後這堆東西呢就是積分那這樣是不是就是一樣了那這個就是這個意思	4-15
我其實只是在求平均而已換句話說我現在看到從一到 t 這兩百五十個 o	4-15
我要知道這兩百五十個 o 裡面的平均但是呢 well 這兩百五十個 o 是可以在每一個裡面啊	4-15
所以我得第一個 identifies 在這裡的不要把這些東西再算進去嘛所以我先要把這個東西算出來嘛對不對	4-15
那所以怎麼辦呢我就要去根據那我要這些 gamma 就是就是要 gamma 就是要在 t 上面的嘛	4-15
對不對所以這個 gamma 的定義就是它在它在這個 t 上這個 gamma 就是我要在 state  j 上面的	4-15
然後呢我的這個後面的 k 就是我要在第 k 個 Gaussian 的位置	4-15
所以把這個都算進去之後呢我得到的其實其實就是	4-15
我把這些東西去求平均但是我只要算它在這裡面的這部分不要算到這些地方去	4-15
只要算這部分我只要算它在這裡的 j 是說我只要算它是在 state  j 裡面的機率	4-15
然後呢這個 k 的意思是說這個 k 的意思是說我現在只要算這個第 k 個 Gaussian 的東西所以呢我是以這個 k 跟這個 j 來算	4-15
那這個其實就是一個 distribution 那麼那這個等於是說我每一個時間的時候它這是除以 total 嘛在每一個時間它除以全部佔多少	4-15
然後呢這個把它的這個 mean 乘過去然後全部積分全部加起來那這樣呢等於是在算這個式子	4-15
所以這樣的結果我就會得到這一個 Gaussian 的這個 mean  OK 那如果這個你可以想像的話	4-15
那底下這個式子的意思是完全一樣的現在求 covariance 了求 covariance 怎麼求	4-15
那就跟你平常所想像的 variance 怎麼求是一樣的如果對單一的 random  variable 跟剛才一樣哦	4-15
如果這是單一的 random  variable 的話我的 variance 怎麼求 variance 是這樣求的對不對	4-15
是我的一個 x 值減掉它的 mean	4-15
然後平方然後然後乘上它的 probability  density  function	4-15
積分嗯 D  X 積分	4-15
這樣的話我得到的是那個 x 的 variance 對不對這個式子應該是很容易講跟那個意思是完全對等的	4-15
就是我要求 variance 是這樣求	4-15
那你如果 variance 這樣求你可以想像的話那我這邊也是一樣啊那我現在這個式子跟剛才一樣我不再寫一次了	4-15
跟剛才這邊的情形是完全一樣的你可以想像中間這個就是 p  of  x 就是這一個這個除以這堆就是那個相當於那個 probability  density  function	4-15
那後面這個呢就是相當於這個 x 減 mean 的平方	4-15
只不過因為我現在是 vector 現在都是都是 n  dimension 的 vector 所以呢我變成	4-15
這個要剪掉之後這個是什麼這是 transpose	4-15
這兩個這個剪掉之後再乘上一個 transpose 之後的話就變成是這個每一個 component 相乘嘛	4-15
對不對就是說你現在因為是我這邊講的都是 vector 所以呢這一個是這樣子的	4-15
它減掉 mean 是一個這樣子的 vector 然後呢它的 transpose 是一個這樣子的 vector	4-15
嗯這樣寫有點反了它很可能它把它當成是當成是 row 來寫的話	4-15
哦沒有錯沒有錯這樣對因為我這樣子的話就是每一個乘一個每一個乘一個每一個乘一個所以就乘出來會變成一個 covariance  matrix	4-15
對的沒錯這樣子也就是說我這邊是減一個一個值減掉它的 mean	4-15
但是我現在變成有一把值分別減掉它的 mean 然後呢那它的 transpose 的話是一把值分在這裡	4-15
那這個跟這個在做 matrix 的相乘的話這後面是它的 transpose 那這個一乘的話這每各自都可以乘一個 component 出來就變成一個 matrix	4-15
然後我現在在求平均的時候就求出所有東西出來那其中的對角線上的這些東西呢就是你原來的每一個 dimension 的這個 variance	4-15
但是我還有這些對角線以外的這堆東西呢是它們的 cross 的 covariance 的參數	4-15
那因此我就可以得到這整個不過這裡面所有的意思都跟這個一樣就是你分別去某一個對不對	4-15
我們如果要寫得詳細一點的話就是把它寫成一個譬如說這個 xi 減掉它的 xi 的 mean	4-15
乘上這個 X J 減掉 X J 的 mean 的這種東西去做平均嘛	4-15
這個是 covariance 的這裡面的每一個 element 的意思是這樣子嘛那這個跟這個的意思是一樣的	4-15
只是我把這個平方變成一個是 i 一個是 j 然後變成這樣子去做就是了	4-15
如果照這樣子來看的話呢當 i 等於 j 的時候就是這些東西對角線上的那就是求出它的它的這些 variance 的值	4-15
就是每一個各自的 variance 的值當 i 不等於 j 的時候就得到這些對角線以外的點就是它們之間的 cross  correlation	4-15
它們的就它們的 covariance 的參數那這樣我就得到這個 matrix 那麼如果這樣看的話你如果可以想像那麼這個式子其實就是這個意思	4-15
所以你可以看成中間的這個一堆其實就是這個分子除以這個分母就是我這個 probability  density  function	4-15
然後呢我右邊乘上這堆東西呢那就是相當於這個嘛或者相當於這種東西只是我現在做出來就是一個 matrix	4-15
那然後這邊來積分嘛積分就是在求平圴嘛求 expectation 或者就是對機率來平均的意思	4-15
因此我這樣我就得到我的 covariance  matrix  OK 　好那麼這樣一來的話這才是我們今天真正用的最多的forward  backward  algorithm 是這一塊哦	4-15
也就是我們剛才講的當你算出 gamma  t 的 J K 以後	4-15
那麼我現在來算它每一個 Gaussian 的 weight 然後我來算每一個 Gaussian 的 mean 每一個 Gaussian 的 covariance	4-15
那麼於是呢我這些東西全部都可以調了全部都可以調了以後那當然那還有的東西是前面的嗯譬如說這個	4-15
 a  i  j 的話就用前面這個嘛 a  i  j 是不受影響這個 pi 就這兩個所以這個 pi 呢就用這個式子就可以求	4-15
 a  i  j 就用這個式子就可以求那只有這個 B K 呢是這個式子是對 discrete  model 這樣求	4-15
對 continuous 的要有 Gaussian 就是這樣子求	4-15
 OK 所以 continuous 的話就是這樣求	4-15
於是呢我就可以都可以求出來了但是真正求不是這樣求的是一個 iteration 的 procedure 怎麼講呢	4-15
嗯應該這樣說我們舉例來講假設你是辨識零到九的十個聲音那你有零的一個 model	4-15
一的一個 model 有二的一個 model 你有一個九的 model 假設這個是你的九的 model 你已經用了一堆人說的九的聲音所 train 好的一個九的 model 在這裡	4-15
現在有一個新的聲音進來你知道它是九我要把這個聲音再 train 進去	4-15
 OK 是這個 problem 是這樣子假設我已經有一個九的 problem 九的 model 已經在那裡了我用了一堆聲音 train 好一個九的 model 在這裡	4-15
現在一個新的聲音九進來的話那怎麼辦那我現在要把這個聲音 train 進去怎麼 train 呢那就用我們剛才講的這一招	4-15
那這一招是怎樣呢其實是一個蠻複雜的過程因為你要先求這一堆 alpha  beta  gamma  epsilon	4-15
所以呢你看譬說說這個 epsilon 裡面要有 alpha  beta 所以你 alpha  beta 都要求嘛	4-15
所以你要一面要求 forward 的 alpha 一面要求 backward 的 beta 通通都要求然後你要求 gamma 然後你要求 epsilon	4-15
當你這些東西都求出來之後等於是說你把那個 o 放到現在這個 model 裡面去了那個 model 是用別人 train 好的聲音	4-15
 train 好的一個 model 你現在把這個新的 o 放進去之後我得到所有的機率就是 alpha  beta  gamma  epsilon 等等都有了之後呢我現在用它來	4-15
估計重新估計這堆東西得到一個新的model	4-15
 於是我的這些個 a  i  j 啦或者是這裡面的 mean 啦 covariance 全部都換新的我得到一組新的 model	4-15
可是你不能相信這個是好的為什麼	4-15
因為這新的 model 怎麼來的我們剛才說我是把這個新的聲音放到舊的 model 裡面去然後呢得到那堆機率 alpha  beta  gamma 這些東西機率之後	4-15
我用那些東西來算出這些所有的新的值	4-15
嗯你很難說它會它會好為什麼因為原來這裡面的 model 是有我另外找了一把人	4-15
另外找了一群人的聲音 train 出來了現在就為了這個聲音你就把它全部都動掉了你動成這樣子了	4-15
一定對嗎這是有點懷疑因為你可以看到我這邊其實是在把這個聲音我把這個新的聲音去算它的 mean	4-15
對不對去算它的 covariance 是這個意思那我把原來這麼多人都動掉了只為了去配合這個人的聲音	4-15
所以呢不見得這個真的是好的我們不敢說它是好的但是它只是說呢我已經把這個聲音又放到這個 model 裡面去求機率	4-15
又然後全部重新調了就是了那麼因此呢它考慮了這個東西也考慮了這個那這個沒有理由它會比較好所以呢我要把這個拿來重新放回來	4-15
再做一次再拿回來再放一次這樣經過一個一堆 iteration 當它會收斂的時候就表示它比較好	4-15
 OK 就這個意思哦也就是說你	4-15
我新的聲音來的時候我用它來得到一個新的但是我沒有理由說這個一定會有多好因為它顯然很參考我現在這個新的聲音	4-15
而對於舊的聲音好像比較不重視了那麼所以不曉得會怎樣所以我就把這個呢再拿來再來一次再拿來再來一次那麼等於說是	4-15
這個讓這個新的聲音跟舊的 model 之間的所有的差異性儘量地慢慢在新的 model 裡面越來越模糊越來越模糊那最後收斂的時候這個應該就好了	4-15
那麼在我們剛才講這裡面整套是用 E M  theory 來證明的我們現在沒有講在 E M  theory 是可以證明底下這個 can  be  shown 這個是用 E M  theory 證明就是說你其實	4-15
每做一次每做一個 iteration 的話這個機率是會提高的	4-15
那其實我們講的就是這個我就希望在這個 model 裡面看到這個的聲音的機率要大我要這個東西儘可能最大	4-15
所以呢但是同樣的我也要看到原來的每個聲音都要機率都要大我不能為了光把這個聲音大光把這個聲音的機率變大別的都變小也不行	4-15
所以呢我要我要一面要有原來所有的這些統計特性在裡面我一面要把新的聲音的考慮進來	4-15
那我那在 E M  theory 可以證明當你這樣做的時候你每做一個 iteration 其實這個機率是會變大的	4-15
因此大到最後他會收斂收斂的那個時候呢就是我們要的結果那這就是我們一開始講說這一個 problem 三是沒有 close  form  solution 的	4-15
我只能夠借助這個 forward  backward  algorithm 那麼經過 iterative 的方式讓它趨進我要的值	4-15
 OK 那麼到這裡呢那我們也可以解釋這裡還有一些重要的問題那第一個問題就是說我需要一個 initial	4-15
開始	4-16
所以呢這個這個 problem 三的 forward  backward  algorithm	4-16
我們說它是一個微調的工作	4-16
我們還要一個 initialization 的過程	4-16
我還需要叫做 model  initialization	4-16
我要有一個好的開始的 model 我才可以算	4-16
就好像我剛才說 OK 　假設這是已經有一群人的聲音我已經做好了一個 model	4-16
我現在把這個聲音把它 train 進去	4-16
這樣講當然可以但是那個 model 怎麼來	4-16
你那個 model 總該有一個要有一個 model 才能夠做嘛對不對	4-16
所以一開始怎麼做這件事情	4-16
model  initialization 是一個很重要的問題	4-16
那麼嗯這個你可以想像這個工作呢是相當於我們講的粗調	4-16
而我們現在講的這個 iteration 的過程是等於是微調精調或者是微調	4-16
所以呢我們第三個 problem 講完我們只是講微調怎麼微怎麼調	4-16
但是 initialization 也就是粗調呢我們還沒有講	4-16
所以我們底下要講	4-16
怎麼做這個粗調怎麼做這個 initialization	4-16
OK 那麼我們這邊講的是微調就是已經有了一個 initial  model 之後	4-16
怎麼樣去把這個東西去調慢慢調得更細更好一點調得更精緻一點	4-16
但是我還有一個粗調的工作就是 initialize 還是要做的	4-16
好那有了這個那我們又講另外一個問題就是這是所有的 EM	4-16
用 EM 的基礎來做都有共同的問題就是說它嗯它可它可能 converge 到 local  optimum	4-16
也就是說 EM 說了半天它其實只是證明這件事情	4-16
你每一個 iteration 都會提高	4-16
那最後它一定會收斂到某一個可以可以這個最大的值是沒有錯	4-16
但是它也許如果是一個這樣的東西你在這邊調的話	4-16
你如果從這裡開始調你調調調調調就調到這裡就停在這裡	4-16
你還會不會走到這邊來呢不會走過來了所以你就會停在這裡對不對	4-16
那你如如果從從這裡開始調的話你也是就回到這來	4-16
你必須要從這裡開始你才會到這上面來	4-16
所以這個時候呢是這個嗯	4-16
也就是說這個嗯 results  depend  on heavily	4-16
depend  on initialization	4-16
depend  on 你從哪裡開始	4-16
因為它幫你往上走	4-16
但是你如果開始點不對的話就走到一個比較可能走到一個更小的地方去	4-16
那麼因此呢這個 initialization 非常重要	4-16
那麼同樣的一堆 data 讓兩個人去 train 它	4-16
那 depend  on 它怎麼做這個 initialization	4-16
最後 train 出來的 model 會不一樣	4-16
所以這是一個很難做的問題就是這樣子	4-16
因為你的 initialization 不一樣的話就會不一樣	4-16
那麼雖然同樣的 data 你會得到不同的結果	4-16
那麼事實上同樣那一堆 data 可以得到很多不同的 model 那會有不同的結果	4-16
那這就是你很容易 converge 到 local  optimum 的問題	4-16
所以呢我們 initialization 非常重要	4-16
好以上這段講的是 problem 三或者說是 forward  backward  algorithm	4-16
那它的目的是解這部分的微調的工作	4-16
那麼再下去呢我們要來講粗調	4-16
ㄦ粗粗調就是 initialization	4-16
這個應該是我的最後是不是這個的最後一章了	4-16
對這是最後一章了	4-16
OK 那麼要講到這個粗調 initialization 我們就要回到 power  point 的這個地方的	4-16
那一般的這個粗調的這個 model  initialization 都是使用 vector  quantization 的方法	4-16
那麼因此呢底下我們會先講一下 vector  quantization	4-16
這個東西是相當有用的在很多地方都很有用	4-16
因此呢你也許在別的課裡面有學過我們會講的快一點	4-16
不過基本上呢在我們這裡也一樣用它來做我們要做的這個嗯 initialization 的工作	4-16
OK 我們先在這裡休息十分	4-16
 OK 我們來講下一段	4-16
我們把 HMM 的那些複雜數學暫時丟掉了	4-16
我們先講到這裡為止	4-16
我們換一些東西不然一直搞那個是很頭大	4-16
我們底下講的是嗯其它的東西了	4-16
那第一個我們先說的是這個	4-16
這個我們要用 v q 來做這個粗調	4-16
我們簡單講一下 v q	4-16
那麼 vector  quantization 是在很多地方都很有用	4-16
那麼你可能別的地方都學過	4-16
不過如果你學過的話我們就算是一個複習就是了	4-16
那麼 v q 的用處在很多地方	4-16
一個常用的是說做為 data  compression	4-16
那麼當然還有另外的用途是 clustering	4-16
那這個底下我們就會解釋	4-16
那麼我們先從 data  compression 的觀點來解釋的話呢	4-16
那 vector  quantization 原始的來源是來自	4-16
這個數位通訊裡面的那個pulse  code  modulation 所謂的 pcm	4-16
那麼它的觀念就是所謂的 scalar  quantization	4-16
是從它衍生出來	4-16
那麼這個的觀念講起來很簡單就是說	4-16
假設我有一個 signal 我有這堆 sample	4-16
我怎麼把它送到遠方去呢	4-16
那一個可能的辦法是我把它整個的 range 切成若干格	4-16
然後每一格代表用一個 bit  pattern 來代表	4-16
舉例來講我這邊切成八格	4-16
如果切成八格的話總共只有三個 bit 就夠了	4-16
也就是這邊只有零零零零零一零一零零一一等等	4-16
於是我的第一個 sample 如果掉到這一格我就是一一零	4-16
表示說它是掉到這格	4-16
第二個呢還是這一格還是一一零	4-16
第三個呢還是這一格還是一一零	4-16
第四個呢還是這一格就一一零	4-16
第五個呢變成一零一	4-16
第六個呢一零零等等	4-16
那我就用這個 bit  pattern	4-16
來描述說我的這些點的位置在哪裡	4-16
那當然如果這樣做的話我這些點的位置其實只說明了它在哪一格裡	4-16
至於在那一格裡的位置它已經丟掉了	4-16
我們不管它在哪一格裡的什麼位置	4-16
只管它在哪一格就好了	4-16
當我把這一串零跟一送到遠方去的時候	4-16
接收端呢它其實無法判斷	4-16
它只知道它在哪一格不知道它那格在哪裡	4-16
所以呢它就會怎麼辦譬如說它就會把這些一一零呢	4-16
它就是一律以一一零這格裡面的中央那一點來代表	4-16
於是呢它就會看成這幾點都是一樣的	4-16
都是中央那一點	4-16
然後一零一呢也一律用中央那一點	4-16
一零零也一律用中央那一點	4-16
因此呢到你遠端它會把這個連起來	4-16
就會變成一個這樣子的曲線	4-16
跟原來會有點不一樣	4-16
那這樣子的過程呢我們稱為 quantization	4-16
那麼在中文當時通常翻做量化	4-16
那麼其實它就是把這個你該有的 range	4-16
切成若干段	4-16
每一段用一個值來代表它	4-16
然後這個段數總共是二的 r 次方	4-16
於是我就只要多少個 bit 就可以了	4-16
那這件事情就是我們這邊講的 quantization	4-16
那麼後來因為要把它變成 n  dimension 變成 vector	4-16
所以這個就叫做 scalar  quantization	4-16
一開始的時候這個就叫做 quantization	4-16
後來因為變成 n  dimension 以後 vector  quantization 這個就叫做 scalar	4-16
那它的意思就是把一個 single  real  number 用一個 r  bit  pattern 來代表	4-16
就像這本來是一個 ream  number	4-16
一個 real  number 但是我變成一個 r  bit  pattern 來代表它	4-16
那我做的方法就像這邊講的一樣我這個 range 就是正 a 到負 a	4-16
就是我這邊的這個正 a 到這個負 a	4-16
那我就把這個 range 呢分成大 L 個段	4-16
每一段叫做 j  k	4-16
所以呢我這邊所以我這個橫這個畫的橫軸就是我這邊的縱軸	4-16
所以這裡面的某一段	4-16
我就叫做 j  k	4-16
就是第 k 段叫做 j  k	4-16
那那裡面有一個代表值就是中間那個代表值	4-16
叫做小 v  k	4-16
那就是我這邊所畫的	4-16
所以呢我總共有多少個呢有 L 個	4-16
L 是我總共的總數	4-16
因此呢我就會有 L 個 j  k  k 等於一到 L	4-16
它們的聯集就是整個的 s	4-16
所以 s 就是整段的	4-16
整段的 real  number 叫做 s	4-16
也就是說這整段就是我的 s	4-16
它就是所有的這些 j  k 的聯集	4-16
那另外呢這些代表值 v  one  v  two 到 v  l	4-16
就這些中間代表值呢的集合呢叫做大 V	4-16
如果是這樣的話呢我的 quantization 不過就是一個從	4-16
大 s 到大 v 的 mapping  relation	4-16
這個 mapping 條件是什麼其實很簡單	4-16
就看你的 sample 掉在哪一個裡面嘛	4-16
譬如說這個 sample 是掉在這一格裡面	4-16
我就把它把那個值 map 到它的代表值	4-16
對不對就是我們這邊的意思	4-16
看它掉在哪一格我就用它的代表值來代表它	4-16
所以我最後就變成這種的就變成黃色的這個	4-16
也就是我把它的在那一格裡面的精確的位置都丟掉了	4-16
只留下它的代表值的值	4-16
那麼如果是這樣子的話呢這個	4-16
那我這個就等於是一個 mapping  relation 嘛喔	4-16
完全看每一個 sample 的值掉在哪一塊裡面	4-16
那麼我就用那一塊的代表值來代表它	4-16
那麼這個時候呢這個 r 的代表值呢我全部可以存在遠端都可以存好	4-16
所以我每一個每一每一個代表值我就只要用 r 個 bit  pattern	4-16
就可以代表了	4-16
所以我在傳送的時候我只要送這些少數個 bit 這個 bit 數目很少	4-16
就可以代表這些東西了	4-16
但是當然我也同時丟掉了重要的 information	4-16
就是這裡面的每一個點到底在這裡面的什麼位置是丟掉了	4-16
我只知道它是那個代表值的位置而已	4-16
那這個過程就是所謂的 quantization	4-16
那如果是這樣想的話呢	4-16
那麼嗯這裡面很重要的一件事這到底要怎麼做	4-16
我們這邊的說我把它等分成八塊	4-16
那當然沒有理由要等分	4-16
我可以做不等分的	4-16
那麼你最容易想的情形就是	4-16
把它分成譬如說中間比較小外面比較大	4-16
我這個隨便畫畫	4-16
中間比較小外面比較大這裡面你可以想像有幾個原因	4-16
第一個原因是說它也許有一個 distribution 是這樣子的	4-16
這我們剛才講的這個它有一個 distribution	4-16
這它的 probability  density  function	4-16
如果是這樣子的話	4-16
我就有理由	4-16
中間比較小外面比較大	4-16
為什麼因為這邊機率那麼大嘛	4-16
經常出現我就做得比較精細一點	4-16
因為我的我每次做的時候我就會把我的真正的值在那個裡面的精確的位置丟掉了	4-16
所以呢我如果這個值越細的話呢那我丟掉的東西越少嘛	4-16
越能夠精密	4-16
當變成這麼大的時候當然就搞不準一定會有很大的誤差嘛對不對	4-16
所以呢你這個越大的話我的 error 越大	4-16
越小的話我的 error 越小嘛	4-16
那麼中間這個機率那麼大我儘量用細一點	4-16
它這個這個 error 比較小	4-16
那外面的話反正難得發生所以呢給它粗一點大一點就算了	4-16
阿這個是一個很容易想的想法就變成這樣	4-16
那這個就是我們講的這個 probability  distribution  of  X  M	4-16
你可以根據這個東西來 design 怎麼做這件事	4-16
那同樣 error  sensitivity 是說呢	4-16
有的時候其實這個 error 大小是跟我們的感覺的敏感度有關	4-16
舉例來講呢你如果是在這些地方的話呢可能我的 signal 本來就很大	4-16
signal 本來那麼大所以中間如果多一點 error 可能影響不大	4-16
可是呢你如果是在這邊的話呢	4-16
它可能是很小的 signal	4-16
這個時候你的一點點的 error 可能都很影響都很大	4-16
所以呢如果這樣來看的話我就會希望說我的 error 呢	4-16
在中間會很小在外面可以比較大	4-16
所以我是會讓外面這格比較大裡面這格比較小等等	4-16
那這就是所謂的 error  sensitivity	4-16
因此呢我們至少可以考慮我們至少 at  least 考慮這兩個因素	4-16
就是 x 本身會有 distribution	4-16
然後包括它的 error 我們可以容許的程度我們敏感的程度來 design 這個東西	4-16
那這個東西是什麼東西就是你如何來分	4-16
我不一定要是等分我可以分得大大小小	4-16
同樣我那個代表值 j  k 這個 v  k 的位置	4-16
也不是一定要在正中央我可以放在邊邊上	4-16
我可以偏離中央也沒關係	4-16
那麼 depends  on 怎麼樣比較好	4-16
那這些都是可以考慮的	4-16
那也就是說你每一個 J L 這個區這個劃分怎麼劃分	4-16
還有裡面的代表值 v  k 倒底怎麼取	4-16
這些東西的加起來就是我們講的一個 quantization 的 characteristics	4-16
也就是一個 code  book	4-16
那我們要想辦法做這件事情	4-16
那這個是所謂的 scalar  quantization	4-16
有了這個之後我們現在可以把它 extend 到我們先說 two  dimension	4-17
如果 two  dimension 的這個變成 vector  quantization 意思是什麼呢	4-17
我們舉例來講就是我把相鄰兩個當成一個 two  d 的 vector	4-17
譬如說這一個跟這一個	4-17
這兩個合成一個變成一個 two  d 的 vector	4-17
這兩個變成一個 two  d 的 vector	4-17
這兩個變成一個 two  d 的 vector 你可以這樣子來看	4-17
那如果是這樣子來的看的話呢我就變成一個 two  d 的 vector 是相鄰兩個 sample	4-17
那麼於是他們的 range 就這樣不是一個 dimension 了是兩個 dimension	4-17
於是呢就會變成	4-17
譬如說這樣變成這一塊	4-17
那譬如說這個軸是 x 的 n	4-17
那這個軸是 x 的 n 加一	4-17
那它變成一個 two  d 的兩兩 two  d 的一個 range	4-17
我的 s 變成這一塊它是在它都是在正負 a 之間	4-17
所以這個 x  n 呢也是在正負 a 之間從正 a 到負 a	4-17
x  n 加一呢也是從正 a 到負 a 之間	4-17
在這裡面了	4-17
那這個時候我仍然可以做相同的事情	4-17
就是把這個 range 我也一樣的分成 l 塊	4-17
不過現在每一塊是 two  d 的 region	4-17
我還是寫成這個 j 　	4-17
因此呢我舉例來講呢你可能可以說呢	4-17
 ok 這裡有一塊	4-17
這裡有一塊	4-17
那麼這塊呢叫做 j  k	4-17
這中間你有一點呢叫做 v  k	4-17
還是一樣	4-17
那我這樣這塊 region 我總共可以分成譬如說 l 個	4-17
 l 個 two  d 的小塊	4-17
那我讓這些 l 個 two  d 小塊的聯集	4-17
仍然是整個的這個 s 這個就整個的 s 整個的 range	4-17
然後每一個 j  k 呢裡面有個代表值是小 v  k	4-17
那麼我的小 v  k 的集合呢叫做大 v	4-17
還是一樣	4-17
如果這樣的話我仍然是一個 two  d 是一個 mapping  relation	4-17
還是從這個 two  d 的 s 對應到這個 v 　	4-17
跟剛才是完全相同的	4-17
那麼這個 mapping 的方式也是一樣	4-17
如果你的 two  d 的那個 vector 掉在哪一塊我就對應到哪一個值	4-17
對不對	4-17
譬如說呢我的 x  one 是這個值	4-17
x  two 是這個值	4-17
於是掉到這一塊	4-17
在這一塊我就用這個代表了對不對	4-17
那這樣呢我這個如果我總共有我總共有 l 塊的話	4-17
是二的 r 次方	4-17
我仍然只要用 r 個 bit  pattern 就可以傳送了	4-17
就可以代表了	4-17
那這個是 v q 如果拿它來看成是這個傳送 data 的問題的話	4-17
我們可以這樣子看這個問題	4-17
那這個時候你可能會想到第一個問題就是說為什麼要這樣子做	4-17
這不是多此一舉嗎	4-17
因為我其實當你看成這樣的話	4-17
我馬上想到就是說那我其實就是把 x  one 的這個軸也切成幾塊	4-17
 x  two 這個軸也切成幾塊	4-17
不就一樣嗎	4-17
於是我就得到這樣子一堆正方形的或者長方形的	4-17
它們不一定要一樣長	4-17
就變成這樣就是啦這不就是那個嗎	4-17
沒有錯如果你這樣做的話	4-17
就跟剛才的 one  d 的這個是完全一樣的	4-17
對不對如果那樣做的話就跟這個是完全一樣	4-17
這個就沒有意義就這樣就好了	4-17
那麼會要這樣的原因就是	4-17
你可以想像這個並不是一個最好的辦法	4-17
因為它等於把你自己限成一堆框框	4-17
這堆框框把你完全限制成這樣子之後你這樣做的話就是原來那樣做法	4-17
但是其實我現在不一定要這樣子嘛我現在就可以變成別的樣子了	4-17
對不對我就變成別的樣子了	4-17
就不是那樣子那效果就會不同嘛	4-17
那麼為什麼會不同呢我們至少有這些個原因	4-17
一個最容易想像的原因就是它們這兩個之間是會有統計上的 correlation 的	4-17
所以它的 distribution 會不一樣	4-17
那麼最容易想的一個例子就是	4-17
如果這個 x  n 跟 x  n 加一是如剛才所說的這兩個相鄰的 sample 的話	4-17
如果是這兩個相鄰 sample 的話呢	4-17
in  most  case 它們值是比較接近的	4-17
你要說一個在這個上面一個在這個下面的機會是很少的	4-17
通常是都是蠻接近的	4-17
因此呢你就可以猜得到它的 distribution 是怎樣的	4-17
它大部分的 distribution 會集中在中間這一堆	4-17
那麼兩端是很少的	4-17
如果說它的 distribution 都在中間這一堆的話	4-17
那我其實我就可以把中間分得很細	4-17
就跟剛才的情形一樣	4-17
我中間可以分得很細	4-17
但這邊的情形是不太容易發生我就可以弄得很粗	4-17
這一大塊就好了這一大塊就好了因為它不太會發生	4-17
OK 那麼因此呢我就可以得到這個比較精緻的做法	4-17
那麼這樣子的話我就不再受限於這堆框框而	4-17
你可以想像這堆框框其實是多餘的	4-17
因為你搞到這邊來	4-17
這邊還是有一樣精細的框框跟這邊一樣	4-17
這個框跟這個框一樣是沒什麼道理因為它們本來就不一樣嘛	4-17
那麼你在這點的話是說一個這麼正一個這麼負其實不太可能嘛	4-17
哦等等所以你這邊就可以很鬆等等	4-17
那如果是這樣做的話呢	4-17
我們舉個例子來講	4-17
如果這邊是這個是這個	4-17
 r 是八個 bit  per  sample 的話	4-17
如果這裡的一個值是八個 bit  per  sample 所以 l 是兩百五十六格	4-17
我這邊分成兩百五十六格那麼每一個 sample 我用八個 bit 去描述它	4-17
我可以得到一個精細的代表	4-17
那麼當我變成兩個的時候	4-17
我如果仍然用這個框框來做的話呢	4-17
我就是要變成這個兩百五十六的平方 l 對不對	4-17
於是呢我其實還是就是十六個 bits  per  sample	4-17
這樣就跟剛才完全一樣	4-17
我這個這個軸也分成兩百五十六格八個 bit	4-17
這一軸也分成兩百五十六格八個 bit	4-17
這樣的話呢我兩個 bit 要十六個嗯十六個 bits  per 兩個 sample	4-17
對不對每兩個 sample 呢	4-17
是要十六個 bits	4-17
所以結果還是八個 bits  per  sample 是一樣的	4-17
但是你如果這樣做的話呢	4-17
我可以把這中間分得很細外面分得很粗	4-17
搞不好這樣我就只要譬如說兩千零四十八個 region 就夠了	4-17
我這邊不見得要分兩百五十六平方這麼多	4-17
我搞不好只要這樣就夠了如果這樣就夠的話呢	4-17
這個就是什麼這就是二的十一次方	4-17
於是呢我就只要十一個 bits  per 每兩個 sample	4-17
於是我一個 sample 呢只要五點五個 bit	4-17
比剛才的八個 bit 就省了很多等等	4-17
那你可以從這個觀點來想的話這個它就有它的意義	4-17
那這就是我們這邊講的這個	4-17
你為什麼要這樣子做	4-17
這樣做為什麼要這個把這些框框丟掉	4-17
而我重新去於這些奇奇怪怪的 region	4-17
那是有原因的	4-17
那原因就是我要做這類的事情	4-17
那我的考慮包括呢我的這個	4-17
這些東西可能是有這個這個統計上的 correlation 在	4-17
就像我剛剛講的這就是他的統統計上的 correlation 在	4-17
然後呢當然我可以有更 flexible 的 choice  of 每一塊可以更 flexible	4-17
那然後呢我的這個	4-17
還有一種可能就是 error  sensitivity 可能是 depends  on 它們 jointly	4-17
換句話說	4-17
如果我的一個 sample 非常準	4-17
一邊非常準的話	4-17
那另外一個搞不好我可以允許比較大的 error 可能沒什麼關係	4-17
譬如說你可以想像	4-17
如果其中一個已經很準了另外一個可能可以差很多都不影響等等	4-17
那這些的話就造成我可以做的空間	4-17
那麼因此呢這樣的話我就可以得到一個比較好的	4-17
那比較大的問題還是這倒底要怎麼做這件事	4-17
那這個如果做的好的話呢這每一個就是	4-17
每一個就是 j  k	4-17
那這裡面的每一個代表值就是 v  k	4-17
那麼這些 j  k 跟 v  k 的組合	4-17
就是所謂的我的 quantization 的 characteristics	4-17
或者說就是所謂的 code  book	4-17
就是嗯碼書啊它們有時候翻做碼書啊	4-17
就是 code  book	4-17
那就是那在哪裡歸在哪裡這樣的意思	4-17
那麼到這裡為止我們大概可以想像做 vector  quantization	4-17
這個嗯為什麼是一個 data  compression 的方法	4-17
原因我們剛才在最前面的這一頁說	4-17
它是一個 efficient  approach  for  data  compression	4-17
我可以把一組 real  number 變成一個 final  number  of  bits	4-17
所以我的 data 大為 compress	4-17
而且我 data 數目可以減少對不對	4-17
我現在這個兩個 sample 我只要十一個 bits	4-17
不像剛才要十六個 bits 譬如說	4-17
那這就是我的 data  compression 的功能	4-17
那當然這個觀念是可以衍生下去	4-17
那麼我們就可以得到更複雜的	4-17
舉例來講這個是 two  dimension 的	4-17
那當然你也可以 tree  dimension	4-17
然後可以變成 n  dimension	4-17
那這個情形都完全一樣這個 formulation 跟剛才都完全相同	4-17
那麼當你有這麼多 dimension 當然我們就沒有辦法畫了	4-17
我們也許最多只能畫一個 three  dimension 的	4-17
如果是 three  dimension 的話就變成一個像這樣	4-17
那它這塊就在這個裡面	4-17
然後那你中間還是一樣把它切成一塊一塊等等	4-17
three  d 的話大概就是這樣子啦	4-17
那你也是一樣你每一塊 three  d 的 region	4-17
我叫做 j  k	4-17
它有一個代表值叫做 v  k	4-17
這是一樣的	4-18
所以呢以此類推就可以變成	4-18
 n  dimension	4-18
所以我這個會有 n 個	4-18
然後呢我這 n 個構成一個 vector	4-18
那每一個值都有一個上下限正負 a 之間	4-18
然後呢我把它切成很多小塊	4-18
就是每一小塊叫做 j  k	4-18
每一小塊有一個代表值叫做 v  k	4-18
然後我仍然是一個 mapping  relation	4-18
depends  on 你那 n 個值的 vector	4-18
掉在哪一塊裡面對不對	4-18
你那 n 個值的 vector 掉在哪一塊裡面我就用那一塊的代表值來代表	4-18
那到時候呢我就變成剩下 l 個代表值	4-18
所以我只要 r 個 bit 就可以代表它等等等等	4-18
那這個都一樣	4-18
那到這裡的時候你就可以發現其實	4-18
我可以變成 n 之後呢	4-18
那麼並不見得這個一定是剛才那譬如說相鄰的 n 個 sample	4-18
可以是任何的 n 的不同的參數都沒有關係	4-18
當你是 n 的不同參數都沒有關係的時候	4-18
那其實我們在做的事情是等於是在這個看這些 data	4-18
這些這些個 data 它們之間的關係了	4-18
那麼這個時候我們再回過頭來講	4-18
現在最大的難題就是	4-18
你倒底怎麼做這堆 code  book	4-18
倒底它應該怎麼求	4-18
那你可以想像它顯然有道理	4-18
但是它要怎麼做	4-18
你憑什麼到這個這個邊界怎麼畫定	4-18
然後這個代表怎麼設定	4-18
那這個其實是經過很多年	4-18
很多人一直想不出來	4-18
因為這個 v q 的觀念其實很早就有了	4-18
但是人家一直想不出怎麼做這件事	4-18
那麼真正做出來的時候是在七零年代的末期	4-18
那做出來的人是因為	4-18
到那個時候 computer 進步到一個階段	4-18
可以處理大量 data 了	4-18
於是他就想到說其實這個 code  book	4-18
這個code book是可以用大量的 training  data 來 train 的	4-18
那麼因此他就想出這個用大量 training  data 用 computer 來 train	4-18
讓它自己收斂 train 出一個 model 來的方法	4-18
那這是七零年代的末期所出來的	4-18
那麼要講這件事情呢	4-18
我們要先 define 任意兩個 vector之間的一個 distance	4-18
因為它是這個演算法完全是用 distance 來算的	4-18
那麼這個 distance 就是在假設這個是 two  dimension 的話	4-18
這上面的任意兩點我怎麼 define 它的 distance	4-18
如果這是 n  dimension 的話	4-18
這上面任意兩點我怎麼 define 它的 distance	4-18
我們先要 define 好一個 distance 之後	4-18
那它就可以用那個 distance 去去算這件事情	4-18
去做出這個去做出這個切割的動作	4-18
那這個就是我們這邊講的	4-18
所以它要先 define  distance 的原因	4-18
那這個 distance 怎麼 define 呢	4-18
它說其實你自己可以 define 照你的要求	4-18
那麼你只要符合這些條件	4-18
什麼是 distance	4-18
就是 s 乘以 s 對應到正的 real  number	4-18
也就是你你這裡面任意取兩個 vector 出來	4-18
任意取兩個 vector 出來都對應到一個正的 real  number	4-18
然後呢它有這些條件	4-18
就是這就是一般的 distance 所需要的條件	4-18
譬如說任意兩個 vector  distance 呢應該是正的 real  number	4-18
如果一個 vector 跟它自己的 distance 呢應該是零	4-18
然後呢它們是可交換的	4-18
你 x 跟 y 的 distance 跟 y 跟 x 的 distance 是沒有區別的應該是一樣的	4-18
然後這個呢是三角不等式	4-18
三角形的兩邊合大於第三邊對不對	4-18
就這個是 x 這個是 y 這是 y 這是 z 那這是 x 跟 z	4-18
那兩邊合呢要大於第三邊	4-18
這都是 distance 的意義	4-18
當你符合這些條件之後你自己可以隨便怎麼定義	4-18
那這邊舉幾個例子是一般常用的 distance	4-18
那這些 distance 都符合這些條件	4-18
那顯然你還可以設很多其實的 distance	4-18
那舉例來講這個就是我們所熟知的歐幾里得距離嘛	4-18
對不對每一個 dimension 都平方的 distance 都平方就是了	4-18
這是歐幾里得距離	4-18
那這個呢我不平方我做絕對值可不可以也可以	4-18
這是所謂的 city  block  distance	4-18
city  block  distance 什麼叫 city  block 呢	4-18
你可以想像就是在假設在街道城市的街道裡面它都是這樣子的	4-18
所以你如果要從這一點走到那點去怎麼走	4-18
你只能夠走這樣子過來	4-18
那這個距離就是你的 x 跟 y 分別都求 difference 取絕對值	4-18
那這就是所謂的 city  block  distance	4-18
那麼你沒有辦法做直線距離	4-18
這個就是 city  block  distance	4-18
那底下這個距離呢	4-18
嗯這個名字比較難念	4-18
不過它的你看它的長相就知道了	4-18
這個就是 covariance  matrix	4-18
所以這個長相其實就是 Gaussian  distribution 後面那個東西	4-18
你現在應該很熟悉那個 Gaussian  distribution	4-18
就是後面有 e 的 minus	4-18
前面有一個 vector 然後乘上一個 inverse  matrix	4-18
然後再乘上一個這個對不對	4-18
那後面的這堆東西就是這個 distance	4-18
就是這個 distance	4-18
那這個東西就是 inverse 的 covariance  matrix	4-18
那這是什麼意思呢	4-18
我們其實很簡單的解釋就是	4-18
你最容易想像就是假設是對角線的	4-18
假設是只有對角線其它都是零的話	4-18
在這個狀況的意思是說	4-18
我這邊的 x  one  x  two	4-18
都要分別 normalize  by 它的 variance 除以它的 inverse 嘛	4-18
就是它們這個 distance 對到 normalize  by 它們的 variance	4-18
那為什麼呢因為我不同的值它可能的 range 有大有小	4-18
譬如說假設這是某一個東西它的 range 都是正一到負一的	4-18
可是這個呢是從正一萬到負一萬的	4-18
如果是這樣的話這是正一到負一這是正一萬到負一萬的話	4-18
你算的 distance 馬上它 dominate 它	4-18
它的差距它的差異就沒有了	4-18
那你怎麼辦	4-18
它們都先 normalize  to 它們的 variance 之後	4-18
它們的差異就一樣了對不對	4-18
像這類的功能就是顯現在這裡了	4-18
那我現在再進一步我可以把這邊變成不是零的	4-18
那有類似的情形	4-18
那這就是	4-18
其實就是我們一般做 Gaussian 的時候後面這個東西本來就是這個意思	4-18
那這麼一來的話呢那這個就是所謂的這個 distance	4-18
那你也可以想很多別的 distance	4-18
譬如說	4-18
我們剛才講的歐幾里德 distance 是 xi 減掉 yi 的平方	4-18
你也可以為每一個 dimension 做一個不同的 weight	4-18
這樣也可以啊	4-18
如果你覺得哪個 dimension 比較重要你該 weight 比較多這也是一種啊	4-18
那同樣呢我們剛才講	4-18
我們剛才講你 design 這種東西的時候	4-18
要考慮的一個因素是 error  sensitivity	4-18
那麼假設你要做的東西是	4-18
耳朵要聽的聲音或者眼睛要看的畫面的話	4-18
看你聽覺視覺會感覺什麼	4-18
會怎麼樣都可以把那個感覺上差異的要求通通都放到這個 distance 裡面去	4-18
那我的目的就是希望	4-18
我看起來覺得跟原來的一樣好看	4-18
聽起來就跟原來一樣好聽都可以	4-18
如果這樣的話我就想辦法把	4-18
怎麼樣子讓我聽起來的差距顯現在這裡面我就把它放進去等等	4-18
所以呢這些不同的因素我都可以放到那個 distance 裡面去考慮	4-18
那有了這些 distance 之後呢	4-18
那我就可以做底下的這個演算法	4-19
那這個呢就是所謂的	4-19
嗯它也有一個名字是 k  means  algorithm	4-19
或者是 low  and  max  algorithm	4-19
那你可以看得出來這是兩個人的名字	4-19
而這是它的原來的意思	4-19
那這個演算法意思很簡單	4-19
雖然不怎麼好用但是呢是容易了解	4-19
所以一般我們都會先講這個	4-19
那這個的意思是	4-19
像這樣我們舉例來講以 two  d 為例	4-19
假設我有一堆 data 在這裡	4-19
假設說我們要做的 data 是聲音	4-19
然後這是 two  d 就是相鄰兩個 sample	4-19
這是一個 vector 的話	4-19
我就把一大堆聲音的相鄰兩個兩個 sample 的 vector 通通拿進來	4-19
就變成一堆 data 在這裡	4-19
然後呢我做哪兩個 step呢	4-19
第一步是先 fix 我的代表值	4-19
然後去找它們的區域	4-19
譬如說我先假設假設有一個代表值在這裡	4-19
一個代表值在這裡	4-19
一個在這裡	4-19
一個在這裡	4-19
那麼我先設這四個值之後	4-19
我想辦法去找這個	4-19
那怎麼找呢那它就是根據每一點去看	4-19
每一點去找	4-19
它跟這四個代表值的 distance	4-19
剛才 define 的 distance 就用在這裡啦	4-19
我每一點都去看它跟這四個的 distance 跟誰最近	4-19
跟誰最近呢它就歸給誰	4-19
於是呢像這個跟它近歸給它	4-19
這個跟它近歸給它	4-19
這個跟它近歸給它這個跟它近歸給它等等	4-19
這個歸給它這個歸給它	4-19
你每一個呢都根據 distance 去算它跟誰近就歸給誰	4-19
當你把這個歸好之後	4-19
其實這個 boundary 就出來了	4-19
譬如說這些東西都歸它於是呢我的 boundary 就是在這裡	4-19
這些東西歸它那 boundary 在這裡	4-19
這些東西歸它 boundary 在這裡	4-19
那麼呢 boundary 在這裡	4-19
那這就是我的第一個 step	4-19
第二個 step 呢反過來	4-19
我假設我的 boundary 確定了	4-19
就我的每一個 j  k 確定了	4-19
我要重新找它的代表值	4-19
於是呢如果這一些已經知道這是一個 boundary 的話	4-19
這是一塊的話呢	4-19
這裡面到底哪一點才是最最能夠代表這些的呢	4-19
我要把這裡面的點重新去看一次	4-19
發現其實我如果把這一點	4-19
搬到這邊來的話	4-19
那麼每一個跟它的距離加起來才是最小的	4-19
它的 distance 才最小所以應該搬到這邊來	4-19
同理呢這一點也不是最理想的我應該把它搬到這邊來	4-19
這一點也不是最理想的我應該把它搬到這邊來	4-19
這一點也不是最理想的我把它搬到這邊來	4-19
那這樣子我就完成我的第二步	4-19
之後我就再繼續 iterate	4-19
我現在這個用新的這四個點做為代表之後重新把這個擦掉	4-19
boundary 擦掉重新再來	4-19
那麼這時候我得到新的 boundary 可能會變成在這裡然後在這裡在這裡在這裡等等	4-19
那樣子經過若干個 iteration 之後它會收斂	4-19
收斂的應該就是比較好的	4-19
那這就是所謂的 k  means	4-19
那你看名字就知道求 k 個 mean 嘛	4-19
那每一個就在求它的 mean 嘛這樣的意思	4-19
那如果說得詳細一點的話呢就是這邊寫的式子	4-19
所以第一步是怎麼做呢	4-19
就是我們剛才講就是把每一點都去求它的 distance看它跟誰最像	4-19
距離最近就歸給它	4-19
所以呢所謂的 j  k 是什麼	4-19
就是所有的那些個 x 它跟 k 第 k 個代表值的距離才是最近的	4-19
跟別的 distance 都比較遠的	4-19
那那些個 x 的集合呢就是 j  k	4-19
那這個目的很顯然就是讓我的 total  distance 最小嘛	4-19
我在 minimize  total  distance 啊	4-19
那我如果每一點都歸給跟它最近的那個 distance 的話	4-19
那這樣子它都歸它它都歸它的話呢	4-19
那這樣的話我的 total  distance 一定最小	4-19
所以呢我其實就是在 minimize  total  distance	4-19
而這個 condition 其實就是所謂的 nearest  neighbor	4-19
那麼我每一點看這幾點誰是我的 nearest  neighbor 就歸給誰	4-19
那第二塊的意思呢是反過來	4-19
那其實就是給我這堆之後呢裡面看哪一點最能代表大家	4-19
那很直覺的就是求它的 mean	4-19
那求它的 mean 其實是針對剛才那個歐幾里得 distance 而言	4-19
我們剛才講你可以 define 各種 distance	4-19
你可以 define 各種 distance	4-19
那如果是歐幾里德 distance 的話	4-19
那麼這個 total  distance 最小的條件就是	4-19
total  distance 最小的條件就是你取它的 mean	4-19
於是呢你每一個我都求它的 mean 就是最好每一個求它的 mean	4-19
那這也就是 k  means 這個名字的來源	4-19
那這樣之後當然我就讓每一塊各自都有最小的 distance	4-19
那這個 mean 其實就是所謂的 centroid 就是你的質心嘛	4-19
那麼這個那這樣這兩招之後就是我這個 iteration 不斷進行	4-19
那我怎麼收斂	4-19
因為你每走你每走一個 iteration	4-19
我的 total  distance 一定在減少	4-19
因為每一次都在 minimize 這個 distance	4-19
那這邊是把每一塊的 distance都在 minimize	4-19
那把每一塊 distance 加起來就是我的 total  distance	4-19
所以我就看 total  distance 應該是一路在減少	4-19
然後什麼時候少到不會少了我就給它停止	4-19
那這個就可以收斂了	4-19
那麼它一定會收斂為什麼因為我的 distance 是正的	4-19
我的 distance 是一個正值	4-19
但是呢我每一個 iteration 之後呢我的 distance 一定會降低所以它最後一定會收斂	4-19
這是所謂的 k  means 的 algorithm	4-20
這個方法簡單而容易了解	4-20
它有一個很大的缺點就是不容易真的用	4-20
為什麼呢因為這個嗯我沒有寫在這裡	4-20
這邊只是說你可以你有一個夠多的 data 就可以跑了	4-20
那麼它一個最大的問題是在我們下一頁講的就是說呢	4-20
這跟我們剛才上面講的那個問題一樣	4-20
它 converge  to  local  optimum	4-20
而且它 depends  on  initial  condition	4-20
它的 solution 顯然不是 unique	4-20
也就是說你一開始的我們說一開始是你要設幾個代表值的點然後才開始跑嘛	4-20
看你那個設的好不好	4-20
你設的不一樣跑的結果就是不一樣你的那個 solution 不是 unique	4-20
solution 不是 unique 然後它會看你 initial  condition 設的好不好就不一樣	4-20
你很可能會收斂到 local  optimum 去	4-20
就跟我們剛剛說的情形是一樣的你現在是要往下走	4-20
但是你不見得走到這來	4-20
你如果從這開始的話你就停在這裡就出不去了	4-20
你如果從這開始你還是停在這裡	4-20
必須要從這開始才會到這來	4-20
那麼這個是剛才那個演算法的缺點	4-20
那麼到後來有人想到一個辦法	4-20
這是真正vector  quantization 後來變成有用是有了這個	4-20
那這也是它最早最成功的一個演算法所謂的 L B G  algorithm	4-20
那麼 L B G 是什麼東西呢是三個人的名字	4-20
actually 這是 L 跟 B 是兩個研究生 G 是他們的指導教授	4-20
那麼他們三個人他們三個人發明的	4-20
那麼這個 L B G 的意思是怎樣呢	4-20
我們簡單地解釋就是它變成 iteration	4-20
然後呢每一個 iteration 重新來一次	4-20
那麼我們可以簡單地來看的話就變成這樣子	4-20
它就是說我們仍然以這個為例	4-20
我第一次的時候呢我只要 l 等於一	4-20
第一次我只要 l 等於一	4-20
所以也就是我只要求一個 vector 的 v q 那就是求它的 mean	4-20
所以呢我就是找出一個這邊的 mean 來譬如說 mean 在這裡	4-20
這是它的 mean 這就是我的第一步我只要等於一就好了	4-20
然後第二步我把它 split 變成二	4-20
由 l  split 變成二 l	4-20
我怎麼 split 呢就是從這個這個當初找到那個核那個代表值開始把它拆成兩個	4-20
這最常用的拆法就是這個方法	4-20
就是一個向一個一加 epsilon 一個一減 epsilon 這個 epsilon 是一個比一小很多的值	4-20
讓它讓它呢在這個核心的附近拆開來變成兩個	4-20
譬如說呢我就把這個拆掉變成一個在這裡一個在這裡	4-20
不過這兩個仍然非常接近核心	4-20
我就從這兩個開始跑	4-20
那麼這兩個開始跑這時候跑什麼呢就是跑 k  means 跑剛才那個 k  means	4-20
那麼於是呢它就會開始把譬如說把它這個切開來	4-20
這樣子然後呢這一堆呢重新求它的 mean 就會跑到這邊來	4-20
這堆重新求它的 mean 就會跑到這邊來	4-20
OK 那這樣之後呢我再跑 L B G 再來一次	4-20
於是呢這個時候我的新的 boundary 到這來了	4-20
然後呢我可以再來一次	4-20
那麼發現呢是這個比較好這發現是這個比較好等等	4-20
當我 l 等於二完成之後就會得到一個比較好的	4-20
這個 point 在哪裡呢你可以看到	4-20
他剛才這個辦法是他從核從一個整個的中心那裡拆成兩個還在附近	4-20
所以基本上它還在中心的部分這兩個都在中心從中心部分開始往外走	4-20
那根據它的 data 的分部的情形走到外面來	4-20
這樣的話大概這個這個 initial  condition 比較好	4-20
那麼這樣子之後呢	4-20
我就可以如果我覺得不夠我就再來一次	4-20
所以當我這個跑完之後我可以再回到 step  two	4-20
我再把 l 分成二 l 　	4-20
譬如說我這個可以再拆成兩個	4-20
這個再拆成兩個這個也再拆成兩個	4-20
然後讓它們再去跑	4-20
那麼再去跑的結果呢它就會把這個拆開來	4-20
於是這個往這邊跑這個往這邊跑	4-20
這個拆開來這個往這邊跑這個往這邊跑	4-20
那這樣的話它慢慢慢慢跑出來我的就會接近比較好的	4-20
所以這個比較會 converge  to  better  code  book	4-20
那這個精神我想很容易想像就是它為什麼會這樣就是我原來的 k  means 裡面太 depends  on  local  optimum	4-20
那麼這個時候 depends  on 你的太 depends  on 你的 initial  condition 了	4-20
所以呢很會收斂到 local  optimum 去	4-20
那用這個方法的話我等於是我先從一個核心開始	4-20
從它向旁邊一點走的話呢我慢慢再往外散	4-20
這樣的話它比較不至於會搞不好然後它大概都會接近它的 optimum  solution	4-20
那我們可以舉一個例子來來解釋	4-20
假設有一堆 data 長得很奇怪如我們剛才所說它專門長成這樣	4-20
那如果專門長成這樣你如果一開始的的點放在這裡放在這裡什麼的它就不太容易收斂的很好這個就很難	4-20
但是呢我如果是用這個方法的話呢你想我會怎樣	4-20
第一次得到的一定是在這裡嘛在中間	4-20
那第二次就算是它拆的時候拆到這邊來了	4-20
拆到這邊來之後變成一個在這裡一個在這裡	4-20
那它很可能會畫的一條線是這樣子的那也沒有關係	4-20
這個時候各自去跑 l b g 各自去跑 k  means 之後那它會怎樣	4-20
那它顯然會往這邊搬它顯然會往這邊搬	4-20
OK然後它再拆開來就算它再拆錯也沒有關係	4-20
它再拆成一個這樣子一個這樣子的話呢它最後還是會它最後還是會它往這邊搬它往這邊搬	4-20
它最後一定會它往這邊搬它往這邊搬	4-20
所以最後呢它們會得到一個比較好的 code  book	4-20
那這個是這個把它由核心去拆成兩個的一個辦法	4-20
當然還有另外一個辦法就是一個用原來的一個用最遠的也可以	4-20
那如果一個用原來的一個用最遠的話呢這個意思是像這樣	4-20
我們也舉個例子譬如說我的 data 也是這樣子的	4-20
我得到一個在這裡	4-20
那我現在怎麼辦我的兩個呢一個就用這個另外一個用最遠的	4-20
這個時候跟它最遠是在這裡 OK 我就在這裡	4-20
如果就在這裡的話呢我一開始就會切在這裡這個歸它這個歸它	4-20
但是之後我再來繼續跑這個 k  means 的話呢它就會往這邊搬	4-20
於是呢這個就會往這邊搬它就會往這邊搬對不對	4-20
於是你就會看到這個會向這邊移動這個也會向這邊移動這個也會向這邊移動所以它慢慢就會過來	4-20
那這樣也是可以的所以這是另外一招	4-20
不過這精神都一樣就是用這個方法來得到的就是這個就是這個喔這是 l b g  algorithm	4-20
那這個呢到這裡我們 v q 講到這裡	4-20
那麼我們可以回過頭來看剛才第二句話	4-20
我們在這裡做 v q 不是為了剛才講的 data  compression 是為了拿來做 clustering  large  number  of  sample  vectors	4-20
當我有一大堆 vector 的時候倒底怎麼樣把它們分成最合理的一群一群	4-20
然後每一個群分別有最有代表性的 vector	4-20
那等於是這件事情那我們很多時候做 vector  quantization 是這個目的	4-20
那這樣的話就像我們這裡其實是這個目的然後可以做這樣的事	4-20
好有了這個之後我們現在底下就可以講我這個 v q 怎麼拿來做這個 HMM 的 initialization	4-20
那就是底下這一段 OK 我們休息十分鐘	4-20
 OK	4-21
我們接下來講這個做 initialization	4-21
那麼這是其實這 initialization 很多種方法那麼這邊講的是最簡單的一種也是最常用的一種	4-21
那這就是所謂的 segmental k  means  algorithm	4-21
那什麼叫 segmental k  means 就是這麼做	4-21
那基本上是怎麼樣就是你你總要有一個 initial  estimate	4-21
所有的 model 怎麼辦呢	4-21
最常用的辦法就是	4-21
分成這個 equal  lance	4-21
怎麼講呢我們舉例來講如果我要 train 一個	4-21
某一個音譬如說	4-21
六	4-21
假設我要 train 四個 state	4-21
假設我要 train 四個 state 的話	4-21
那怎麼辦呢我有	4-21
好幾個六這是一個六這是一個六這是一個六這是一個六這是一個六	4-21
有長有短	4-21
那麼於是我都一樣的都把它等分成為四段	4-21
這個也等分為四段	4-21
這個也等分為四段	4-21
因此呢我的最早的 initial 呢是	4-21
讓它們的	4-21
等分的東西去 train 一個 model	4-21
譬如說它所有的這前四分之一	4-21
所有的前四分之一	4-21
拿來做這個 model	4-21
所有的最後四分之一嗯做那個 state	4-21
所有的最後的四分之一	4-21
做這個 state 等等	4-21
這樣子哦那你	4-21
這樣子之後那你這邊就有一堆	4-21
對不對這裡面是一堆 vector	4-21
這裡面是一堆 vector 基本上它們大概都是屬於這個六的聲音的	4-21
前面的四分之一然後我就假設拿來 train 這個 model	4-21
 train 這個 state	4-21
那這些呢都是最後的拿來 train 這個 state	4-21
那這個時候你這個 state 是什麼呢	4-21
我需要一堆 Gaussian 嘛	4-21
我舉例來講我如果需要十六個 Gaussian	4-21
的話那怎麼辦	4-21
我就是把這裡的一堆 vector 一堆 vector 拿來做 vq	4-21
做十六個	4-21
做這個 l 等於十六的 vq 嘛	4-21
於是呢我就	4-21
那麼於是我就會得到	4-21
這一堆有一個這一堆有一個	4-21
這樣總共十六個	4-21
那我就當它們是 Gaussian	4-21
其實這每一個你當然沒有理有這是 Gaussian 了	4-21
它就是一堆 vq	4-21
就把它們做成一堆東西然後有一個代表值有一堆有一個代表值我就那個	4-21
把那堆東西的 mean 跟 variance	4-21
就當成是 Gaussian 的 mean 跟 variance	4-21
這個其實寫在底下這個	4-21
我們一直都這樣子做哦	4-21
包括後面的是這樣子做一開始也是這樣	4-21
所以呢你就是說呢你	4-21
那這樣你才可以得到得到第一堆這些這些 model 裡面的 perimeter	4-21
那麼因此呢你就可以說是這個	4-21
你總得要有一個比較好的 initial 嘛那怎麼來呢	4-21
你就是把你的聲音	4-21
先用等分為假設它們都 equal  lance	4-21
然後你的前的四分之一做第一個 state	4-21
最後四分之一做最後的 state 等等	4-21
那你這裡面一堆 vector 一堆 vector 嘛你就把它拿來做 vq	4-21
那麼這樣子的話呢你可以得到	4-21
得到這些東西的這個	4-21
然後這堆呢我就求它的 mean 跟 covariance 等等	4-21
那就是那其實這個步驟跟前面是一樣的就是把這個	4-21
所有的這些 observation  vector 呢	4-21
那變成一個	4-21
用 vq 的方法	4-21
變成 m 個 cluster	4-21
我這邊變成 m 其實就是剛才的 l 啦一樣的	4-21
你把它變成個	4-21
 l 個 cluster 不過現在因為我們前面講 Gaussian 的時候我們都說有幾個 Gaussian 有 m 個嘛	4-21
就是 m 個 Gaussian	4-21
 m 個 Gaussian  distribution	4-21
那麼你就把它變成這個 l 就等於 m 就是了嘛	4-21
然後呢於是呢你就可以把那一每一個 cluster	4-21
第 n 個 cluster 到 state  j 就得到它的 mean	4-21
然後呢每一個 cluster 裡面的	4-21
它的求 covariance 就是你要的 covariance	4-21
你就把這些 data 去算它的 mean 去算它的 covariance	4-21
就可以了	4-21
然後那你這個這個怎麼算這個是那個 weight	4-21
 Gaussian 的 weight 怎麼算就數有幾個 vector 嘛	4-21
就是 number  of  vector 去除以全部的 vector	4-21
那在這個 case 就是四分之一嘛	4-21
哦不是就是說你現在如果掉在這裡面了	4-21
這裡面我假設有一百五十六個 vector	4-21
總共有七百五十個 vector	4-21
那這個就是它的 weight	4-21
它就這樣算嘛就是算 vector 的數目除以全部 vector 的數目就是它的 weight	4-21
那其它的我這邊沒有詳細講那其它通常都用假設譬如說 A I J 怎麼辦	4-21
 A I J 我們通常都是假設假設一個值	4-21
那通常就是這個你只讓它這邊有一個這邊有一個	4-21
你別的都讓它是零最多再讓它多一個這個	4-21
其它都讓它是零然後你就設一個簡單的數字就讓它開始跑	4-21
所以這是一開始的時候我先假設一個 initial 的 estimate	4-21
 of 所有的 parameter 用這個方法	4-21
那這樣的話我就有一個一開始的 model	4-21
有了 model 之後呢	4-21
你 step  one 是幹什麼呢	4-21
跑 viterbi	4-21
重新切一次	4-21
雖然說這裡有四個 state	4-21
沒有理由每一個 state 各是四分之一	4-21
所以呢剛才只是因為你必須要有個開始	4-21
沒有開始沒辦法做所以	4-21
我開始先用四分之一做	4-21
四分之一做完我有了這個起始 model 以後	4-21
我就不能再去相信它是四分之一等分的於是怎麼辦	4-21
我現在就可以這些東西每一個都去跑	4-21
我們上週講的上次講那個 viterbi	4-21
 viterbi 會幫我把重新切一次對不對	4-21
所以 viterbi 的結果可能會說	4-21
這邊的第一個 state	4-21
 viterbi 是根據現在有的這些 state 裡面的這些參數	4-21
去切它會說這個可能這個是在這裡這個是在這裡這個是在這裡	4-21
那麼這個是在這裡這個是在這裡這個是在這裡	4-21
譬如說這樣子	4-21
它會重新切一次	4-21
重新切好之後你這邊就得到第二次的	4-21
這個來 train 它	4-21
同理呢這個也是得到一個第二次的	4-21
這個來 train 它	4-21
因此呢當你這樣 viterbi 切出來之後呢	4-21
我再重算一次	4-21
重新我現在用紅色這堆 vector	4-21
重新跑一次 vq	4-21
然後呢重新找它每一個	4-21
那每一個 vq 之後每一個就是一個 Gaussian	4-21
算它的 mean 跟 covariance	4-21
之後我又得到一組新的出來了	4-21
那我這個做完之後我就可以算一次這個機率因為我還是以這個機率為準	4-21
我就算這個機率看這個分數	4-21
看是不是提高了	4-21
有提高就回過頭來繼續做	4-21
有提高就回過頭來繼續做	4-21
我又回去呢	4-21
再切一次	4-21
用再跑一次 viterbi 再切一次	4-21
然後呢再重新做一次	4-21
這個 vq 然後再算一次 Gaussian 這樣子	4-21
那麼等到這個機率收斂為止	4-21
我的粗調就到此為止	4-21
沒有辦法再細調了	4-21
那這個時候我就把這個做出來的東西放到剛才的那個	4-21
 forward  backward 的 algorithm 的那個起始值去	4-21
開始用那個 forward  backward 來跑那個是跑得比較細	4-21
那個比較細之後可以得到一個微調然後可以得到比較好的結果	4-21
這個基本上就是	4-21
所謂的 segmental k  means 也是我們最常用的簡單的辦法就是這麼做的	4-21
那這個我後面有兩張圖是在講這件事	4-21
這個比較容易看就是我們講的這件事	4-21
就是說現在假設你已經有了第一個	4-21
一開始這個等分 train 出來已經有的話	4-21
你下一步怎麼辦就是跑 viterbi	4-21
你的這一堆這一堆就是 viterbi 這一樣橫軸就是時間軸就是一個一個的 vector	4-21
縱軸就是 state 跑 viterbi 就跑出一條	4-21
最可能的路徑	4-21
這條路徑也告訴我說這一堆 vector 到這邊為止	4-21
歸 state  one 這一堆呢歸 state  two 這一堆呢歸 state 三	4-21
於是我 state  one 的這個裡面的 Gaussian 怎麼 train 呢	4-21
就拿這些來 train 因此呢就是把這些東西拿來做	4-21
其實這個應該是做這個	4-21
嗯 LBG 啦 LBG 是一堆 k  means 所以寫 k  means 也沒有錯啦	4-21
就是做 LBG 啦所以呢你先做一個 global 的 mean 就是做 l 等於一的	4-21
 mean 在這裡然後把它拆成兩個小的就是我們剛才下課前講的把它拆成兩個小的	4-21
於是這兩個小的可以分別做成兩個	4-21
然後呢做好之後呢這個再拆成兩個小的這個拆成兩個小的就變成四個	4-21
如果你現在四個 Gaussian 夠了的話就你就拿這四個來做 Gaussian	4-21
於是呢它就有它的 mean	4-21
跟 covariance 跟它的 weight 都可以求出來	4-21
那這樣的話我就得到它第一個 state 的四個 Gaussian	4-21
等等那這樣這就是我們剛才講的這個情形	4-21
那這個是 continuous 的做法	4-22
 discrete 是一樣的大概不需要講	4-22
因為我們其實不用了不過你可以想像它一樣的事情只是說呢	4-22
我現在就變成如果在那裡面你是不是用一堆 Gaussian 而是用一堆點來規定的話規定好一堆點的話	4-22
那其實那堆點怎麼來的它們當初在	4-22
七零年代八零年代的時候它們其實	4-22
八零年代的時候它們其實沒有辦法做 Gaussian 都是做這些點	4-22
那這些點怎麼做這些點都是用 v q 做的其實就是用 v q 做出來	4-22
然後看看它們有夠多 data 之後用 v q 來算它們大概有哪些點最具代表性	4-22
就用這些點然後這個每一點到底上面給它多少機率	4-22
給它多少機率呢就也是一樣用數的嘛	4-22
就是有多少個 vector 在做那個 v q 的時候被歸到這個來	4-22
好那就是就是我的機率嘛	4-22
那用底下這張圖講的也就是這件事就是假設你 viterbi 跑成這樣子	4-22
但是呢那我我這些 vector 呢我現在先看它 v q 到哪幾個點去	4-22
然後規在哪裡的時候就看在那裡面	4-22
舉例來講這個 state 裡面的話呢你會發現它有四個裡面	4-22
有三個都歸這個一個歸這個所以就是四分之三跟四分之一等等等等	4-22
這是 discrete 不過當然我們現在沒用就是了	4-22
好那這樣我們四點零就講完了	4-22
那四點零我們是等於是把這些 h m m 的最基本的數學模型跟它們的 training 的方法	4-22
講完那麼我們五點零還是在講 h m m	4-22
我們要進入五點零只不過五點零講的是另外一回事	5-1
就是我們把它當成是用來處理聲音	5-1
我們剛才四點零為止沒有當它是聲音	5-1
我們只當它是數學模型然後就是一堆 arbitrary 的 number 在那裡跑	5-1
沒有當它是聲音	5-1
但是其實它是聲音啊	5-1
所以呢我們必須要想一想在聲音的狀況之下它會怎樣	5-1
那就是五點零講的	5-1
那這種東西我們稱之為 acoustic  modeling	5-1
那這部分牽涉的內容稍微廣一點從基本的聲學語音學一直到後面怎麼樣做	5-1
 model 還包括什麼什麼很多東西都在裡面	5-1
所以 cover 東西比較多	5-1
那麼比較好的 reference 是課本的這些章節	5-1
那我們其實後面最重要要講的是	5-1
它的最後這個最重要的方法	5-1
就是 training  TRI  PHONE  model 的這件事情	5-1
那這個這個也就是今天最主流的方法	5-1
那這個的原始 paper 來自這一篇	5-1
那這一篇其實是我們台大的一位校友的博士論文	5-1
非常了不起的所以後來變成這個經典作品大家都在用所以我們也引這篇有興趣可以讀一讀	5-1
那麼我們前面先簡單講一點我們	5-1
我想我可以稍微再講幾分鐘	5-1
我們禮拜六禮拜六補課是蠻辛苦的所以我們也不要太辛苦	5-1
不過	5-1
我的飛機快要飛了所以我們要	5-1
 OK 那麼一開始的時候我們先第一個問題是說我們可以做 HMM 但是倒底要用什麼來做	5-1
 HMM 可以短可以長可以大可以小	5-1
那麼我們我們這個說過	5-1
 HMM 的好處就是它可以小的可以兜成大的	5-1
你如果有一個基本音的 model 是這樣子	5-1
這個基本音的 model 是這樣子	5-1
那麼它們串起來可以變成一個比較大的	5-1
譬如說詞或者什麼東西是可以這樣做的	5-1
所以呢從這個觀點來講好像是越小的 model 越好因為它可以串成比較大的東西	5-1
但是你也不能小到太小	5-1
你要能夠好做等等的原因	5-1
那麼基本上來講呢	5-1
嗯我們可以最小是所謂的 phoneme	5-1
 phoneme 就是最小的基本音這待會解釋	5-1
不然呢再來就是 syllable	5-1
 syllable 就是我們通常講的音節	5-1
譬如說這個 minimum	5-1
就是這是三個音節	5-1
那麼這個	5-1
 same 這是一個音節等等	5-1
那麼這個是 syllable 我們講是音節	5-1
然後 word 這就是一個 word	5-1
對不對或者 phrase 是比 word 還要大的幾個 word 構成一個 phrase 都可以做	5-1
都可以做這個 HMM 那倒底什麼比較合適	5-1
那我們先要了解一件事情我們這邊在講 phoneme 這個是一個語音學上一個專有名詞	5-1
這個東西是什麼我們來解釋一下	5-1
這個 phoneme 是一個語言裡面的最小的單位最小的聲音的單位	5-1
它可以幫助我們區別 one  word  from  the  other	5-1
就是說我們 word 跟 word 之間的區別是靠這些東西來區別的	5-1
那它的最小的單位叫做 phoneme	5-1
舉例來講像一個以英文而言這個是 bet 這是 pet	5-1
那差別就是這個 b 跟 p	5-1
所以呢這個 b 或者 p 呢的音呢就是一個 phoneme	5-1
同理呢這個是 bad 這個是 bed 那這兩個差別是什麼就是這個音	5-1
那也是一樣這個它就是 phoneme	5-1
從這裡你就可以了解其實我們講的 phoneme	5-1
這是一個語言學上非常講起來好像很好聽的話其實簡單的講就是每一個子音每一個母音都是一個 phoneme	5-1
那跟這個 phoneme 很像的叫做 phone	5-1
那在語音學上來講 phone 跟 phoneme 是有區別的	5-1
那麼這個 phoneme 是這個東西叫做 phoneme 是一個最小的單位	5-1
但是 phone 呢是 phoneme 的 acoustic  realization	5-1
這是什麼意思呢是說一個相同的 phoneme 可以有很多不同的 realization	5-1
舉例來講這個 set 跟這個 meter 這兩個 t 是同一個 phoneme 這個 phoneme 就是 t	5-1
是同一個 phoneme 如果講這個的話講這個的話是同一個	5-1
可是它們是兩個不同的 realization	5-1
你也可以想像這兩個聲音本來就不一樣 set 的這個 t 跟 meter 這個 t	5-1
是顯然不一樣的是兩個不同的 realization	5-1
這是語音學上它們的解釋這兩個是區別的	5-1
但是從九零年代以後因為一大堆 engineer 在做這些東西	5-1
那 engineer 始終讀不太清楚語音學然後他們最後就把這兩個混為一談	5-1
因此呢在後來今天在看的很多文獻跟書裡面	5-1
其實 phone 跟 phoneme 已經混為一談	5-1
那麼因此呢有的課本乾脆就說在我這本書裡面這個 phone 跟 phoneme 是一樣的隨便講哪個都可以	5-1
那這一點就是 engineer 跟語音學家始終不一致的地方	5-1
那語音學家始終認為你們不懂的人在胡搞一通	5-1
但是 engineer 覺得我們做出來可以 work 就好所以這是不一樣的地方	5-1
那在這個情形之下很重要的我們需要知道的就是 context	5-1
什麼是 context 呢就是相鄰的 unit 在左邊右邊	5-1
譬如說如果這是一個 phoneme 的話這是它前面的這它左邊的 context	5-1
這個呢是它左邊的 context 這邊是它右邊的 context 這叫做 context	5-1
因此呢那麼很重要的一件事就是我們必需了解在聲音裡面	5-1
存在一個現象叫做 CO ARTICULATION	5-1
什麼叫 CO ARTICULATION 呢就是	5-1
我的聲音是受到左右的 context 的影響就它的 neighboring  units	5-1
左邊右邊 unit 影響我的聲音就變了	5-1
這是很重要一點就雖然這是同樣一個音	5-1
這同樣一個音這邊是接這個後面是前面是這兩個因為左邊不一樣它就會不一樣	5-1
那這兩個是同樣的音這左邊不一樣它就不一樣這右邊不一樣它也不一樣	5-1
那為什麼會這樣那其實說穿了很簡單因為我們人發音的任何一個音的區別主要是靠口型跟脣齒舌之間的關係	5-1
你這樣關係你的啊ㄜㄧㄨㄩㄕㄙ都是因為你的口型跟脣齒的關係所造成的	5-1
那你發這個音需要這個口型發那個音需要那個口型的時候你不可能在一瞬間變成另外一個	5-1
你一定是 continuously 變過去的	5-1
因此你要從這個音變成那個音的時候互相都會影響所以前面會影響後面後面會影響前面	5-1
因此呢你可以想像譬如說我們講八跟逼這兩個波是一樣的嗎顯然不一樣	5-1
因為這個你也知道這個八當我發這個波的時候其實後面已經是要發八了所以那個八跟這個逼顯然是不一樣的	5-1
就好像 tea 跟 target	5-1
這個顯然不一樣	5-1
 tea 我後面是接了 e 了這個 tar 我後面是接 ar 　所以這兩個本來就不一樣	5-1
那這個呢就是 right  context  dependency	5-1
我受到右邊的影響這個也是這個也是右邊的影響	5-1
這就是所謂的 right  context  dependent	5-1
就是說我的右邊一定受同理左邊也是	5-1
譬如說 it 跟 at 這兩個 t 有一樣嗎會不一樣	5-1
因為這個 t 是從這個過來的這個 t 是從這個過來的這兩個口型不一樣所以會影響到這個 t 不一樣那這個呢就是	5-1
這個 left  context  dependent 它受左邊的影響這個是受右邊的影響	5-1
 OK 這個是受右邊的影響這叫做 right  context  dependent	5-1
這個受左邊的影響這叫做 left  context  dependent	5-1
所以呢我就會有這個 right  context  dependent 跟 left  context  dependent 的區別	5-1
當然我們真正的狀況應該是 both	5-1
真正我們發的每個音是受到左右兩邊的影響	5-1
你可以想這裡面的每一個音其實它都是受左右兩邊的影響	5-1
那只是說呢你如果左右兩邊都考慮進去太複雜了	5-1
有的時候難度比較高所以我要怎麼樣子做到比較有效	5-1
這個是這個 context  dependency	5-1
那麼也就是我們這邊講的 CO ARTICULATION	5-1
就是我們發聲會受到它的 neighboring  unit 的影響	5-1
會受到左邊跟右邊的 context 的影響	5-1
那還有呢我們也可以說它是這個 intra  word 跟 inter  word 這兩種	5-1
怎麼講呢就是所謂的這個 intra  word 就是假設我的影響僅限於一個 word 裡面	5-1
假設這個 word 裡面它的左右都會受影響	5-1
但是呢我的影響不跨過一個 word 的 boundary 這叫做 intra  word	5-1
就是說在一個 word 裡面它們前後互相影響	5-1
可是我的影響不會跨假設啦都是 engineer 在做這些問題的時候	5-1
做一些假設因為你你不做那些假設你有時候很難做	5-1
你如果假設它們影響不會跨過一個 word  boundary	5-1
所以這個音不受它的影響	5-1
這個音不受它的影響這個呢叫做 intra  word  context  dependency	5-1
那如果變成 inter  word 的話呢就變成讓它可以跨越 word  boundary	5-1
於是呢它可以影響它它也可以影響它的話呢那就是 inter  word  context  dependency	5-1
所以呢如果講真正從語言學家來講的話它這個所有的都是存在的所以應該是 inter  word	5-1
應該是 both  right 跟 left  context  independent	5-1
但是從 engineer 來講呢 engineer 就會希望說我們如果能夠做一些 assumption	5-1
讓這問題簡化我可以得到比較好的答案的話呢	5-1
看它 work 不 work 嘛如果 work 就好了這個是這個這些東西的來源	5-1
那麼就我們講的國語而言	5-1
那麼那麼你知道我們的每一個字就是一個單一的 syllable	5-1
就是中文的特性就是每一個字都是單一的 syllable	5-1
所以你察注音符號檢字表裡面的巴拔把爸逋不補布一直到淤於雨育每一個都是一個 syllable	5-1
那每一個單一的 syllable 對應到的是	5-1
一個或者一個以上的字	5-1
這個是字那麼通常都會以一個以上的通音字	5-1
那麼因此我們有這個 many  to  one 的一種這個 mapping  relation	5-1
但是反過來也是 one  to  many 因為有這個還有破音字嘛等等	5-1
所以有同音字還有破音字這是我們的特有的一個現象	5-1
那對每一個 syllable 而言呢	5-1
傳統的聲韻學	5-1
它們分做聲母跟韻母	5-1
那麼在早年的語音學家它們把它翻成英文就叫做 initial 跟 final	5-1
所以呢我們後面也會以這個為例	5-1
我們這門課絕大部分的內容我們都以 language  independent 的東西來解釋所以講的都是以英文為準	5-1
那麼這些課本啊 reference  paper 大部分都是以英文為主所以大部分都是用英文	5-1
但是碰到有中文的地方我們會說一下中文的部分	5-1
像這個 case 的話呢我的這個我們就會說到後面會說到一下聲母跟韻母	5-1
什麼是聲母跟韻母呢其實就是我們的每一個音都會分成兩個	5-1
譬如說八這個是聲母這個是韻母	5-1
逼這個是聲母這個是韻母	5-1
那麼如果說是天的話呢那這個是聲母這個是韻母	5-1
那麼如果是六的話呢那這個是聲母這個是韻母	5-1
所以基本上聲母是前面那個字音韻母是後面這些東西	5-1
那這個但是呢這個	5-1
這是用聲母韻母來分這是聲韻學上的分法	5-1
那麼如果用它們西方語言學的 phoneme 來分的話就不一定是這樣子了	5-1
譬如說這個天呢你可以想像是ㄊ一ㄢ它是四個 phoneme	5-1
ㄊ一ㄚㄢ天那麼六的話是ㄌ一ㄜㄨ六	5-1
那麼你可以看成是四個 phoneme 兜起來的	5-1
所以呢這是如果用西方語言的 phoneme 來分的話可以看成是這樣	5-1
那麼其實這三個兜成一個韻母這三個兜成一個韻母等等諸如此類	5-1
這樣你大概了解一下我們講的東西	5-1
那我們比西法語言多一個就是聲調	5-1
那你知道就是我們這就是所謂的四聲加輕聲	5-1
那麼我們有聲調不是說西方語言沒有	5-1
只是說我們用聲調來辨異	5-1
西方語言不辨異	5-1
辨異	5-1
所謂辨異就是說你不同的聲調代表不同的字代表不同的詞有不同的意義	5-1
那麼西方語言也有聲調只是它不拿來辨異	5-1
它可以說 how  are  you  today  how  are  you  today  how  are  you  today 都可以	5-1
那意思其實是差不多的只差一點點不能說完全一樣但是只差一點點	5-1
但是我們如果把聲調一變的話就是不同的	5-1
那這是我們一些不同的地方就是了	5-1
好那我想也許我們今天就停在這裡好不好	5-1
好這個週末補課大家辛苦	5-1
喂喂喂	5-1
喂喂	5-1
有沒有聲音有齁	5-1
喂ok 好	5-1
我們上次我們補課在講從講五點零開始阿	5-1
這個週末補課大家都辛苦	5-1
我是今天早上八點降落在桃園機場	5-1
經過二十六小時的飛行才降落的	5-1
ok 我們上週在講說這個喔四點零講的h m m 的時候我們只當它是一堆數學模型	5-1
沒有當它是聲音	5-1
那我們在五點零是把他看成是聲音的話	5-1
那麼看看有何不同的情形	5-1
那麼這個時候那麼我們要考慮的因素就包括	5-1
到底要用什麼樣子的單位來做h m m	5-1
當然可以是phrases	5-1
可以是words	5-1
可以是syllables	5-1
可以是phoneme	5-1
那麼所謂的phoneme 是我們所講的最小的一個聲音的單位	5-1
也就是我們所說的基本單位的音	5-1
那麼最簡單解釋就是每一個母音每一個子音都是phoneme	5-1
那麼我們常常把phoneme 跟phone 混為一談	5-1
那麼很多人他們其實都很偷懶就是phoneme 太長了我就唸前面	5-1
就是phone 嘛	5-1
那所以他講的phone 其實就是phoneme	5-1
那麼雖然語言學家說phone 跟phoneme 有點不一樣等等阿	5-1
那麼這裡面影響最大的一個問題應該是所謂的co articulation	5-1
也就是說每一個phone 的發音其實是context dependent	5-1
也就是說它跟它的左右的neighboring units 或者說跟它的前後音都有關係	5-1
那麼這就是所謂的co articulation	5-1
也就是我們發的聲音會跟前後音不同而不同	5-1
那麼我們說過呢因為你發的每一個音都是因為某一種口形	5-1
你的ㄚㄨㄧㄕㄙㄕ都是你的某一種口形所造成的	5-1
那你那個口形不可能在一瞬間變成另外一個口形	5-1
所以它前面要發什麼音後面要發什麼音這個口音一定是變過來的	5-1
因此你的聲音自然就受前後音的影響	5-1
這就是所謂的co articulation	5-1
因此我們的聲音顯然是受到左邊的影響	5-1
受到右邊的影響	5-1
那就是所謂的left 跟right context dependency	5-1
那麼就我們國語而言	5-1
我們的每一個音我們的每一個字都是一個syllable	5-1
那一個syllable 都可以分成聲母和韻母	5-1
還有就是第幾聲的聲調阿等等	5-1
好我們底下要講的就是那麼到底要用什麼來做	5-2
做這個h m m 的unit 呢	5-2
基本它有幾個最重要的考慮	5-2
這三點	5-2
第一個要accuracy	5-2
也就是你如果要用那個聲音那個單位來拼聲音的話	5-2
顯然它必須要足夠精確來描述真正的acoustic realization	5-2
好也就是說你如果用每一個音這個這是一個這是一個譬如說一個p 是一個h m m	5-2
r 是一個h m m	5-2
i 是一個h m m 如果這樣拼的話他們拼起來真的能夠變成一個primary 嗎	5-2
那這個字是不是能夠精確的用那些音拼的出來	5-2
第一個問題就是accurate 就是accuracy	5-2
這些小音是不是真的能夠精確的拼出那個音來	5-2
第二個問題是train ability	5-2
也就是指裡面有沒有辦法有夠多的data 去train 那些model	5-2
換句話說你如果說是這個你必須要考慮到每一個音都能每一個model 都要有夠多的data	5-2
它都是統計的model	5-2
如果沒有夠多的data 的話你沒有辦法真的train 那些聲音喔	5-2
所以要有夠多的data	5-2
那第三個也就是所謂的generalized ability	5-2
也就是你的一個新詞必須要有夠多的任何一個new word 都要能夠用已經有的unit inventory 拼出來	5-2
我們不能因為在我們語言是活的	5-2
語言是活的所以永遠有新詞	5-2
永遠有新new word	5-2
你不能因為有個new word 你必須要去train 一個new 的model	5-2
那麼你不能為了一個必須是要用已有的unit inventory 去拼才可以	5-2
這是所謂new word 的問題	5-2
那麼new word 在不同的語言它的新詞出現的比例高低是有不同的	5-2
那麼就中文而言我們中文是新詞出現特別多的一種	5-2
原因是我們的每一個音都代表很多字	5-2
每一個字都有意思	5-2
這些意思很容易拼成任何的新詞	5-2
所以我們很容易創造新詞	5-2
我們舉個最簡單的例子	5-2
四不一沒有這就是一個新詞阿	5-2
這個廢統還是終統這都是新詞	5-2
所以我們隨時都可以創造很多新詞	5-2
反分裂法	5-2
這個都是新詞	5-2
那麼中文算是因為我們每個字都是有意思所以很容易湊成新詞	5-2
同樣的鼎泰豐也好是	5-2
這個什麼小福還好也好什麼	5-2
我們很容易創造很多新詞	5-2
在英文裡面新詞比起來是比較少的	5-2
但是也有很多阿	5-2
最多的就是專有名詞嘛	5-2
譬如說microsoft 那就是一個micro 跟一個soft 兩個字拼起來	5-2
當它創了那個公司的時候它就有那個新詞	5-2
intel intel 是比較不同它不是一個in 跟一個tel	5-2
它是根本就是一堆字母臨時就是拼出來的阿等等	5-2
所以那他們也有一堆新詞	5-2
不過他們的新詞顯然比我們少	5-2
中文是新詞最多的阿	5-2
那麼因此你不能因為有個四不一沒有你趕快就要就要當成一個新詞	5-2
然後去找一堆人去唸四不一沒有然後才能夠辨識這個詞呀	5-2
你必須要是已有的單位能夠拼出來的才可以	5-2
這就是所謂的generalized ability	5-2
當你同時考慮這三種因素的時候	5-2
那麼我們可以舉例來說明他們哪些會怎樣	5-2
那麼例如說呢這個words	5-2
就英文而言呀words	5-2
就中文而言呀就是詞	5-2
到底好不好呢	5-2
它最大的好處是絕對accurate	5-2
舉例來講你如果是primary我就真的用primary 來做成一個model 的話	5-2
你就用夠多的聲音primary primary primary 去train 它	5-2
它當然會變成很精確的描述這個primary	5-2
這是這是這絕對是accurate 阿	5-2
只要有夠多的data	5-2
所以呢用word 來做單位的話絕對是夠accurate	5-2
但是呢它trainable 可能只限於small vocabulary	5-2
你如果是辨識兩百個詞兩百個words 三百個words 還可以	5-2
你如果要辨識比較英文常用的words 的數目大概至少是三萬以上吧	5-2
當你要三萬個words 你都要每一個word 你都要用夠多的data train 的出來就很累了	5-2
所以呢它不見得是trainable	5-2
你除非是small vocabulary 的應用	5-2
譬如說你的這個辨識人名阿	5-2
你的那個這個name dialing	5-2
手機上可以說我說amy 它就幫我接到amy	5-2
這種這種是這只是少數	5-2
是small vocabulary 是trainable 的	5-2
可是如果vocabulary 變大的話就會變的很難	5-2
那麼最大的問題是它不generalized	5-2
出來一任何一個new word 沒有辦法做了	5-2
除非那個new word 再做一個新的model	5-2
重新唸一堆聲音給它它才能train 的出來	5-2
所以這是word 的問題	5-2
那麼最急最想到最小單位就是phone 嘛	5-2
這裡講的phone其實就是我們之前講的phoneme	5-2
好我們沒有把phone 跟phoneme 區別開來	5-2
就這邊講的phoneme 就這些子音跟母音	5-2
用這些來做好不好	5-2
當然好	5-2
第一個它trainable	5-2
為什麼trainable	5-2
因為它的總數有限嘛	5-2
你任何一個語言的phone 的數目phoneme 的數目大概是不到一百個	5-2
幾十個吧	5-2
所以數目不多	5-2
所以是trainable	5-2
然後generalized理論上我都可以拼成所有的字嘛	5-2
所有的word 都是用它拼出來的嘛	5-2
所以generalized 嘛	5-2
可是有個最大的問題就是不容易變成accurate	5-2
因為什麼因為context dependency	5-2
換句話說你如果每就是我們剛才舉的例子你說這個每一個p 有一個h m m	5-2
r 有一個h m m	5-2
這個e 有一個	5-2
阿每一個都有一個它們真的拼起來真的可以變成一個primary 嗎	5-2
這是一個很大的問題	5-2
那麼為什麼是一個很大的問題	5-2
是因為你每一個每我們說過就是剛才講的這個co articulation	5-2
也就是說每一個音都受到前後音的影響	5-2
發音就不一樣了	5-2
因此你不要以為那個e 在這裡	5-2
我只要有一個e 的model	5-2
這個e 在這裡它就可以變成e	5-2
其實不然	5-2
因為這個e 只要前面接的是不同的音後面接著不同的音它的音都不一樣的	5-2
同樣這個m	5-2
前面接個不同的音後面接個不同的音都不一樣的	5-2
所以呢它你如果有辦法把這幾十個phoneme 通通都train 出來	5-2
變成model 把它拼起來可以嗎這是鐵定不可以的	5-2
它不容易變成accurate	5-2
因為我們每個音都是context dependent	5-2
那麼因此呢phone 有它的困難	5-2
那syllable 如何呢	5-2
syllable 非常depends on 是在哪一種語言	5-2
因為不同的語言裡面的syllable 數目差很多	5-2
因此可行性就差很多	5-2
舉例來講	5-2
日文總共只有五十個syllable	5-2
這個算是一個非常容易control 的一種單位	5-2
那麼各位學日文就知道	5-2
ka ki ku k e ko	5-2
就是它就有五種母音	5-2
再加上十種子音	5-2
五十個syllable	5-2
這是在很容易control 範圍之內所以在日文而言	5-2
syllable 是一個非常好的一個單位	5-2
來做這些事情的	5-2
中文就比較麻煩了	5-2
你知道我們的國語有多少個syllable 呢	5-2
大約一千三百個這個	5-2
數目其實是不大	5-2
比起日文來是大很多了	5-2
那這一千三百個是指說你去查國語辭典的	5-2
注音符號檢字表	5-2
從八拔把罷撥柏跛播逋不補部一直數數到淤於與欲阿	5-2
大概是一千三百個而已	5-2
不算多	5-2
為什麼說不算多	5-2
你跟英文比的話	5-2
英文的syllable 是超過三萬個的阿	5-2
比我們多很多	5-2
英文為什麼它的syllable 會超過三萬呢	5-2
是因為英文有非常多的前後都可以帶的子音阿	5-2
我們隨便舉個例子就可以知道	5-2
譬如說prompt	5-2
這是一個syllable	5-2
一個syllable它只有一個母音在這裡	5-2
可它前面可以帶兩個不同的子音	5-2
後面可以帶三個子音阿	5-2
那譬如說script	5-2
它只有一個一個母音在這	5-2
但是它前面可以帶三個子音	5-2
後面帶兩個子音	5-2
阿等等	5-2
那像這些東西都是一個一個不同不同的syllable	5-2
所以這也是一個syllable 這也是一個syllable那這樣syllable 就多了很多	5-2
那中文只有一千三百個為什麼這麼少	5-2
就是因為中文沒有那麼多複雜的子音	5-2
它的母音前面只有一個子音	5-2
後面常常沒有子音阿	5-2
我們中文譬如說ㄅㄨ	5-2
這是一個母音前面帶一個子音	5-2
後面沒有	5-2
基本上就是差不多是這樣子的	5-2
那麼後面有的情形很少很少	5-2
那麼那當然這個是指國語而言	5-2
如果是方言的話不同的語言又不一樣了	5-2
譬如說閩南語的話	5-2
閩南語的話這個數字超過兩千	5-2
超過兩千	5-2
那原因是它了很多	5-2
而且包括它後面可以帶一些子音阿	5-2
閩南語的是有一些syllable 後面可以多帶一些子音	5-2
那麼國語比較沒有等等	5-2
所以這都不一樣	5-2
所以呢這個syllable 好不好作單位呢	5-2
這也是depends on 也是作哪一種語言等等	5-2
那turns on 呢	5-2
最理想的最理想的應該是什麼呢	5-2
那麼最理想的似乎應該是所謂的tri phone	5-2
tri phone這個名詞是在九零年代末期	5-2
人家想出來的	5-2
那麼雖然後來很多人說這個名詞取的不好	5-2
不過人家已經都叫它叫tri phone 了	5-2
tri phone 聽起來好像是三個phone 連在一起叫做tri phone	5-2
其實不然它是指一個phone	5-2
阿tri phone 是指一個phone	5-2
但是呢它前後只要接的音不一樣就算不一樣	5-2
譬如說這個是一個這是一個phone	5-2
就是這個這個喔的這個音	5-2
它前面帶的是r 後面帶的是m	5-2
所以這是一個tri phone	5-2
是指這個音前面帶著前面是跟著r 的後面是跟著m 的	5-2
那其實不是三個連在一起	5-2
而是前面後面都不算阿	5-2
它只是算中間這個	5-2
但是因為前面受到它影響後面受到它影響	5-2
所以呢這個它是單獨一個	5-2
你換另外一個的話是不一樣的	5-2
你如果前面是接這個	5-2
後面是接這個	5-2
那當然是另外一個阿	5-2
你只要前面不一樣後面不一樣都算不一樣	5-2
但是它講的其實是講中間這一個	5-2
ok	5-2
那這個叫做tri phone	5-2
那麼我剛才講後來很多人認為這個名字取的不好	5-2
是因為這個一看好像是三個連在一起	5-2
其實它是只有一個	5-2
它並不是這三個連起來叫做tri phone	5-2
它tri phone 是指中間這一個阿	5-2
是指中間那一個但是前後不同就算不同	5-2
那麼這麼一來的話呢	5-2
它的好處它是一個最小的單位	5-2
它是一個最小的單位	5-2
那麼喔你就把左邊右邊不同都算不同	5-2
但是呢壞處是數目很大	5-2
假設某一個語言	5-2
現在多數的語言大概它的phone 的總數大概是這樣子的order 嘛	5-2
六十的話	5-2
六十的三次方就是這麼多個	5-2
這麼多個之後數目非常大	5-2
更大的一個問題是說很多時候你找不到夠多的data 來train這個	5-2
很多時候你找不到夠多的	5-2
譬如說你一定要這個phone 是前面是這個的後面是這個的	5-2
這樣的phone 可能就不多了阿	5-2
那很多的phone 可能你根本找不到哇	5-2
阿這是最大的問題	5-2
所以呢tri phone 的好處是very good generalized ability	5-2
因為你如果真的有一個tri phone 的一個set 的話	5-2
你可以拼成任何的一個文字嘛	5-2
對不對任何的一個word 都可以拼	5-2
所以你如果真的有tri phone 的話你是可以拼成所有的	5-2
所以是最好的generalized ability	5-2
但是呢另外兩個條件都不太好	5-2
trainable是有問題的因為它train 不好	5-2
因為你數目太大了然後很多很多的tri phone 根本找不到	5-2
所以呢	5-2
你就不容易train	5-2
然後呢	5-2
也因為不容易train 的結果其實就就不容易accurate	5-2
因為你如果所有的音都能夠train 的很好的話顯然是可以accurate	5-2
但是因為你train 不好	5-2
所以就沒有辦法accurate	5-2
那麼怎麼辦呢	5-2
那麼他們通常的辦法就是作所謂的parameter sharing	5-2
什麼叫parameter sharing	5-3
就是指說	5-3
有些tri phone 實在沒辦法我們就讓一些個tri phone 雖然不太一樣我們把它拼成一個	5-3
當成一個算了	5-3
阿那我們底下的一頁的兩個例子就是這一纇的叫做所謂的parameter sharing	5-3
那麼第一個例子就是	5-3
像這樣	5-3
假設這是一個tri phone	5-3
這另設是另外一個tri phone	5-3
他們都是相同的東西	5-3
但是呢就是最後面接的不一樣	5-3
就譬如說他們我們說這兩個好了	5-3
它兩個都是這個	5-3
所以呢母音本身是一樣的	5-3
前面那個子音也是一樣的	5-3
只是最後這個是接這個這個是接這個	5-3
所以因此他們是兩個不同的tri phone	5-3
他們雖然是兩個不同的tri phone	5-3
所以呢你看到它它畫三個state 的話呢	5-3
最後一個state distribution 不太一樣	5-3
就是因為最後一個state 是因為後面接不同的東西的關係所以最後state 會不一樣	5-3
前面兩個state 是很像	5-3
但是因為我找不到夠多的data 去train 這兩個不同的tri phone	5-3
那我就乾脆把他們merge 成為一個	5-3
把它合成一個算了	5-3
那麼這一個呢它最後就它把它們混在一起	5-3
就把相同的tri phone這個寫錯了喔這是p h o n e 阿	5-3
把相同的tri phone	5-3
類似tri phone 把它當成一個	5-3
然後呢就用共同的data 來train	5-3
譬如說這個總共只唸到三次	5-3
這只唸到兩次	5-3
那至少有五次總比三次兩次好嘛	5-3
阿那我就把他們兜在一起	5-3
當成一個model	5-3
恩	5-3
那這是一種辦法	5-3
這就是所謂的generalize tri phone	5-3
他們稱為generalize tri phone 就是指說有一堆tri phone	5-3
我實在沒有辦法各自train	5-3
我就當它們很像	5-3
合成一個算了	5-3
那麼這樣的話呢我可以至少至少這樣的話呢這兩個可以獲得	5-3
但他們就不夠accurate	5-3
那另外一個情形是像這邊的這種	5-3
那我有另外一個辦法就是	5-3
這個至少前面這兩個state 很像	5-3
我就把它們當成是同一個	5-3
只有最後一個state	5-3
那不像就拆開來	5-3
那這樣畫的意思其實是就是這樣的意思嘛	5-3
知道就是	5-3
那也是像這樣	5-3
是同一個phone	5-3
前面是接一樣	5-3
只有最後後面接的東西不一樣	5-3
使得它們最後一個state 不一樣	5-3
既然這樣的話如果這個音只有三次這個音只有兩次	5-3
那我至少前面這個就我就用五次一起train	5-3
所以我這兩個state 呢至少是用相同的讓它們有	5-3
所以這兩個我讓它們有相同的distribution 至少train 起來比較好一點	5-3
那最後後一個就讓它們不一樣好了	5-3
阿這這是另外一種方法	5-3
這是所謂的shared distribution model	5-3
那麼前面這個情形我們說是sharing and model level	5-3
就是這兩個model 一起share	5-3
那這邊呢就是指state level	5-3
就說在state level 讓它們一起share	5-3
那也就是說如果它們這個state 確實不太一樣的話	5-3
就讓它們不merge 起來	5-3
讓它們拆開來阿	5-3
那這種都是所謂的sharing	5-3
所謂的這個這裡所謂的sharing 就是我們剛才所說的意思	5-3
我們剛才說呢	5-3
parameter sharing	5-3
其實所謂的parameter sharing 就是它們有共同的	5-3
有共同的parameter	5-3
或者說有共同的mean 跟covariance	5-3
共同的gaussian parameter	5-3
阿那所謂的共同parameter 其實也就是有共同的training data	5-3
如果這個有三次這個有兩次我我就用五次一起train 嘛所以有共同的training data 阿	5-3
那麼這樣的話呢	5-3
這個tri phone 的最最大的好處是generalized ability	5-3
你如果完整	5-3
真的能夠做到的話你可以做的最好	5-3
但是最大的問題就是不好train	5-3
然後呢不容易拼的好	5-3
因此呢我們就是就是要用這個parameter sharing 的方法	5-3
那麼這樣sharing 之後呢是在train ability 跟accuracy 中間作一個trade off	5-3
或者作一個balance	5-3
這個意思是說	5-3
你如果是越讓它sharing 越多的話	5-3
這種sharing 越多	5-3
當然就越好train 哪	5-3
可是train 出來就越不精確嘛	5-3
這當然不精確嘛對不對	5-3
這兩這兩個怎麼會當成那一個呢	5-3
這當然不精確嘛	5-3
所以你這種這種狀況這種sharing 發生的的越多你一定越不精確嘛	5-3
所以accurate 一定比較差	5-3
但是這樣之後這個比較trainable	5-3
對不對所以train ability 跟這個accurate 是互相矛盾的	5-3
同理那邊也是一樣	5-3
你的sharing 做的越多的話你越不accurate	5-3
但是你越是trainable 阿	5-3
那麼因此呢就是剛才講的	5-3
那麼因此呢就是剛才講的就是你這個train ability 跟這個accuracy 之間是一種balance	5-3
或者是一種trade off	5-3
也就是兩者是無法得兼的	5-3
好那麼有了這樣的了解之後	5-3
底下我們這個五點零整個在講的都是在講這件事	5-3
就是如何來train 這個tri phone 阿	5-3
那麼tri phone 是今天所有的語音系統的主流	5-3
也就是最成功的model	5-3
一律都是用tri phone train	5-3
但tri phone 都有共同的問題就是說事實上你是train 不好的	5-3
因為一定有一大堆tri phone 是看不到的	5-3
那是根本沒有data 或者data 太少	5-3
是train 不出來的	5-3
那麼因此呢你顯然是需要用一堆方法	5-3
來做到這個accuracy 跟train ability 之間的balance	5-3
這就底下我們要說的事	5-3
那麼在我們之前的這個reference 這邊講到	5-3
unseen tri phone 阿	5-3
所謂的unseen tri phone 就是你永遠有一大堆tri phone 你的data database 根本就沒有	5-3
你根本就看不到阿	5-3
這所謂的unseen tri phone	5-3
那麼哦以這個以我們的經驗而而而立	5-3
譬如說這個哦我們要train 一一套tri phone 的model	5-3
很可能會發現裡面有一半的tri phone 根本沒有data	5-3
也就是說你蒐蒐蒐集了幾十小時的聲音	5-3
發現它只中間只呈現了一半的tri phone	5-3
另外一半它們根本就沒有出現	5-3
那些根本沒有出現的tri phone 呢就是所謂的unseen tri phone 阿	5-3
這所謂的unseen tri phone 那怎麼辦	5-3
我們要有辦法來做它阿	5-3
那就是底下我們五點零整個都在講這件事	5-3
就是怎麼樣來做這樣子的tri phone	5-3
那麼為了要做這件事情起見我們就開始講底下一堆事情	5-4
因為我們後面真正要做的最成功的方法是就是用這個所謂的這個cart 也就是classification and regression trees	5-4
用這個來做	5-4
而這個的基本的原理是用所謂的information theory 裡面的entropy 的觀念阿	5-4
那麼因此呢我們這邊要從information theory 開始說起	5-4
那麼就在說這段	5-4
那麼information theory 本身是一個博大精深的學問	5-4
那麼在古典通訊原理裡面是一個非常重要的基礎	5-4
那麼你或許在別的課也學過等等	5-4
那麼也許沒有學過也許學過	5-4
不過我們這裡只用到裡面一個最基本的東西	5-4
所以我們利用這個機會在這邊簡單的說一下它的一個最基本的觀念	5-4
就是在古典的通訊原理裡面的information theory	5-4
裡面所說的所謂的information 的major	5-4
那麼那麼這個東西turns out 是非常有用的	5-4
那麼用在非常多的地方	5-4
那麼包括用在我們這邊的要train tri phone 也是用這個方法	5-4
所以我們簡單的來說一下它是	5-4
那麼假設有一個information 的source s	5-4
那麼它送出一系列的symbol	5-4
那麼每一個symbol 都是一個random variable	5-4
譬如說m one m two m 三到m j 等等	5-4
其中m j 就是指在時間j 或者說是第j 個的那一個symbol	5-4
那基本上它們每一個symbol 都是一個random variable	5-4
那個random variable 都有一定的可能的值	5-4
譬如說有大m 個可能的值	5-4
是x one 到x m	5-4
ok 那麼因此呢	5-4
我們說這個是這個information source	5-4
它所送出來的的所謂的information 就是一系列的symbol	5-4
那麼在在時間j 的時候的第j 個symbol 就是m sub j	5-4
那麼它呢是一個random variable	5-4
它可以有大m 種可能的值	5-4
就是x one 到x m	5-4
那麼這個random variable 呢它基本上是有一定的機率的	5-4
所以呢第x i 個第i 個呢有一個機率就是p 的x i	5-4
那麼那當然囉那就就表示說這個阿第j 個m 的它turns out 它的值是第i 個x 的機率	5-4
就是p 的x i	5-4
而這些所有的p x i 的加起來	5-4
i 從一加到m 的話應該是正一	5-4
每一個是機率所以都是大於零的值	5-4
加起來是一	5-4
這樣樣講有點抽象	5-4
那麼我們用最具體的例子來講	5-4
那麼也許就是零跟一吧那假設說	5-4
我這個送出來就是一個一系列的零跟一的話	5-4
那其實這裡的每一個就是	5-4
第一個bit 就是m one	5-4
第二個bit 就是是m two	5-4
第三個bit 就是m 三	5-4
那每一個m j 其實就是一個bit	5-4
那那個bit 本身是random variable	5-4
它可以是一可以是零	5-4
所以呢它基本上呢它的x one x two 呢就是兩個	5-4
就是一跟零	5-4
然後它們的機率呢各是二分之一	5-4
那也就是我們這邊講的p 的x one 等於p 的x two 等於二分之一	5-4
它們加起來要等於一等等	5-4
那這樣子也許是最具體的一個簡單的例子	5-4
那這邊講的當然是比較一個general 的說法	5-4
這個比較general 而這個呢比較specific	5-4
當然這個specific 例子呢我們也不是一定要這樣	5-4
我也可以把它複雜一點譬如說我兩個兩個當成一個symbol	5-4
我如果兩兩當成一個symbol 來看的話	5-4
我的我就變成是有四個積x one x two x 三x 四	5-4
我我每兩個bits 是一個symbol 的話	5-4
我有四種symbol	5-4
分別是譬如說零零零一一零一一	5-4
那如果是這樣的話呢	5-4
我的機率各是四分之一	5-4
各是四分之一	5-4
那這就是我的四個	5-4
這樣子也是可以	5-4
那你如果這樣看的話呢它仍然是在這樣子的model 之下	5-4
那就是我的每一個symbol 是有四種可能	5-4
是有四種可能它們機率各是四分之一等等阿	5-4
這就就是我們這邊舉的這個這個情形	5-4
那在這個情形之下呢	5-4
那麼在information theory 裡面	5-4
它希望為你所看到的每一個symbol	5-4
不管是一個還是不管是這邊的一個bit 還是這邊的兩個bit 的一個symbol	5-4
你為每看到一個symbol 定義一個它到底它給我多少information	5-4
這個在這個在這個information theory 裡面他們稱之為information 的major	5-4
它到底給我多少的量information	5-4
的major	5-4
那麼也就是指quantity of information	5-4
到底這個event 到你看到一個m j 等於x i 的時候	5-4
當你看到這個bit 等於一的時候	5-4
或者當你看到這個bit 是零這個這個symbol 是是零一的時候	5-4
也就是說當你看到m j 等於x i 的時候	5-4
到底獲得多少的information	5-4
那麼在information theory 裡面它仔細的分析說這個information 怎麼定義呢	5-4
它說它應該要有這些個我們所希望有的probability	5-4
第一個就是當你看到一個symbol 出來的時候	5-4
當你看到一個information 的時候	5-4
它是譬如說m m two 是等於一的時候	5-4
這個bit 等於一的時候	5-4
或這個bit 等於零的時候	5-4
你得到information 絕對是正的	5-4
所以你的information 絕對應該是正的	5-4
它就把這個叫information 量嘛	5-4
i of x i	5-4
就是指你看到一個event	5-4
它是x i 的時候	5-4
某一個m j 等於x i 的時候	5-4
那麼你得到的information 量它那個量應該是正的	5-4
那第二個呢如果那個的機率趨近於一的話	5-4
你得到information 應該是零	5-4
這話什麼意思呢	5-4
這個如果那個information 如果那個x i 的機率是一的話	5-4
你得到的information 的量應該是零	5-4
我發現我現在走路	5-4
ok 好謝謝	5-4
如果我不能走動的話那一半的黑板就不能用了	5-4
ok 那麼喔什這話是什麼意思	5-4
簡單的解釋就是說你如果你的那個那個bit 那個symbol 出那個出來的全部都是一好了	5-4
如果它永遠是一的話	5-4
這個時候我的一的機率就是一	5-4
這個時候它還帶有information 嗎	5-4
應該沒有帶information	5-4
因為我都可以猜下個一定下個一定是一嘛	5-4
我都可以猜得到一定是一嘛對不對	5-4
我永遠猜得出來它是一嘛	5-4
所以看到一的時候有沒有看到information 沒有看到嘛	5-4
那麼因此呢它就有這個有這個definition	5-4
也就是說我的要求就是應該是如果這個x i 的如果這個x i 的機率是一的話	5-4
它就應該沒有帶information	5-4
那第三個條件是說呢如果是機率越小的	5-4
帶的information 越多	5-4
機率越多的帶的information 越少	5-4
所以呢如果x i 的機率小於x j 的話	5-4
x i 的information 就大於x j	5-4
什麼意思呢我們我們說都是一未免是太太誇張一點	5-4
那我們說是這樣子	5-4
它是一很多零很少	5-4
那在這個情形之下呢就是一的機率比零的機率大很多	5-4
如果這樣的話	5-4
你想我看到一個一所看到的information	5-4
比看到一個零所看到的information	5-4
一樣多不一樣多	5-4
顯然不一樣多	5-4
因為我這邊幾乎可以猜下一個是一八成都會猜對	5-4
所以呢再下一個我再猜是一八成還是會對	5-4
所以呢這個給我一個一的話這個information 是量是很少的	5-4
反過來呢我今天如果看到一個零的話	5-4
這個是給我非常豐富的information	5-4
為什麼因為我絕對不管誰不敢隨便猜它是零	5-4
要要猜零會八成會錯嘛	5-4
我要猜零而會對的機率是很低的嘛	5-4
所以今天告訴我那個是零的話這是給我非常豐富的information	5-4
所以這所以在這個case 如果一的機率大於零的機率的話	5-4
那麼我一所帶的information 呢	5-4
應該是比零所帶的information 要少很多ok	5-4
那麼也就是說我如果看到一個零的話	5-4
應該是看到非常多的information	5-4
看到一個一的話	5-4
大概是沒有看到太多information 因為我都猜的出來	5-4
看到零的話我是不敢隨便說它是零的	5-4
所以呢它顯然是給我比較多的information 的	5-4
那麼因此呢這就這個這是一個decreasing function of x of 這個probability	5-4
當我機率小的時候我的information 量是大的	5-4
機率大的時候information 量是少的	5-4
第四個條件是說它們是additive 的	5-4
這個比較難解釋什麼叫做information 量是additive 的	5-4
你只能講就是說哦當我不斷的增加獲得新的information 的時候	5-4
我的information 量應該是不斷地在增加嘛我不斷看到新的東西出來	5-4
我的information 量應應該是不斷地加起來的它是additive 的阿	5-4
就是我就一路我一路看到新的東西我就是得到information 量應該是一直可以加上去的	5-4
那麼用我們這邊的例子來講的話呢	5-4
簡單的解釋應該應該可以說是你如果看成是一跟零零跟一各是單獨的一個event 的時候	5-4
那麼你看到它有多少information 看到它有多少information 的話	5-4
跟你現在把一一當成是一個symbol 來看的information 應該是一樣多的	5-4
你在這邊可以兩個看成一個你	5-4
如果兩個看成一個你所那一個能夠給我多少information	5-4
跟每一個看成一個它給我的information 應該是就是兩個加起來嘛	5-4
ok 所以呢我如果看到這個零跟看到這個一	5-4
得到多少information	5-4
我把它們當成是individual event 來算的話	5-4
看到看到一個零得到多少information	5-4
看到一個一得到多少information	5-4
跟我把零一當成一個event	5-4
看到它多少information照說應該是一樣的	5-4
所以呢這個給我的information	5-4
這個給我的information	5-4
跟這個給我的information	5-4
照說應該是一樣的阿	5-4
那這是所謂的additive 的意思	5-4
那麼當有了這四個條件的時候	5-4
那麼當初發明information theory 的人	5-4
很聰明他就想了一個方法他說很簡單就是log	5-4
你只要取這個機率的log 的倒數	5-4
就符合這四個條件	5-4
那麼那麼機率的這個這個這個分之一的log	5-4
其實就是負的這個log	5-4
那這個呢就符合這四個條件	5-4
這也就是information 量的quantity information	5-4
或者是information major	5-4
的算法就是取log	5-4
那這是什麼呢	5-4
這個我們簡單的畫一個圖就知道	5-4
你知道log 是這樣的	5-4
log 是這樣的	5-4
但是呢我現在是我現在橫軸是log 是什麼log 它它的log 是機率呀	5-4
那機率的log 是最大到一為止嘛	5-4
所以他沒有上面沒有沒有這些呀	5-4
它只有這一段	5-4
只有這個這邊沒有啦	5-4
這沒有只有這個啦	5-4
然後呢你現在是負的嘛	5-4
現在是負的嘛	5-4
所以倒過來你就得到一個這個圖	5-4
就是這樣子的	5-4
這個是一點零	5-4
橫軸是那個機率p of x i	5-4
縱軸就是它的information 的量	5-4
那這條曲線就是這個log 的這條曲線	5-4
變成負的	5-4
就變成這樣子	5-4
那這個的意思其實很簡單就是符合剛才講的這四個條件	5-4
第一個呢它永遠是正的	5-4
它是一個function of probability 所以呢它是在零跟一之間	5-4
在零跟一之間	5-4
在零跟一之間它永遠是正的	5-4
那麼當你如果是趨近於一的機率趨近於一的時候它就是零嘛	5-4
當你機率趨近於一的時候它就變成零	5-4
那也就是說當我如果是永遠都是一的話那個一沒有給我information	5-4
所以它就是零	5-4
那麼同時它是一個decreasing function	5-4
機率越大的information 量越少	5-4
機率越小的information 量越大	5-4
所以呢這是這這是一個一個這樣倒過來的這就是剛符合剛才的關係	5-4
那最後一個所謂的它要additive 的這一點呢	5-4
其實我們如果從這樣來看就很簡單	5-4
因為你的機率是相加的相乘的	5-4
當你譬如說你要看到你要看到到這兩個零一的話	5-4
那麼就是它的看到它的機率乘上它的機率	5-4
如果你看成是independent 阿	5-4
我們姑且假設independent 比較容易解釋	5-4
independent 的時候呢看到這個的機率跟看到這個機率相乘才是看到這個的機率	5-4
所以這個機率是是二分之之一這個機率是二分之一的話你看到這個機率是四分之一嘛	5-4
那那因此呢你如果取log 的話	5-4
就是兩個相加嘛	5-4
所以呢當你把它當成這一個的時候	5-4
你看到它的機率就變成四分之一嘛	5-4
那麼因此呢這個取log 之後就是這兩個的log 相加嘛	5-4
ok 所以呢你如果這樣來想的話這個這個additive 這個也可以解釋啦	5-4
就是如果是independent 的話	5-4
如果這些m m j 彼此都是independent 的話那麼它們的發生的機率	5-4
就應該都是相乘的	5-4
取log 就是相加的所以就是additive 的	5-4
那麼這麼一來呢我們就有這個information 量的這個定義出來了	5-4
有了這個定義之後我們就可以再繼下一步定義這個東西	5-4
就是平均到底每一次會出現多少	5-4
什麼意思呢	5-4
就以這邊為例	5-4
假設說我這個一出現機率很高在這裡	5-4
p of 一等於零點九	5-4
就在這裡	5-4
因此呢它的量很小很小	5-4
那麼零的機率呢只有零點一	5-4
它的機率很大很大	5-4
它的它的這個帶的這個information 量很大很大	5-4
那我真正到底門我每次看到一個bit	5-4
那個bit 就是m	5-4
m j 啦我們這邊講的m j	5-4
我每次看到一個bit 到底它給我多少information 呢	5-4
那應該是它可能是零點九	5-4
是一可能是零點九	5-4
但是呢它的機率只有喔不對喔它可可能可能是這個值	5-4
就是i 的這個一	5-4
i 的一是是這個值	5-4
但是呢它這個值很小	5-4
但是它的它的機率是零點九	5-4
然後呢這個零的帶的information 量很大	5-4
但它的機率呢是只有零點一	5-4
那這樣子平均起來才是我的那就是這裡的這個東西	5-4
那這就是這個的意思	5-4
也就是說我現在有有零有一嘛	5-4
那麼零出現的機率很小	5-4
所以我帶的information 量很大	5-4
這個量很大	5-4
可是因為出現機率很小阿	5-4
我十次才出現一次呀	5-4
那麼一的時候出這機率很高	5-4
但他帶的information 很少	5-4
這個很小可是呢我十次裡面有九次是它呀	5-4
所以那你平均你每看到一個bit	5-4
在這個例子而言你每看到一個bit 到底看到多少呢就是平均嘛	5-4
平均一下的結果	5-4
那麼這就是我的average quantity of information	5-4
就是average in average 你每你每看到一個m j	5-4
到底看到多少information 呢	5-4
那麼你看到的應該是它們的平均	5-4
所以就把它們平均一下	5-4
所以呢當你看到的是p 的是x i 的時候	5-4
它的這個information 量是這樣	5-4
但是它的機率是那樣	5-4
你要把機率平均一下阿	5-4
所以就是這個i of x 的這個平均值	5-4
那這就是你所要的你所看到的看到一個m j 的時候它的information 量	5-4
這個就是這個i 的這個也就是這個h of s 的定義	5-4
它有一個名字叫做entropy	5-4
那麼為什麼叫這個entropy 的名字這個我們底下會再進一步的解釋	5-4
不過我們現在先這樣說	5-4
這是 entropy 的定義就是平均你每次看到一個bit 或者一一個symbol 或者一個m j	5-4
那麼平均你看到多少的information 的量	5-4
或者說就是quantity of information carry by 這個event of 一個random variable	5-4
這邊等於一個都等於是一每一個都等於是一個random variable	5-4
因為它有一堆random 的值	5-4
它有一堆值然後都有機率	5-4
所以每一個都是一個random variable	5-4
那麼到底你看到一個random variable 是多少的時候	5-4
到底得到多少information	5-4
那麼平均可以得到就是所謂的entropy	5-4
那麼這這樣講有點抽象我們底下舉一些例子來看喔	5-5
對了還有漏掉的我們要講一下	5-5
這邊講到說取log	5-5
取log 當然很重要的是你到底log 的基底是取什麼對不對	5-5
你這個是log 是什麼呢	5-5
那麼在課本裡面它會說你depends on 你取什麼	5-5
最常用的是基底是取二	5-5
如果是log 是二的話	5-5
那這個單位就叫做bits	5-5
那麼這個有它的道理的我們底下就會解釋為什麼是這樣	5-5
但是呢這個是最常用的一個一個基底	5-5
當你不一定要要用二你用其他也可以	5-5
譬如說那個用自然對數的e 也可以	5-5
你用log 十也可以等等	5-5
但是最常用的是二	5-5
當你用二的時候呢	5-5
算出來那個數字的單位叫做bits	5-5
那麼這裡有點有點confuse	5-5
因為我們平常也常常用bit 來講另外一件事情	5-5
我們常常講的bit 就是講這一個這一個零或者一叫做一個bit	5-5
那麼這個bit 是binary digit 的簡稱	5-5
binary digit 叫做bit	5-5
那麼這是一個零或者一叫做一個bit	5-5
但是這裡不是	5-5
這裡的bit 是information 的量	5-5
是取log 出來的值	5-5
它可以是任何的real number	5-5
這裡一定是整數	5-5
你如果講這一個bit 就是指一個bit 沒有半個bit 嘛阿	5-5
所以呢bit 一定是一個整數嘛	5-5
可是這裡不是這裡bit 是一個real number 阿	5-5
是一個real number	5-5
那麼為了區別起見我這邊凡是後面寫個括號of information	5-5
就是指這個bit 是information 的量	5-5
而這個我如果沒有寫of information 的話呢	5-5
就是指它是我們所熟悉的bit 的意思	5-5
那為什麼會這樣子搞得一個bit 有兩個意思呢	5-5
這個其實都都是有原因的這個我們底下就會解釋	5-5
好那麼底下我們就舉一些例子來看這些東西這樣會比較清楚一點	5-5
那麼最簡單的例子就是我們這邊所熟悉的零跟一	5-5
如果說我我送出來的如果說我的這個就是零跟一	5-5
而且它們機率各是二分之一	5-5
這是個最容易想像的case	5-5
就是那麼零的機率是二分之一	5-5
一的機率也是二分之一	5-5
就是我們這邊講的這個case	5-5
在這個情形之下你很容易算	5-5
你怎麼算它的information 你就是把這個機率各取log 嗎對不對	5-5
因為我們剛才講我就是把那個機率把那個機率取log 嘛	5-5
取log 就是我的這個information 的量	5-5
所以我就取log 你就會發現那個零給我的是一個bit 的information	5-5
一也給我一個bit information	5-5
平均起來它還是給我一個bit information	5-5
所以呢也就是說呢	5-5
譬如說這一個這一個binary digit	5-5
我們把它寫清楚這樣比較不會混淆	5-5
這一個binary digit 給我多少information	5-5
一個bit of information	5-5
對不對	5-5
這一個binary digit 給我一個bit 的information	5-5
同樣呢這一個零也是一樣的	5-5
這個零也是一個binary digit	5-5
也給我一個bit information	5-5
所以呢這些既然每一個bit 都是給我一個bit information	5-5
所以呢我就是每一個bit 就是帶一個bit information	5-5
ok 所以呢平均起來我每一個symbol	5-5
或者每一個bit	5-5
每一個symbol 或每一個給我的就是一個bit 的information	5-5
那這裡你比較容易可以想像為什麼它要用information 量也要要這樣樣取然後也叫做bit	5-5
因為它就是一個bit	5-5
這一個bit 就給我一個bit information	5-5
那頭第二個例子呢是我如果有四種的話	5-5
那麼就變成我有四種	5-5
那麼我有這四種四種的時候呢我們說它各是四分之一的機率	5-5
m 一m 二m 三等等等等	5-5
那他呢可以有四種	5-5
也就是說有x one x two x 三x 四有四種	5-5
如果這四種的機率各是四分之一的話	5-5
那你可以想像這四種既然有四種不同東西	5-5
我可以其實它就可以分別代表零零零一一一一零	5-5
代表這四個兩個bit pattern	5-5
那麼因此呢如果它們各是四分之一的話	5-5
我也很容易算它們每一個給我的information 量呢就是四分之一的log 嘛	5-5
我一樣的就用剛才的那個式子	5-5
去算四分就算那個機率的log 嘛	5-5
我取log 之後我得到的	5-5
就是兩個bit 嘛	5-5
所以呢我每一個symbol	5-5
這個symbol 呢	5-5
給我的是什麼	5-5
它帶的是two bits of information	5-5
它這一個帶的就是兩個bit information	5-5
剛才根據我們來算就是算它是四分之一嘛	5-5
它的機率各是它它機率各是四分之一的話它就是帶了兩個bit information	5-5
那麼那麼因此呢那我我現在每一個都是都是兩個每一個都是兩個bit 所以平均還是兩個	5-5
於是呢平均起來呢每一個都是帶了兩個bit information	5-5
那麼因為它帶了兩個bit information	5-5
所以呢你也可以很容易想像	5-5
欸我就是其實就是每一個相當於一個two bit pattern	5-5
那它就是等於是這個是零零這這是零一	5-5
它每一個就是帶了兩個bit	5-5
那所以從上面這兩個例子來看的話	5-5
是比較容易想像它為什麼要取bit 的這個名字	5-5
然後它為什麼要用log 二	5-5
為什麼用二來做這個log 的基底哦	5-5
為什麼要用二來做這個log 的基底然後叫做bit	5-5
這樣比較容易想像	5-5
因為這樣就跟我們平常的binary 平常的這個binary 的這些哦想法是一致的	5-5
不過當然剛才這兩個case 都很簡單	5-5
因為他們都是是機率都是一樣的	5-5
就是二分之一或者四分之一是完全一樣的	5-5
如果不一樣會怎樣呢	5-5
我們底下的第三個例子就是如果不一樣的話	5-5
如果不一樣的話我們舉例來講	5-5
像這個例子就是不一樣的	5-5
那麼如果說一個是四分之一一個是四分之三	5-5
那這裡應該會多一點零	5-5
這樣子	5-5
那麼我零的機率是四分之一	5-5
一的機率是四分之三	5-5
如果是這樣的話呢你也可以算算看那麼零給我幾多少的information 呢	5-5
因為零是四分之一呀	5-5
這就如我們剛才所說的因為零比較少	5-5
所以我如果看到一個零的	5-5
話我的information 給我看到information 是比較多的	5-5
那麼因此呢你這樣一算的結果你發現那個零給我就是兩個bit information	5-5
所以呢譬如說這樣應該講說這一個binary digit	5-5
也就這一個bit	5-5
零是一個bit 阿	5-5
那這一個不過這個是一個binary digit	5-5
它呢它給我多少	5-5
兩個bit of information	5-5
因為這個比較少看到難得看到一下	5-5
它其實給我的是兩個bit information	5-5
那反過來呢一比較多呀	5-5
所以呢你如果看到一的話呢	5-5
一的information 就少啦	5-5
譬如說這個一的話呢	5-5
它只給我多少呢	5-5
這一個binary digit	5-5
這一個bit 這是一個bit 這也是一個這個bit 呢只給我多少呢	5-5
你用四分之三去算的話呢	5-5
發現取四分之三的log 之後只有零點四二個bits of information	5-5
ok 所以呢這個呢是只有它的information 就很它只有零點四二bit	5-5
它是兩個bit	5-5
它有零點四二	5-5
它是兩個bit	5-5
零點四二兩個bit 零點四二等等等等	5-5
這樣子	5-5
那這樣子有的帶的多有的帶的少	5-5
那平均帶多少呢	5-5
那你就可以算它的平均就是這個h of s	5-5
那這個h of s 其實就是	5-5
零是兩個bit	5-5
可是呢它的的機率是四分之一	5-5
一是零點四二個bit	5-5
但是呢它的機率是四分之三	5-5
對不對	5-5
你這樣算起來它平均是多少呢	5-5
算起來是零點八一	5-5
ok	5-5
我這個這個零的話呢它有兩個bit information	5-5
不過呢它的機率只有四分之一	5-5
一的話呢只只有零點四二bit information	5-5
但是呢它有四分之三	5-5
平均起來是零點八一	5-5
那什麼意思呢是變成說呢	5-5
現在的in average 那麼每一個every bit 這個bit 我們寫清楚是binary digit	5-5
每一個binary digit 呢只給我多少	5-5
零點八一個bit of information	5-5
換句話說平均起來每一個bit 只給我零點八一個bit information	5-5
跟剛才每一個bit 給我一個bit information 是不一樣的	5-5
在這邊的話	5-5
二分之二分之一的時候呢每一個bit 就給我一個bit information	5-5
我現在呢因為一個是多一個是少	5-5
結果每一個bit 呢只給我零點八一的information	5-5
就差了阿	5-5
那這個就說明它為什麼這樣define	5-5
它其實是在說明你的每一個bit	5-5
每一個binary digit 這個bit	5-5
它到底給我的information 是不是充足	5-5
還是是不是efficient	5-5
那顯然呢當你是各二分之一個時候	5-5
一個bit 就是是一個bit information	5-5
可是你如果不是各二分之一而是一個多一個少的話呢	5-5
一個bit 帶了info information 就不到一個bit 了	5-5
那麼從這個引申下來	5-5
其實就有一個非常有名的在information theory 裡面有所謂的叫做binary entropy function	5-5
我把這個擦掉了哦	5-5
這個應該知道了沒什麼難的	5-5
所謂的這個binary entropy function	5-5
這個意思我們待會也會再解釋不過簡單的講就是說假設我現在這個假設我現在的這個送出來還是只有零跟一	5-5
然後呢譬如說一的機率是p	5-5
零的機率呢是一減p	5-5
p 現在是一個random variable 哦	5-5
p 是一個variable 從零到一之間	5-5
一個是一一個是一減p	5-5
那麼這個時候它的這個h of s	5-5
就是我們這邊講的這個東西	5-5
平均每一個symbol 給我多少information 這個東西呢	5-5
你就會發現它就是我們講的p log p	5-5
加上一減p log 一減p	5-5
就是這個	5-5
這個東西p log p 加上一減p log 一減p	5-5
也就是這個log p 是一的機一所帶的information	5-5
乘上一的機率	5-5
這是零所帶的information	5-5
乘上零的機率對不對	5-5
那其實這個就是我們剛才在算就是在算這個式子	5-5
那你如你如果把這個整個都畫出來的話呢	5-5
就得到一個這樣子的圖	5-5
一個這樣子的一個對稱的圖	5-5
畫的有點不對稱不過應該是對稱的	5-5
零到一	5-5
橫軸是p	5-5
縱軸呢就是在零點五的地方	5-5
是一點零	5-5
那這個縱軸就是這個bits	5-5
就是information 的量阿of information	5-5
縱軸又是bits of information	5-5
那這個就是我們剛才的剛才講的所有的example 都是這裡面的圖這裡面你一個點	5-5
譬如說我們剛才擦掉的就是零跟一各是零點五	5-5
就是這一點	5-5
當p 等於零點五的時候	5-5
一減p 也是零點五	5-5
所以呢也就是零跟一各是二分之一的情形	5-5
當零跟一各是二分之一的時候呢	5-5
每一個binary digit	5-5
每一個bit 帶的量	5-5
就是一個bit 的information	5-5
就是一個bit information	5-5
那我現在剛才這邊的這個例子是四分之一跟四分之三	5-5
一是四分之三零是四分之一	5-5
所以呢是相當於p 是四分之三	5-5
零是四分之一	5-5
那就是這一點	5-5
零點七五	5-5
那這樣子算上來的話呢	5-5
你得到的是多少呢就是零點八一	5-5
就是我們剛才那個零點八一的那一點	5-5
是不是零點八一	5-5
就是這個零點八一的這個這個例子	5-5
你如果一個是p 是四分之p 跟一減p 一個是四分之一一個是四分之三的話就是零點零點七五的地方	5-5
或者是這邊也可以	5-5
零點二五也可以	5-5
都一樣都是零點八一	5-5
那同樣呢你也可以猜得到	5-5
這點是哪一點	5-5
這點是零	5-5
p 等於零也就是說呢根本就沒有一	5-5
全部都是零嘛	5-5
也就是根本就是零零零零零一整串都是零	5-5
一整串都是是零的話就是我們說它不帶information	5-5
因為我都猜的出來後面一定是零	5-5
每一個我都猜得到它是零	5-5
根本不要看了都是零	5-5
所以你給我一個零我沒有給我任何東西	5-5
所以都是零都是沒有information 的	5-5
所以呢它的總共的info information 量就是零	5-5
所以在在縱縱軸上就是零的位置	5-5
同理呢這一點呢是p 等於一	5-5
就是整串都是一整串	5-5
都是一的話	5-5
這個任何一個一也不給我任何information	5-5
所以它給我information 量是零	5-5
ok	5-5
那這樣我就不知道一個一個整個的一個一個function	5-5
變成是一個function of  p	5-5
那麼於是呢這個時候剛才的這些這個例子跟這個例子	5-5
都是它的一個special case	5-5
對不對這個例子是指各二分之一的時候	5-5
每一個就是一個bit information	5-5
那這個例子呢是說一個四分之一一個四分之三的時候呢	5-5
給我的是零點八一個bit information 等等	5-5
那這樣就可以發現呢在這個case 而言	5-5
那麼只有有零跟一各是零點五的時候	5-5
是最efficient	5-5
因為每一個bit 就給我一個bit information	5-5
所以呢這個是在是在它的頂端是最高的那一點	5-5
你如果不是各二分之一而是一個多一點一個少一點的話呢	5-5
in average 你所帶的information 是減是降低的	5-5
是比較不efficient	5-5
所以呢只有在最只有在零跟一各是二分之一的時候它給給你帶的information 是最efficient	5-5
然後一個bit 就是一個bit 阿	5-5
這是為什麼它叫做bit 的的意思	5-5
它取了這個取了這個這個量我用bit 來做information 的量的原因	5-5
那就是這樣的意思	5-5
那麼也因此呢我們這邊剛才看的這個例子如果有四個的話呢	5-5
你如果各是四分之一的話就相當於兩個bit 阿等等	5-5
好那如果這點可以了解的話我們可以再進一步衍申出更多的東西出來	5-5
那麼一個最容易想像的情形就是這樣	5-5
假設我有三個好了	5-5
我有三個會怎樣	5-5
譬如說我的m j 我會有三種	5-5
x one x two x 三	5-5
如果我有三種可能的話	5-5
那我說x one 的機率是p	5-5
x two 的機率是q	5-5
然後呢x 三的機率呢就是一減p 減q	5-5
如果是這樣的話我我也一樣可以像剛才一樣算這個算這個function	5-5
只不過現在剛才是一個是p 一個是一減p	5-5
所以是一個function of p 是比較好算的	5-5
現在我有一個p 有一個q 有兩個變數怎麼辦呢有	5-5
兩個變數我沒有不不太容易畫那樣子	5-5
所以我可以想個辦法就是把p 固定	5-5
p 固定的話呢就變成q 變成variable	5-5
所以q 呢我就讓它從一減p 哦從零到一最小可以是零	5-5
最大是到一減p	5-5
如果這樣的話我可以畫一樣的可以畫一個q 的圖	5-5
我仍然可以算這三個三種symbol	5-5
這三種可能的symbol 所造成的平均每一次看到一個東西的的時候它的information 量是多少呢	5-5
這個答案我們不去算了不過答案你可以猜的出來也是這樣	5-5
跟剛才那個圖是一樣的	5-5
所不同的是minimum 那點不是零而已	5-5
這個就是零跟一減p	5-5
也就是說我的q 的值	5-5
是在從零到一減p 的中間	5-5
那麼q 的值在零到一減p 的中間	5-5
那麼這個那麼在什麼時候最大呢	5-5
是在二分之一一減p 的時候最大	5-5
那什麼意思呢跟剛才那個完全一樣	5-5
譬如說這點呢就是x one 的機率是p	5-5
x two 的機率呢是二分之一減p	5-5
x 三的機率也是二分之一減p	5-5
那就是這一點	5-5
那如果這一點是什麼呢	5-5
這一點就是q 等於一減p 啦	5-5
那麼因此呢這個x 三就沒有了	5-5
所以呢就變成一個是p	5-5
一個是一減p 零	5-5
這個根本就沒有了給我個x 三根本不會發生	5-5
只有這個x one 跟跟x two 會發生x 三根本不會發生了	5-5
這就是在這裡	5-5
那反過來呢如果在這裡的話呢	5-5
那是p 零一減p 是x 二根本不會發生	5-5
x 二根本不會發生	5-5
x 二根本不會發生就就是x 三是一減p	5-5
那在這裡面你也可以發現呢這個簡單的現象就是說	5-5
我如果把x one 的機率固定的話	5-5
這個最可以給我in average 每一個symbol 給我最多information 是在二跟三機率相同的時候	5-5
在二跟三機率相同的時候給我最多的information	5-5
那如果說是什麼時候最少呢	5-5
根本就沒有三	5-5
或者根本就沒有二	5-5
這時候就只有兩種東西了	5-5
這裡根本沒有三的話只有兩種	5-5
這裡根本沒有二也只有兩種	5-5
如果只有兩種的話我的information 就少了嘛阿	5-5
所以它就會減減少就變成這樣	5-5
那這個圖的情形呢畫的呢	5-5
其實跟那個是完全一樣	5-5
只不過我們這邊因為有三個變數	5-5
所以呢有兩個變數所以我們只固定其中一個來畫	5-5
好如果這個圖可以想像的話	5-5
那我們可以再進一步衍生	5-5
你就可以想像哦如果說是我這邊是假設x one 機率是固定	5-5
這個時候機率最大是發生在二跟三機率相同	5-5
都是二分之一減p	5-5
同理我也可以把q 固定	5-5
q 固定的話呢它跟什麼時候這這這個information 量最大呢	5-5
是它跟它相同都是一減q 的時候	5-5
我也可以把它固定	5-5
那麼什麼時候最大呢是它跟它機率相同的時候等等	5-5
以此類推你就可以猜的出來	5-5
真正information 最大的時候	5-5
就是這三個各三分之一的時候	5-5
對不對	5-5
也就是應該要這這三個機率各是三分之一的時候	5-5
如果各是三分之一的時候它們機率是最大	5-5
那如果可以這樣子衍生的話	5-5
我們就可以這些都是可以證明的	5-5
不過在information theory 裡面都有證明	5-5
不過我們這邊就不證	5-5
我就這麼用嘴巴講一下	5-5
那麼我們就可以得到一個重要的結果	5-5
那就是這邊所說的底下這個這個講的就是這就是這件事情	5-5
也就是說	5-5
你如果總共有m m 是什麼	5-5
m 就是我所總共的總	5-5
我這邊x 一x 二x 三總共有大m 個嘛	5-5
大m 是這裡symbol 的總數	5-5
所以呢剛才這邊的這邊m 就是二	5-5
我們那邊舉例的時候m 就變成三	5-5
這m 就是這個東西	5-5
那麼如果m 是這個的話呢	5-5
那麼你什麼時候這個就是h of s	5-5
就是我們這邊所在算的這個所謂的這個source 的entropy 的這個東西	5-5
的這個東西這個平就是平均也就是h of s 就是這個entropy	5-5
也就是我們所說的平均每一個symbol 到底給我多少information 的這個東西	5-5
那麼它的值呢應該是它的maximum 發生在什麼情形之下	5-5
發生在我們每一個機率都是m 分之一的時候	5-5
那麼剛才在這邊看到的時候呢就是什麼時候它的它的peak 在哪裡	5-5
peak 在我這兩個都是二分之一的時候	5-5
因為我大m 這裡在我這個case 大m 等於二嘛	5-5
在我這裡大m 等於二所以呢	5-5
就是它的peak 發生在各是二分之一的時候	5-5
那我們這邊講如果這個這邊舉舉的case 是大m 等於三	5-5
這邊講的例子是大m 等於三	5-5
於是這個peak 發生在什麼時候	5-5
我去同時調p 跟q 的話	5-5
會在p 跟q 各是三分之一的時候	5-5
那個時候會最大	5-5
那如果是四的話呢	5-5
那我我最大的應該就它們各是四分之一的時候	5-5
那就是等等	5-5
那麼因此也就是不管是大m 等於幾	5-5
它的這個上限都它的peak 就是這一點	5-5
這一點	5-5
或者說是這一點	5-5
都發生在它們equally probable 的時候	5-5
或者說呢就是h of s	5-5
is maximum s when all symbols are equally probable	5-5
也就就是說我的p 的x i	5-5
都等於大m 分之一	5-5
那麼我的每一個symbol 它們的機率都一樣	5-5
就是這邊的的這m 個symbol 呢它們機率要完全一樣都是m 分之一	5-5
當它們的機率有都完全一樣都是二m 分之一的時候	5-5
它的這個它的這個這個entropy 呢是maximum	5-5
那個時候呢就是各是m 分之一	5-5
那如果是m 分之一的話呢那這很容易易算	5-5
我們帶回去算帶回去這個式子的話	5-5
你就知道	5-5
每一個的機率是log 的m 分之一	5-5
然後呢它們的然後平均的話呢	5-5
這個這個然後平均我我我一樣都是都是乘以個m 分乘以m 分之一嘛	5-5
然後加起來加m 個嘛	5-5
所以平均起來就是這個值	5-5
平均起來就是這個值	5-5
因為每一個每一個都是m 分之一嘛阿	5-5
因為每一個就是log 就是這個就是log m 嘛	5-5
負的log m 嘛	5-5
每一個就是log m	5-5
然後再平均起來還是log m 嘛	5-5
所以呢我所得到的就是log m	5-5
所以它的上限就是log m	5-5
就剛才而言就是log 二就是一點零	5-5
那如果是三個的話呢	5-5
那這上面就是log 三	5-5
就是log 三	5-5
如果四個的話呢就是剛才這個例子的話呢	5-5
那這個例子的話呢就是log 四就是二嘛等等	5-5
所以呢我的上限就就是log m	5-5
發生在什麼時候發生在每一個symbol 都equally probable 的時候	5-5
就是上限就是log m	5-5
那它的下限呢就是零	5-5
你只要有其中有一個的機率是一別的都是零的話	5-5
像這一點	5-5
它全部都是這就是這個全部都是零的情形	5-5
它根本就沒有information	5-5
這點就全部都是一的時候它也沒有information 等等	5-5
那其實你這個m 個裡面只要存在某一個x j	5-5
存在某一個x j 它的機率是一	5-5
而其他的所有的j 都是零的話	5-5
那它就會變成零	5-5
那麼因此它的上限下限就是這樣子	5-5
那這其實是information theory 裡面非常重要的定理	5-5
是該證明的不過我們不證它了	5-5
我們就這樣講一下	5-5
那這個在這個情形之下的話	5-5
那你會說ㄟ那這邊不是零呀	5-5
這邊有有一個不是零的值	5-5
是因為我我永遠有p 嘛	5-5
我這邊已經讓p p 等於這個有一個p 在這裡	5-5
所以這個其實就是你加起來不不是零就是因為有個p 的關係	5-5
那你所以像這個情形的話	5-5
一個是p 一個是一減p 一個是零	5-5
所以在這裡並沒有任何這三個裡面沒沒有任何一個機率是一的	5-5
所以它不會在零的地方	5-5
這裡也是一樣	5-5
沒有任何一個機率是一的	5-5
所以呢並不符合剛才的這個下限的條件	5-5
所以它們都不是零	5-5
它是一個有一個非零的值	5-5
有一個非零的值因為它沒有一個是機率是一的	5-5
那麼剛才在這裡是零是因為它真的有一個是機率是一的的關係阿等等	5-5
好那這個是一個非常基本的	5-5
關於information 量的一個這個簡單的這個這個這個說法	5-5
那有了這個之後我們底下就可以用它來推我們底下要做的事情了	5-5
好我們先在這裡休息十分鐘	5-5
ok	5-5
我們剛才在解釋這個這個式子	5-5
就是這個entropy 這個東西	5-5
那麼它的上限是log m	5-5
下限是零	5-5
上限發生在這個狀況	5-5
它們是equally probable	5-5
下限發生在只有一個是一其它都是零的情形	5-5
這是什麼意思我們如果再衍生一下的的話	5-5
就可以這樣子來看	5-5
我現在如果把這個看成是一個機率p of x i	5-5
這是x one x two 等等到x 的m	5-5
那麼這個就是我的我的這這m 個我們之前講的這m 個可能的值它們的機率就是p 的x i	5-5
那麼這樣子畫是什麼case 呢	5-5
這樣子畫的case 就是我們剛才講講的發生上限的情形	5-5
也就是也就是在嗯這裡	5-5
那麼如果他們各是m 分之一的話	5-5
那這個時候我的我的這個所以每一個機率各是m 分之一	5-5
那這個時候我的entropy 呢其實就是log 的大m	5-5
那這是它的上限	5-5
下限是什麼呢下限是只有其中的一個是一	5-5
對不對其中那個是一	5-5
其它全部都是零	5-5
其它根本不會發生那	5-5
如果只有那個是一的話呢其實它不帶任何information 因為你猜都猜得出來這樣子	5-5
所以這是x one 這是x m	5-5
那只有其中的某一個x j 是一	5-5
那這個呢就是它的的entropy 就是零	5-5
那就是我們這邊所看到的下限	5-5
那在這個上限跟下限的的中間那當然還有很多個case	5-5
你可以想像的是哪些case 呢我們舉例來講	5-5
它不再是完全一樣m 分之一而是有多有少	5-5
但是都有	5-5
有多有少	5-5
這樣我總共這是x m 這是x 一	5-5
它有多一點有少一點這樣子的話呢	5-5
那它是在比它那這是什麼情形	5-5
就好比我們剛才看到的	5-5
從這點向下移動嘛對不對	5-5
這點pick 的這點就是剛才的上限	5-5
就每個都是m 分之一的話等於是在這點上面	5-5
那你現在有有有有多有少它就在這邊來在這邊來	5-5
所以以呢平均起來就在這些地方	5-5
所以那就是這個情形	5-5
那再來的話呢	5-5
它可能慢慢集中到譬如說說若干個這附近的	5-5
譬如說這裡有幾個或者這邊有幾個	5-5
其它的很少很少	5-5
這個是x m	5-5
這個是x one	5-5
如果它集中在少數幾個而多數都變成零了的話	5-5
那就更少一點	5-5
那如果再過來的話呢	5-5
它可能就只集中在在這附近有一點	5-5
其它都沒有了	5-5
這是x m	5-5
這個x one	5-5
ok那你可以想像差不多是一個這樣的情形	5-5
就是由完全的uniform distribute 的	5-5
簡單講就是uniform distribute 的	5-5
或者是equally probable 的時候	5-5
這是最大的	5-5
然後慢慢的它開始有大有小了	5-5
然後慢慢集中在一個比集中在少數的地方了	5-5
然後別的地方會變成零了	5-5
最後集中在一個了	5-5
這就變成零	5-5
所以這就是它的這個分佈的情形	5-5
那你如果是這樣子看那你就可以猜的出來	5-5
它這個東西其實有非常豐富的意思	5-5
它可以拿來做很多用途	5-5
其實我們講了半天我們底下已經也就是要用這個	5-5
用這個的意思你可以看成是在最底下的是什麼是純度最大	5-5
純度最高	5-5
什麼叫純度最高就是說它的distribution 集中在一點上面	5-5
其它都沒有	5-5
這非常純的	5-5
這純度最高的	5-5
那當然然在上面就是純度最低的就不純了嘛	5-5
就完全的是完全沒有有純度可言	5-5
那在這上面呢這什麼我們可以說是亂度最大	5-5
所以這個可以描述一個distribution 純度或者亂度	5-5
那麼這邊是最亂的嘛	5-5
最完全最混亂的	5-5
你每一個都有一樣的機率	5-5
而這邊是最不最不混亂最純的	5-5
完全就是一種情形	5-5
那你也可以說這個是是這個不確定性最高	5-5
因為它的它的每一種都都一樣的可能所以最不確定嘛	5-5
這邊是最確定嘛	5-5
這個這個底下呢這是確定性最高	5-5
那我也可以說是這個它的distribution 的這個分這個嗯等等	5-5
那所謂的這個就是uncertainty	5-5
那這所謂的純度就是purity	5-5
對不對	5-5
這個那所謂的純度就是最高就是指highest purity	5-5
那亂度最大就是highest randomness	5-5
這是random 的情況最高	5-5
所謂不不確定性就是uncertainty 最高對不對	5-5
那然後我也可以用這個來說就是一個這個the spread of a probability distribution	5-5
我也可以說它就是代表the spread of a probability distribution 也就是說它的分散集中的的程度	5-5
一個機率的分的分分佈的分散集中的程度	5-5
分散或集中的程度	5-5
ok	5-5
那麼因此呢我們我們後面會一再的用到這個東西	5-5
其實是就是在用這件事	5-5
就是我只要算出這個東西來的話	5-5
其實就是給我一個distribution	5-5
給我一個p of x i 這個東西的話	5-5
那我來算這個東西	5-5
其實算出來就可以告訴我either 是算它的randomness	5-5
或者是算它的uncertainty	5-5
或者是算它的purity	5-5
或者是算它的spread	5-5
那這個是最不spread 嘛這是最集中在一點上	5-5
那這是最spread 的情形	5-5
所以算它的這個這個分散集中的程度等等	5-5
那我都用這個來做	5-5
好有了這個之後	5-5
底下這一頁倒是沒有太多複雜的東西	5-5
這個看起來很複雜其實倒只是一些簡單的數學	5-5
它在講兩件事情	5-5
這都是後我們後面要用的現在利用這個機會就講的	5-5
那第一個所謂的jensen's inequality	5-5
這個沒有什麼特別只是我們在算的那個東西其實就是p log p 嘛	5-5
你現在如果再回去看剛才這個式子	5-5
我算的這個東西哦ok 我們還漏了那麼為什麼這個東東西叫做entropy	5-5
那我們現在取取這個東西的名字叫做entropy 的意思就很清楚	5-5
entropy 原來是熱力學裡面的一個名詞	5-5
那你如果去看熱力學裡面的講它也就是在講那些分子它的這個混亂的亂度	5-5
所以entropy 其實就是在講亂度	5-5
那麼因此呢	5-5
那麼在熱力學裡面的那個entropy 它它計算公式也就是這個	5-5
就是這是一個機率	5-5
p of x i 這是一個機率的distribution 就是這個東西	5-5
這個東西就是p of x i	5-5
那麼你如果把p 的x i 乘取了log 之後再乘以p x i	5-5
然後去加起來的話	5-5
就是p log p 嘛	5-5
p log p 然後summation over i 的這個式子	5-5
在熱力學裡面	5-5
也就是用這個式子來算	5-5
的這個entropy	5-5
所以這個就把熱力學那個公那個名詞借過來	5-5
那麼這邊的意義其實跟熱力學裡面一樣	5-5
也一樣是在描述亂度跟純度	5-5
描述它的uncertainty 等等	5-5
都一樣的的意思	5-5
那麼因此	5-5
我們這邊才會有這個說法	5-5
就是說呢	5-5
它也叫做degree of uncertainty	5-5
它也可以說是quantity of  information	5-5
也就叫做entropy	5-5
那對一個random variable 而言呢	5-5
有一個distribution 我都可以這樣算	5-5
然後都可以得到這個東西來算它的這些東西等等	5-5
好那麼有了這個的話呢	5-5
那這個jason inequality 只是我們後面要用的一個不等式	5-6
那沒有什麼特別就是這個p log p	5-6
就是我們算entropy 的式子	5-6
給我一個機率	5-6
一個機率的分佈一個distribution 就是這個東西	5-6
我取log 之後分別對每一個p 去加起來	5-6
這就是p log p	5-6
所以這個式子其實就是我們剛才的	5-6
這個h of s 這個entropy 的式子就是這個式子	5-6
那麼它說呢	5-6
你如果把這個p 換成另外一個q 的話	5-6
一定會比較大	5-6
嗯就這樣子意思	5-6
我們算entropy 是p log p	5-6
可是你如果換成另外一個distribution q 的話	5-6
一定會比較大	5-6
q 是another probability distribution	5-6
所以q 一定也是要加起來等於一的	5-6
加起來要等於一它也是正的	5-6
所以是另外一個機率的distribution 不過不是p 就是了	5-6
那你只有把這個p 換成一個q 的話	5-6
這個p 換成q 的話都會比較大	5-6
什麼時候等號成立	5-6
當q 等於p 的時候等號成立	5-6
這個叫做jason inequality	5-6
那底下沒什麼特別它這只是在証明	5-6
這個証明其實很簡單這個証明其實就是用這個式子就是log x	5-6
永遠小於x 減一	5-6
這個是沒有什麼特別	5-6
你在學數學的時候就學過這個	5-6
就所謂的log x 永遠小於x 減一	5-6
其實有很多種証明的方法	5-6
然後等號成成立是在	5-6
只有那一點等號成立	5-6
它就這樣了這個是log x	5-6
這個是x 減一	5-6
x 減一是一條直線	5-6
那然後呢它們兩個只有在一點等號成立	5-6
就是x 等於一的時候等號成立	5-6
那別的時候它都比它小	5-6
那就用這個式子	5-6
那麼我現在的x 呢	5-6
就把它寫成q 除以p	5-6
q 的x i除除以p 的x i	5-6
當成是x 的話	5-6
那麼log 的這個x 呢就會小於等於x 減一	5-6
然後我分別都乘上p 的xi 去summation over i	5-6
它仍然是小於等於它	5-6
ok	5-6
然後呢	5-6
你右邊的這個式子就是等於零的	5-6
原因是這個這個兩個一消掉了嘛	5-6
就是q q 的x i 減一的summation	5-6
那個加起來這個q x i 加起來也是是一嘛所以就等於零了嘛	5-6
所以呢	5-6
左邊這個式子就小於等於零	5-6
左邊這這個式子就是p log p 減掉p log q 嘛	5-6
嗯p log q 減掉p log p 嘛	5-6
那就得到上面這個式子哦	5-6
所以這個沒有什麼這只是一個証明而已	5-6
它就是用這個log x 小於等於x 減一的這個式子	5-6
然後只有一個等號成立就是x 等於一的那一點	5-6
它用這個	5-6
然後把x 代q 除以p 代進去	5-6
然後就這樣可以証明它等於這樣的意思就是了	5-6
那它的簡單的解釋它的說法就是說呢	5-6
你本來算entropy 應該是p log p 的	5-6
但是你如果把那個p 算做另外一個機率的話	5-6
你弄錯了用另外一個機率來算的話呢你entropy 一定是增加的	5-6
或者說呢你用一個估計不正確的distribution 去算	5-6
你得到的degree of uncertainty 是增加的	5-6
你可以用這個方法來解讀這個式子的意思	5-6
不過基本上這只是一個一個不等式我們後面會用它來証明一些東西就是了	5-6
我們只是這樣而已	5-6
那同樣的有了這個東西我們也可以寫成另外一個式子	5-6
你看這個底下這個式子的話呢這個p log 的的p 除以q	5-6
其實就是這個一樣的不過倒過來寫就是了	5-6
那我們剛才証明這個東西小於等於零現在這個倒過來就這個大於等於零	5-6
那這個p log p 減掉p log q	5-6
它一定是大於零的東西	5-6
這個東西呢我們給它一個名字	5-6
叫做cross entropy	5-6
或是relative entropy	5-6
就是如果你現在有兩個distribution 一個叫做p 一個叫做q 的話	5-6
那我們就可以算它們的這個cross entropy	5-6
或者relative entropy	5-6
就這樣算	5-6
那這個其實是在算兩這是算兩個distribution 之間的distance	5-6
這個有一個名字就是所謂的k l distance	5-6
那麼換句話說其實	5-6
你給我兩個distribution 我可以算很多東	5-6
算很多種distance	5-6
假設這是一個這是一個p of x	5-6
那我給你給我另外一個q of x	5-6
它是另外一個	5-6
這是q of x	5-6
那麼q 跟p 之間是有是有差距的	5-6
到底它們的差距是多少	5-6
怎麼算p 跟q 的distance	5-6
這有很多種算法你如果去查	5-6
查書的話	5-6
given 一個q 跟一個p 怎麼樣算它的distance	5-6
有很多種distance 的算法	5-6
有很多種definition	5-6
各有它不同的意思	5-6
其中的一種叫做k l distance	5-6
就是這個	5-6
kull back leibler	5-6
這個這個distance	5-6
那這個distance 算法就是用這個來算	5-6
那其實即使是這個distance 也有好幾種算法	5-6
那有很多個版本	5-6
那這個distance 那你看你可以看得出來它的意思就是說	5-6
你去算它的entropy	5-6
因為給我一個distribution 本來就可以算entropy	5-6
那我如果算entropy 的話那我當把其中的一個	5-6
當成另外一個的時候	5-6
它的它的會entropy 會差多少	5-6
就表示它們之間的就表示entropy 會差多少嘛	5-6
就是p log p 跟p log q 會差多少	5-6
那就是它們之間的差異	5-6
所以這個呢是等於是這個它們的它們之間的這個這個uncertainty 的差異	5-6
那麼這個	5-6
那麼做為一個它們之間的這個這個distance嗯	5-6
這個也是我們後面會用的就是了	5-6
那麼嗯在這裡的話呢當然我們這邊都是用discrete 的版本來解釋	5-6
當然這個都可以變成continuous distribution	5-6
你現在給我我一個continuous distribution 我也一樣可以做這件事情	5-6
那這個我們就不多講就是了	5-6
好關於這個我們就就說到這裡	5-6
底下我們就要來講我們真正要用的是	5-6
用在底下這裡	5-6
那這是另外一件件事情就是所謂的cart	5-7
就是classification and regression trees	5-7
這個在嗯這是另外一個領域裡面一個重要的方法	5-7
這個領域嗯這個東西在嗯either 是這個data mining 裡面	5-7
or 是machine learning 裡面	5-7
or 是pattern recognition 裡面	5-7
它們大概都會講一堆這種這種東西	5-7
那我們等於是說把它拿來用	5-7
那麼嗯這邊舉的是一個非常簡單的例子來說明這個在幹嘛	5-7
那底下我們是要用它來做tri phone 的train tri phone	5-7
而train 的時候要用什麼	5-7
要用剛才的entropy	5-7
所以我們在講的這堆東西是這樣來的	5-7
好那我們先說這個是在幹嘛我們們這個這個是用一種tree 的structure	5-7
像這樣一種tree 的structure 來幫助我們來這個把一大堆的data 把它如何把它結構化	5-7
讓它的structure 清楚哦等等	5-7
這是什麼意思我們舉這個例子來說明這個例子其實是課本的例子	5-7
你如果看課本的話在我這邊的給你的這個課本的reference 裡面哦	5-7
哦在這個裡面	5-7
它就是就是就有這個例子	5-7
那我其實就把課本上的例子拿下來講就是了	5-7
它說假設說這個區公所裡面拿到一群人的data	5-7
這個區公所裡面有這一區的這一鄰或者這一里的居民的data	5-7
那它要根據這一群的data 把這一群人呢分成根據他們們的身高分成五群	5-7
最高的一群	5-7
普通高的一群	5-7
跟中等身材的一一群	5-7
跟稍微矮的	5-7
跟最矮的	5-7
分成五群	5-7
那區公所的這一群人的data 它有很多很多資料	5-7
譬如說他的年齡他他的性別他的工作等等等等	5-7
就是沒有身高	5-7
那你就要根據你所知道的訊息譬如說年齡啊性別啊工作等等	5-7
根據這些去判斷說沒有身高那你現在要根據身高來分成五群怎麼分	5-7
那這個時候我們要做的事情其實是跟剛才講的是很像的	5-7
是這樣的一件事	5-7
那麼我們姑且可以想像成說	5-7
ok 這些人的身高也是有一個distribution 的	5-7
那這個distribution 也許不太容易看	5-7
我們就把它當成是discrete	5-7
那麼最矮的人的身高是叫做 x x one	5-7
最高的人叫做x m	5-7
那麼他們各有一定的distribution 在這裡	5-7
那我現在要把這些人的身高	5-7
所以所以這些這些就就是它們身高的distribution	5-7
我要把它們身高分成這五群	5-7
那最高的這一群在這裡譬如說	5-7
這是最高的大t	5-7
然後這邊呢是是小t 是第二高的	5-7
中間這些是中等身材的m	5-7
然後這邊是比較矮的	5-7
叫做s	5-7
然後呢這邊是最矮的這群叫做大s 等等	5-7
我等於是要做這件事情	5-7
我要把它們的這個這個根據他們的身高分成五群	5-7
那我有很多data	5-7
有的是說他的年齡有的是說他的性別說他的工作等等	5-7
就是沒有身高	5-7
我有根據這些其它的data 來分他們的身高	5-7
那怎麼分呢	5-7
你想個辦法做一個tree	5-7
那就他這個case 而言他說呢ok	5-7
我先說他的年齡是不是大於十二歲	5-7
這個理由是說假如小於十二歲就是小孩	5-7
小於十二歲的話大概是小孩小孩總是最矮的	5-7
所以呢凡是呢所以呢如果說凡是小於十二歲的話呢	5-7
就歸成最小的這一類最矮的	5-7
那大於十二歲以後不表不表示他一定最高	5-7
這時候怎麼辦呢我們再來分	5-7
那第二個呢他就說呢我根據什麼來分呢	5-7
根據他的職業	5-7
如果他是職業籃球隊的哦那顯然是最高的	5-7
所以呢只要他是職業籃球隊的我就分到就是屬於這個	5-7
那就是最高的	5-7
那麼如果那這是把最矮的跟最高的分出來了	5-7
那中間這堆人怎麼辦呢不知道	5-7
不過呢他有一個data 他說呢每週喝多少牛奶	5-7
ok 那supposed 每週喝牛奶喝很多的會比較高	5-7
所以呢每週喝牛奶多的呢就屬於第二高哦就屬於這一層	5-7
那那每週喝牛奶喝完之後怎麼辦呢	5-7
那剩下呢他就說看他是男的還女的	5-7
那這一群人可能男的比較高女的比較矮	5-7
所以呢他就如果又不是小孩對不對	5-7
又不是小孩所以不然如果小孩就男生可能特別矮	5-7
又不是打職業籃球隊籃球隊的話女生也會很高對不對	5-7
那到了這群人的時候那大概就是ok 男生會比女生高	5-7
所以呢男生的話呢就是屬於中間這群	5-7
女生就屬於稍矮的這群哦這樣子	5-7
那它這樣就構成一個tree	5-7
也就是說我在這個tree 的每一個note 是根據一個question 來分	5-7
說它是屬於哪邊這樣把它分出來	5-7
那問題是你這個tree 夠不夠好	5-7
怎麼樣才能做到一個有效的做法呢	5-7
那一個可以做的方法就是用我們這邊講的這個entropy	5-7
那這個entropy 是幹嘛呢其實就是在做我們這邊講的事情	5-7
那你可以想像成	5-7
我們剛才那個tree 是怎樣	5-7
就是把一個node 拆成二個node	5-7
不斷地在做這件事情	5-7
假設我原來這個node 是叫做n	5-7
我把它拆成a 跟b 的話	5-7
它不斷地在做這個用一個question 或者用一個criterion 來判斷	5-7
把那群人拆成兩半	5-7
它就不斷在做這件事情之後構成一個tree	5-7
最後把它拆成剛才看到這五群	5-7
那它怎麼樣做這件事呢	5-7
那基本上這個你可以想像	5-7
最理想的情形是說	5-7
我們如果畫小一點它這樣子拆成這樣子	5-7
剛才這樣子拆法	5-7
我一開始上面的所有的人都在這裡	5-7
但是我會希望最後拆出來的時候呢在這邊的是什麼	5-7
是某一群	5-7
就剛好是譬如說這一群	5-7
其它都是零	5-7
剛好就是這一群	5-7
那這邊呢剛好是另外一群	5-7
其它都是零	5-7
對不對	5-7
那這邊剛好是另外一群	5-7
最好是這樣子	5-7
就是這一群其它都是零	5-7
也就是說如果要這樣子做的話呢	5-7
很可能就是我希望每一次一拆的時候	5-7
就是從這邊切開來或者從這邊切開來或從這邊切開來	5-7
我每一次如果我每一次的這個split 這個node	5-7
這個node 如果這樣子split 的時候每一次都是可以這樣子切這樣子切這樣子切的話	5-7
那它就一塊一塊就切出來了	5-7
可是我如果是做得不好會切成這樣子的話呢	5-7
就不好了	5-7
這意思是說我如果每一個這樣split 的時候呢我是從這裡再這裡這樣切或者這樣子切的話	5-7
都有效我這樣的話就就就就就可以把它這樣一次這樣切一次這樣切把它切開來	5-7
就得到一群一群的	5-7
可是我如果這個node 做不好的話我如果弄了一個這樣子切的話呢	5-7
這樣一切就一點用都沒有	5-7
等於白切了	5-7
那麼舉例來講	5-7
它如果區公所用的一個是譬如說這個學歷	5-7
學歷可能就跟身高沒有關係	5-7
它這邊用的都是剛好跟身高會有關係的	5-7
所以你這樣一切的時候看我儘可能我儘可能是希望是這種的對不對	5-7
我如果弄個學歷的話搞不好大學畢業的有最高的也有最矮的	5-7
那麼幼稚園畢業的也有從最高到最矮都有嘛	5-7
那所以你這樣如果用學歷這樣一切的話	5-7
可能就沒有意義了	5-7
那當然實際上你可能雖然你要避免切成這樣切法	5-7
但你要真的這樣切可能是做不到的	5-7
那你真正會做成怎樣的你很可能會做成這樣子	5-7
就是我切一個可能是這樣子切的	5-7
譬如說那麼於是呢比較多的在這邊	5-7
這邊比較少	5-7
但是你不至於說完全沒有啦	5-7
嗯就是就是你可能切成一個這樣子的	5-7
那那我們怎麼來想這件事情呢	5-8
那我們就來define 我們要做的事情其實就是要算這個entropy	5-8
因為所謂的你希望直的切下來	5-8
你希望每一個直的切下來是意思是什麼	5-8
就是我希望純度	5-8
就是我希望這個純度啊我要	5-8
我原來是亂度	5-8
對不對我一開始的時候是亂度最大上面這個case 嘛	5-8
我一開始是最上面的是最亂的	5-8
然後呢我希望把它如果這樣子一刀一刀切下來的話呢	5-8
越到底下我越純	5-8
越純的話呢就是越好	5-8
那從entropy 來講呢就是我的entropy 要越變越小	5-8
我entropy 的越小的越多就是越純嘛	5-8
越小的越多就是越走向純嘛	5-8
所以呢我就是要把entropy 降得越多越好	5-8
因此怎麼辦呢	5-8
我就是為每一個node 去算在這裡的entropy	5-8
我這個可能是在這上面這裡	5-8
我就可以算在這裡的entropy	5-8
然後今天如果有有有一有一個辦法讓我切一刀的話	5-8
那我不知道我這一刀切出來到底是切成這樣還是切成這樣	5-8
還是切成這樣	5-8
那我就是算這兩個的entropy	5-8
那就是底下這一頁所講的事情	5-8
也就是說這一個node 我算它的的這個什麼東西p 這個p log p	5-8
其實就是entropy	5-8
這裡的不太清楚這這個c sub i 這應該有一個下標是i	5-8
c sub i 除以n 就是percentage of data sample	5-8
for class i at  node n	5-8
也就是說	5-8
也就是說嗯在這裡	5-8
這裡每一個就是一個每一個cross 就是一個i	5-8
它的data 的它的有幾個人	5-8
我們算人數好了就是data sample 的數目就是有幾個人嘛	5-8
那就是c sub i	5-8
那我總數呢就是n	5-8
total 的人數是n	5-8
嗯如果我們用剛才的人數來算的話	5-8
total 的人數是n	5-8
這是第i 個人數的cross 是c sub i	5-8
你如果這樣子看看的話呢那麼這個c sub i 除以n	5-8
其實就是一個機率嘛	5-8
所以它說percentage of data sample for class i at node n	5-8
嗯嗯嗯等一下	5-8
這個n 不是總數	5-8
n 是指這個node 的意思	5-8
c sub i 是這裡的data 是沒有錯	5-8
然後n 是指這個node 這個node	5-8
所以呢所以這個n 呢就是在這個node n 這邊在node n 這邊的所有的data	5-8
然後我就就去算對cross i 的人數的percentage 就是一個機率嘛	5-8
所以呢就是這些機率	5-8
所以這裡的每一個就是所以這裡的每一個機率其實就是p 的c sub i	5-8
at node n 就是它的機率	5-8
因此呢我這邊所謂的這個p 然後乘以log p 再加起來這件事情	5-8
就是在算這裡的entropy	5-8
當這裡有個entropy 這裡這樣算之後我現在來切	5-8
用某一個方法	5-8
根據某一個criterion 把把它切成兩個的時候	5-8
那個depend on 我怎麼做對不對	5-8
我們說最理想的是這樣切	5-8
這樣的話呢就左邊跟右邊這樣是切得最乾淨的	5-8
但是你很可能做不到這樣	5-8
那你做到的可能是一個這樣	5-8
譬如說這樣	5-8
於是呢你左邊呢可能會變成是這邊這樣然後很小了	5-8
右邊這個會變成是這樣然後這邊很小了	5-8
這邊都不是沒有	5-8
只是少而已	5-8
那這樣也不錯啦這也是進步啦對不對	5-8
所以呢我這邊可以得到一個這個b 的	5-8
這邊可以得到a 的	5-8
啊都一樣	5-8
這就是在node b 這邊有一個distribution	5-8
我也可以算它的entropy	5-8
這邊的在node a 這邊有一個distribution 我也可以算它的entropy	5-8
那我希望這個entropy 從這裡到這邊呢是降下來的	5-8
那降得越多就越好嘛	5-8
那降得越多就表示我這一刀切得越接近這樣子切法	5-8
對不對你如果這樣子切的話就等於沒有降低	5-8
那就等於是白做了ok 哦	5-8
那麼因為這樣的關係我現在就可以定義這個delta 的entropy 就是entropy reduction	5-8
就是在node n 減掉node a 加b 的	5-8
就是我本來在這裡的時候entropy 是多少	5-8
變成這樣經過這個criterion 切下來之後	5-8
變成這兩個那它的entropy 是多少	5-8
那這個時候呢這兩個entropy 的差就是我的delta 的delta 的這個這個的這個entropy 就是entropy reduction	5-8
看看我減多少	5-8
那麼於是呢我就可以選擇一個在每一個地方我都選擇一個最好的切它的方法	5-8
就是一個所謂的question	5-8
當然不見得是所謂的question 應該就是一個criterion 你怎麼切它的一個切它的一個criterion	5-8
不過在我們剛才的例子裡面我們都用entropy 都用question 來解釋	5-8
譬如說年齡是不是大於十二	5-8
他的的他的職業是不是職業籃球隊的	5-8
它每週是不是喝這麼多牛奶	5-8
這都是用一個question 哦	5-8
我們就稱它為question	5-8
那其實不見得是一個question 就是一個criterion 就是了	5-8
當我把它用用這個一堆我有一堆存在的criterion	5-8
但是到底用哪一個criterion 在哪裡切	5-8
我也可以一開始就先選男生還還是女生	5-8
我也可以一開始就看根據它的牛奶	5-8
我也可以一開始就根據它的職業	5-8
嗯你憑什麼要先選哪一個呢	5-8
那就根據看我在根據這裡來看	5-8
我從頭開始	5-8
到底我手邊有的data 裡面	5-8
哪一個讓我一拆開來	5-8
entropy 降低最多	5-8
就表示我的程度增加最多	5-8
那到了這裡我再來看還有什麼東西讓我降低最多的	5-8
我就從那裡來拆	5-8
這裡我再看哪個讓降低最多我從那裡來拆等等	5-8
所以我每一次都選擇我每一次都選擇entropy 降低最多的那一個	5-8
來做這件事	5-8
所以呢我這個h n  的entropy 減掉a 跟b 的entropy	5-8
做為我的 entropy reduction	5-8
然後它是一個function of q	5-8
就是我的question	5-8
或是我的criterion	5-8
因此我每一次呢就在所有的q 裡面去選擇那個降低最多entropy 的那個question 從那裡開始切哦等等	5-8
那這裡還有一個小問題	5-8
就是你如果光是這樣做的話會發現不對	5-8
為什麼不對呢	5-8
這個我們舉個簡單的例子	5-8
如果說如果說一開始是完全uniform 的	5-8
總共是m 個	5-8
那這個就是log m	5-8
這個entropy 就是log m	5-8
假設我選了一個非常笨的方法	5-8
它這一刀是這樣子切的	5-8
那切完之後呢在這邊也仍然是一個log m	5-8
在這邊也仍然是一個log m	5-8
對不對所以這邊也是log m	5-8
這個也是log m	5-8
這應該表示說這個這一刀是白切了	5-8
entropy 完全沒有改變	5-8
可是你現在如果算一算的話呢	5-8
這個h a 加h b 是兩倍的log m	5-8
上面是一倍的h m log m	5-8
嘿它還增加了	5-8
這邊是一個log m 這邊變成兩個log m 這不太對吧	5-8
那為什麼不太對呢你仔細想一想呢其實是應該怎麼樣	5-8
我這邊假設這邊有一百個人	5-8
其實是六十五個人到這來三十五個人在這裡的話	5-8
應該是這個乘以零點三五	5-8
這個乘以零點六五	5-8
這樣比較合理嘛	5-8
對不對	5-8
也就是說假設這邊有一百個人你這邊後來是六十五個人到這裡三十五個人到這裡的話呢	5-8
你應該是這邊兩個雖然都都等於白切了	5-8
是兩個都是log m	5-8
你不可能變成兩倍的log m 嘛那變成還變大了嘛	5-8
沒有變大只是一樣而已等於沒有做而已	5-8
那其實這邊的entropy 應該是它乘以零點六五它乘以零點三五加起來	5-8
那就是跟它一樣	5-8
表示沒有表示沒有做任何事	5-8
那麼因此你後面還要乘這個東西	5-8
還要乘這個比例才對	5-8
那後面的這個比例就是這個p 的n	5-8
ok 所以p 的n 呢就是prior probability probability of n	5-8
也就是說有在你那個node 那裡	5-8
到底out of total number of sample 有多少	5-8
就像total sample 是一百個人	5-8
你現在有六十五個人在這裡三十五個人在這裡的的話呢	5-8
這個零點六五跟零點三五就是這個比例	5-8
就是這個p 的a 跟p 的b	5-8
這個就是p 的a 這個就是p 的b	5-8
那這樣的話呢乘在這個裡面這樣就不會錯了	5-8
嗯所以呢我們這邊還多了一個這個東西	5-8
所以這個就做weighted entropy	5-8
我還加一個weight	5-8
那這樣的話呢就合理了	5-8
那底下這段沒有什麼特別	5-8
它只是說呢這個delta entropy 就是這個entropy reduction 這個東西呢你可以証明它其實是這個式子	5-8
那這個東西是什麼呢這個東西就是我們剛才所說的那個cross entropy	5-8
我們上一頁不是說過這個嗎	5-8
嗯再上一頁	5-8
就是這個東西	5-8
你今天如果給我兩個distribution 一個是p 一個是q 的話	5-8
那麼它們的cross entropy 就是p log p 除以q 的這個東西	5-8
或者p log p 減掉p log q 的這個東西叫做這個東西	5-8
你現在如果那這只是一個式子它說你可以証明它	5-8
那麼這個entropy 的reduction 是跟這個有關的	5-8
那這個a of x 就是到了這邊的的distribution	5-8
b of x 就是到了這邊的distribution	5-8
那麼它們分別跟n of x 就是	5-8
原來在這裡的distribution	5-8
它們到底改變了多少	5-8
這個就是剛才講的那個那個entropy 的這個cross entropy	5-8
或者說是它的k l distance	5-8
從這邊變到這邊到底distance 改變了多少了那個distance	5-8
那你其實就是這兩個distance	5-8
那你這個這個可以想像	5-8
這個物理意義是合理的嘛	5-8
就是我原來原來這邊是n 啊	5-8
原來這個就是n of x	5-8
當我這樣一切之後這個叫做a of x	5-8
這個叫做b of x	5-8
那因此呢我就是算a 跟n 差改變多少的k l distance	5-8
跟b 跟n 改變多少的k l distance	5-8
然後分別weighted by 它們的這個p a p b 就是這個東西	5-8
就是它們的weight	5-8
那這就是我真正改變的entropy	5-8
那麼如果是這樣子的話呢那那那我就一路去把它降低就對了	5-8
那它這邊也提到說我這邊都每一次都算這個p n	5-8
我這邊算p a p b 就算這個零點六五零點三五這個東西呢	5-8
它有另外一個意義也是很合理的	5-8
就是我其實是是把reliability of the data 算進去	5-8
我量越多的數目越多量越多的這個這個統計特性越可靠	5-8
所以我就weight 比較重嘛	5-8
weight 比較數目比較少比較不可靠我就weight 比較較輕嘛	5-8
所以這個其實也是把把這個statistics 的reliability 算進去	5-8
那這樣的話呢你就可以發現我現在每一步	5-8
都在讓我這個tree 不斷不斷地這個repeatedly reduce 我的entropy	5-8
那也就是說我等於是每一次如何split 用什麼方法來split 都是選擇一個 entropy 降低的最多的那個來拆	5-8
然後這樣一路拆下來一路拆下來	5-8
所以呢我每拆向下走呢我的entropy 都在降低之中	5-8
那這棵樹	5-8
整棵樹的的entropy 呢	5-8
就是它的最後的terminal 的entropy	5-8
那最後一路拆拆拆拆到最後的這個	5-8
在terminal entropy 就是我整棵樹	5-8
那當然我這個terminal 如果如果到最後的每一個純度最高的話entropy 一定最小	5-8
那這樣是最理想	5-8
所以呢這就是這樣的意思	5-8
好那如果這一點比較了解了的話	5-8
那我們現在可以來看我們現在怎麼來做tri phone	5-8
那麼要做tri phone 的時候呢	5-8
這個好像該休息哦我們休息十分鐘吧哦	5-8
 OK 我剛才漏掉說一件事就是 哦	5-8
就是這個 cross  entropy 的這個東西是不對稱的	5-8
嗯這 ASYMMETRICAL	5-8
也就是說	5-8
這個 p 跟 q 的位置如果是反過來是 答答案就是會反嘛	5-8
你會看到因為這是 p log p 除以 q 嘛	5-8
所以如果這兩個倒過來的話就變成 q log q 除以 p 是不一樣的東西	5-8
哦 所以它是一個不對稱的叫做 cross  entropy 這是一個不對稱的 measure	5-8
那麼 但是呢有人把它拿來當做 K L  distance 當成 distance 的時候呢 distance 常常覺得應該是對稱的啊	5-8
哦這個 a 跟 b 的 distance 好像應該 b 跟 a 的 distance 應該是一樣應該是對稱的啊	5-8
所以你當然也可以去定義一個對稱的也可以	5-8
那麼最常用的一種辦法就是讓它兩兩個加起來除以二嘛哦	5-8
那這樣就會變成對稱嘛	5-8
在這個定義裡面它是不對稱的	5-8
但是你可以定義另外一個東西就是 d 這個 d 的 p  of  x  q  of  x	5-8
加上 d 的 q  of  x  p  of  x	5-8
然後除以二對不對	5-8
你只要定義一個這樣東西的話它就變成是對稱的	5-8
所以你如果要一個 symmetric 的的 distance 也可以	5-8
就這樣就變成 symmetric 了	5-8
那至於說嗯我們這個地方剛才講說這裡這個當你一個 n  split 成為兩個 node  a 跟 b 的時候	5-8
它們的這個這個 entropy  reduction  turns  out 剛好是這個式子哦	5-8
那這個式子基本上其實就是數學推出來的	5-8
那我剛才講過這個你意義上也可以解釋	5-8
就是你是這個是這個這個嗯 n 跟 a 差多少	5-8
n 跟 b 差多少對不對 嗯	5-8
那這個地方如如果你覺得有點這個其實這個整個的式子的推導是來自這篇 paper	5-8
你如果詳細去看的話	5-8
在那個 paper 裡面會講得很清楚	5-8
就是這一篇啦哦	5-8
這一篇嗯我在上次講有提到過	5-8
這一篇 paper 的作者其實是我們台大資訊系的一位學長從前的博士論文	5-8
當初是他做的	5-8
後來變這個變成經典之作	5-8
所以這邊拿來講就是	5-8
值得你去看一下你如果要看的話哦	5-8
那剛才那個推導或者那裡面相關的討論在那個裡面有	5-8
那基本上你可以看成就是我的 distance 我的這兩個 distribution	5-8
a 跟 b 跟原來的 n 差多少的一個關係就是我這個 entropy  reduction 嗯	5-8
好那麼再來我們來講我們真的怎麼做這件事	5-8
我現在不再講那些東西而是說我們現在真的要要要 train 這個 TRI PHONE	5-9
我們真正回到我們原原來問題是要 train 這個 TRI PHONE	5-9
TRI PHONE 為什麼會有 會會難 train 呢	5-9
你記得我們一開始說過	5-9
TRI PHONE 的問題是在它的數目太大了	5-9
那麼我就算只有三只有六十個 phone 的話	5-9
TRI PHONE 就是二十一萬六千個	5-9
而且很多是看不到的	5-9
是很多很多 TRI PHONE 裡面 data 跟本就沒有 嗯	5-9
就是所謂的 unseen  TRI  PHONE	5-9
就是你的 data 裡面沒有的	5-9
並不是任何一個 phone	5-9
並不是任何一個 phone	5-9
這個左邊接一個什麼右邊接一個什麼你在 data 裡面都可以找得到	5-9
可能 data 裡面都找不到	5-9
那麼因此呢這就很大的麻煩哦	5-9
那以我們國語為例	5-9
我們國語的其實很多 phone 不會連起來	5-9
所以真正會真正有的 TRI PHONE 大概四千六百個左右	5-9
比這個少很多啦哦	5-9
我們真正大概四千六百個左右	5-9
但是你如果拿一個二十小時 database 來數一數裡面的 TRI PHONE	5-9
大概只有兩千個左右	5-9
根本一半都不到	5-9
有一大半根本就沒有 嗯	5-9
所以那就是這這是真正的問題	5-9
就是一大堆 unseen  TRI PHONE 就是你沒有的 TRI PHONE 怎麼辦	5-9
那這個辦法就是我們剛才講了半天的這些東西的辦法	5-9
那怎麼做呢	5-9
那麼我們先看它它它基本上就是什麼呢我我每一個 bass  phone	5-9
包括所有的 possible  context  dependency 的每一個 state 去做一棵 tree	5-9
譬如說我如果總共有五十個 phone 的話	5-9
每一個 phone 是五個 state 的話	5-9
我總共就是兩百五十個 tree	5-9
什麼意思	5-9
譬如說 啊 這是一個 phone	5-9
那麼這個 phone 的話呢它其實左邊可以接很多東西啊很多很多	5-9
右邊可以接很多東西啊很多很多	5-9
對不對那那光是這個就有很多很多個 TRI PHONE	5-9
那這樣子那裡面有有的 TRI PHONE 我就就是沒有我我我無法 train	5-9
那怎麼辦呢那我現在不要那麼多了	5-9
我現在就就只把把所有的啊拿出來	5-9
不管左邊什麼右邊什麼我通通都放在一起得到一個啊的	5-9
那麼假設這個啊我要五個 state 的話	5-9
五個 state 的啊的話我假設說裡面的我要 train 它最後這個 state 怎麼 train	5-9
我就把所有的啊的最可以做為最後一個 state 的所有的 data 拿來	5-9
在這裡做一棵 tree	5-9
在這邊長這個 tree 等等等等哦這是這個意思	5-9
那我現在就是這個啊	5-9
不管他是我現在不管它左邊是什麼右邊是什麼	5-9
我左右的 context 都不管我只管它中間這一個	5-9
然後呢如果光是光是管中間這個的話我要五個五個 state 構成一個 HMM 的話	5-9
我就做五個 state	5-9
然後呢每一個 state 我就做一棵 tree	5-9
那到時候每一個 tree 後面會產生某一些個 TRI PHONE 的這個 state 長怎樣	5-9
哦我們底下看到的是這樣	5-9
所以呢這這這這是第一句話講的意思	5-9
就是說呢我我所謂的 bass  form 所謂的 bass  form 就是包括所有的 context  dependency	5-9
不管左邊接什麼右邊接什麼我都不管了	5-9
我就只管這個 bass  form	5-9
這個 bass  form 裡面假設是要一個 HMM 的話	5-9
那麼它的每一個 state 我去做一棵 tree	5-9
我就 con  construct 一棵 tree  for 每一個 state  of 每一個 bass  form	5-9
因此呢假設我像我啊總共有五十個這樣子的 form 的話	5-9
我就做了就有五十個 HMM	5-9
每一個如果要五個 state 的話	5-9
我就做了兩百五十棵 tree	5-9
OK 好那這兩百五十棵 tree 怎麼來呢	5-9
我們就舉例來講假設是這個啊的這個音的最後一個 state	5-9
這個 tree 的話我就是把所有的阿 所有的啊的聲音要拿來 train最後一個 state 的所有的 data 都放在一起	5-9
那這個時候呢這些 data 放在第一個 node 這裡然後我要開始長這個 tree	5-9
這個時候因為這裡面它可能是所有的這些跟所有的這些都有影響嘛	5-9
因此呢它這裡面的這個時候它的 distribution 是非常複雜的	5-9
現在不再是我們剛才那邊畫的那麼單純	5-9
只有一個 dimension 說有 m 個不是那樣子	5-9
現在是三十九維	5-9
所以是非常複雜的一堆	5-9
尤其現在是因為各種狀況它可以接它可以接各種各樣	5-9
以這個為例我們如果放它說它是最後一個 state 的話	5-9
最後一個 state 顯然是比較受到後面的影響	5-9
後面有各種各樣的的 phone 都在這裡	5-9
所以它就會各種各樣	5-9
所以呢雖然說是這三十九維這裡面是非常複雜的 distribution 一一團亂	5-9
那我希望能夠把它變成一個 tree 之後	5-9
到最後能夠不要那麼亂	5-9
當然我不可能做到說像這邊純度那麼高那麼純	5-9
大概是做不到	5-9
但是我至少要能夠做到就是比較清楚一點譬如說呢	5-9
這個是這樣子的	5-9
那這個呢是另外一個樣子的	5-9
這個是這樣子的	5-9
它們各自不像這邊這麼一團亂	5-9
而至少呢都有一定的純度	5-9
而可能譬如說某些個 TRI PHONE 就用這個	5-9
某些個 TRI PHONE 就用這個這樣子	5-9
OK 我的目的是這樣子	5-9
也就是說因為不可能每一個 TRI PHONE 都會有夠多的 data 去 train	5-9
所以顯然是是有幾個要共用這一個	5-9
那它們的 data 都拿來 train 成這一個	5-9
那另外有幾個要 train 成這個	5-9
它們 data 要拿來變成這個	5-9
那問題是到底哪些個可以該兜在這裡共用 data 共用共同 train 一個	5-9
哪些個該在這裡共用這個呢	5-9
那我就看誰跟誰比較像	5-9
這麼複雜三十九維的我怎麼知道誰跟誰比較像呢	5-9
就用這個辦法	5-9
就去看這個 entropy 由這邊慢慢變到這邊來怎麼樣子 entropy 降到最低	5-9
就可以假設這裡面是最純的	5-9
這個時候如果有三個 TRI PHONE 共用這個的話	5-9
那至少這三個 TRI PHONE 是蠻像所以它們兜在一起純度還是蠻高的	5-9
有兩個 TRI PHONE 共用這個的話	5-9
它們它們兜在一起純度還是蠻高的	5-9
表示它們應該是像的	5-9
OK 所以我的目的是這樣所以我要把這個三十九維的東西這樣這樣子把它把它這個這樣做下來	5-9
那我每一次到底用什麼 criterion	5-9
那麼我們剛才在這裡的時候是用一堆 criterion 來判斷	5-9
然後來判斷哪些人的身高怎樣	5-9
那我現在到底是用什麼東西來判斷呢	5-10
那其實這個這個這想的非常好的辦法就是所謂的 phonetic  knowledge	5-10
什麼是 phonetic  knowledge 就是去找語言學家	5-10
根據語言學家來說哪些東西會影響我的前後	5-10
哪些呢我們舉底下這個例子你比較了解	5-10
這是一個這是一個我們從前做中文的做國語的例子啦哦	5-10
就是假設我是要做ㄅ的第一個 state	5-10
這是講ㄅ的第一個 state	5-10
啊 我這個例子不太好不一致	5-10
就是這是ㄅ的第一個 state	5-10
這晡後面接ㄨ的後面接ㄨ的那個ㄅ的第一個 state	5-10
這是一個 Hidden  Markov  model 的這第一個 state	5-10
我把所有的這個ㄅ的 data 拿來我要用它來做一棵 tree	5-10
這樣子那這個時候我用什麼樣的 criterion	5-10
這都是從語言語言學裡面學來的	5-10
因為既然是這樣的話呢最主要的問題是這是ㄅ的第一個 state	5-10
最影響它的是它到底左邊接什麼	5-10
左邊接什麼才是一個關鍵	5-10
那左邊的接什麼是會什麼呢	5-10
會影響的是語言學上的發音的道理	5-10
所以它左邊是一個母音嗎	5-10
還是子音	5-10
如果左邊是母音的話	5-10
那麼母音又可以分成這什麼什麼這是語音學上的分法	5-10
這是根據語音學裡面 你那個母音發音的時候 它的 發音的位置或者共振的位置或者什麼	5-10
它是在後面一點還是在低一點還是在遠一點這口這什麼什麼等等哦	5-10
那這些東西 really 就會影響	5-10
那你可以想得到就是說我的我我如果說是這個ㄅ的最左邊的這個這個ㄅ的第一個 state 的話會影響它其實最主要是左邊的	5-10
那麼它的左邊會是一哪怎麼樣的一種音	5-10
是母音還是子音是哪一種母音哪一種子音	5-10
然後那種母音那種子音它的口型會怎樣	5-10
它發聲會怎樣	5-10
那些 really 影響到這裡	5-10
所以呢它就去找所有的這種語言學上的這些區別	5-10
用這個來做這些個 question	5-10
所以譬如說在這裡的話呢你先問什麼呢	5-10
它左邊是不是母音	5-10
如果左邊是母音的話就歸到這裡來	5-10
左邊不是母音的話呢那就是 silence	5-10
那因為在在國語而言	5-10
這個ㄅ什麼東西前面不會有不是母音的嘛	5-10
那如果不是母音的話它它就是就是沒有就是 silence	5-10
這就會變成這樣子	5-10
然後如如果左邊是母音的話那再來看三十說左邊是不是一個 low  vow	5-10
如果是的話大概是屬於這幾個	5-10
如果不是的話是屬於這些	5-10
那這樣子一路再分下去	5-10
那麼看它左邊是什麼右邊什麼再看它是是屬於這個還是這個	5-10
於是分到最後就會發現	5-10
譬如說這兩個可以共用同樣一套	5-10
因為它們走到最後它的 distribution 是像的	5-10
那這兩個可以共用一套	5-10
因為走到最後它們是像的	5-10
那這個呢它不跟別人搞在一起它就自己一個 嗯	5-10
那它這個自己一個它跟別人那個等等	5-10
那這幾個都會都會很像就跟它們搞在一起哦等等	5-10
差不多是這樣的意思	5-10
那麼因此呢這樣你比較容易想像	5-10
那這樣子總共大概這個原來是最早是它們做英文	5-10
大概用了兩三百個 question 哦	5-10
就是說完全就是根據根據這個語言學家語音學的知識去分	5-10
左邊會是什麼右邊會是什麼	5-10
左邊有沒有什麼右邊有沒有什麼等等	5-10
這個語言學大概有兩三百個 question	5-10
然後你就從這個一路走下來	5-10
那麼根據到底要從哪一個開始問起	5-10
就是看你問的是哪一個 question 我的 entropy 降得最多我就用那個 question 然後這樣一路走下來	5-10
那在這些 question 裡面其實都是比較簡單的	5-10
你如果仔細看的話這些 question 就是 yes 還是 no	5-10
嗯是 yes 還是 no 所以是比較簡單	5-10
嗯是 yes 還是 no 所以呢只是說根據語言學的知識它它是不是什麼東西	5-10
嗯你如果是前面那個式子其實還更難	5-10
譬如說十二歲為什麼是十二歲	5-10
那其實你也可以是十三十四十五十六都可以嘛	5-10
十九八七六都可以嘛	5-10
所以你還要再選那個 threshold 是多少	5-10
像這裡你也是要選 threshold 是多少	5-10
嗯 那在這個例子反而不需要	5-10
因為在我們真正做的時候其實不需要	5-10
因為其實都是在根據就是不是 threshold	5-10
只是在看說	5-10
你是不是哪一種具有哪一種左邊右邊是不是有哪一種特性的子音或者母音等等之類的東西	5-10
那這樣總共大概有兩三百個 question	5-10
然後就用這個來做這個選擇	5-10
於是你可以想到這件事情其實是嗯它是 both  data  driven 跟 linguist  knowledge  driven	5-10
它靠兩樣東西同時在操作	5-10
第一個是語言學的知識	5-10
因為你可以想像會影響它的 TRI PHONE	5-10
左邊右邊會影響這個東西的其實是在它的左邊跟右邊是接在哪一種音	5-10
然後那個音的發音會怎樣	5-10
所以呢我就用我的 linguist  knowledge 來做我的 question  set	5-10
然後一路來選	5-10
所以這個是 driven  by  linguist  knowledge	5-10
但是另一個另一方面我是 driven  by  data	5-10
因為根據現在我把這堆東西都都拿來的時候它是這麼亂的	5-10
然後我到底用哪一個 question 可以把它拆得比較清楚	5-10
我是根據 data  driven	5-10
嗯所以根據這個算它的這個 entropy 來算	5-10
這是我在算它的 data  driven	5-10
所以呢它是同時 data  driven 跟 linguist  knowledge	5-10
我一面用人的知道一面用 data 這兩樣組合來建一個這樣的 tree	5-10
那於是呢我這個 tree 呢就從頭相 開始向下走	5-10
那我用所有的 available  data	5-10
就是譬如說我現在要 train 這個	5-10
train 這個啊的最後這個 state 的所有 data 我都拿來	5-10
然後就開始用它的這個長像非常亂的一個長相開始來算它的 entropy	5-10
然後根據這些 question 來看它怎麼怎麼建這棵 tree	5-10
那麼然後呢我一路長這個 tree 呢	5-10
當然我最後我要有一個這個停止長 tree 的一個停止再 split 下去的一個 criterion	5-10
分到哪裡應該要停住呢	5-10
那當然你可以想到第一個就是entropy  reduction 不小到一個程度對不對	5-10
我在哪兒再分下去 entropy 沒有沒有再變小的話就不必要再分了	5-10
entropy 的 reduction 小到一個程度就不必了	5-10
另外呢就是我的 data 量少到一個程度也不必了	5-10
對你可以 define 我這個 data 量嘛 對不對	5-10
我的 sample 如果少到一個程度	5-10
已經不足以 train 出一個東西來當然也就不要了	5-10
所以呢你可以這裡用這個方式來 define 你的這個這個這個 stop  criterion	5-10
於是呢這棵 tree 會長到哪裡	5-10
在什麼地方停住	5-10
在什麼地方停住是 depends  on 在那個地方的狀況	5-10
看它的 data 的量看它還會不會再降低 entropy 等等	5-10
於是呢當你這個 tree 長好之後	5-10
所有的 unseen  TRI PHONE 你就從頭開始	5-10
這個這個延著這個 tree 向下走就好了	5-10
走到哪裡就歸誰	5-10
我我我現在這個長的這個這邊都是看到的 data	5-10
我這個 training  data 看到哪些	5-10
我把看到的 data 拿來放在這裡 然後讓它一路長下來對不對	5-10
我讓一路長長完這都看到的	5-10
那還有一堆一大堆沒有看到我們剛才講有一半的 TRI PHONE 沒有看到怎麼辦	5-10
沒有看到的譬如說這個左邊要這個右邊要這個	5-10
嗯就是沒有	5-10
沒有怎麼辦	5-10
我就讓它從這上面往下走	5-10
那每一個地方因為它都它它它的問題都是說我是左邊是怎麼樣的右邊這個都是語言學的知識	5-10
所以我可以根據語言的知道來判斷它應該往哪邊走	5-10
所以我就可以	5-10
雖然沒有看到沒有 data 的 TRI PHONE	5-10
我完全根據這棵 tree 上面的語言學的知識	5-10
就走走到它該走的地方	5-10
最後就是這個 traversal across tree  by  answering  the  questions  leading  to  the  most  appropriate  state  distribution	5-10
你凡是沒有的	5-10
沒有看到的你就你就根據它左邊跟右邊的語言學知識來走	5-10
走到哪裡就歸那裡	5-10
你就用那個 data 當成那個 TRI PHONE	5-10
就這樣子	5-10
所以呢這就是所有的 unseen  TRI PHONE 都有位置都有 model 的個辦法	5-10
嗯 那麼嗯這樣子做的話呢	5-10
我的 Gaussian  mixture 最後呢就是	5-10
哦 就是凡是有相同的 linguist  property 的就會 tie  together  sharing  the  same  data 然後 same  parameter	5-10
就是說你你到到時候這一堆會會長在一起嘛	5-10
就像譬如說這兩個就會長在一起	5-10
那沒有什麼理由	5-10
就是一方面就是我我如果這樣一路走過來的話	5-10
表示說它們左邊右邊的這些音	5-10
它們的這個 linguist 特性就是很像的	5-10
那一方面呢那如果是這樣的話呢那是有理由說是這個	5-10
一方面你也等於是說它們的純度是最純的	5-10
或者說它們本來就長得像	5-10
那那那麼因此呢這麼一來的話呢	5-10
那麼這些個 Gaussian 就是所謂的 tie  together	5-10
所謂 tie  together 就是我用同樣的 training  data 最後 train 出這一組來	5-10
最後 train 出一組來	5-10
那麼這個時候呢就是所謂的 tie  together 的意思	5-10
那麼因此呢它們就是用相同的 training  data 來 train	5-10
所以是 sharing 用相最後就用相同的 parameter 啊	5-10
這就是 sharing 的意思	5-10
所以這樣的時候到到了最後 leaf  node	5-10
到了 leaf  node 這裡的每一每一組 Gaussian	5-10
它們都是一組大家一起 share 的	5-10
那麼這是這個一個非常簡單的解釋	5-10
那其實要做這個是很有學問	5-10
怎麼樣 train 得好還是有很多很多進一步的問題	5-10
舉一個例子來講	5-10
tree  PRUNING 就是說你這個 tree	5-10
你有的時候長長得太長得太茂密了	5-10
可能最後分得太細不見得好的時候	5-10
你有一些 criterion 把它砍掉一點	5-10
把它砍掉一點讓它這個比較不要那麼茂密	5-10
麼可能效果會更好	5-10
這是所謂的 tree  PRUNING	5-10
另外呢你的 question 也可以是所謂的 composite  question	5-10
所謂 composite  question 就是像這樣子嘛	5-10
假設你可以把這個這個是 and 這個是 or 嘛	5-10
嗯你可以把左邊又是這樣右邊呢又不能那樣這有一個 這個 bar就是否定嘛 對不對	5-10
左邊要這樣右邊不能那樣這是 and	5-10
然後呢或者是嗯左邊是這樣這都可以	5-10
你這樣就變成一個 composite  question	5-10
那你也可以用這類方式來做	5-10
所以這中間的學問還有很多	5-10
那我想我們這邊並沒有詳細的講	5-10
我這邊只是把一些基本的原理大概我們大概解釋	5-10
大概是這樣一回事	5-10
那這個 TRI PHONE 的這樣的方式的的 train 是最重要的主流	5-10
也就是說我們今天絕大多數的最成功的系統都是用這個方法來 train 的啊	5-10
那這個嗯到這裡為止我們大致把這個最主要的 HMM 的 training 的大概都已經說完	5-10
那麼我們其實沒有說得很清楚	5-10
我們大概只是選擇裡面幾個重要部分把它說得清楚一點	5-10
然後還有很多中間有一些地方是沒有說清楚的	5-10
那不過嗯我想應該 OK	5-10
我們在後面大概下週或者下下週會給各位做第一題習題	5-10
第一習題就就是在做這些事情	5-10
那這個習題倒是這你不用擔心這個程式要寫起來不得了	5-10
不用你寫因為都有現成的 toolkit 哦	5-10
所以你只要是用這個現成這個去上網 download 下來	5-10
然後這個主要的那個怎麼操作那個 toolkit	5-10
那麼大部分的的助教都已經告訴你怎麼做	5-10
所以不會太難做	5-10
你如果認真得去做那一次的話	5-10
大概就會了解中間所有東西	5-10
現在看是還有很多問題因為很多功能其實沒有講得那麼清楚	5-10
因為真的要講清楚是是太複雜	5-10
那我覺得我們也不太可能真的把每樣都講得那麼清楚	5-10
但是後面就會給你一個這個習題	5-10
你如果認真去做一次你大概就會知道裡面所有東西	5-10
OK 好我們今天就講到這裡	5-10
ok 我們補課的時候在講的事情就是	5-10
怎麼樣做這個tri phone 的training	5-10
那麼做tri phone 最大的問題就是有一堆unseen event 我們說過就是因為有很多個unseen 的tri phone	5-10
你所需要的很多東西其實根本就沒有data	5-10
所以呢必須要跟別人去share	5-10
那麼share 的辦法呢就是用我們這邊講的這個這個tree 的結構來做這件事情	5-10
而這個tree 的發展的這個過程之中呢我們就是讓它每一步都找一個有最大的這個entropy 的變化的那個地方的那個question 來分這個tree 等等	5-10
那這個基本原則是來自我們前面講的這個information theory 裡面的entropy 等等	5-10
那這部分詳細的這原始paper 是這一篇	5-10
所以你如果想詳細地看可以看這篇	5-10
那在這本課本裡面這一段其實也在講這件事情是一樣的	5-10
那所以呢你這邊都這邊都可以看得到	5-10
那前面這邊它是在講一些關於我們這邊說的一些嗯像是這個phoneme 啦	5-10
像這些個co articulation 等等的現象它有一些說明所以都是蠻不錯的一些reference	5-10
是可以參考的	5-10
那麼到這裡為止呢我們大概把嗯所有的h m m 怎麼train	5-10
然後這些東西我們等於是講完一次	5-10
那這些東西其那你如果仔細想一想我們其實從第三四點零開始	5-10
整個的h m m training	5-10
從頭到尾其實是一個非常複雜的過程	5-10
中間很多東西譬如說我們講講這個basic problem 三是在講這個用用這個iterative forward backward algorithm 讓它作微調能夠train 得更好	5-10
我們後面有講另外一個是segmental k means 是其實怎麼做initialization 等等	5-10
那我們這邊講的tri phone 是另外一塊	5-10
那我們並沒有真的足夠的時間把整個都講那麼清楚	5-10
我等於只是挑裡面的幾塊把它說清楚而已	5-10
那你自己可以去想中間怎麼link 起來	5-10
那有些地方沒有說得很清楚	5-10
那麼我們後面會給各位一個習題	5-10
就是把這個東西做一次	5-10
那嗯你不用擔心如果這個習題如果這個這個程式如果你自己寫的話會寫死人	5-10
這個這個程式複雜到難以想像的程度	5-10
不過現在都沒什麼問題因為我們都有現成的工具	5-10
所以其實你只要用那個那個工具都可以download 下來	5-10
自己上網就可以download 下來然後你就自己可以train	5-10
那嗯所以呢我不曉得今天助教會不會準備好如果準備好的話今天就會給你那個習題	5-10
那這個你嗯你只要從頭到尾仔細得走一次	5-10
那它的那一整套的htk 的程式就是所謂的我們給你用的htk 是今天國際語音界最普遍使用的一套程式就是h m m toolkit	5-10
那它有一整套的manual	5-10
很厚你如果把它印出來是很厚一本畫一樣	5-10
它詳細說裡面所有東西怎麼樣怎麼樣怎麼樣	5-10
你如果真的下工夫的話你想要真的弄清楚的話仔細走一遍	5-10
裡面所有東西	5-10
就其實是可以很清楚裡面所有程序	5-10
如果你沒那麼有興趣的話就照我們習題裡面給你的那些scrip 走一次	5-10
大概也會了解中間的程序	5-10
那嗯應該是這樣講就是說	5-10
我們今天在做這樣的事情的時候	5-10
那麼如果打個比喻的話好像是我們要蓋一棟房子	5-10
那麼當我們蓋一棟房子的時候	5-10
嗯如果說是譬如說我們需要有冷氣	5-10
我們就去冷氣行裡挑一個我們需要的冷氣放在這裡就拿來用了	5-10
我們需要一個爐子我就去爐子的店去買一個爐子來放就可以用了	5-10
我不需要從頭去	5-10
自己去做冷氣怎麼做把它做好然後爐子怎麼做把它做好其實是不需要	5-10
我們做的事情是蓋房子	5-10
因此呢像這類基本的程式	5-10
嗯我們可以不要自己寫我們都不要自己寫	5-10
然後都用現成的	5-10
然後這個但是我們要做的事情是要如何把這些現成的工具兜起來做到我們想做的事情	5-10
所以這是我們現在的工作是是這樣子	5-10
這個跟十多年前是不一樣的十多年前我們做語音的時候因為沒有這種東西所以我們每個學生都要自己寫一套h m m 的程式	5-10
那時候很累很累那現在都不需要了這是我們要做的事在不同就是了	5-10
那嗯這個htk 的這個程式是這個原始程式是英國的劍橋大學的學生寫的	5-10
英國的劍橋他們在九零年代的時候開始很認真地做hidden markov model	5-10
那麼嗯他們覺得做得不錯他們也就去參加美國的這個比賽	5-10
結果一比美國人看不起他們覺得他們一定不會	5-10
美國人認為h m m 是他們發明的	5-10
認為英國人一定很爛一定不會	5-10
結果一比果然是最後一名	5-10
嗯結果大家就笑說啊英國人實在很爛	5-10
結果英國人回去就很認真地從頭寫這個程式	5-10
第二年再去比賽就是第一名	5-10
打敗美國所有的團隊	5-10
包括i b m 啊包括這些a t and t 全部都被打敗	5-10
然後它變成第一名	5-10
從那個以後這個嗯美國人也就承認英國人這方面最厲害	5-10
所以後來他們就把他們的程式變成一套變成一套軟體	5-10
當時開了一家公司是賣這套軟體的很貴	5-10
當時htk 一套大概是換算成新台弊是幾十萬才買得到	5-10
那後來到了九零年代的末期的時候	5-10
這個微軟把這家公司買下來	5-10
然後就把這個把它這個軟體放在網路上讓大家使用	5-10
所以後來這個程式就變成大家都可以用	5-10
那我們用的就是這套	5-10
那這個那當然事實上微軟放在網路上的是htk 裡面的最基本的版本	5-10
是最基本的版本所以這個嗯不是最好的版本	5-10
最好的最有效率最快的真正可以拿去賣錢的可以這個的	5-10
那一套它其實留著沒有放在網站上	5-10
那放在網站上的是效率不太好的	5-10
但是是夠夠用的就是了	5-10
所以呢這個雖然慢效率不好	5-10
但是是是夠豐富所有我們想要做的事情都可以做	5-10
所以我們也用這個來做為這個嗯習題的教材	5-10
所以如果either 是本週or 是下週	5-10
但是因為發現下週又放假了我們實在是很頭大	5-10
好容易才把進度補起來	5-10
下週又放假了所以呢我比較希望今天如果助教趕得出來就今天把這個習題給你們	5-10
ok 好底下我們利用這個機會稍微說一下國語的部分	5-11
那這個我們不要多說大概稍微提一下	5-11
我們基本上我們說在這門課我們講的所有東西我們都假設是	5-11
英文或者是language independent 的東西	5-11
但是我們碰到跟中文有關我們稍微提一下	5-11
那麼在中文而言那麼我們中文是一字一音的結構	5-11
我們的每一個字都是一個所謂的一音是什麼就是一個syllable	5-11
一個syllable 也就是裡面有一個母音	5-11
那我們總共有多少個syllable	5-11
這個大約一千三百多個	5-11
這個括號裡面的數字就是總數	5-11
但是這個總數不是很精確	5-11
因為不同的字典不同的人說法都不太一樣	5-11
因為永遠存在一些音是	5-11
有的人認為有的字典算進去有的字典不算	5-11
譬如說油捱這類的音	5-11
倒底算不算呢不一定	5-11
所以呢就會多一點少一點	5-11
不過我們這個syllable 的總數	5-11
大概是一千三百多個	5-11
那麼這裡面呢我們可以分成有四聲的tone 就是我們的聲調	5-11
跟沒有四聲的	5-11
所謂的沒有四聲不是說沒有四聲是說我們的每一個音都有聲調	5-11
但是如果你不記聲調的話是四百多個	5-11
也就是說我們有八拔把爸吧	5-11
五個對不對	5-11
八拔把爸吧	5-11
你如果當成五個的話	5-11
那這個總數是一千三百多個	5-11
你如果只當成是一個	5-11
只是它的音調有變化當成一個的話	5-11
那大概就是四百多個	5-11
這個數字也不一定	5-11
最多大概人家講四百一十六個	5-11
所以就是四百多個	5-11
那另外呢我們有所謂的聲調	5-11
聲調就是這四加一四就是這四聲	5-11
一呢就是輕聲	5-11
那為什麼不是五呢	5-11
在語言學上來講輕聲跟四聲不一樣	5-11
那四聲是各有一個固定的pattern	5-11
輕聲是沒有	5-11
就是說你譬如說第一聲是這樣第二聲是這樣你知道第三聲	5-11
第三聲是這有所謂全晌是這樣	5-11
半晌是只有到這裡	5-11
第四聲是這樣	5-11
那所以呢它的這個這個四聲是有固定的pattern	5-11
至少長一定的長相	5-11
輕聲基本上呢被認為是沒有長相的	5-11
也就是說depends 它在什麼地方它會不一樣	5-11
根據它們的標準的語言學的說法	5-11
那麼輕聲最主要是depends on 它前面是第幾聲	5-11
它就會不同	5-11
所以來吧去吧走吧吃吧	5-11
你的這個前面是第幾聲你的那個吧就是不一樣的等等	5-11
所以呢輕聲在它們語言學上叫做neutral tone	5-11
這個neutral 的意思是中立嘛對不對	5-11
也就是它不屬於這四個裡面的任何一個	5-11
它是跟著前面的人走的	5-11
所以是叫做neutral tone	5-11
所以我們把它說成四加一而不是五	5-11
然後呢這個那麼所謂的base syllable	5-11
前面加一個base 的意思就是不計聲調	5-11
的時候叫做base syllable	5-11
那每一個base syllable 呢我們又可以分成這個我們提過了就是聲母跟韻母	5-11
那麼聲母其實就是子音ㄅㄆㄇㄈㄉㄊㄋㄌㄍㄎㄏ這種都是聲母	5-11
它們都是子音	5-11
總數是二十一個	5-11
這倒是確定的	5-11
不過呢有的時候有的人說是二十二個	5-11
那是因為還有一個所謂的空聲母	5-11
因為我們有一些音是不要聲母的	5-11
譬如說愛	5-11
這是沒有聲母的	5-11
你如果是敗的話	5-11
就是有一個ㄅ就是有聲母的	5-11
它可以沒有聲母	5-11
那這種情形稱之為空聲母	5-11
如果把那種情形也算進去就是二十二個	5-11
那麼韻母的話呢	5-11
這個一般而言韻母的核心部分所謂的nuclear	5-11
是它的最重要的部分	5-11
前面跟後面的這兩個東西是所謂的optional	5-11
可以有可以沒有	5-11
什麼叫做medial 呢	5-11
medial 是我們有一些音是三個注音符號拼出來的	5-11
那三個注音符號拼出來的中間那一個注音符號叫做medial	5-11
那有三個就是ㄧㄨㄩ	5-11
那你知道一的話我可以由譬如說ㄒ一ㄝ些	5-11
ㄨ的話有ㄉㄨㄢ斷譬如說	5-11
有什麼ㄐㄩ捐譬如說這樣子	5-11
那中間的這三個ㄧㄨㄩ叫做medial	5-11
所以我們的medial 總共有三個	5-11
那我說它是optional 是說有很多音它沒有這個的嘛	5-11
我那這些其實它自己可以做為自己可以做為nuclear	5-11
譬如說我也可以ㄉㄨ都對不對	5-11
那這個ㄨ就不是medial	5-11
這個ㄨ是屬於nuclear	5-11
那我也可以ㄅ一必	5-11
ㄅ一必的話這個一也是屬於nuclear	5-11
也不是屬於medial	5-11
medial 只有在這三個注音符號的中間的時候這叫做medial	5-11
所以我們有三個是medial	5-11
那ending 是什麼呢	5-11
我們有兩個ending	5-11
那就是n 跟n g	5-11
那麼n 跟n g 出現在哪裡	5-11
出現在我們注意符號的ㄣ跟ㄥ或者是ㄢ跟ㄤ	5-11
那ㄣ跟ㄥ你知道所謂的ㄣ跟ㄥ其實就是前面接一個ㄜ嘛	5-11
如果我前面接一個ㄜ然後再接ㄜ	5-11
應該說是這前面接一個ㄜ的話	5-11
ㄜㄣ就是這個ㄣ	5-11
那麼ㄜㄥ就是這個ㄥ	5-11
那如果我前面不是接ㄜ我前面是接ㄚ的話呢	5-11
ㄚㄢ就是ㄢ	5-11
接ㄚㄥ就是ㄥ	5-11
所以我們其實是有這四個韻母	5-11
這四個音它裡面是帶著這兩種ending 的	5-11
那麼這兩種ending	5-11
所以呢我們總共有兩種ending	5-11
那也是只有這四個case	5-11
才有的跟這三個case 才有的	5-11
所以這個三個medial 跟兩個ending 是optional	5-11
不見得有	5-11
那麼這個其它有所謂的nuclear	5-11
那這樣加起來的總數	5-11
總共有十二種不同的phoneme 在裡面	5-11
那你說加起來超過十二嘛沒有錯超過十二因為有的是重覆出現	5-11
譬如說這個ㄨ	5-11
也算medial	5-11
也算nuclear	5-11
這個一也是在這邊也算在這邊也算等等	5-11
所以加起來是超過十二種	5-11
那我們總共有十二種	5-11
那這裡面呢絕大部分都是母音	5-11
只有什麼case 是子音呢	5-11
就是這兩個ending	5-11
這兩個ending 是所謂的鼻音	5-11
這是na nasal ending	5-11
這是鼻音	5-11
鼻音其實是子音	5-11
所以這兩個是子音等等	5-11
那這些東西就是所謂的phone	5-11
那這裡面也有重覆	5-11
所以呢加起來大約是三十出頭差不多是這樣子	5-11
這是我們的所有的音	5-11
那當你有了這些音的時候我們也可以用這些東西做單位	5-11
來拼我們的tri phone	5-11
所以我們可以做我們的tri phone	5-11
那同理我們也可以做其它的東西	5-11
因為今天我們如果是要做國語的話	5-11
嗯你可以用的你當然可以用phone 來做然後可以做tri phone	5-11
那這裡面問題就是說到底什麼單位叫做phone	5-11
那麼其實不同的語言學家有一些不同的說法	5-12
那麼因此呢它們有另外一個名詞叫做phone like units	5-12
也就是說呢這個我不太能夠define 什麼是phone 或者phoneme	5-12
那我乾脆就自己define 一堆東西然後就說就跟phoneme 很像就對了	5-12
那麼我們舉例來講呢我們剛才講的這個這個線	5-12
那你如果真的要拼的話可能應該是c e a n	5-12
這四個這四個phone	5-12
先	5-12
但是不同的人也許有一些不同的意見	5-12
他認為不是這樣變成只有三個也不是說不可以哦	5-12
所以這個沒有一定	5-12
不過這個基本上這就是所謂的phone 跟phone like unit 因為有的文獻裡面	5-12
嗯其實在西方語言或者是其它各種奇奇怪怪的語言你知道全世界有幾千種語言	5-12
所以各種語言裡面到底以哪些做單位其實並不見得是有一致的意見	5-12
我們隨便舉個例子	5-12
你拿一個什麼土耳其話你會一個什麼突厥語什麼的	5-12
那它們到底怎麼哪些單位其實很難講的	5-12
所以呢有所謂的phone like units	5-12
所以你可以用這些單位來做tri phone	5-12
　那你也可以用聲母韻母來做	5-12
那通常我們嗯比較喜歡用這個	5-12
因為這個數目這個比較簡單	5-12
因為聲母韻母其實非常清楚	5-12
我們上次說過	5-12
所謂聲母韻母就是你總之第一個是聲母	5-12
後面兩個後面的兩個或者一個叫做韻母	5-12
那麼所以聲母韻母是比較容易切得清楚的	5-12
那麼聲母韻母也是一個可以用的單位	5-12
當你選擇好單位之後這是你選擇單位	5-12
當你選擇好單位之後	5-12
你要用怎麼樣子的context dependency	5-12
那我們說過就是你可以只看左邊也可以只看右邊	5-12
那就是right context dependent 和left context dependent	5-12
或者兩個都看	5-12
那就是像tri phone 就是左右兩邊都看	5-12
那麼我們後我這邊只有寫只看右邊而沒有只看左邊的是因為我們做後來做過實驗都知道	5-12
你如果只看一邊的話看右邊比較好	5-12
看左邊比較差	5-12
那為什麼呢你也可以猜得出來是因為	5-12
其實我們的音受右邊的影響比受左邊的影響來的大	5-12
那麼譬如說ㄅ一必跟ㄅㄨ不	5-12
這兩個ㄅ顯然不一樣	5-12
是因為它的右邊或者後面的那個逼或者是逋	5-12
那個逋就不一樣了	5-12
可是你前面是譬如說這個前面是逋必	5-12
還是前面是這個嗯大逋	5-12
前面是啊還是嗚對它的影響是比較小	5-12
不是沒有	5-12
就是說前面對它的影響是比較小	5-12
這個不同對它的影響比較小	5-12
後面這個不同對它影響是很明顯的大	5-12
那麼因為這樣的關係所以是右邊的context 是顯然比左邊重要	5-12
你如果只算一邊的話我們通常喜歡算右邊	5-12
是有這個原因的	5-12
那麼或者當然比較好是兩邊都算嘛	5-12
那還有呢就是intra syllable 跟跟這個intra plus inter	5-12
也就是我們一個syllable 裡面	5-12
你譬如說這個早上	5-12
你的這個早裡面的ㄗ跟ㄠ之間的影響	5-12
這是intra syllable	5-12
那麼這個上	5-12
如果是這個ㄠ影響到ㄕ的話這個影響是跨越了一個syllable 的邊界的	5-12
跨越了syllable 邊界的影響我們稱為這個inter syllable ok	5-12
所以呢你一種情形是說我只算我只算syllable 以內的	5-12
我讓它它影響它它影響它那	5-12
它影響它是右邊的影響那它影響它是左邊的影響	5-12
你可以只在這個裡面做	5-12
如果只在syllable boundary 裡面做的話呢	5-12
那這種的context dependency 是所謂的這個intra syllable only	5-12
那當然比較好的是說我把這個都考慮進去	5-12
我也讓它影響跨過syllable boundary	5-12
但是如果是這樣的話當然你的影響就更複雜	5-12
所以呢總之呢你可以有不同程度的context dependency	5-12
那這個就影響到你後面真正你要train 的tri phone 或者什麼東西它的總數會有多少	5-12
然後那些因為如果你某一些不算的話就會比較少嘛	5-12
那你如果都算的話就會很多那你就會有更多unseen 的	5-12
那更多unseen 的話你就更需要靠一些東西來補	5-12
等於是這樣意思	5-12
那在這個情形下就有很多種做法哦	5-12
那我們這個是隨便舉例	5-12
不一定只有這些還有很多	5-12
舉例來講我們有所謂二十二個聲母我剛才有講過	5-12
我們前面講的是二十一個嘛對不對	5-12
這二十一個	5-12
那為什麼變成二十二個	5-12
就是把空聲母加進去是二十二個	5-12
那麼一種最常做的做法就把二十二個聲母把它extend 變成一百一十三個	5-12
right context dependent 的聲母	5-12
那也就是說後面接譬如說這兩個就是不同的ㄅ嘛	5-12
那這個接一的ㄅ跟接ㄨ的ㄅ是不一樣的	5-12
所以呢這個是逋的那個ㄅ	5-12
這個是一的那個ㄅ等等	5-12
那這樣就一堆不同的ㄅ	5-12
ㄆ有一堆不同的ㄆ等等	5-12
那這樣一來呢我可以總共變成一百一十三個	5-12
那這是一個簡單的做法	5-12
我也可以用phone 來做	5-12
那麼倒底有多少個phone 呢這個也不一定	5-12
那麼一個做法大約三十多個啦	5-12
那三十多個其實是屬於剛才這邊你把這堆加起來差不多是這樣的數字	5-12
那這三十多個裡面呢	5-12
我如果只算intra syllable 的right context dependent 的話	5-12
大概可以變成一百四十五個左右	5-12
你如果把intra 跟inter 都算進去	5-12
那就大概是四百多個將近五百個	5-12
那也有另外一個辦法是說你syllable 跟syllable 之間是韻母跟聲母去銜接嘛對不對	5-12
譬如說早上	5-12
像早上這個case 是說	5-12
這個ㄠ後面要接這個ㄕ嘛	5-12
那你這個是所以inter syllable 之間的銜接的context dependency	5-12
你可以看成是在這個syllable boundary 的前面的韻母影響後面的聲母	5-12
那如果是這樣的話呢	5-12
我們這也是當時曾經做過一種辦法就是你把這個你把這個韻母分根據它們的結尾的音分成十二個group	5-12
有些韻母的結尾其實是差不多的	5-12
譬如說這個彎是一個韻母這個這個嗯	5-12
單的ㄢ也是一個韻母	5-12
這兩個韻母其實是很像的嘛等等	5-12
所以呢那這兩個韻母的結尾可以算是相同的	5-12
我們就可以用同一個	5-12
就像我們剛才那邊講的另外一個case 是譬如說結尾都是這個ㄣ	5-12
那一個是ㄣ一個是ㄢ	5-12
ㄣ跟ㄢ的結尾都是ㄣ所以我就算它是同一個group 等等之類的	5-12
那如果是這樣子的話呢	5-12
那我韻母可以根據它的ending phoneme	5-12
分成十二個group	5-12
那聲母可以根據它開頭的那個發音的情形	5-12
那分成七個group 這樣子的話它們之間的銜接的狀況可以分成八十四個八十四種	5-12
用這樣子來分群這我們當時也做過這樣等等	5-12
所以這個是有很多種變化depends on 你要怎麼做	5-12
那當然最複雜的情形是做tri phone	5-12
這個也是效果最好的一種	5-12
那麼我們如果把這些tri phone 通通算進去的話的話呢	5-12
國語的tri phone 沒有那麼多	5-12
因為不是所有的組合都會出現	5-12
那麼這樣算起來會出現的組合大概tri phone 大概是四千六百多個	5-12
那我們上次提到過	5-12
我們要train 這四千六百多個tri phone 的話你拿一個十多小時的data base 來數一數	5-12
會發現大概只有一半有只有一半	5-12
有聲音另外一半根本不在data 沒有data 等等	5-12
那就是要用那個decision tree 來做的	5-12
那我想剛才也許漏掉一件事情是說	5-12
韻母有三十七個怎麼這麼多	5-12
注音符號我們總共才差不多這麼多嘛	5-12
沒有錯	5-12
我們的韻母有很多是所謂的這個結合韻母	5-12
就是像這種嘛	5-12
像這個的話	5-12
耶啊淵啊這種都是這種韻母很多嘛	5-12
所以呢你把這些東西統統算進去的話	5-12
ㄦ這種都是韻母	5-12
那這些東西統統加起來我們是有這麼多個	5-12
好以上大概是一個簡單的介紹	5-12
那這個是很多年以前我們做的實驗我想只是一個例子	5-13
說明說你用不同的unit	5-13
然後你用不同的給它不同的context dependency 的話	5-13
可以得到不同的結果	5-13
所以呢這depends on 這個非常depends on 你用什麼data 去什麼database 去train 它	5-13
然後你用什麼database 去測試它	5-13
就會得到不同結果	5-13
所以這個你不需要把它看成是一個絕對的正確數字	5-13
而只是一個相對高低的參考	5-13
那麼用不同的data 去train	5-13
用不同的data 去測試就會不一樣	5-13
那麼我們如果是本週或是下週給你那個習題也是一樣	5-13
那個習題也是只是一個測試	5-13
那麼這個不同的狀況得到的答案就不一樣	5-13
那麼在這個情形而言	5-13
這邊的好像正確率很低才三十四十五十六十	5-13
那其實也是嗯包括另一個重要的原因是	5-13
我們沒有用辭典也沒有用language model	5-13
那我們知道是你加了辭典之後	5-13
辭典會告訴我哪些音才會兜成哪些字	5-13
什麼詞	5-13
所以這些音兜起來沒有一個詞的話這些音就不會存在嘛就一定是不對的嘛	5-13
所以辭典是有助於找到正確的音	5-13
同樣的language model 有助於連哪些詞連起來	5-13
那如果沒有language model 那當然也少了很多知識	5-13
所以我們這個是沒有用辭典也沒有用language model	5-13
純粹就是辨識這個syllable 對不對	5-13
所以正確率會比較低	5-13
那同樣的我們給你的習題也是這樣子	5-13
就是只是辨識syllable 完全就是看那個h m m 辨識syllable 的情形	5-13
那所以比較低是嗯ok 的	5-13
那在這裡我們可以只是簡單看一下它們之間的這個相對的高低的關係	5-13
譬如說這個呢就是我只用phone 為單位	5-13
c i 就是context independent	5-13
我不考慮所有的context independency 的話	5-13
那它只有最低	5-13
這是最低的	5-13
我同樣的情形也是context independent	5-13
可是我用聲母韻母為單位	5-13
立刻就高了不少	5-13
那從這個觀點來講也就是說這個這個聲母韻母來自古典的聲韻學	5-13
那麼這個古代的人他們就有所謂的聲韻學	5-13
他們就分析這個中文的聲音的時候他們就是用聲母韻母是有他的道理的	5-13
那這樣子我的總數其實這個總數其實還多一點	5-13
但是事實上這個結構可能還更清楚	5-13
更清楚的分這個每一個syllable 有所謂的聲母韻母這樣的關係的話呢	5-13
似乎是一個更好的單位	5-13
所以呢你可以看到它它這個我同樣都是context independent	5-13
那麼用phone 的話呢低的多	5-13
用聲母韻母if 就是initial final	5-13
我馬上就高了很多	5-13
那然後呢我如果把context dependency 考慮進去的話	5-13
我這兩個是跟這個來比	5-13
我如果還是用phone 為單位	5-13
但是我呢讓它左邊有context dependency	5-13
還是讓它右邊有context dependency	5-13
那你馬上看到呢你看左邊就是比右邊低嘛	5-13
不過不管左邊右邊都比不做要高很多嘛	5-13
所以你是context independent 差嘛	5-13
你加了一邊的context dependency 就高了很多	5-13
那這裡面的右邊加得比左邊多嘛	5-13
這就是我們剛才講的就是這個右邊影響你的右邊影響你的音厲害影響得大	5-13
前面影響比較小後面影響比較大	5-13
那所以我們如果只算一邊	5-13
大概都只算右邊那等等	5-13
那麼然後呢這邊我所謂的inter	5-13
加了inter 兩個字就是指做了inter syllable 的	5-13
做了inter syllable 的context dependency	5-13
那就是所謂的加了inter 的	5-13
那如果沒有寫就是用phone 為單位	5-13
那麼因此你可以看到呢這邊用了inter 之後我都會更高一點	5-13
那最高是什麼是後面這一群裡面的最後那幾個	5-13
那這一群都是tri phone	5-13
那麼tri phone 其實是有很多種不同的	5-13
你如果詳細講它train 的過程裡面有一些不同的做法	5-13
會不太一樣	5-13
那這個細節我們都沒有說你如果有興趣的話自己可以去查文獻可以看得到	5-13
那tri phone 這有不同的做法	5-13
那最好的是在這裡tri phone	5-13
那麼這個的話呢它是六十一在這個case 比這個三十一幾乎高了一倍	5-13
所以這是tri phone 是turn out是確實是最有效是沒有錯	5-13
就是我們這邊講的	5-13
tri phone 永遠是比較好	5-13
而且呢是這個嗯你有加inter syllable 都會比較好	5-13
然後呢這個但是你倒底怎麼train tri phone 是有是重要性	5-13
depends on 你怎麼做	5-13
你做到最好就會最好	5-13
差不多是這樣	5-13
那我想這部分我們說到這裡	6-1
那這個是我們五點零就說到這裡	6-1
那等於是我們把這個h m m 的部分	6-1
講到這裡為止我們現在要進入六點零	6-1
六點零要講的是language model	6-1
那language model 最基本的原理就是這個bigram trigram 這些個n gram	6-1
這些我們都已經說過了	6-1
那這裡面其實還有很多進一步的學問要講的	6-1
那我們現在來說這些language model	6-1
那麼那這部分的話大概這幾本書都一樣的提到大概類似的東西	6-1
所以呢我這邊寫一個or	6-1
就是說你其實大概只要選其中一個來看就夠了	6-1
你如果看這本的話你如果看這本的話	6-1
大概是這些	6-1
看這本的話大概是這些	6-1
看這本大概是這些	6-1
那這裡面應該說是講得最淺最容易看的	6-1
可能是第二個	6-1
那個講得比較完整都說到的可能是第一個	6-1
那比起來三呢就是稍微難看一點就是它講的這個中間跳的東西比較多一點就是了	6-1
大概是這樣	6-1
那我想你只要所謂or 就是你只要選裡面的一個來看就好了	6-1
那在開始講之前我們再回到我們前兩天補課的時候所說的information theory 裡面的這個entropy 的東西	6-1
因為我們底下還是要用這個來解讀這裡面講的事情	6-1
我們舉一個非常簡單的例子就是	6-1
這個這個entropy 呢是我們說它的上限是log n 下限是零	6-1
那它所描述的是這個一個變化的這個uncertainty	6-1
或者說是一個distribution 的分散的程度	6-1
或者是它的純度等等的	6-1
意思我們上次都已經說過	6-1
那麼我們是在講一個這樣的東西	6-1
如果我有一個一個information source	6-1
出來一堆m one m two m 三一直到m j 等等	6-1
那麼m j 是指第j 個	6-1
那每一個是什麼每一個是一個random variable	6-1
它都可以是譬如說x one x two 到x 大m 對不對	6-1
這就是那每一個都有個機率	6-1
這個就是p 的x one p 的x two 等等	6-1
那麼因此呢我們可以畫一個distribution 說	6-1
它們怎樣	6-1
這個是x one 的機率x two 的機率	6-1
一直到x m 的機率	6-1
這是我們上次補課的時候所說的我們基本上在講這麼一件事情	6-1
如果說它的每一個時間t 的時候	6-1
j 的時候送出一個m j 的一個symbol	6-1
那它們都可以是這大m 的裡面的一個	6-1
那每一個都有一個機率它都有一個distribution	6-1
那這些東西就是我們講的p of xi 的這些東西	6-1
那然後呢那我們就說呢我們現在可以根據這個	6-1
來算它的entropy	6-1
這個entropy 就是h of s	6-1
就是這個東西	6-1
那麼它的上限是log m 下限是零	6-1
什麼時候上限是log m 呢	6-1
就是如果這些機率都一樣	6-1
變成一個完全相同的uniform	6-1
的從x one 到x m	6-1
是完全平的時候	6-1
那這個時候它的它的這個entropy 最大	6-1
就是所謂的它的值就是log m m 就是總數	6-1
那什麼時候最小呢	6-1
最小的時候是只有一個對不對	6-1
那個x j 的機率是一	6-1
其它全部都是零	6-1
這個時候呢就變成零	6-1
所以它的這個它的這個這個entropy 呢是介於這個之間	6-1
這是我們上次所說的事情	6-1
好那我們現在以這個以這個狀況	6-1
我們以這個狀況來現在看假設是我現在的這個是一個language source	6-1
這個跑出language 出來	6-1
舉例來講假設你在用你的手機或什麼在接收某一個massage	6-1
假設這個massage 是這樣進來	6-1
那麼如果說假設這個massage 一個字母一個字母進來	6-1
一次跳出一個字母一次跳出一個字母來	6-1
所以進來一個t 一個h 然後一個i 一個s 哦我知道這個字是this	6-1
然後等等那因為跳進來的話	6-1
你可以想像成這裡每一個字母就是一個m j	6-1
就是一個字母	6-1
那這樣來看我就把每一個字母看成是那這裡講的random variable	6-1
這裡的每一個是一個random variable 嘛	6-1
那我就把這裡面的每一個字母看成是random variable 的話	6-1
那你想我這樣的話總共有多少個	6-1
我的entropy 有多大	6-1
那你可以想這是那就是那這就每一個字母就這些x m	6-1
那我這個m 呢應該是二十六乘以二還要再多	6-1
為什麼二十六個字母	6-1
所以我只有二十六種	6-1
乘以二是為有大寫有小寫嘛所以乘以二	6-1
然後還有一些標點符號啊還有空白也算是一個symbol 啦等等	6-1
所以大概是比這個還要多一些	6-1
不過應該小於六十四	6-1
六十四是二的六次方	6-1
所以你如果取log m 的話呢	6-1
應該是大約是六個bits	6-1
比六個bits 還要少	6-1
所以呢你可以說如果它是一個字母一個字母跳出來的	6-1
每一個字母看成是一個random variable 的話	6-1
那那個字母帶給我的information 是多少	6-1
大概是比六個bits 少一點	6-1
這個是這句話的意思	6-1
所以你一個字母大概給我這個一個字母給我大概六個bits 少一點的information	6-1
那這個其實是可以仔細算的我們這個只是很粗的這樣子講	6-1
我用這個上限等於log m 的這個來看	6-1
那上限這個是在講上限嘛	6-1
就是log m 就是六個bits	6-1
那事實上其實你是可以算的	6-1
那你知道我們的英文字母其實每一個字母它的機率都可以算的出來	6-1
那麼有的字母是機率比較高的有的字母是機率比較低的	6-1
我把這個二十六個字母如果算成a b c d 這樣算到z 的話	6-1
你知道什麼字母是機率很高的譬如說t	6-1
是機率很高的	6-1
那麼e 也是機率很高的	6-1
什麼是機率很低的	6-1
z 機率很低的	6-1
那麼q 機率很低的等等	6-1
你其實每個字母的機率都算得出來的	6-1
所以你可以真的可以照我們那個公式就是p log p	6-1
這個東西不是p log p 嗎我們上次講過	6-1
就是p xi log p xi	6-1
然後summation over i 嘛	6-1
就是這個東西嘛你可以算嘛	6-1
你其實可以算精確的算出來每一這樣子的話一個字母倒底給我多少information	6-1
這可以算得出來的	6-1
那我們這邊沒有去算它我只是舉個例子算它的上限	6-1
這個上限是log m	6-1
那麼這個log m 大約是六個bits of information per character	6-1
所以呢你如果說假設說我的手機可以一個一個字母跳出來看到收到我的message 的話	6-1
每一個字母給我多少information	6-1
大概是六個bit	6-1
那這個應該很接近我們的直覺	6-1
因為你知道我們本來英文字母一個字母就是用這個ascii representation 就是六個bit 嘛	6-1
所以六個bits 大概給我一個字母是沒有錯的	6-1
那如果它不是一個字母一個字母跳出來是一個字一個字跳出來的話	6-1
就第一次跳出來就是一個this	6-1
後面跳出來是course	6-1
一個一個字跳出來的話	6-1
我就把每一個字看做一個random variable 可不可以	6-1
也可以	6-1
我如果把一整個字看做一個random variable 的話	6-1
那就是那我這就變成一個一個word 了	6-1
一個一個word 的話我總共有多少個word 呢	6-1
那英文的word 數目很多了	6-1
我們假設是三萬個的話	6-1
三萬個大概是二的十五次方這個寫錯了應該是二	6-1
是二的十五次方	6-1
那你如果算成二的十五次方就表示說呢	6-1
我那也是一樣其實英文的每一個字	6-1
每一個word 都有它的機率	6-1
譬如說這個字的出現機率都可以算的嘛	6-1
就是unigram	6-1
所以你可以算它的機率	6-1
你機率算出來你可以算那個p log p	6-1
你可以算這個entropy 算得出來的	6-1
那我現在也是只是簡單地用log m 這個上限來說的話	6-1
假設它是三萬個它的上限就是二的十五次方	6-1
那就是十五個bit	6-1
所以你可以說喂一個如果這樣一個字一個字跳出來的話呢	6-1
這個字給我大概十五個bit information	6-1
你大概可以這樣子看	6-1
那這個是我們用information theory 裡面的entropy 的觀點	6-1
來解讀這個這個language source 譬如說英文	6-1
假設我現在這個出現的是中文的話	6-1
那還有不同的情形啊	6-1
假設你用聽的	6-1
我是一個一個音聽到	6-1
我聽到這個音聽到這個音我一個一個音聽到的話	6-1
我聽到一個音我得到多少information 呢	6-1
那我們剛才說我們音的總數大概一千三百個	6-1
這大概是二的十一次方	6-1
因此呢你聽到一個音大概是聽到十一個bit	6-1
或者更少一點	6-1
我還是一樣我用log m 來算	6-1
假設它的上限是log m	6-1
那麼我就是用這個log m 來算	6-1
假設這樣的distribution 的話	6-1
那麼那麼譬如說你在聽你的你聽你的手機的電話	6-1
我每聽到一個音	6-1
那那個音等於是這裡面的一個random variable	6-1
那它有一千三百種嘛	6-1
所以呢我所得到的information 應該是log m	6-1
為上限的一個information 的量	6-1
所以大約是十一個bit 或者更少	6-1
那也是一樣其實每個音有它的distribution	6-1
有些音機率比較高有些音機率比較低的	6-1
我們隨便舉例你也知道	6-1
機率比較高的是什麼譬如說ㄕ啊	6-1
因為施時使是都有很多常用詞	6-1
常用字所以ㄕ是機率很高的	6-1
那有的機率很低的等等	6-1
那那你也可以真的去算把它們機率高低通通都算出來	6-1
你也可以得到真正精確的每一個音給我多少information	6-1
但是我們不算我們只算上限的話	6-1
你聽到一個音大概是這樣	6-1
如果如果我聽不出它的它的聲調了	6-1
我剛才是including 的tone 是一千三百個	6-1
我如果聽不出聲調的話會怎樣	6-1
我們假設一個外國人他聽不出聲調來	6-1
對他而言不管是第幾聲聽起來都一樣	6-1
那這個時候他只聽到四百多個聲音	6-1
那那個時候大概是九個bits	6-1
那這個合理嘛因為你少了四聲	6-1
四個聲調大概是兩個bit 嘛對不對	6-1
所以你如果聽不出聲調來的人	6-1
他聽到一個字大概聽到九個bit 的information	6-1
那當然你也可以是一個字一個字跳出來當成字來看	6-1
如果一個一個字來看的話呢	6-1
那depend on 我們算是多少字	6-1
我也可以算這樣的事情那假設我們常用字的數目是八千的話	6-1
那八千是這樣子二的十三次方	6-1
所以呢你每看到一個字	6-1
每跳出一個字來看到一個字	6-1
大概是十三個bit information	6-1
那這樣只是一個general idea 你比較有數我們怎麼在算這些東西	6-1
那這底下這個只是隨便意思意思我們舉個例子了解一下這個玩一玩而已	6-1
譬如說這女孩相對於girl 這可能是一個最match 的例子	6-1
為什麼呢	6-1
這裡有兩個字	6-1
如果一個字是十三個bit 的話	6-1
那兩個字大概是二十六個bit information	6-1
那這四個英文字母呢一個英文字母如果是六個bit 的話呢	6-1
四個大概是二十四個bit	6-1
這個二十四個bit 這個二十六個bit 很接近嘛	6-1
所以也就是說你如果從英文來看從中文來看	6-1
in general 它們給我們的information 的量是接近的	6-1
就因為我們這些language 所靠的東西是大概類似的concept 所以是差不多的	6-1
那當然這裡只是一個特別的例子那剛好看起來很接近	6-1
有些是不接近的啦	6-1
這裡也是兩個字這邊有這麼多字母嘛	6-1
三個字有這麼多字母嘛所以這個就不見得接近這只是一個例子而已	6-1
好以上只是一個簡單的一些例子在說我們真正要說的是底下這個perplexity	6-2
當我們在講language model 的時候	6-2
所有的課本所有的文獻都講perplexity	6-2
那麼我們來解釋一下perplexity 是什麼	6-2
怎麼樣用它來看language model	6-2
當我聽到一段聲音的時候	6-2
我先去判斷它的第一個字是什麼	6-2
第一個字是w one	6-2
但是w one 其實假設我有咦我已經跳掉了	6-2
w one 其實是有這個很多個w one 其實是有很多個可能啊	6-2
譬如說我有六萬個詞的話	6-2
我有譬如說英文我們假設英文我們有六萬個可能的word	6-2
所以呢其實w one 有六萬個可能	6-2
那麼你可以想像它可以是我總共有這麼多可能	6-2
它可以是這個word 可以是那個word 我有六萬個可能	6-2
同樣的呢w two 呢第二個字呢	6-2
也有六萬個可能對不對	6-2
所以呢其實我每一個word 都有六萬個可能w 三還是有六萬個可能	6-2
那其實language model 在幹什麼它就是在告訴我說它們不是都是六萬個可能	6-2
而是你如果知道了前面	6-2
後面的可能性其實就集中了	6-2
那舉例來講呢	6-2
這個w one 的這六萬個可能呢	6-2
很可能是我們剛才所說的	6-2
這六萬個可能是我們剛才所說的嗯這樣子畫	6-2
假設這邊總共是六萬個	6-2
從x one 到x m	6-2
我總共有六萬個	6-2
這六萬個裡面有高有低嘛	6-2
這就它們unigram 有高有低	6-2
那麼有的word 機率比較高有的word 機率比較低	6-2
這裡面就有六萬個可能	6-2
當我如果知道了w one 之後我再看w two 的話呢	6-2
它還是有六萬個可能	6-2
可是這六萬個可能會怎樣	6-2
它distribution 會不一樣	6-2
它會集中在某些地方比較高	6-2
另外的地方就很小了	6-2
為什麼呢根據bigram 對不對	6-2
根據bigram 如果知道前面是w one 的話	6-2
那這個剛才w one 的地方這個因為完全沒有任何前面的字的information	6-2
所以我只好根據原來的distribution	6-2
原來distribution 是怎樣就怎樣	6-2
那這個看它是怎樣就怎樣	6-2
所以呢是根據原來的distribution	6-2
但是當我有了第一個word 有了第一個word 之後的第二個	6-2
的第二個的時候呢	6-2
它就這個distribution 跟這個不一樣	6-2
它基本上應該集中在若干個word 上面	6-2
其它的就很低了	6-2
因為我有bigram	6-2
那等到第三個的時候呢那就更好了	6-2
因為我有了前面兩個之後的我有trigram	6-2
那就更清楚地說ok 它是這些高這些機率高	6-2
別人的機率很低了	6-2
那這又是另外一種distribution 對不對	6-2
那麼因此呢那其實是說這個一開始的這個distribution 最散	6-2
應該說這個的entropy 最大	6-2
這個的entropy 最大	6-2
然後當我有了這有了w one 之後再看w two	6-2
因為有了bigram	6-2
所以呢我這個entropy 會小一點	6-2
稍微集中在若干比較少的數目的	6-2
稍微集中在若干比較少的數目的word 上面	6-2
當我有了這兩個之後第三個我會更好一點等等	6-2
那你如果這樣子看的話我們language model 的目的在做什麼事情	6-2
language model 是在做是在做一種linguist constraint	6-2
to help in word selection	6-2
對不對	6-2
就是說你你本來是完全沒有知識的時候我六萬個word 都要選	6-2
那當我有了前面一個word 我下一個word 的話	6-2
我就集中在少數譬如說一千個word 裡面去選就好了	6-2
當我有了這兩個之後我在這邊選我搞不好就在嗯兩百個裡面去選就好了	6-2
那就是我的有越來越多的constraint 在給我	6-2
讓我來選字的時候比較容易選的	6-2
language model 你可以看成是這樣的一件事情	6-2
如果是這樣的一件事情的話呢	6-2
那一個language model 它真正的功能是什麼呢	6-2
就是倒底給我多強的constraint	6-2
這個constraint 倒底多強	6-2
那這個constraint 如果越強的話	6-2
也就是讓我這邊要選的越少的話是越好嘛	6-2
就是我們剛剛講如果這邊要選六萬個	6-2
這邊只要選一千個了	6-2
到這邊只要選兩百個了	6-2
那這樣的話呢就表示說我的constraint	6-2
那如果這邊不是兩百個只要選五十個那不是更好對不對	6-2
所以就是說你如果越到後面的話	6-2
我如果這個constraint 越緊	6-2
我其實是越有幫助越不會錯嘛	6-2
那這個所謂的這個這個六萬個還是兩一千個還是五十個這個是什麼	6-2
這個其實就是我們之前講的entropy 對不對	6-2
entropy 就是在告訴我們這件事情	6-2
就是它倒底是uniform 的這麼多呢還是集中在這裡	6-2
那我們上上次說過你可以把這個看化成很多種嘛	6-2
你是這個完全uniform 的完全不知道呢	6-2
還是你會集中在某些地方	6-2
還是更集中在某些地方	6-2
別的就沒有了	6-2
還是完全集中在一個	6-2
那這個就是在算entropy	6-2
所以呢那麼你你其實entropy越小的話就表示我這個constraint 越強	6-2
那我就是用這個entropy	6-2
在我就是用這個entropy 在描述這個constraint	6-2
那麼因此這個我們這邊底下所講的東西都是在講entropy	6-2
是這樣來的	6-2
其實這些這些perplexity 這些東西都是跟entropy 有關	6-2
那其實entropy 就是在描述這個linguist constraint	6-2
那什麼意思呢	6-2
應該這樣子講	6-2
這六萬個word 裡面誰的機率高誰的機率低	6-2
我有很複雜的distribution	6-2
而且每一個word 出來	6-2
它的bigram 都不一樣	6-2
你可以想像我現在如果前面換成另外一個字的話bigram 又不一樣了	6-2
這又是另外一個	6-2
對不對	6-2
你這邊出來了兩個字之後	6-2
後面又有另外一個trigram	6-2
那這兩個字只要不一樣這個又不一樣所以這個實在太複雜了	6-2
那我們怎麼辦	6-2
我們就用剛才的這個entropy	6-2
我就算這個entropy 就說明它的constraint	6-2
ok	6-2
那我算entropy 來說明constraint 意思是怎樣呢	6-2
我們舉例來講	6-2
假設這個那我的一個辦法是這樣	6-2
譬如說即使是這樣它還是即使用entropy 去算這還是很複雜嘛	6-2
那我怎麼樣把這個entropy 轉成比較簡單的觀念呢	6-2
我就是把它用二的次方	6-2
所以你看我這個式子其實就是算entropy	6-2
p x i 乘以log	6-2
這就是算entropy 嘛	6-2
那每一個xi 就是我的一個word	6-2
這裡的每一個xi 這裡的每一個xi 就是我的一個word	6-2
這是出來第一個word 這是出來第二個word	6-2
那麼這裡出來每一個word 的	6-2
所謂出來每一個word 就是我們剛才講的這個這個例子裡面ok	6-2
這是第一個word	6-2
假設我這個這個一個一個word 出來	6-2
那麼每一個word 有一個機率	6-2
那個機率就是xi	6-2
那這樣的話呢我這個我的這個entropy 就是log xi 乘上咦跳到下一頁去了	6-2
就是這個那這就是我的entropy	6-2
但是我怎麼辦呢我把那個entropy 再一個二的次方	6-2
叫做perplexity	6-2
這個二的次方是什麼意思呢	6-2
是一個size of virtual vocabulary in which all words are equally probable	6-2
什麼意思就是說	6-2
如果我這邊算出來是某一個entropy 的值	6-2
我把它二的幾次方之後變成另外的東西	6-2
變成一個譬如說一千八百七十二點六七	6-2
如果我二的把這個entropy 算出來之後	6-2
我算出來之後我二的這個次方之後得到一千八百七十二點六七的意思是	6-2
如果剛好真的有一千八百點六七個word	6-2
它們是equally probable 的話	6-2
它的機率就是log m	6-2
log two 的這個一千八百七十二點六七呢	6-2
我們叫做m ’好了	6-2
那它就是log two 的m ’	6-2
那你可以想像好比是存那這個的這個的constraint 跟這個的constraint 是一樣的	6-2
因為它的entropy 是一樣ok	6-2
我們講這個entropy 怎麼算	6-2
就是log m	6-2
如果我有m 個而這m 個是equally probable	6-2
機率完全相同的時候的m 的話	6-2
它的entropy 就是log m 嘛	6-2
那當然我真的算的時候不見得不見得會剛好是	6-2
你我算出來的entropy	6-2
我現在就是把它把它二的如果現在算出這個東西來	6-2
然後給它二的這個次方不就是m 嘛對不對	6-2
所以等於就是說假設我剛好是一千八百七十二ok 是一個整數那好那就是這麼多個	6-2
但是實際上你算出來不見得是整數	6-2
是一個這個任何一個real number	6-2
那也沒有關係你就想像成這個譬如說我算出來是一千八百七十二	6-2
你可以想像好比是好比是相當於是	6-2
因為你這個三萬個這麼複雜你很難想像它到底是什麼	6-2
我們不如都把它轉成一個轉成一個virtual vocabulary	6-2
那裡面所有的word 或所有的unit 都是equally probable 的	6-2
那假設我這樣來轉的話	6-2
那如果我算出來的那個我算出來的那個entropy	6-2
我用二的這個次方去算得到一千八百七十二點六七	6-2
我就可以想像這是一個m ’假設我真的就是一千八百七十二點六七的話	6-2
那它們的而它們是equally probable 的話	6-2
這個entropy 跟這個entropy 是一樣的	6-2
那如果是這樣的話呢	6-2
那假設我的這個那假設這個是我的第一個字	6-2
那到了第二個字的時候呢	6-2
因為有了前面的這個我有了bigram	6-2
結果我算出來的東西呢	6-2
這個entropy 就小了很多嘛對不對	6-2
當我這個有了bigram 之後	6-2
這個entropy 這個distribution 又更集中了嘛	6-2
變成變成更集中了	6-2
當我這邊變成更集中了也許我這邊算出來變成一個更小的	6-2
譬如說呢	6-2
我變成一個譬如說你可以想像成我現在的這個m ’呢	6-2
只有兩百五十七點一二	6-2
那也就是說假設存在一個virtual vocabulary 只有兩百五十七點一二的話呢	6-2
那它的entropy 它們機率equally probable 的話	6-2
它的entropy 就是這個的log	6-2
那	6-2
那那等於是說我的這個bigram	6-2
已經把我從原來一千八百多個的的這個constraint 縮小到只有兩百多個了	6-2
那如果是這樣的話呢	6-2
到了trigram 的時候呢這個可能更小了	6-2
譬如說我其實entropy 更小因為它更集中了	6-2
entropy 更小的時候呢我其實只有一個譬如說三十八點一四	6-2
那這個是我的m ’	6-2
那也就是說我的entropy 變成log 的這個三十八點一四了	6-2
那你如果這樣看的話這個就這就是我更小的一個constraint ok	6-2
那這個意思是說	6-2
我們因為你對每一個不同的word 你的bigram trigram 都不一樣	6-2
給你的這個三萬個word 的這個distribution	6-2
你其實真的一直都在考慮這三萬word 的distribution	6-2
不過三萬word 的distribution 實在是每個distribution 都不一樣	6-2
你給你每一個bigram trigram 都不一樣實在有夠複雜我們希望用一個簡單的數字來呈現它	6-2
那個簡單數字就是我用另外一個distribution	6-2
它是uniform 的	6-2
看它總數是多少	6-2
那個總數越小就是constraint 越小	6-2
那那個總數其實就是entropy 的二的entropy 次方ok	6-2
那這麼一來的話這個東西其實就是我們講的size of virtual vocabulary in which all words are equally probable	6-2
那麼等於說在這個case 呢	6-2
我們本來第一個word 相當於要考慮一千三百七十二個字	6-2
它會equally probable 的那樣的選擇的	6-2
它的困難度是那樣的程度	6-2
到了bigram 這困難程度變成譬如說	6-2
相當於在兩百五十七個裡面	6-2
要選一個	6-2
這兩百五十七個是equally probable 的	6-2
到了trigram 的時候呢我剩下是這麼多個裡面要選一個	6-2
等於是這樣的意思	6-2
那這些個數字就是我們在講的linguist constraint	6-2
那我的language model 就是希望把這個把這個linguist constraint 一路把它縮小	6-2
然後我們可以看到它縮到多小	6-2
那這個東西叫做perplexity of the language	6-2
那你如果有一個language source s	6-2
譬如說是英文	6-2
或者譬如說是中文	6-2
或者說是文言文	6-2
或者說是白話文或者說是什麼你都可以這樣子來算	6-2
你可以算它的這個東西	6-2
那這個呢其實就是我們的每一個word 的出來的entropy	6-2
然後二的entropy 次方	6-2
相當於一個virtual vocabulary	6-2
那麼那底下我這個例子是跟剛才講的是一樣的例子啦	6-2
就是說你假設你算出來的那個是十個bits information	6-2
就相當於一千零二十四個	6-2
每一個都有相同的機率	6-2
那在那個情形之下呢	6-2
你的這個perplexity 所謂的perplexity 就是一零二四	6-2
perplexity 這個字你去查字典的話呢	6-2
是混淆度	6-2
也就是說你現在還無法選你的selection	6-2
你的constraint 就是只有這樣子	6-2
我有這麼大的混淆度	6-2
其實這個混淆度就是entropy	6-2
因為entropy 其實也就是混淆度的意思	6-2
那用另外一個字來講就是所謂的branching factor	6-2
所謂的branching factor 的意思是說就是你這邊你可以想像有多少個選擇	6-2
你要說三萬個六萬個也沒錯但是你也可以說就是一千八百七十二個選擇	6-2
對不對這你如果說這個word 有六萬個選擇的話是說	6-2
這六萬個選擇機率有大有小	6-2
所以呢倒底六萬個是都怎麼好選法你其實講不出來	6-2
那其實它的因為有大有小的關係	6-2
其實相當於共有一千八百個才要選的	6-2
這一千八百個是一樣的	6-2
那我就說它的branching factor 是一千八百	6-2
這是所謂的branching factor 的意思	6-2
那有了這個第一個word 之後我下一個word 的branching factor 就降低到兩百多了	6-2
因為我有了bigram	6-2
我就知道其實相當於我要選的第二個字是相當於是只有兩百五十七個裡面選一個	6-2
那這兩百五十七個是一樣的機率就是了	6-2
你這樣來看我的branching factor 變成兩百五十七	6-2
那麼等到有了這兩個字之後呢我的第三個word 呢我的branching factor 剩下三十八	6-2
那就是說呢我現在的這個嗯只有三十八了等等	6-2
那麼我們如果這樣看的話呢這個就是所謂的branching factor	6-2
好有了這個之後呢	6-2
我們剛才所講的是一個language source 本身的perplexity	6-2
底下我們要講給我一個language model	6-2
我就可以算這個language model 的perplexity	6-2
那這個language model perplexity 也就是這個language model 的的嗯它的給我的linguist constraint	6-2
或者它給我的branching factor	6-2
那麼怎麼算呢	6-2
那麼你現在可以這樣想	6-2
其實一個language model 給我的就是每一個word 的機率	6-2
given 前面的condition	6-2
所以呢一個language model 就是給我這個下一個possible word w i 的機率	6-2
given 它的condition c i	6-2
例如假設對某一個word sequence 是w n 的話	6-2
這是我們某一個word sequence	6-2
我們講過後面這個應該沒問題你知道	6-2
就是我現在第一個word 的unigram	6-2
然後呢有了第一個word 之後第二個word 的bigram	6-2
以及第三個以後呢假設我這是用trigram 來算的話	6-2
就是前兩個的第三個的trigram	6-2
那這樣來看的話不管是哪一種unigram trigram bigram 這些gram 都一樣	6-2
都是一個某一個condition 下的下一個word	6-2
在這個case 的話因為它是unigram	6-2
所以那個condition 就是沒有condition 就是空集合	6-2
就是c i 就是空集合	6-2
那這個bigram 的話我的condition 就是前面一個word	6-2
第一個word 之後下一個是bigram	6-2
那後面的trigram 它的condition 就是前面兩個word	6-2
所以不管是unigram trigram bigram 它們等等的這些n gram	6-2
我都可以看成是一個condition c i 之後的下一個word w i	6-2
那如果是這樣的話呢	6-2
那其實用我們剛才這個圖來解釋的話呢	6-2
那這個每一個不是一個機率	6-2
就是一個distribution 就是一個這種東西	6-2
一個這種東西distribution	6-2
因為就是你現在給我這個condition	6-2
那這個word 有六萬種對不對	6-2
給我這個condition 這個word 有六萬種	6-2
給我這個condition 這個word 有六萬種	6-2
那這個六萬種呢有一個它的distribution	6-2
那那個distribution 不好算我就都算它的perplexity	6-2
或者說算它的branching factor	6-2
或者說就是把它考慮成為一個另外一個virtual vocabulary 它的size 是m ’	6-2
我都把它看成這樣子	6-2
那我怎麼做呢	6-2
那given 一個language model 我必需要有一個test corpus	6-2
那如果沒有一個test corpus 我是無法算這件事	6-2
那麼我們這話怎麼講我們底下就解釋	6-2
假設我有一個test corpus d	6-2
它裡面有n 個sentence	6-2
所以呢這是一個測試的database	6-2
那麼這個測試的database 裡面呢有總共有n 個sentence	6-2
所以呢每一個sentence 是大w 的一到n	6-2
那大w i 呢就是裡面有一堆word	6-2
就是小w 是它裡面的word	6-2
它的長度是n i 個word	6-2
所以呢它有n 個sentence	6-2
其中第i 個sentence 就是大w i	6-2
它的長度是n i 個word	6-2
所以呢這是小w 就是它裡面n i 個word	6-2
然後呢我總共的word 的數目是大n d	6-2
總共word 的數目是大n d	6-2
如果是這樣的話那我怎麼算它的perplexity	6-3
就是底下這個式子	6-3
這式子看起來有點複雜	6-3
不過其實很簡單	6-3
那它的意思應該就是說	6-3
用我們剛才這個來講的話就是說	6-3
我有一個測試的database	6-3
就是大d	6-3
它裡面有第一個句子叫做大w one	6-3
第二個句子就做大w two 等等等等	6-3
那麼這個長度是n one 這個長度是n two 等等等等	6-3
那全部有多少個word 呢有大n 個word	6-3
而不是大n d 個word	6-3
那就是這邊所說的事情	6-3
那如果是這樣的話那我怎麼算呢	6-3
我其實就是把這裡面的每一個word 　	6-3
每一個word 都可以根據前面它的condition 不管是bigram trigram 什麼gram	6-3
根據它前面的condition 都可以算這個word 的機率嘛	6-3
就是這個東西	6-3
就是這個機率嘛	6-3
那我就這個機率把整個sentence 通通取log 全部加起來	6-3
所以每一個第一個第一個可能是要算它的unigram	6-3
第二個算它的bigram 後面算它的trigram 等等	6-3
我把這機率全部加起來	6-3
然後呢那我就得到這一個	6-3
我就得到這一個sentence 的和	6-3
就是括號裡面這個東西對不對	6-3
我就是把每一個機率這個從第一個字的unigram	6-3
加上第二個字的bigram 第三個字的trigram 後面每一個每一個都有trigram	6-3
我都這樣去算	6-3
那麼把它全部加起來	6-3
就是這個式子	6-3
然後呢我每個句子都做這件事	6-3
全部加起來	6-3
然後呢除以全部的字數	6-3
那其實是什麼就是就是我在算所有的字的機率	6-3
等於說是我每一個字都在算它的它的unigram	6-3
它的bigram	6-3
然後它的trigram 等等	6-3
我就把每一個通通都這樣算把每一個字出現的也就是說我的我的language model 本來就是在給我這一堆機率	6-3
只是說這堆機率顯然你沒有辦法自己算	6-3
一定要給我一篇文章給我一篇文章	6-3
之後我才可以算每一個字每一個字的機率是多少	6-3
你沒有給我一篇文章我沒辦法算嘛	6-3
所以呢你就要給我一篇文章	6-3
這篇文章就是我們所謂的test corpus	6-3
那我有了這篇文章之後我就可以算每一個到底機率是多少	6-3
那這個式子其實只是在算所有的機率的平均對不對	6-3
你把所有的機率取log 再加起來再除以所有的字數	6-3
就是在算平均嘛	6-3
所以我就是把它這全部加起來這所有的機率我每一個字那每一個機率是什麼	6-3
其實每一個機率就是我們這邊講的這麼一個東西	6-3
它其實是一個distribution 裡面的某一個嘛	6-3
那它其實是在算這種東西嘛	6-3
那麼因此呢我其實是在算平均	6-3
那這個全部加起來算平均是因為我取了log	6-3
那如果取了log 加起來算平均的話	6-3
相當於我先全部都這個取n 分之一然後加起來再取log 是一樣的嘛	6-3
那你如果取n 分之一相當於是什麼相當於是做幾何平均嘛	6-3
相當於在做幾何平均嘛	6-3
那是什麼意思呢	6-3
那其實我最後的這個perplexity 就是二的這個次方	6-3
那二的這個次方的意思就跟我們之前講的這個二的這個次方是一樣的意思	6-3
你其實是在算一個你其實是在算一個virtual vocabulary	6-3
那這個講起來其實用底下這個講起來就很容易看	6-3
就好比這樣子	6-3
我的某一句裡面	6-3
這是這個是unigram	6-3
它算出來相當於是一零二四分之一	6-3
這個是bigram 算起來是五百一十二分之一	6-3
這是trigram 算起來是兩百五十六分之一	6-3
這是trigram 這是一百二十八分之一	6-3
這是trigram 等等等等等等等等	6-3
你把所有東西都平均起來看看到底這個language model這個language model 給我每一個word 都有一個機率	6-3
那其實這個機率就是告訴我這個一零二四或者五百一十二或者兩百五十六就是告訴我這個size m pron 這個size	6-3
就是告訴我它的branching factor	6-3
就是告訴我這個constraint 大還是小	6-3
就告訴我那個branching factor	6-3
那這個東西那我現在要算它的平均	6-3
怎麼算	6-3
什麼叫平均	6-3
我不是把一零二四跟五百一十二這些東西來平均不對的	6-3
我應該是把這些個機率一零二四分之一這些東西來做一個幾何平均	6-3
幾何平均之後再inverse 回來	6-3
譬如說平均出來變成三百一十二	6-3
那意思就是說in average	6-3
我每一次要看下一個word 是有三百一十二個可能	6-3
所以呢in average 我的branching factor 是三百一十二對不對	6-3
是這個意思	6-3
所以我們這個式子寫了半天其實就是只不過是做這件事情而已	6-3
ok	6-3
那我等於是在算這個language model 給我每一次要predict 下一個字的時候	6-3
每一次要算下一個字的時候就是在看你給我多好的多強的linguist constraint	6-3
讓我這個selection 的range 多大或者是多小	6-3
那因此呢那麼如果這是一零二四分之一相當於是說它有一零二四個這個一零二四分之一相當於我這邊這邊講的一八七二一樣的意思嘛	6-3
等於說我其實branching factor 是一八七二	6-3
我一零二四裡面我要選一個	6-3
這相當於是五百一十二裡面選一個	6-3
那平均起來在這整篇文章裡面	6-3
這個整篇文章來數一次平均一下就知道我這個language model 到底給我怎麼樣的constraint 呢	6-3
那不是把一零二四跟五一二去做平均	6-3
而是把它們的分之一這個機率來做幾何平均	6-3
之後的inverse 得到三百一十二	6-3
那才是平均那這個意思等於是說in average	6-3
我要算每一個下一個字的時候是有三百一十二個選擇	6-3
等於是在三百一十二個	6-3
這個變成三百一十二	6-3
對不對就等於說我這個m 等於三百一十二	6-3
我在三我有一個等於說in average 每一次要選下一個字是有三百一十二equally probable 的字裡面	6-3
要選一個	6-3
那這樣的話呢這些東西其實就是所謂的一個average branching factor	6-3
所謂的average 是什麼average in the sense of geometric mean of	6-3
就是這個一零二四的倒數	6-3
五百一十二的倒數它們的幾何平均	6-3
那麼如果這樣看的話呢上面這個其實就是在做這件事	6-3
我們這個這邊只是在做它的平均嘛	6-3
平均之後再做二的幾次方回來其實就是在做這件事是一樣的意思	6-3
所以呢這個是一個perplexity	6-3
這是這個language model 它的混淆度	6-3
或者說也可以說就是它所提供的那個linguist constraint	6-3
那個linguist constraint	6-3
也就是capability of language model	6-3
它可以predict 下一個word 那麼那麼這個這個的能力嘛	6-3
那麼你在這個language model 就是在幫我predict 下一個字	6-3
那麼它的這個constraint 有多強	6-3
因此呢是越小越好對不對	6-3
越小的話表示我的constraint 越緊	6-3
那表示我的越好	6-3
那麼因此呢我們可以根據那個language model 大小	6-3
來看它的這個language model 好不好	6-3
不過呢這個language model 我們不能自己來算	6-3
一定要有一個test corpus	6-3
所謂test corpus 就是要有一個文章嘛	6-3
一篇文章或者一萬篇文章要有這個東西之後	6-3
每一篇文章算出來都不一樣的嘛	6-3
所以一定要有一個測試的文章或者是一群文章	6-3
然後做為test corpus	6-3
這個時候你的language model 就可以去算	6-3
你就可以知道它的能力有多強	6-3
所以它是function of 這個p 是這個language model	6-3
就是這個p	6-3
我們就用這個我們就用這個p 的這個代表一個language model	6-3
那這個language model 就是這個p	6-3
然後這個d 就是我的這個test corpus	6-3
它是一個function of 這兩個東西	6-3
它是一個function of 這兩個東西	6-3
那這樣子的話呢我就可以算出這個language model 好壞	6-3
那這個也就是我們講的perplexity	6-3
也就是它的這個average 的branching factor	6-3
那也就是我的這個也可以說就是一個跟我們這邊意思是一樣的就是size of 一個virtual vocabulary	6-3
就是in average 如果我得到三百一十二的話呢	6-3
就是in average 我每一個字都相當於好像有一個三百一十二個word 的一個vocabulary 在那裡	6-3
讓我來算	6-3
等於是這樣的意思	6-3
ok 好我們停在這裡休息十分鐘	6-3
好有了剛才的那個perplexity 的定義之後	6-4
你了解它的意思我們先來看一個比較簡單的例子	6-4
這個例子很簡單這樣你比較有點感覺它們在講什麼	6-4
這邊所做的事情是我們把網路上的這個新聞網站去download 一堆新聞	6-4
就是我們中文的新聞網站台灣的新聞網站download 一堆新聞	6-4
然後這些新聞裡面它的網站它自動分類了	6-4
這政治新聞啊什麼等等財經新聞體育新聞等等等等社會新聞這樣	6-4
總共各download 量這麼多	6-4
然後總數是這麼多	6-4
那這個呢我們拿它來train	6-4
當成這個training corpus 然後去train language model	6-4
所謂train language model 就是去算那個n gram	6-4
去算unigram bigram trigram	6-4
當你language 算好之後	6-4
我就可以來算	6-4
那我另外還要找一堆testing corpus	6-4
因為我要算我們剛剛講要算這個perplexity 一定要有個文章才能算嘛	6-4
我另外要再找一堆testing 的	6-4
去算這個testing 的我沒有寫在這裡就是了	6-4
那另外算找一堆testing 的這個嗯corpus 來算它的language model	6-4
那我們剛才講的這件事情其實也可以這樣子看就是說	6-4
你顯然是要用兩個database	6-4
一個是train 它一個是test 它	6-4
所以譬如說我這個是一個這個training training corpus	6-4
我用這個來train 那堆language model	6-4
所以呢我就會我用這個language model 的training 的演算法	6-4
去算它的n gram	6-4
所以這邊得到的就是n gram	6-4
就是我們剛才所寫的w i 的機率	6-4
這個東西	6-4
這就是我的n gram	6-4
我有了n gram 之後呢	6-4
我要有另一個testing corpus	6-4
d	6-4
就是我們講的testing corpus	6-4
p 跟d 嘛	6-4
就是我們前面這一頁所說的p 跟d	6-4
算出這個東西出來這是perplexity evaluation	6-4
ok	6-4
所以我顯然需要兩個兩個database	6-4
這個是拿來train	6-4
train 出來這些東西之後我用另外一個去測它	6-4
那因為光是這個不能算嘛我一定要有個東西去測它所以拿這個來來測它然後算出這個來	6-4
那我剛才講的這個例子也是這樣	6-4
那麼我們這邊是	6-4
分別為每一類的新聞做一個language model	6-4
我有政治新聞的language model我有體育新聞的language model 等等等等	6-4
那你看到這個總共這麼五十八點一的裡面	6-4
政治新聞占了十九點六台灣的新聞裡面政治新聞比例之高啊	6-4
這個台灣人之熱衷政治可以想像	6-4
那那我現在假設是	6-4
這裡的每一類我都做了個language model 之後我都在每一類我也都找個testing corpus 去去測的話	6-4
算出來的perplexity 在這裡	6-4
這是第一類第二類這是右邊這一	6-4
我現在講的是domain specific	6-4
就是每一個domain 做一個language model	6-4
所以我政治新聞做一個language model	6-4
財經新聞做一個language model 等等等等的話呢	6-4
我然後我就分別用政治新聞去測	6-4
我有另外政治新聞的testing corpus	6-4
財經新聞有財經新聞的testing corpus 分別去測的話	6-4
得到的perplexity 是右邊這條灰色的	6-4
那你可以看到呢譬如說在這裡最低的是什麼我們說越低越好嘛	6-4
因為它代表的是我的我的這個constraint	6-4
對不對	6-4
我的branching factor 是越低越好嘛	6-4
最低的是五號	6-4
五號是體育新聞	6-4
它體育新聞其實只用了最小的training data 它只有這麼小的量	6-4
就train 出一個來	6-4
它就只有三百多	6-4
那這什麼意思呢你可以想像其實體育新聞是一種最簡單的語言	6-4
因為它講的其實就是一堆體育的事件	6-4
球賽啊	6-4
這隊多打一球那隊多贏一球然後它輸了一球所以它多了兩分然後它贏了	6-4
等等	6-4
喔	6-4
它大敗它什麼東西就這樣子	6-4
所以它的內容其實它的辭彙也很單純	6-4
它的語言結構也很單純就是這些事情	6-4
那這就是體育新聞所以呢它雖然有很小的training corpus 它得到的perplexity 也是最低的	6-4
但是你注意看一下	6-4
我們的政治新聞perplexity 也很低	6-4
我們的政治新聞裡面其實是內容如果我們去看的話是千變萬化	6-4
裡面的這些政治人物說的話是多的不得了很有學問這樣那樣	6-4
但是如果我們去分析他們說的語言的話	6-4
它的辭彙也就是那些辭彙	6-4
調過來調過去	6-4
然後它的句型也就是那些句型而已	6-4
所以它講的話其實跟打一場球是一樣的	6-4
只是在	6-4
只是在打另外一種球好像是不同的球就是了	6-4
那就是我們政治新聞裡面的	6-4
這個說穿了就是口水了口水就是這樣	6-4
那你說什麼是perplexity 最高的	6-4
四號	6-4
四號你看這麼高比人家高好多好多	6-4
四是什麼呢	6-4
這個其實是文教新聞	6-4
就是這個文教也就是包括教育文化啦你可以想像有教改	6-4
有大學校長的問題有這個什麼這個哪一個這個嗯哪一個博物館怎樣啦有一個什麼這個歌劇來表演啦又是什麼	6-4
什麼樣的都在裡面	6-4
所以文教是一個最複雜的環境	6-4
它的辭彙也最豐富它的語言也最豐富所以它的perplexity 是最高的	6-4
我想這是一個簡單的例子大概你應該這樣你比較了解我們講的perplexity 是什麼	6-4
那左邊這個藍色的線是什麼呢	6-4
左邊藍色的線是說我現在做一個general domain 的	6-4
就是我現在把所有的東西	6-4
我不分領域我當成一個然後train 一個language model 的話	6-4
然後再分別去去那個language model 就等於是全部平均在一起啦	6-4
全部平均在一起之後我再拿那個去算剛才的那些個測試的database	6-4
我剛才有政治新聞有財經新聞我那些我再去測的話呢	6-4
那你會發現它們的perplexity 都比較高	6-4
這藍色這個都比較高	6-4
換句話說它的我所得到的機率的這個我所得到的機率的這個branching factor 都比較大	6-4
那意思也就是說我的language model 比較差	6-4
對不對	6-4
perplexity 越高就是越差嘛	6-4
那換句話說	6-4
為什麼比較差就是因為其實每一個domain 它有它自己的語言結構	6-4
它有它自己的辭彙跟語言結構	6-4
所以財經新聞跟體育新聞跟政治新聞跟社會新聞它們內容顯然是不一樣的	6-4
所以它們之間的n gram 的關係是不同的	6-4
你如果做個general 的domain 的language model 把它們全部平均在一起當然也可以	6-4
但是呢也就因此你的constraint 是比較差嘛	6-4
你如果針對每個domain 自己去算的話都會比較小	6-4
因為它的domain 是比較精緻	6-4
它的那個變化比較少變化所以比較perplexity 比較小	6-4
這就是這邊講的domain specific 這個domain specific 的language model 呢	6-4
我用domain specific 的corpus 去train 的話	6-4
那麼它都會有一個比較好的	6-4
都會比這個general domain 的language model 來的好	6-4
而這裡面呢體育新聞是最低的perplexity	6-4
那它是語言結構最單純的	6-5
那這個例子大概有一點感覺我們再回過頭來剛才上一頁	6-5
還有一樣東西沒有講	6-5
就是這個cross entropy	6-5
那麼	6-5
這個倒沒什麼特別這個只是在你去讀這些書的時候	6-5
你如果讀前面我們講的這些這些書裡面不管哪一本它都說	6-5
這個language model 的perplexity 是一種cross entropy	6-5
它們都說它是一個cross entropy	6-5
這個perplexity 為什麼是一個cross entropy 呢這點不太容易了解	6-5
所以我們這邊解這一頁其實是在解釋說perplexity 為什麼是一種cross entropy	6-5
那麼我們在禮拜天補課的時候曾經說過這個	6-5
所謂的cross entropy 的定義是一個這樣的東西	6-5
也就是我有兩個distribution 一個是p 一個是q	6-5
然後我p log p 除以減掉p log q 的這個東西	6-5
通常是大於零的	6-5
這個東西叫做cross entropy	6-5
它是p 跟q 兩個distribution的一個	6-5
它是p 跟q 兩個distribution的一個distance measure	6-5
或者叫做k l distance	6-5
這是我們上次所說的	6-5
那麼在那個情況之下呢我們也說過這個p log p 比p log q 大	6-5
我們可以把它寫出來一個是p log p 一個是p log q	6-5
那這兩個東西寫起來就變成這樣的一個式子	6-5
這個式子就是所謂的jason inequality	6-5
那這個也是我們上次所說過的	6-5
那這個也是我們上次所說過的	6-5
那我現在要講的是說	6-5
它們課本上說這個perplexity 是一種它們說perplexity 是一種cross entropy	6-5
這句話所講的cross entropy 不是這個cross entropy	6-5
是哪一個是這一個	6-5
喔	6-5
也就是說	6-5
這個什麼叫做cross entropy 其實不同的人它的取的名字講的東西不太一樣	6-5
所以你不要被confuse 了	6-5
那麼很多人講的cross entropy 是講這一個	6-5
這是我們週末補課的時候講的是這個	6-5
但是呢也有很多人把這個叫做cross entropy	6-5
就是p 跟q 之間的p 跟q 之間的差異	6-5
p 跟q 之間的差異	6-5
那麼	6-5
這個叫做那我我這邊為了區別起見我把它寫成x 的	6-5
那麼這個叫做那我我這邊為了區別起見我把它寫成x 的p 跟q	6-5
這個x 也是cross 的意思嘛	6-5
那那麼有的人是把這個叫做cross entropy	6-5
那它們課本上所謂的perplexity 是一種cross entropy	6-5
是指是一個這種東西	6-5
那我們底下要解釋的是為什麼這個東西叫做	6-5
那我們底下要解釋的是為什麼這個東西叫做為什麼這個東西就是我們講的perplexity	6-5
那麼這個東西是什麼就是這我們禮拜六補課講的	6-5
就是說	6-5
就是說它是一種entropy	6-5
當p of x 是被當成q 了弄錯了的話	6-5
它整個的entropy 會變大對不對	6-5
我本來是算p log p 的	6-5
但是假設這個p 不是正確的p 是變成q 的話	6-5
它會變大	6-5
那那個變大的那個差異叫做cross entropy	6-5
那這是它們的這個定義	6-5
那底下我要講的事情是說	6-5
我們剛才講的這個perplexity	6-5
我們剛才講的這個perplexity剛才講的這個perplexity 其實就是一個這個東西	6-5
所以它們說它是一個cross entropy	6-5
為什麼是這個東西呢	6-5
因為它其實就是一堆	6-5
因為它其實就是一堆log 的平均值	6-5
喔	6-5
那這個式子其實就是我們上一堂課講的	6-5
你看我們上一堂課講的這個perplexity	6-5
這樣算的式子是這樣寫的	6-5
這是什麼就是一堆p	6-5
這是什麼就是一堆p一堆probability 然後這個取了log 之後全部加起來平均嘛	6-5
對不對	6-5
一堆probability 取了log 之後全部加起來然後全部平均嘛就是這個式子嘛	6-5
那跟我們現在寫的這個式子是完全一樣的	6-5
一堆probability 取了log 之後全部加起來然後平均嘛	6-5
對不對	6-5
喔	6-5
所以這個式子跟剛才是完全一樣所以我們那個perplexity 可以說就是這個	6-5
那其中的這個q 就是我們所說的這個language model 的這些n gram 的機率	6-5
就是我們這邊所講的q	6-5
那我其實這些perplexity 的這個東西就是這些q 的全部加起來平均	6-5
那如果是這樣的話	6-5
那麼我們要說呢這個其實可以看成是p log q	6-5
也就是一個cross entropy	6-5
為什麼呢	6-5
根據所謂的law of large numbers	6-5
這個名詞你一定是熟悉的	6-5
你在小的時候就偷這個	6-5
機率還是統計什麼就學過所謂的law of large numbers	6-5
什麼是law of large numbers 呢	6-5
我們說假設有一個random variable	6-5
你做了很多的experiment 去測它是多少	6-5
你得到一堆值	6-5
譬如說我們說它的值	6-5
跟它的次數	6-5
假設說是投一個擲一個骰子或者什麼東西	6-5
我的值是a one 的時候	6-5
我總共得到幾次呢總共得到n one 次	6-5
值是a two 的時候我得到	6-5
n two 次	6-5
等等值是ai 的時候我得到n i 次等等等等	6-5
我總共做了	6-5
n 次實驗	6-5
我做這麼多次實驗加起來有這麼多個n 次	6-5
假設是擲一個骰子這就是一二三四五六	6-5
各有多少次當然也可以是其它的各種random variable 的實驗	6-5
那麼這個時候到底平均值是多少呢	6-5
那麼我們的算法可以是這樣算就是summation 的ai n i	6-5
然後再除以n	6-5
對不對	6-5
這就是平均	6-5
我就是把ok 得這麼多的這麼多次	6-5
那麼我就其實就是把全部乘起來	6-5
然後全部加起來	6-5
那總共有大n 次我就除以n 這就得到平均值	6-5
那這個平均值你可以有另外一個寫法	6-5
就是把它寫成	6-5
這個summation 的	6-5
嗯ai 乘上n i 除以n	6-5
那這個其實就是機率pi	6-5
也就是說	6-5
發生a one 有幾次是n one 除以n 的這樣的機率對不對	6-5
發生a two 是n two 除以n 等等	6-5
所以這個就是這個pi 就是發生	6-5
值是ai 的機率	6-5
我就每一個值乘上它發生的機率乘起來不就是平均嘛	6-5
我就可以算成這樣	6-5
那所謂的law of large numbers 是說呢你做了夠多的實驗之後	6-5
那麼這個limit	6-5
n 趨近於無限大的時候	6-5
這個東西會等於它	6-5
當你實驗做的不夠多的時候這個這個機率不準嘛	6-5
實驗做的不夠多這個這個不準你不見得是	6-5
等於但是當你實驗做到夠多次的實候n 趨近於無限大的時候你就可以用這個來算	6-5
這個叫law of large numbers	6-5
那這個應該很容易沒有問題你如果這個了解的話那我們這邊講的事情是一樣的事情	6-5
那我們說左邊的這個式子其實就是在	6-5
就是我們剛才算的perplexity	6-5
就是在把所有的機率加起來	6-5
那相當於就是	6-5
在我這個情形就變成是	6-5
a one 加a one 加a one	6-5
總共加幾次加n one 次	6-5
再加上a two 加上a two	6-5
總共加了嗯不是小	6-5
n one 次總共加了n two 次	6-5
等等等等	6-5
這個a one 加了這個	6-5
a one 加了n one 次其實就是	6-5
就是a one n one	6-5
這邊都寫錯了這都是小寫	6-5
小寫這樣比較不會弄錯這是	6-5
這樣子	6-5
那麼你如果說這個	6-5
對不對你現在如果a one 總共	6-5
發生了n one 次你就全部給它加起來就是a one 乘上n one	6-5
然後a two 總共發生n two 次等等	6-5
就是這裡面的一項就是這裡面的一項	6-5
等等等等全部加起來的話你其實可以分別	6-5
你如果知道	6-5
你如果知道a one 發生的機率是多少乘上它的機率就好了嘛	6-5
就是這個意思	6-5
那你如果從這來看的話呢這就是我們那些language model 的機率	6-5
就是相當於這邊的	6-5
這些個東西	6-5
這些個a one	6-5
就是因為我就是要算這些機率的平均嘛我的perplexity 就算這些機率的平均嘛	6-5
那我怎麼算我就是把它們全部加起來平均嘛就是左邊那個式子	6-5
就是把它們全部加起來平均嘛	6-5
但是你可以看成是用這個東西乘上它出現的a one 乘上它出現的機率	6-5
這樣也可以啊	6-5
如果你知道它的機率的話	6-5
所以呢你看我這個等號左邊寫的就是說	6-5
averaging by all sample 你把全部通通都加起來	6-5
你把它們全部通通加起來來做平均	6-5
就是averaging by all samples	6-5
那右邊呢是averaging 如果它的機率是知道的話	6-5
如果你求的出來這個機率你知道的話	6-5
對不對我現在的對應到那上面的式子	6-5
這個ai 就是我的q of x	6-5
我現在在求那個q of x	6-5
就是這些機率所以q of x k 其實就是	6-5
就是我們前面在算的那些perplexity 的機率	6-5
那我要算這些東西的平均	6-5
那我們的算法就把它全部加起來除以n 嘛	6-5
但是你如果知道它每一個的機率是多少的話	6-5
譬如說你知道它的ai 的機率是pi 的話	6-5
你其實這樣算就可以啦	6-5
當你寫成這個式子的時候	6-5
那麼於是呢就是當它的值是某一個值的時候它的機率是多少你把它寫成這樣子的話	6-5
那這個東西其實就是上面這個式子	6-5
它就是一個cross entropy	6-5
那你如果這樣看的話呢這個q of x	6-5
就是我們的這個	6-5
而這個p of x 是什麼	6-5
就是真正的機率	6-5
所謂所謂的真正的機率是	6-5
這裡面算出來的機率	6-5
那麼	6-5
我現在的這個這個	6-5
n gram	6-5
是用這個算的	6-5
問題就在這裡因為這兩個不一樣	6-5
我必須用一個training data 去train 出這些n gram 來	6-5
這個n gram 所得到的這些機率跟你現在測試這篇文章裡面的本來就不見得一樣	6-5
那這兩個不一樣其實就是	6-5
我們這邊講的p 跟q 是不大一樣的就是這兩個p 跟q 不大一樣	6-5
所得到的cross entropy 就是這個p 跟q 的	6-5
它不大一樣	6-5
那麼換句話說	6-5
我要求的我這邊在算的這個perplexity	6-5
是在算這裡面的n gram 的機率	6-5
從這裡面來算的	6-5
那麼我得到的就是這個東西	6-5
但是我平均怎麼平均法	6-5
是在這上面做平均	6-5
對不對	6-5
我是在這上面做平均是在這上面做平均所以呢	6-5
我是在用這個做平均如果我知道它的每一個case 是機率是多少的話	6-5
如果每一個pi 是知道機率是多少我可以把它寫成這個式子	6-5
那麼在那個情形的話呢我的這個仍然是我的q of x	6-5
而這個東西呢就是我的這邊所寫的p of x	6-5
那我如果知道說	6-5
有有百分之多少是這個值有百分之多少是這個值	6-5
我就根本用這個加就可以加出來嘛	6-5
那那這個東西p of x 機率是什麼	6-5
其實是在這個testing 的這個d 裡面的機率	6-5
而不是那個	6-5
那這兩個會不同	6-5
所以呢true probability 這個bar 加一個bar 這個true probability 是指這個	6-5
是在這裡面的true probability	6-5
跟我真正的那個是有差的	6-5
那麼因為這個的關係所以我就得到這個perplexity	6-5
所以這個perplexity 可以寫成這個式子	6-5
這個式子可以看成這個式子	6-5
那它就是一個cross entropy	6-5
那什麼是cross entropy 呢也就是我的真正在這個測試環境裡面	6-5
在這個測試環境裡面的這個機率	6-5
其實跟你的training 裡面的不大一樣	6-5
那中間的差異所構成的	6-5
恩	6-5
所以我們說它是一個cross entropy	6-5
當你true statistic 在測試裡面是interpolate estimate as 這個東西	6-5
by the language model	6-5
ok	6-5
就是說你真正的	6-5
你應該是測試環境裡面的那個統計特性你現在用那個來做	6-5
恩	6-5
因此你可以說它是一個cross entropy	6-5
ok	6-5
那麼當然越大越不好	6-5
這個越大越不好也就是我們剛才所說的越小越好是一樣的	6-5
在這邊來講就是越大就是它們差異越大	6-5
這個對不對我們說過這個這個東西本來就是兩個distribution 的的difference	6-5
所以越大表示它們差的越多就表示你估計的越不對	6-5
恩	6-5
這個是這個我們解釋一下是為什麼它叫做cross entropy 的意思	6-5
好	6-6
有了這個之後再底下我們要說的是一個實際的問題就是我們真正在做language model 的時候其實不是這麼簡單來算n gram	6-6
n gram 我們這個老早就講過了	6-6
那個n gram 講起來很簡單	6-6
就是算這種東西嘛對不對	6-6
就是算這種東西嘛	6-6
這bigram 就這樣算嘛	6-6
講起來非常簡單如果真的那個簡單就沒什麼學問了	6-6
事實上的n gram 不是那麼容易算的	6-6
為什麼	6-6
因為有data sparseness 的問題	6-6
什麼是data sparseness 的問題就是有很多event 就是不會發生	6-6
也就是unseen event	6-6
又來了	6-6
那麼因為我們這邊所有的從h m m 到language model 到n gram	6-6
都是用統計的觀點來做的事情	6-6
那統計永遠是盡信統計不如無統計	6-6
那麼統計你不能那麼相信它因為統計永遠有一堆unseen event	6-6
在你的統計資料裡面沒有看到的東西不表示它不會發生	6-6
只是它沒有	6-6
那就像我們之前講的tri phone 我有一堆unseen tri phone	6-6
我得把它train 出來	6-6
看不到也得train 出來	6-6
一樣那這裡我的n gram 也有一堆unseen 的n gram 我要怎麼處理	6-6
同樣的問題	6-6
那麼這個例子我們前面已經說過了就是	6-6
假設有一句話叫做jason immediately stand up	6-6
這是多麼普通的一句話	6-6
但是很可能你算它的n gram 機率就是零	6-6
為什麼呢	6-6
你有很多jason	6-6
你的training data 裡面很多jason 它後面就是不接immediately	6-6
你有很多immediately 它前面就是不接jason	6-6
所以呢	6-6
這個jason 後面接immediately 的機率就是零	6-6
只要這個bigram 是零的話它就是零了	6-6
那你這個句子永遠不會辨識正確	6-6
一定就錯掉了	6-6
可是沒有道理啊這麼普通的句子怎麼可能是零呢	6-6
顯然就是一個unseen event	6-6
沒有出現在database 裡面但是我們必須讓它有值	6-6
不能讓它沒有值	6-6
所以怎麼辦	6-6
trying to assign some non zero 的probability to all events	6-6
即使它們沒有出現	6-6
所有的event 你都要給它一個值	6-6
恩	6-6
即使沒有出現都要給它一個值	6-6
那是怎樣	6-6
就我們剛才的說法來講	6-6
我們用每一個event 有一個機率這樣來看的話	6-6
會有一堆是沒有的	6-6
會有一堆沒有怎麼辦所有沒有的你都要給它一個值	6-6
或者有大有小或者怎樣你都要給它一個值	6-6
當你給它一些值之後顯然要把別的值弄小一點	6-6
因為加起來才是一啊對不對	6-6
所以你那些大的要變小一點	6-6
然後呢讓這些沒有的都讓它有一點點	6-6
或者有大有小都要讓它有	6-6
那這個過程叫做smoothing	6-6
恩	6-6
那就是所謂的language model smoothing	6-6
那你看就知道這個意思是smoothing 就是把高的弄低一點	6-6
然後塞到這些低的地方去嘛	6-6
所以是一個smoothing 的過程	6-6
這是我們所謂的language model smoothing	6-6
那怎麼做這件事情呢	6-6
一個最簡單的想法是所謂add one smoothing	6-6
這是最簡單的想法不過這個方法很不好	6-6
你做出來效果很差就是了	6-6
講起來是很直覺	6-6
就是我把所有的event 次數都加一	6-6
我所有的次數都加一就是once more occurs once more than actual does	6-6
我所有次數都加一所以所有零次都給成一	6-6
一都變成二二都變成三三都變成四	6-6
全部都加一的話這樣都有了	6-6
那麼舉例來講	6-6
要train 這個bigram 的時候這個bigram 我們說過本來就是這樣子嘛	6-6
對不對	6-6
就是我這兩個詞連在一起的count 除以前面那個全部的count	6-6
這就是我的bigram	6-6
那我全部都加一的結果呢就是	6-6
那你可以看得出來我的分母所謂的所有的這個前面那個word 的count 就是它後面接的任何的一個word 的count 嘛	6-6
那我這樣通通都加一的結果呢就是分子的這個count 加了一	6-6
分母就加了我的詞的總數	6-6
因為後面接的每一個都有一次都要加一嘛	6-6
恩	6-6
所以就加就變這樣	6-6
這就是所謂的add one smoothing	6-6
它有一定的效果它讓我沒有零的了	6-6
全部都有是沒有錯	6-6
但是這樣做出來的效果並不好	6-6
所以後來沒有人真的用這個方法	6-6
不過這個通常是這個最容易最直覺想像的所以呢我們就說一下	6-6
因此呢我的bigram 就變成分子加一	6-6
然後分母呢就是加一個total number of 不同的word	6-6
這樣我就把所有的後面會接的通通都算進去都加了一就是了	6-6
那比較有效的smoothing 通常是底下這兩個觀念的衍伸	6-6
真正的做法不見得是這樣但是呢大概用到這兩個觀念	6-6
一個是所謂的back off	6-6
一個是所謂的interpolation	6-6
那麼什麼是back off 呢	6-6
這個back off 的這個觀念很簡單	6-6
這個式子寫成這樣是有夠頭大	6-6
其實很簡單我們不要看這個式子的話	6-6
用簡單的符號來寫的話呢	6-6
應該是可以寫成這樣的意思	6-6
p n 的bar 是等於嗯p n 如果p n 大於零	6-6
如果a 的p n 減一如果p n 等於零	6-6
喔	6-6
其實只是這樣的意思	6-6
那我這裡所謂p n 就是n gram 的那個機率	6-6
像你仔細看這個東西搞了半天其實它就是一個n gram 嘛	6-6
就是given 前given 從這個這個w i 的機率	6-6
given i 減一i 減二一直到i 減n 加一這不就是n gram 嗎	6-6
所以就是我這邊寫的p n 就是n gram 的意思	6-6
那我要求的n gram 呢如果那個n gram 存在的話大於零就是那個count	6-6
這個是從i 減n 加一一直到i 的那個word sequence 兜在一起的那個count	6-6
如果真的是大於零的話這個就是就是這個n gram 是存在的嘛	6-6
那我就用就用原來這個n gram	6-6
如果它不存在呢如果count 是零的話沒有的話怎麼辦呢	6-6
我就用n 減一gram	6-6
所謂的lower order 就是n 減一gram	6-6
所以你看這個變成n 減一gram 所以是從從這個word i 的機率given 從i 減n 加二到i 減一	6-6
也就是n 減一gram	6-6
我就把n 減一gram 拿來然後呢乘上一個某一個factor a	6-6
這個的意思是什麼呢	6-6
是只要是n 減一gram 大的話n gram 應該也會大	6-6
n 減一gram 小的話n 也會小	6-6
為什麼呢我們舉底下一個例子這個例子是課本上的一個例子	6-6
你知道這個thou 跟you 是同義字	6-6
這個是古字這個是現代字	6-6
你如果去念莎士比亞的作品什麼羅密歐茱麗葉他們講的話的you 都是thou	6-6
喔	6-6
所以thou 就是you	6-6
但是呢這個是古字這個是現在字所以現在來看的話呢	6-6
這個機率是很大	6-6
這個機率是很小	6-6
如果這個機率比這個機率大很多的話	6-6
那你在任何一個字後面看到它的機率也是一樣的嘛	6-6
譬如說	6-6
你see 後面看到它跟see 後面看到它	6-6
顯然也是這個大這個小嘛	6-6
就是這個意思ok	6-6
你如果這一個字機率比它大的話	6-6
那麼它在那麼它在別的後面看它也是機率比較大嘛	6-6
那因此呢	6-6
我們剛才的這個是unigram	6-6
比較大的unigram	6-6
它如果後面要接什麼的話它的前面有什麼東西的話呢它的bigram 也是比較大的嘛	6-6
那bigram 比較大trigram 也是比較大的嘛	6-6
trigram 比較大的話four gram 也是比較大的話	6-6
所以呢只要n gram n 減一gram 比較大n gram 大概也是比較大	6-6
n 減一gram 比較小n gram 大概也是比較小	6-6
那反過來這個n 越小這個gram 越容易算得出來越會存在	6-6
對不對	6-6
因為n 越小的話你只是在算一個越小的單位的機率越會出現越容易算到	6-6
n 越大越算不到嘛	6-6
所以呢如果一個那麼大的一個count 不存在的話	6-6
我就退一步算n 減一gram 的機率	6-6
然後呢基本上你可以假設	6-6
這個n gram 跟n 減一gram 應該是有一個比例關係	6-6
當然問題是這個a 怎麼求你得有辦法求這個a 就是了	6-6
恩	6-6
你如果有辦法求這個a 的話呢	6-6
你就可以用n 減一gram 來估計n gram	6-6
那這個a 我們這邊寫就是說它是一個function of 這些東西	6-6
那depends on 你這些東西是什麼要怎麼算這個a	6-6
喔	6-6
那我們後面還會說到一些怎麼算這a 的情形不過基本上就是這樣	6-6
所以呢就變成說是所以我這邊畫一個bar	6-6
就是你真正經過這個smoothing 之後	6-6
經過這個smoothing 之後的這個n gram 就是畫一個bar 的	6-6
那沒有經過smoothing 直接照我們原來公式去算的就是沒有畫bar 的	6-6
所以呢你的smoothing 的功能只是在如果那個count 不存在的時候	6-6
就用n 減一gram 來做	6-6
然後乘上一個factor	6-6
那個factor 你要算就是了	6-6
那這個方法叫做back off	6-6
也就是退一步	6-6
退到一個n 減一gram 去算	6-6
退到lower order	6-6
就是這樣的意思	6-6
那跟這個很像的就是interpolation 意思其實是很像的	6-6
那這個式子也是這樣看起來很複雜	6-6
其實你如果用這樣來寫比較簡單的話呢	6-6
就是寫成寫成這樣	6-6
就是我的n gram 是什麼呢	6-6
就是b 乘上	6-6
嗯	6-6
就是b 乘上n gram 加上一減b 乘上n 減一gram	6-6
我看對不對	6-6
yeah 沒有錯	6-6
也就是說呢怎麼講呢就是即使即使你的n gram 有non zero counts	6-6
即使你有non zero counts 你也不要太相信那個n gram	6-6
你不如用n 減一gram 去跟它interpolate 一下	6-6
阿	6-6
也就是說我們剛才講你n 減一gram 永遠比n gram可靠	6-6
為什麼你的數的數目比較多嘛	6-6
你你	6-6
你n 減一gram 出現出現的機率一定比n gram 要來的大	6-6
所以數的數目比較多之後一定比較reliable	6-6
所以你永遠去跟n 減一gram 去做一個平均做一個interpolate	6-6
這樣得到的n gram 會比直接算這個的n gram 要來的可靠一點	6-6
恩	6-6
那問題是這個b 是多少	6-6
那b again 是一個function of 這堆東西	6-6
就你要想辦法估計這個b 之後	6-6
拿n 減一gram 來跟它來跟它做一次這個interpolation	6-6
那也是一樣我這邊加了bar 的表示是我smooth 之後的	6-6
我是用smooth 之後的n 減一gram	6-6
跟我新得到的n gram 去做一個interpolation	6-6
得到我smooth 之後的n gram	6-6
等等	6-6
恩	6-6
那這樣子呢即使我有count 是non zero 我也這樣做	6-6
這樣會比較好	6-6
這是所謂的interpolation	6-6
這是基本觀念是這樣	6-6
那詳細的做法那就有底下的這些做法了	6-6
恩	6-6
那麼你如果去看前面給你那些reference 課本裡面	6-6
它都會說好幾種重要的smoothing 的方法	6-6
因為事實上language model 都需要做smoothing 就是因為我們剛才講的這個問題	6-6
就這些unseen event	6-6
那這堆問題事實上都蠻多的	6-6
那麼你永遠都要做這些事情	6-6
那	6-6
嗯	6-6
有好幾種方法	6-6
那麼我們沒有辦法說哪種比較好	6-6
因為可能在不同的狀況	6-6
有的時候這個比較好有的時候那個比較好	6-6
那麼不見得一定是誰比較好	6-7
所以每一種都存在都可都值得用	6-7
喔	6-7
那我們沒那麼多時間講每一種所以我們就舉一個例子	6-7
這個例子就是所謂的good turing smoothing	6-7
那麼good turing 的基本觀念	6-7
good turing 顧名思義你知道這是兩個人的名字	6-7
一個人叫做good 一個人叫做turing	6-7
恩	6-7
不是不是說他很好	6-7
那麼	6-7
那麼	6-7
他是用這個good turing 的一個方法來做這件事情	6-7
阿	6-7
那它的基本精神是怎樣呢	6-7
就是說properly decreasing 的frequency for observe event allocate some pro frequency to unseen event	6-7
這講起來其實就是我們剛剛講的話了	6-7
就是你有一堆unseen event 怎麼辦你一定要把一些機率給它嘛	6-7
你就要把你就要把你看得到的event 一些frequency count 降低	6-7
降低去丟給它	6-7
喔	6-7
那怎麼做這件事情呢	6-7
我們用底下的一頁的簡單的例子來講	6-7
這是它的一個假說	6-7
倒不見得一定說是最正確的方法	6-7
只是它用這個假設來這樣子做	6-7
恩	6-7
那這個這個這個例子在這裡	6-7
這個例子是假設你去釣魚	6-7
假設你去釣魚	6-7
那麼我希望根據我釣到的魚來判斷海裡的所有的魚它們的distribution	6-7
那當然就是有一堆魚你沒釣到嘛	6-7
那就是unseen event	6-7
那你怎麼估計那些你沒釣到的魚	6-7
恩	6-7
它說呢假設我去釣魚總共鈞到六種魚	6-7
我這一號二號三號四號五號六號	6-7
就是我總共鈞到六種魚	6-7
鈞到幾條呢	6-7
一號魚顯然是很貪吃所以一來就吃到它馬上就釣起來	6-7
所以我總共釣到十條	6-7
二號魚鈞到三條三號魚釣到兩條	6-7
四號五號六號各釣到一條總共釣到十八條	6-7
於是呢我總共看到這我總共得到這十八條魚	6-7
我根據這十八條魚裡面譬如說一號有十條之多	6-7
然後二號三號等等等等	6-7
那裡面我有四五六是各一條	6-7
這樣總共有十八條	6-7
我要根據這十八條魚來判斷海裡總共有多少魚	6-7
做個統計的話	6-7
顯然是有問題	6-7
因為還有一大堆魚沒有看到	6-7
假設海裡有一千種魚的話	6-7
我只看到六種	6-7
還有九百九十四種呢都是unseen	6-7
恩	6-7
那因此怎麼辦呢我要在這十八裡面把某一些魚算成是unseen 的	6-7
然後呢我再來	6-7
然後我把每一個魚的其它的每一種魚的機率都降低	6-7
阿	6-7
它等於是這樣的一種這樣的一種想法	6-7
ok 好我們在這裡停十分鐘	6-7
恩	6-7
那我們這個	6-7
休息十分鐘的時候請助教進來set up 我們的那個習題	6-7
這樣各位可以可以可以開始去看我們要做的習題	6-7
ok	6-7
OK 好	6-7
哦 我們現在先請助教來講一下我們的第一題的習題哦	6-7
那 第一習題第一題的習題就是我剛才講過就是我們把這個整個 HMM 從 從這個 哦 training 到最後測試我們都給各位做一次	6-7
那這個 因為我們進度太慢了	6-7
本來這個應該是在期中考以前做完 哦	6-7
那 那因為我們進度太慢所以現在才剛剛把它講完而已	6-7
各位可能也需要一點時間來把它讀清楚然後再來做等等	6-7
那因為下週又放假了實在很頭大	6-7
那麼我們好容易才把進度補上來又放假了	6-7
但是要再補課嗎 嘶	6-7
我想大家都補怕了我也補怕了	6-7
所以 也許我們下週先放假再說 哈	6-7
那麼先放假再說 但是我又怕進度耽誤太多 所以我們呢 我們就先給各位這個習題 哦	6-7
那麼什麼時候交 我們待會再再再討論什麼時候交	6-7
不過我們至少先給各位習題 這樣子要趕快念的人可以先趕快做這樣子 哦	6-7
那麼關於我們補課的時候這錄影帶的問題 有人因為某些原因不能沒有來上	6-7
想要借錄影帶這個問題呢 我們會再研究清楚怎麼做	6-7
然後在網路上 在網站上公告怎麼進行好不好 哦	6-7
就是說你補課的時候因故不能來 需要想要這個 看這個錄影的話我們用什麼方式來做到	6-7
那麼因為基本上這個是我們學校的進修推廣部來幫我們做這件事情	6-7
那麼他們擁有這個智財權等等的問題我們不能隨便 copy 給人家等等 有這個問題在	6-7
所以我們要想一個比較有效的辦法來做這件事情	6-7
哦 所以我們有什麼辦法做的的做的時候會在網站上公告	6-7
那基本上各位去上網注意看我們想辦法做到這件事這樣 OK	6-7
那現在還不知道我們再研究一下這樣子	6-7
好那現在先請助教來講這個題目好不好	6-7
嗯大家好我們第一個作業就是有關於 HTK 的	6-7
然後至於這個 toolkit 可以到這個網頁的這個地方去下載有關它的東西	6-7
然後嗯這網頁是你要必須要 log  in 才可以給你下載的	6-7
所以你可能需要先註冊一個帳號	6-7
然後註冊完以後第三點是請大家稍微注意一下的	6-7
因為嗯我們這一 這一個作業裡面我們提供了同學一些批次檔可以直接使用	6-7
但是因為嗯就是 HTK 最新的版本的時候有些東西跟我們原先的批次檔是不符合的所以執行上可能會有錯誤	6-7
所以希望同學要記得是要下載舊的 這個是前一個版本的	6-7
那大家稍微注意一下是先進入 download 的頁面下以後會找到一個叫 browse  HTK  software  archive	6-7
然後下面會有一個連結叫 HTK  software 在這裡面會有所有的 HTK 的 version	6-7
然後大家就大家就點點選這一個下載	6-7
然後這片投影片我們已經放到那個網站上了	6-7
然後如果說你是在工作站上執行 HTK 的話	6-7
然後你就是把程式碼自行 compile	6-7
然後嗯 如果是你是在 windows 上的話我們在這一份作業裡面我們也有直接附執行檔	6-7
所以你可以直接在 windows 上也可以跑就對了	6-7
就不用再去下載那些 tool	6-7
嗯然後我們的資料一樣是公告在網站上就在平常的地方下載	6-7
那這個稍微注意一下不要隨便改變目錄結構	6-7
因為那個批次檔都是已經寫好的	6-7
所以如果你隨便改變可能會導致 error	6-7
好然後接下來是大概講解一下嗯執行的流程	6-7
然後反正就是那個我們在在那個 BAT 這個資料夾下面有寫著每一個批次檔	6-7
然後嗯嗯 小妞可以開一個批次檔的範例	6-7
應該已經打開了	6-7
對所以是大概是這樣子的啦	6-7
所以大概 大家可以看一下這個東西然後 HTK 的網站上面也有 HTK  book	6-7
所以大家可以 check 一下看每個指令是在做什麼的	6-7
對然後這個我們的第一步是抽取聲音的特徵參數然後抽完之後	6-7
下一張	6-7
抽完之後我們就要算 mean 跟 variance 下一張	6-7
對然後接下來這個地方就是因為因為接下來的批次檔會在這個位置去讀檔案所以請大家要複製一份上面都寫的其實還蠻清楚的	6-7
然後嗯你可以用程式複製或是直接把它手動 copy 過來都可以	6-7
對然後接下來就是再重新估算重新估算它的 mean 跟 variance	6-7
然後這樣的嗯剛才的流程就已經完成了是只有一個 mixture 的 model	6-7
那我們還可以繼續反覆反覆這兩個動作	6-7
就是前兩頁的那兩個動作就可以一區估算比較多的 mixture 這樣通常會有比較好的效果	6-7
就是像這邊講的	6-7
然後我們的範例就是我們的批次檔裡面我們只寫 有寫到四個 mixture	6-7
啊如果大家要更多我們就是就是自行跑	6-7
然後嗯看結果會有適當的加分	6-7
然後最後存會把 model 檔存在這個地方	6-7
然後我們跑完 training 之後我們也要跑 testing 所以 testing 的話就是拿來跑 viterbi 然後批次檔是執行這個	6-7
然後最後跑完以後我們還可以跟正確答案比較這也是寫好了所以大家就可以知道你跑完你所 train 的 model 在 testing  data 裡面正確率是多少	6-7
然後我們這個這個範例的正確率其實沒有特別的高	6-7
對 然後作業要求很簡單就是至少把所有的範例程式跑一遍就是把批次檔有寫給你的東西你把它跑一遍	6-7
然後如果你有改進正確率就是結果會 improve 的話那我們會適當加分	6-7
然後希望大家把那個就是這個 homework  one 的這個來信主旨寫成這樣子不要寄成很奇怪的名字這樣可能不小心就刪掉這樣	6-7
就是 DSP  homework  one 後 後面是你的學號	6-7
然後寄給嗯我或者是另外一位助教都可以	6-7
就是 data 一樣是在網頁上就直接下載就全部都包在一起了連這個說明文件都在一起	6-7
OK 　好我補充一下哦	6-7
補充一下我想剛才講的 HTK 就是我早上一開始講過就是這個那套這個 HMM  toolkit	6-7
那這個照剛才助教講的就是你怎麼樣去去註冊登記然後就可以下載	6-7
你要下載正確的版本才不才才不會不 match	6-7
那然後這個我們會給你這個這個測試語料跟訓練語料	6-7
那是一堆所謂的連續平衡句在後面 嗯	6-7
那這些個語料基本上來講都是有智慧財產的不能隨便外流的 阿	6-7
所以呢也就是說你知道我我們這些東西都是這樣	6-7
你如果沒有 data 什麼都不能做	6-7
那這個這個有天大本領沒有 data 就是不能做所以呢所有事情是 rely  on  data  data 是最重要的	6-7
所以這個在很多時候 data 都是最重要的財產	6-7
那麼我們的這些 data 都是跟人家有簽過約的都是不得外流的	6-7
那不過我們當時簽的約都有一個但書就是教育目的給少數人學習練習是可以的	6-7
阿 所以呢我們都拿來給各位當習題用	6-7
但是呢就是這個拜託各位 make  sure 就是不要流出去只是你用而已	6-7
就是你 download 下來你用	6-7
用完之後不要給別人哦你就是就是只是放在你那裡就是我沒有叫你還給我	6-7
但是就是	6-7
但是你不要給別人 阿 你給別人就不對了 阿	6-7
然後因為這樣的關係我們我們基本上是這個只在網站上於三天七十二小時	6-7
你要在三天之內 download 完畢	6-7
哦 你什麼時候做沒有關係但是你三天之內沒有 download 的話我們就收起來了	6-7
然後就 哦 那基本上你也不應該再轉給別人所以你如果沒有 download 你就沒有了就是了	6-7
我們從什麼時候開始	6-7
現在放好沒有	6-7
嗯現在已經放好了	6-7
現在已經放好了	6-7
所以 就是從從今天開始	6-7
所以那就從今天中午十二點開始嘛好不好	6-7
所以就是今天中午十二點	6-7
今天是禮拜二所以到禮拜五中午十二點為止好不好	6-7
三天之內七十二小時之內 download 完畢	6-7
過了時間我們就就就收掉了就是了 哦	6-7
哦 然後哦我們這邊講的就是這些 這些步驟我想你你像這個助教剛才都已經講過了	6-7
那你一開始先要 train 這個 HMM	6-7
那 train 的時候這裡所謂的一個 mixture 就是一個 Gaussian	6-7
所以你一開始先做第一個 Gaussian	6-7
之後你可以把它 split 變成兩個變成四個等等	6-7
那它這邊說這個這邊所寫的只有做到四個 Gaussian 哦	6-7
範例只有做到四個	6-7
當然你要再多跑你可以變成八個十六個都可以做	6-7
但是並不表示越多就一定越好因為 data 不夠多 哦	6-7
我們給你的 data 其實是很少的	6-7
所以呢你如果 train 太多的 Gaussian 之後一定 data 不夠多那 Gaussian 會都不好了	6-7
所以呢會有一個有一個上限就上不去了這個你自己可以做實驗就知道	6-7
那麼這個我們給你的這個叫做 我看看哦我們叫做連續平衡句	6-7
那麼所謂的連續平衡句是這個這個 balance 的 sentence	6-7
balance  sentence 意思是說我在 minimum  number  of  sentence 裡面要 cover 所有的音	6-7
哦 就是說我其實就是用最少的數目的的句子	6-7
minimum  number  of  sentence 我讓所有的音都出現	6-7
那其實這個很難我們剛才我們之前講過就知道	6-7
你要有那麼多 phone 你要每一個音都出現其實不是那麼容易	6-7
一定是你如果隨便弄一堆句子的話一定是那堆高頻的音出現很多	6-7
一些低頻的音沒有出現 哦	6-7
那麼因此呢都要經過特別選的	6-7
所以像這個 balance  sentence 怎麼選呢也是用 computer 選的啦 哦	6-7
就是說我們用一個很大的語料庫	6-7
然後這個 有一個程式去選這些	6-7
我儘可能選那些這個一句裡面有最多不同的音的句子	6-7
然後我一定優先選那些低頻的音 哦	6-7
越是低頻的音先優先選出來	6-7
然後這個那它同時就會把高頻的音帶進來	6-7
之後看還缺什麼音就把那些缺的音再把他選進來等等	6-7
這也是一個程式做出來	6-7
所以呢我們那些 你如果去看那些連續平衡句的話	6-7
它那個句子也是特別的句子	6-7
那麼那樣的話在最少的句子裡面把所有音都都有	6-7
所以你才可以 train	6-7
所以這個你去 其實 train 出來是我們國語所有的音所有的 syllable 的	6-7
我們現在 train 的單位應該是 initial  final	6-7
就是聲母韻母	6-7
哦 所以你就會把所有的聲母韻母都 train 的 哦	6-7
到但是呢其實我們的連續平衡句的 database 是非常多的	6-7
應該是有一百個男生一百個女生	6-7
然後每一個人都念了多少遍 哦	6-7
但是現在總共只給你三百句什麼的	6-7
是那裡面很小的一個 subset	6-7
所以也就是說 train 出來一定不好 哦	6-7
那麼因此你到時候做測試的時候你測試出來正確率一定是低的啦 哦	6-7
你如果正確率低不要覺得是差	6-7
是因為我們給你的 data data 很少的關係 哦	6-7
所以呢並不是以那個正確率高低來看 哦	6-7
那這個總共這個裡面有三百句是訓練語料	6-7
有五十句是測試語料	6-7
那你用那三百句 train 好之後用那五十句去測 哦	6-7
那然後再 等等 你可以算正確率等等	6-7
你可以算你要做幾個 Gaussian 等等 哦	6-7
那 所以要寄到哪	6-7
寄給助教寄 email  address 有寫嗎	6-7
寫在網站上 哦	6-7
所以 OK 你就 就照網站上講就行了	6-7
那麼	6-7
哦 我想剛才應該裡面有提到說有些地方你如果要做得比較詳細可以加分	6-7
之外呢我想你如果仔細看那個 HTK 的 HTK 的網站可以 download 整套的 manual	6-7
有詳細的裡面所有東西的說明 哦	6-7
你如果真的有興趣你可以仔細地去 去了解	6-7
把裡面做的事情了解清楚	6-7
然後這個在交的時候 交的比較完整有更清楚說明我想那些都是可以加分的	6-7
那我剛才提到就是說整個的過程	6-7
是非常複雜的一堆	6-7
那我們裡面有一些我們講得比較清楚	6-7
譬如說哪個怎麼做哪個怎麼做	6-7
但是有的很多	6-7
其實我們沒有說那麼清楚	6-7
那麼因此呢看你 你要自己用心到什麼程度	6-7
那你其實用就就就照助教這幾個 step 這樣跑你也可以跑得出來	6-7
那你要多下一點功夫多去看也可以就可以看更多 哦	6-7
那 那個 那本 那 那個詳細的 manual 你如果詳細看的話可以更了解 哦	6-7
那都可以加分	6-7
所以我想這個是看你要下多少功夫而已	6-7
所以我們就給你大概這個讓這個 哦	6-7
下禮拜如果我們又又放假的話那我們至少就是你如果要要趕快多多念一下的話你就可以先來做這些事情	6-7
好 那我們要來討論一下的是 什麼時候要交	6-7
今天是三月二十八	6-7
下週是四月四號吧好像	6-7
十一號十八號二十五號 哦	6-7
那現在下週變成 下週變成放假	6-7
所以呢	6-7
哦 如果兩週以後就是十一號	6-7
但是好像十八號開始是期中考是吧	6-7
好像這一週是期中考週 哦	6-7
所以呢我們一個合理的時間如果是兩週的話	6-7
是在期中考開始之前交	6-7
但是如果這個有點太趕的話因為其實我們才剛剛教完	6-7
這個 TRI PHONE 什麼什麼東西	6-7
那你如果太趕的話我們晚一點再交也可以	6-7
不過又碰到期中考好像晚也沒有太大意義	6-7
所以我們決定一下看要什麼時候交	6-7
各位覺得	6-7
那我們的	6-7
這個裡面是 這個習題裡面是包括 training 跟 testing	6-7
你 testing 其實就是跑 viterbi 我們前面講過的 viterbi	6-7
所以其實我們都有說過	6-7
但是我們其實都只有講裡面的一小塊	6-7
並沒有真的整個 link 起來過 起來過	6-7
那這個題目應該是讓你把所有東西都 link 起來就是了	6-7
我想一種可能是在十一號交	6-7
那是兩週以後	6-7
一種可能是再晚一點	6-7
乾脆十八號二十五號	6-7
譬如說	6-7
不過 好像也沒有太太好因為這時候是期中考	6-7
有沒有意見	6-7
哈 我們的期中考	6-7
我會傾向於晚一點	6-7
哦 那麼因為我在五月中要出國	6-7
我希望在那一週考期中考	6-7
這樣子的話	6-7
阿 五月中 阿	6-7
那 如果是那樣的話就等於補考了 等於補課了啦 哦	6-7
否則的話又變成又又必須至少還要再補課啦	6-7
阿 那如果說我五月中的出國的那一次	6-7
來考期中考的話我們期中考就會比校例規定期中考大概晚好幾週 大概晚個三四週吧	6-7
那這個 那一方面因為我們的 期中考範圍本來就比較 比一半多一點所以應該是要稍微晚一點	6-7
那乾脆如果再晚個一兩週就變成我出國時間這樣的話呢就變成少補一次課	6-7
哦 那所以是那樣的話我們的期中考其實還還還有一段時間 哦	6-7
所以看我們要不要定哪 哪一個時候	6-7
早一點就是十一號	6-7
十一號好不好	6-7
不好	6-7
十八	6-7
二十五	6-7
十八好了啦	6-7
十八這樣有三週應該差不多了啦好不好	6-7
OK 好那這樣我們就定了就是十一月十八號嗯	6-7
四月十八號	6-7
四月十八號 那麼就是有三週的時間	6-7
那麼交的方式就是直接 mail 助教你去上網看就有了這樣子好不好	6-7
 OK 那然後就是我們剛才講補課的時候的錄影帶怎麼處理	6-7
那你也上網去看我們有決定怎麼辦的時候	6-7
在網路上告訴你這樣子好不好 哦	6-7
OK 好 那這樣子我們今天就上到這裡 嗯	6-7
好 我們上週停了一次課	6-7
很久不見	6-7
那我們兩週前正在講六點零language model	6-7
然後我們說到smoothing	6-7
那麼我們說所謂smoothing 的問題	6-7
是指這個data sparse ness	6-7
也就是說有一堆unseen event	6-7
就跟就跟在五點零我們講的unseen 的triphone 一樣	6-7
主要就是有一堆unseen event	6-7
你不能完全根據data 來看	6-7
這個例子我們說過很多次就是像這個例子	6-7
jason immediately stand up	6-7
這個名字非常普通	6-7
但它很可能在你的大量的training data 裡面它後面就是不接這個字	6-7
這個字也很普通	6-7
在你大量的training data 裡面搞不好前面就是不接這個字	6-7
那這樣的話呢這個bigram 就機率就是零	6-7
那麼這個呢就是機率就是零	6-7
所以這樣的一句話它就是出不來	6-7
那麼因此呢你就不能完全用train 的來得到你的bigram	6-7
那因為就是有一大堆unseen event	6-7
那這種情形 嗯 也就是說我們盡信統計不如無統計	6-7
你一定要想辦法讓所有的unseen data 不能機率是零	6-7
那麼unseen data 機率不能讓它是零的情形就是說你現在雖然有的data 機率很高	6-7
有的是零	6-7
那你顯然要讓所有的零我們都讓它有一點點data	6-7
所有的零都要讓它有一點data	6-7
有一點機率	6-7
如果零都有一點機率的話就表示那些不是零的機率要降低一點這樣整個機率才是一嘛	6-7
不是零的降低一點然後把所有的零的都把它讓它多一點	6-7
這樣的一個想法就是所謂的smoothing	6-7
也就是aside some non zero probability to 所有的events	6-7
即使never occur in the training data 你也不能假設它一定完全沒有	6-7
所以我們都要讓它多讓它多放一點進去	6-7
這是這個sparse ness 的觀念	6-7
那最原始的想法就是add one	6-7
所有的event 次數都加一	6-7
所以零都變成一	6-7
一都變成二	6-7
不過這個方法呢並不好	6-7
那我們說比較好的想法來自一個是back off	6-7
一個是interpolation	6-7
所謂的back off 的觀念就是說你用n 減一的gram	6-7
就是說你用這個next lower order	6-7
你退一步退到next order 也就是說你如果n gram	6-7
你如果n gram 找不到的話我就用n 減一gram	6-7
像這個地方其實這個就是n gram	6-7
那n gram 找不到的話我就用n 減一gram	6-7
這是n 減二所以n i 減n 加二嘛所以就是n 減一gram	6-7
那理由就是n 減一gram gram 永遠都比n gram 要reliable	6-7
你曉得這個this is a	6-7
a 這個前面要在is 之後的a	6-7
這個是bigram	6-7
那麼你如果這個找不到的話至少a 一定比較多嘛對不對	6-7
is a 一定比較少a 一定比較多嘛	6-7
所以呢這個unigram 這個這個一定比bigram 容易找到	6-7
那你如果要找this is a 的話	6-7
那這個找不到的話呢is a 八成是有的	6-7
也就是n 減一gram 永遠比n gram 要機會要來得多嘛	6-7
所以呢你這個如果n gram 找不到的話就用n 減一gram	6-7
來替代	6-7
做某一種scaling	6-7
那這個是這個back off 的意思	6-7
就退一步退到n 減一gram 去	6-7
然後呢interpolation 是說你想辦法用一個比較reliable 的來跟它interpolate	6-7
那在這邊的例子呢就是你就乾脆就用n 減一gram來interpolate	6-7
所以呢你就用一個n 減一gram	6-7
你得到的n gram 也許不可靠你就用一個n 減一gram 來跟它interpolate 那這樣比較可靠一點	6-7
那這一類的想法	6-7
那這兩個只是基本的精神back off 跟interpolation	6-7
那真正做的時候呢就是嗯要有一些做法	6-7
那麼你如果去看我們前面給你的reference 的話	6-7
這些書上課本上都會說到不只一種	6-7
你不管是這一本還是這一本還是這個大概都會說到不只一種這個smoothing 的方法	6-7
那其實我們今天看到所常用而有效的smoothing 方法不是一種	6-7
是好幾種	6-7
那麼我們沒有辦法說哪一種比較好	6-7
因為嗯有的有的狀況這種比較好有的狀況那種比較好	6-7
換句話說因為這非常depend on 你是用哪一堆data 去train	6-7
然後你的training data 缺哪些東西	6-7
那麼然後你是在什麼情況之下都不一樣所以呢在這個狀況這個方法比較好	6-7
在那個狀況它還是比較好	6-7
所以呢事實上是存在好幾種方法都不錯	6-7
然後你真的要比的話呢有的時候它比較好有的時候它比較好這樣子	6-7
那我們不會有那麼多時間講那麼多種所以我們就講一個例子	6-7
就是good turing	6-7
那good turing 的基本精神呢	6-7
就是我們這邊講的就是說呢我想辦法decreasing relative frequency	6-7
對於observe events	6-7
然後呢把這些機率呢allocate 到unseen event 去	6-7
其實就是我們這邊講的這些本來是unseen 的我都給它一些機率	6-7
這這些都要給它機率的話就表示說我這些看到的機率要降低一點	6-7
凡是observe 的data 機率要降低一點然後呢給這些unseen 的機率	6-7
那它的這個這個這上面就是剛才講的	6-7
我把這個這個observe 的event 的機率降低一點	6-7
然後呢把這些降降掉的機率呢allocate 給unseen event	6-7
那這底下所說就是它的formulation	6-7
假設說我的event 總共有大k 種	6-7
那麼小k 等等代表某一個event	6-7
然後呢那種event 的發生的次數	6-7
叫做n 的k	6-7
那就以剛才的例子而言	6-7
一號魚就是k 等於一嘛	6-7
二號魚就是k 等於二	6-7
然後n 呢等於k 就是它看到幾次嘛	6-7
所以以剛才的例子而言	6-7
k 等於一就是一號魚就看到看到十次就是釣到十條嘛等等	6-7
那麼n 等於六就是六號魚一條等等	6-7
好那那我現在就以n of k 代表是k number k 的event	6-7
看到的次數	6-7
那你總共看	6-7
那你這個這個那當然這個啊你裡面有一堆是unseen 的	6-7
那些unseen 的event 的n of k 都是零嘛	6-7
unseen event 的n of k 等於零	6-7
但你現在把所有的event 通通加起來就是你的總共的observation 數目是大n	6-7
就是我總共看了這麼多次	6-7
ok	6-7
那它現在重新排一次	6-7
它說我們重新這樣的看	6-7
我重新這樣子看	6-7
就是event 出現的次數	6-7
跟這種次數的不同的event	6-7
就是出現r 譬如說出現零次的	6-7
叫做n 零	6-7
出現一次的	6-7
叫做n one	6-7
出現二次的叫做n two	6-7
出現r 次的叫做n r	6-7
什麼意思	6-7
所謂出現零次的	6-7
是說你那些unseen event 那些unseen event	6-7
到底有幾幾種unseen event	6-7
就是n 零	6-7
以我們剛才的例子	6-7
就是九百九十四	6-7
你有九百九十四種unseen 都是你沒看過的	6-7
所以unseen event 就是有n 零	6-7
種那你只看到一次的那種event	6-7
叫做n one 種	6-7
那以剛才那個例子而言	6-7
我只看到一次就是三條魚嘛	6-7
就是就是三種	6-7
就是三種	6-7
三種event	6-7
不同event 的總數	6-7
不同event 總數所以剛才看到只只看到一條的那種呢	6-7
有三次	6-7
所以這不同event 的有有三種魚啊	6-7
所以n one	6-7
那麼看到有兩次的有幾種	6-7
n two	6-7
看到r 次的有n r 種	6-7
我們重新不用剛才這個n of k 而現在用n r 來代表	6-7
所以呢這邊講就是這個意思	6-7
n r 是number of 不同的event	6-7
occur r 次哦	6-7
就是你現在發生你看到r 次的	6-7
不同的event 有n r 種	6-7
那麼different event 的	6-7
它的次它的這個出現的次數是r 次	6-7
所以出現r 次的有n r 種	6-7
好如果是這樣的話呢我的總共的event 數目剛才是n k	6-7
summation over k 我現在n r	6-7
要乘上r 之後summation over r	6-7
也是一樣的	6-7
對不對	6-7
就是說零次乘以n 零	6-7
沒就全部沒有看到次數	6-7
一次乘以n one 兩次乘以n two 嘛	6-7
那全部加起來像剛才的話加起來的就是十八嘛	6-7
十八條就是這樣來的	6-7
就是一乘三啊什麼什麼什麼加起來	6-7
那就是十八	6-7
那這就是r 乘以n r	6-7
好那麼good turing 的基本的原則就是底下所說的這些	6-7
那這些是什麼呢其實就是我們剛才講的	6-7
我們說	6-7
你等於是把只看到一次的那十八分之三	6-7
當成是看到沒看過的機率	6-7
你就從現在所看到的十八條魚裡面	6-7
有三條是第一次出現的	6-7
你就可以假設十八分之三是會看到沒有看過的魚的機率	6-7
所以呢一次的那種就當成是unseen event	6-7
那十八分之三是什麼就是這個	6-7
一乘以n one	6-7
你把這個一乘以n one 的總次數歸給它	6-7
因為這邊是零次	6-7
對不對	6-7
這個乘以它就是就是它次數就是就每一種的次數都在這裡嘛所以這是零	6-7
這個一零乘以n 零一乘以n one 二乘以n two r 乘以n r	6-7
這通通加起來就是總次數所以這每一個乘起來都是它的次數	6-7
就是這個r 乘以n r 的這個次數	6-7
那麼你就把這個一乘以n n one 的這個呢就歸給它	6-7
算是沒有看過的	6-7
那麼這麼一來我這是可以了我unseen event 可以這樣分之後	6-7
那一次的沒有了怎麼辦呢	6-7
那我就把兩次的這些event	6-7
歸給它	6-7
那兩次的有了歸給他之後那兩次的沒有了怎麼辦呢那我就把三次的歸給它	6-7
就這樣子	6-7
那所以呢r 次的這些東西呢就歸給r 減一次	6-7
那r 次又沒有了怎麼辦呢	6-7
我用r 加一次	6-7
它有n 的r 加一	6-7
我把它歸給r 次	6-7
ok	6-7
所以這個想法呢跟有一點像是都減一的味道	6-7
這個發生一次的	6-7
這個東西呢給零次	6-7
發生兩次的呢給一次	6-7
發生三次給兩次	6-7
發生r 加一次的給r 次	6-7
好我們剛才有一個說是全部給它加一	6-7
這個有一點像減一不完全是啦	6-7
不過有一點這樣的味道	6-7
那這個想法呢就是這個所謂的good turing	6-7
那麼嗯寫在底下這句話裡面	6-7
就是所有發生的次數	6-7
那麼你的這個	6-7
原來是第r 次乘上r 乘上n r 就是	6-7
發生r 次的總共的發生r 次的所有的event 總共的次數是r 乘以n r	6-7
我現在變成是r 加一乘上n 的r 加一了	6-7
ok	6-7
原來發生r 次的總共的event 應該是r 乘上n r	6-7
現在變成r 加一乘上n r 加一	6-7
所以呢	6-7
就是就是這一句話所說的意思	6-7
當你變成這樣子之後呢	6-7
那麼你unseen event 到底被分到多少呢	6-7
就是n one	6-7
n one 次一乘以n one 的分給n 零個event	6-7
於是呢我每一個event 被分到的機的次數是多少是	6-7
n one 次除以n 零	6-7
嘛那以剛才那個例子就是三條魚分給九百九十四種	6-7
每一種魚是九百九十四分之三的機率	6-7
的的次數	6-7
所以呢你的每這個unseen event	6-7
所以呢你的每這個unseen event 每一個分到的次數是n one 除以n 零	6-7
那如果是這樣的話呢	6-7
那我總共有n 零個event 的嘛	6-7
所以這邊總共被分到多少呢	6-7
就是n 零乘上這個就是n one	6-7
就是這個n one 次分給n 零了	6-7
那這就是底下最後這一行所說的	6-7
我所有的account 給unseen 的呢就是我每一個unseen 的被分到的次數是n one 除以n 零	6-7
然後我總共有n 零個所以就是我總共是n one 次分給unseen event	6-7
好那如果是這樣的話	6-7
當然我現在全部都可以重算一次	6-7
它重算一次的話呢	6-7
我現在這個	6-7
譬如說我r 次的原來是r 次	6-7
總共有n r 種	6-7
現在這個次數送給它了	6-7
之後我用了r 加一的n 的r 加一的這些次數給它了嘛	6-7
所以呢我重新算一算它變成幾次呢	6-7
那它就它就被當成是r 的star	6-7
我重新算它是r 的star	6-7
那麼這個二呢也有二的star	6-7
一也有一的star	6-7
零也有零的star	6-7
就是我重新用這個方法來算之後	6-7
它的每一個event 到底有幾次呢	6-7
以剛才為為為例的話呢	6-7
這個零的star 就是n one 除以n 零	6-7
也就是我現在unseen event 的每一個unseen event 發生的次數	6-7
是n one 次除以n 零嘛	6-7
那那個呢叫做新的零的次數就叫做零的star	6-7
同樣呢我也可以有一的star	6-7
二的star	6-7
那r 的star 是什麼呢	6-7
就是r 加一乘上n 的r 加一除以n r 嘛	6-7
也就是我把r 加一乘上n 的r 加一	6-7
這是這邊的總共的次數	6-7
每一個發生r 加一次	6-7
然後有n 的r 加一種	6-7
所以這麼多次之後呢	6-7
歸給上面這個了	6-7
那麼上面這個其實只有n r 種啊所以呢就要它它乘它除以它嘛	6-7
所以呢就要它它乘它除以它嘛	6-7
ok	6-7
所以這就是r 的	6-7
ok 所以這就是r 的star 的意思	6-7
r 加一乘上n 的r 加一就是我這邊的n 的r 加一種每一種發生r 加一次	6-7
這麼多的次數我現在分給它了	6-7
分給n r 之後	6-7
那麼現在它的次數呢我們叫做r star	6-7
那就它乘它除以它	6-7
所以呢那就是good turing 的基本精神就是這麼做	6-7
那當然如果這麼做的話呢	6-7
那麼我r 的star 就不是零嘛所以unseen event 都有一個次數	6-7
就是這個次數	6-7
那如果是這樣的話	6-7
那我的總次數仍然沒有改變	6-7
他用這個推的意思就是說總次數沒有改變嘛	6-7
那你看就知道沒有改變嘛它它只是換一換位置而已總次數沒有改變	6-7
那他這個式子這樣寫的方法意思是說呢我現在r 這個r 次的變成r star 次了	6-7
r star 次仍然是有n r 種	6-7
對不對r star 次仍然是有n r 種	6-7
然後我summation over 所有的r 的話呢	6-7
那麼現在r star 呢我換成這個式子	6-7
r star 換成上面這個式子乘以n r 的話呢	6-7
那其實相當於這樣子	6-7
那這樣加起來還是n 嘛	6-7
所以呢我的總次數沒有改變	6-7
這就是good turing 的的estimate 的基本的精神就是這樣子	6-7
那這個方法基本上是不能算是完全對但是是一種做法來解決	6-7
因為你真的不知道unseen event 到底是多少	6-7
和用這個方法來做	6-7
那這個方法其實是有一些基本的問題的	6-7
那有它有兩個最明顯的問題存在	6-7
第一個問題是說它把所有的unseen event 的機率看成一樣	6-7
那你你可以想像我海底的沒有看到的魚有九百九十四種	6-7
難道它們都機率都一樣嗎	6-7
這裡面還是有機率高跟機率低的嘛	6-7
那顯然不是這樣平分的嘛	6-7
那他現在是用平分的	6-7
這個是不合理的地方	6-7
那你照說這個應該不是平分	6-7
這是第一個問題	6-7
然後第二個問題是ok 它分給它它分給它它分給它這都講得通	6-7
那最最大的那個怎麼辦	6-7
假設說原來是出現最多的是r 跟n r	6-7
出現最多的那個event 是出現大r 次	6-7
它有n r 種的話	6-7
現在呢這堆呢都分給r 減一去了	6-7
所以出現最多的這一群其實應該是機率最高的那一群沒有了對不對	6-7
這是機率最高的那一群是最重要的那一群現在沒有了因為都分給r 減一去了	6-7
所以最高的沒有了	6-7
所以這是它的兩個明顯的問題需要解決的	6-7
那你如果去看我們說過good turing 這是兩個人的名字	6-7
他們是統計學家他們其實做的是生物統計	6-7
你如果去看他們原始good turing 的paper 的話	6-7
他是在他們都不是做language model 他們是在做細菌的統計	6-7
那麼在統計細菌的時候	6-7
有千千萬萬的細菌所以這個不是問題	6-7
沒有沒有這樣的問題因為細菌千千萬萬	6-7
所以你這個加是summation 是加到無限大去	6-7
這個加到無限大去所以沒這個問題	6-7
可是在我們這裡這個問題是存在的	6-7
那因此呢怎麼解決這個時問題呢	6-7
ok 所以我們剛才所說的就是用這個方法來做	6-7
所以我們剛才這裡講你如果把十八分之三歸歸給看不見的九百九十四種魚的話	6-7
那麼你的原來六號魚應該是十八分之一	6-7
現在就變成十八分之一的star	6-7
那一的star 就是我們剛才講的那件事情	6-7
就是你把二兩次的那個歸給一次這樣來算	6-7
那一的star 之後你這樣這樣我分的是十八分之一star	6-7
這樣算就會變成二十七之一	6-7
它的機率就會降低了就是這個例子在說明	6-7
那麼我們講這個good turing 有它的問題所以怎麼辦呢	6-7
那問題是怎麼樣做這件事	6-8
那good turing 這個是兩個統計學家他們當年在解決統計問題統計問題有一堆這種問題	6-8
所以呢他當時想了一個方法就是所謂good turing 的smoothing	6-8
那這個我們也許用下一頁的這個例子來解釋是最清楚的	6-8
我們上次上課的時候就停在這裡	6-8
假設說你現在出海去釣魚	6-8
那總共呢釣到十八條魚	6-8
這十八條魚總共有十總共有六種	6-8
n 這個一號魚二號魚三號魚四號魚五號魚六號魚總共有六種	6-8
那麼一號魚呢特別笨特別貪吃	6-8
所以呢海面裡很多一釣總共釣起來十條是一號魚	6-8
二號魚呢有三條三號魚有兩條	6-8
四五六各有一條等等	6-8
這樣呢你總共釣到十八條魚	6-8
那你想根據你釣到十八條魚來估計海裡面的所有的魚的distribution 是怎樣的	6-8
那這個事實上就是一個非常不太可能的事情	6-8
那麼你海裡面有千千萬萬的魚你怎麼憑什麼用你釣到十八條來統計呢	6-8
那其實就是我observe 的event 就只有這麼多	6-8
然後它只有這六種	6-8
我們假設說我海裡總共有一千種魚	6-8
海裡面有一千種魚	6-8
但是我們現在呢總共只釣到了十八十八條	6-8
這十八條裡面呢是這樣distribution	6-8
那我憑這個十八條我我怎麼判斷這個整個所有的魚的distribution 呢	6-8
那他說你第一件事情你就要知道我現在只看到六種魚	6-8
所以我九百另外還有剩下九百九十四種	6-8
我是沒有看到的是unseen event	6-8
我必需把這十八裡面的一些機率分給它	6-8
剩下的機率再分給這十六條啊這這這六種	6-8
那你到底要分多少機率給這個九百九十四呢	6-8
他有一個非常簡單的假說	6-8
這個假說不見得很合理但是就是它的做法	6-8
那麼有一點道理但不是很對	6-8
但是呢他就是這麼做的	6-8
那他這個做法就是說呢	6-8
這十八條魚裡面現在有三條只釣到一條了有這有這三種	6-8
四號五號六號這三種魚只看到只釣到一條	6-8
既然只釣只釣到一條的話呢就相當於是說	6-8
你釣起來之前你是沒有看過的	6-8
也就是說這四號五號六號這三種魚	6-8
你釣起來的那個時候你是沒有看過的	6-8
所以把它看成是你沒有看過的魚	6-8
如果這樣的話呢你沒有看過的魚總共是十八分之三	6-8
ok 你就是把只出現一次的當成是unseen	6-8
也就是說你你可以想成這個他的想法等於說你這個你現在這十八次裡面有三次釣到的是沒看過的新魚	6-8
所以你就可以假設是說你凡是那種這這個十八分之三也就是你會釣到沒看過的新魚的機率	6-8
那如果這樣子來想的話呢	6-8
這個說法是有一點道理	6-8
不見得完全對但是呢他就是這樣做	6-8
所以呢我這個十八分之三呢就是沒有看過的新魚	6-8
所以呢你那些沒有看過的九百九十四種魚呢	6-8
你就把它除以十八分之三除以九百九十四	6-8
那所以呢那九百九十四種魚呢就給它這個機率	6-8
那你現在的再看到的這十條	6-8
這這六這六種魚呢	6-8
你你就把十八分之十五重新按照比例分給這六種魚	6-8
ok	6-8
這就是good turing 的最基本的原理	6-8
那麼因此呢我就變成說是把這十八分之三分給九百九十四種	6-8
那麼每一種呢沒看過的魚每一種有這樣的機率	6-8
那看過的這六種魚呢	6-8
來平來分這個十八分之十五	6-8
這十八分之十五按照它們的比例重新分一次	6-8
於是呢譬如說	6-8
六號魚分到的本來應該是十八分之一	6-8
它現在變成二十七之一了	6-8
就少了一點	6-8
那這個他他有一個算法	6-8
那麼這樣的話呢我就是讓它這個己經看到呢就是分這十八分之十五	6-8
所以呢就會少了一點	6-8
他就變成二十七分之一了	6-8
等等	6-8
這是所謂的good turing 的觀念	6-8
那說得更清楚一點我們就回到前一頁的這個	6-8
那它的這個這個這上面就是剛才講的	6-8
我把這個這個observe 的event 的機率降低一點	6-8
然後呢把這些降降掉的機率呢allocate 給unseen event	6-8
那這底下所說就是它的formulation	6-8
假設說我的event 總共有大k 種	6-8
那麼小k 等等代表某一個event	6-8
然後呢那種event 的發生的次數	6-8
叫做n 的k	6-8
那就以剛才的例子而言	6-8
一號魚就是k 等於一嘛	6-8
二號魚就是k 等於二	6-8
然後n 呢等於k 就是它看到幾次嘛	6-8
所以以剛才的例子而言	6-8
k 等於一就是一號魚就看到看到十次就是釣到十條嘛等等	6-8
那麼n 等於六就是六號魚一條等等	6-8
好那那我現在就以n of k 代表是k number k 的event	6-8
看到的次數	6-8
那你總共看	6-8
那你這個這個那當然這個啊你裡面有一堆是unseen 的	6-8
那些unseen 的event 的n of k 都是零嘛	6-8
unseen event 的n of k 等於零	6-8
但你現在把所有的event 通通加起來就是你的總共的observation 數目是大n	6-8
就是我總共看了這麼多次	6-8
ok	6-8
那它現在重新排一次	6-8
它說我們重新這樣的看	6-8
我重新這樣子看	6-8
就是event 出現的次數	6-8
跟這種次數的不同的event	6-8
就是出現r 譬如說出現零次的	6-8
叫做n 零	6-8
出現一次的	6-8
叫做n one	6-8
出現二次的叫做n two	6-8
出現r 次的叫做n r	6-8
什麼意思	6-8
所謂出現零次的	6-8
是說你那些unseen event 那些unseen event	6-8
到底有幾幾種unseen event	6-8
就是n 零	6-8
以我們剛才的例子	6-8
就是九百九十四	6-8
你有九百九十四種unseen 都是你沒看過的	6-8
所以unseen event 就是有n 零	6-8
種那你只看到一次的那種event	6-8
叫做n one 種	6-8
那以剛才那個例子而言	6-8
我只看到一次就是三條魚嘛	6-8
就是就是三種	6-8
就是三種	6-8
三種event	6-8
不同event 的總數	6-8
不同event 總數所以剛才看到只只看到一條的那種呢	6-8
有三次	6-8
所以這不同event 的有有三種魚啊	6-8
所以n one	6-8
那麼看到有兩次的有幾種	6-8
n two	6-8
看到r 次的有n r 種	6-8
我們重新不用剛才這個n of k 而現在用n r 來代表	6-8
所以呢這邊講就是這個意思	6-8
n r 是number of 不同的event	6-8
occur r 次哦	6-8
就是你現在發生你看到r 次的	6-8
不同的event 有n r 種	6-8
那麼different event 的	6-8
它的次它的這個出現的次數是r 次	6-8
所以出現r 次的有n r 種	6-8
好如果是這樣的話呢我的總共的event 數目剛才是n k	6-8
summation over k 我現在n r	6-8
要乘上r 之後summation over r	6-8
也是一樣的	6-8
對不對	6-8
就是說零次乘以n 零	6-8
沒就全部沒有看到次數	6-8
一次乘以n one 兩次乘以n two 嘛	6-8
那全部加起來像剛才的話加起來的就是十八嘛	6-8
十八條就是這樣來的	6-8
就是一乘三啊什麼什麼什麼加起來	6-8
那就是十八	6-8
那這就是r 乘以n r	6-8
好那麼good turing 的基本的原則就是底下所說的這些	6-8
那這些是什麼呢其實就是我們剛才講的	6-8
我們說	6-8
你等於是把只看到一次的那十八分之三	6-8
當成是看到沒看過的機率	6-8
你就從現在所看到的十八條魚裡面	6-8
有三條是第一次出現的	6-8
你就可以假設十八分之三是會看到沒有看過的魚的機率	6-8
所以呢一次的那種就當成是unseen event	6-8
那十八分之三是什麼就是這個	6-8
一乘以n one	6-8
你把這個一乘以n one 的總次數歸給它	6-8
因為這邊是零次	6-8
對不對	6-8
這個乘以它就是就是它次數就是就每一種的次數都在這裡嘛所以這是零	6-8
這個一零乘以n 零一乘以n one 二乘以n two r 乘以n r	6-8
這通通加起來就是總次數所以這每一個乘起來都是它的次數	6-8
就是這個r 乘以n r 的這個次數	6-8
那麼你就把這個一乘以n n one 的這個呢就歸給它	6-8
算是沒有看過的	6-8
那麼這麼一來我這是可以了我unseen event 可以這樣分之後	6-8
那一次的沒有了怎麼辦呢	6-8
那我就把兩次的這些event	6-8
歸給它	6-8
那兩次的有了歸給他之後那兩次的沒有了怎麼辦呢那我就把三次的歸給它	6-8
就這樣子	6-8
那所以呢r 次的這些東西呢就歸給r 減一次	6-8
那r 次又沒有了怎麼辦呢	6-8
我用r 加一次	6-8
它有n 的r 加一	6-8
我把它歸給r 次	6-8
ok	6-8
所以這個想法呢跟有一點像是都減一的味道	6-8
這個發生一次的	6-8
這個東西呢給零次	6-8
發生兩次的呢給一次	6-8
發生三次給兩次	6-8
發生r 加一次的給r 次	6-8
好我們剛才有一個說是全部給它加一	6-8
這個有一點像減一不完全是啦	6-8
不過有一點這樣的味道	6-8
那這個想法呢就是這個所謂的good turing	6-8
那麼嗯寫在底下這句話裡面	6-8
就是所有發生的次數	6-8
那麼你的這個	6-8
原來是第r 次乘上r 乘上n r 就是	6-8
發生r 次的總共的發生r 次的所有的event 總共的次數是r 乘以n r	6-8
我現在變成是r 加一乘上n 的r 加一了	6-8
ok	6-8
原來發生r 次的總共的event 應該是r 乘上n r	6-8
現在變成r 加一乘上n r 加一	6-8
所以呢	6-8
就是就是這一句話所說的意思	6-8
當你變成這樣子之後呢	6-8
那麼你unseen event 到底被分到多少呢	6-8
就是n one	6-8
n one 次一乘以n one 的分給n 零個event	6-8
於是呢我每一個event 被分到的機的次數是多少是	6-8
n one 次除以n 零	6-8
嘛那以剛才那個例子就是三條魚分給九百九十四種	6-8
每一種魚是九百九十四分之三的機率	6-8
的的次數	6-8
所以呢你的每這個unseen event	6-8
所以呢你的每這個unseen event 每一個分到的次數是n one 除以n 零	6-8
那如果是這樣的話呢	6-8
那我總共有n 零個event 的嘛	6-8
所以這邊總共被分到多少呢	6-8
就是n 零乘上這個就是n one	6-8
就是這個n one 次分給n 零了	6-8
那這就是底下最後這一行所說的	6-8
我所有的account 給unseen 的呢就是我每一個unseen 的被分到的次數是n one 除以n 零	6-8
然後我總共有n 零個所以就是我總共是n one 次分給unseen event	6-8
好那如果是這樣的話	6-8
當然我現在全部都可以重算一次	6-8
它重算一次的話呢	6-8
我現在這個	6-8
譬如說我r 次的原來是r 次	6-8
總共有n r 種	6-8
現在這個次數送給它了	6-8
之後我用了r 加一的n 的r 加一的這些次數給它了嘛	6-8
所以呢我重新算一算它變成幾次呢	6-8
那它就它就被當成是r 的star	6-8
我重新算它是r 的star	6-8
那麼這個二呢也有二的star	6-8
一也有一的star	6-8
零也有零的star	6-8
就是我重新用這個方法來算之後	6-8
它的每一個event 到底有幾次呢	6-8
以剛才為為為例的話呢	6-8
這個零的star 就是n one 除以n 零	6-8
也就是我現在unseen event 的每一個unseen event 發生的次數	6-8
是n one 次除以n 零嘛	6-8
那那個呢叫做新的零的次數就叫做零的star	6-8
同樣呢我也可以有一的star	6-8
二的star	6-8
那r 的star 是什麼呢	6-8
就是r 加一乘上n 的r 加一除以n r 嘛	6-8
也就是我把r 加一乘上n 的r 加一	6-8
這是這邊的總共的次數	6-8
每一個發生r 加一次	6-8
然後有n 的r 加一種	6-8
所以這麼多次之後呢	6-8
歸給上面這個了	6-8
那麼上面這個其實只有n r 種啊所以呢就要它它乘它除以它嘛	6-8
所以呢就要它它乘它除以它嘛	6-8
ok	6-8
所以這就是r 的	6-8
ok 所以這就是r 的star 的意思	6-8
r 加一乘上n 的r 加一就是我這邊的n 的r 加一種每一種發生r 加一次	6-8
這麼多的次數我現在分給它了	6-8
分給n r 之後	6-8
那麼現在它的次數呢我們叫做r star	6-8
那就它乘它除以它	6-8
所以呢那就是good turing 的基本精神就是這麼做	6-8
那當然如果這麼做的話呢	6-8
那麼我r 的star 就不是零嘛所以unseen event 都有一個次數	6-8
就是這個次數	6-8
那如果是這樣的話	6-8
那我的總次數仍然沒有改變	6-8
他用這個推的意思就是說總次數沒有改變嘛	6-8
那你看就知道沒有改變嘛它它只是換一換位置而已總次數沒有改變	6-8
那他這個式子這樣寫的方法意思是說呢我現在r 這個r 次的變成r star 次了	6-8
r star 次仍然是有n r 種	6-8
對不對r star 次仍然是有n r 種	6-8
然後我summation over 所有的r 的話呢	6-8
那麼現在r star 呢我換成這個式子	6-8
r star 換成上面這個式子乘以n r 的話呢	6-8
那其實相當於這樣子	6-8
那這樣加起來還是n 嘛	6-8
所以呢我的總次數沒有改變	6-8
這就是good turing 的的estimate 的基本的精神就是這樣子	6-8
那這個方法基本上是不能算是完全對但是是一種做法來解決	6-8
因為你真的不知道unseen event 到底是多少	6-8
和用這個方法來做	6-8
那這個方法其實是有一些基本的問題的	6-8
那有它有兩個最明顯的問題存在	6-8
第一個問題是說它把所有的unseen event 的機率看成一樣	6-8
那你你可以想像我海底的沒有看到的魚有九百九十四種	6-8
難道它們都機率都一樣嗎	6-8
這裡面還是有機率高跟機率低的嘛	6-8
那顯然不是這樣平分的嘛	6-8
那他現在是用平分的	6-8
這個是不合理的地方	6-8
那你照說這個應該不是平分	6-8
這是第一個問題	6-8
然後第二個問題是ok 它分給它它分給它它分給它這都講得通	6-8
那最最大的那個怎麼辦	6-8
假設說原來是出現最多的是r 跟n r	6-8
出現最多的那個event 是出現大r 次	6-8
它有n r 種的話	6-8
現在呢這堆呢都分給r 減一去了	6-8
所以出現最多的這一群其實應該是機率最高的那一群沒有了對不對	6-8
這是機率最高的那一群是最重要的那一群現在沒有了因為都分給r 減一去了	6-8
所以最高的沒有了	6-8
所以這是它的兩個明顯的問題需要解決的	6-8
那你如果去看我們說過good turing 這是兩個人的名字	6-8
他們是統計學家他們其實做的是生物統計	6-8
你如果去看他們原始good turing 的paper 的話	6-8
他是在他們都不是做language model 他們是在做細菌的統計	6-8
那麼在統計細菌的時候	6-8
有千千萬萬的細菌所以這個不是問題	6-8
沒有沒有這樣的問題因為細菌千千萬萬	6-8
所以你這個加是summation 是加到無限大去	6-8
這個加到無限大去所以沒這個問題	6-8
可是在我們這裡這個問題是存在的	6-8
那因此呢怎麼解決這個時問題呢	6-8
ok 所以我們剛才所說的就是用這個方法來做	6-8
所以我們剛才這裡講你如果把十八分之三歸歸給看不見的九百九十四種魚的話	6-8
那麼你的原來六號魚應該是十八分之一	6-8
現在就變成十八分之一的star	6-8
那一的star 就是我們剛才講的那件事情	6-8
就是你把二兩次的那個歸給一次這樣來算	6-8
那一的star 之後你這樣這樣我分的是十八分之一star	6-8
這樣算就會變成二十七之一	6-8
它的機率就會降低了就是這個例子在說明	6-8
那麼我們講這個good turing 有它的問題所以怎麼辦呢	6-8
後來真正由good turing 所發展的用得最多的是所謂的katz	6-8
那我們剛才講其實嗯有效用而用得多的方法不是只有katz 還有別的啦	6-8
那我們沒有那麼多時間講那麼多種	6-8
所以我們就以他為例	6-8
那katz 的基本精神就是這樣子做	6-8
但是呢他解決我們剛才講的兩大問題	6-8
第一個問題是	6-8
零零的event 並不是平分的	6-8
它們應該要有高低	6-8
就這九百九十四種魚還是有高低的嘛	6-8
不是平分的	6-8
第二個問題就是我機率最大的沒有了怎麼行呢這個不通嘛	6-8
所以呢解決這兩個問題的話呢就變成所謂的katz	6-8
那我們來說一下katz 是怎樣的	6-8
katz 的基本的想法跟剛才的是非常像的	6-8
它就是從剛才的good turing 來的但是呢它稍微有一點不一樣	6-8
我們還是一樣一個是次數一個是不同的event 數	6-8
那麼如果沒有的是n 零種	6-8
一次的是n one 種	6-8
兩次的是n two 種等等等等	6-8
那它的基本原則是什麼呢	6-8
第一個你如果次數夠多的話是reliable 的就不要動它	6-8
我們舉例來講	6-8
那如果說是這個嗯r 零次好了	6-8
我們以這個r 零為一個上限	6-8
r 零次數是n r 零種	6-8
嗯不是r 零次的那種不同的event 有n 的r 零那麼多個	6-8
我就以這個為為限	6-8
在這個以上的我就不動它了	6-8
這是r 零加一一直到大r	6-8
這個是n 的r 零加一一直到n 的大r	6-8
這些我都不動了因為越上面的話是越重要的	6-8
它的統計越準嘛	6-8
統計準我就不要動它	6-8
所以呢large counts are reliable 所以unchanged	6-8
我這以下的我全部都不動	6-8
要動的呢就是動這裡的	6-8
從一到零到r 零為止	6-8
我只動這個比較少的這個少的本來本來反正本來就不準	6-8
本來就不準	6-8
因為次次次數太少反正本來就不可靠我來動這裡	6-8
這是第一個原則	6-8
第二個原則呢就是small small counts 嗯就把它discount	6-8
那麼換句話說呢	6-8
我這個出現一次的我把它算成這個我打一個折扣叫做dr	6-8
那麼dr 呢就是這邊的discount ratio	6-8
打一個打打一個折扣	6-8
discount ratio for event with r times	6-8
就是我像這個一次的這些東西呢現在不是一次	6-8
算什麼零點九次還是零點七次	6-8
我打一個折扣	6-8
那麼這個打一個折扣這個這個折扣的原則是根據剛才的	6-8
也就是說根據剛才的good turing 的話呢	6-8
這個應該變成一的star 嘛	6-8
所以呢這個應該是d one 這個是d two dr 零	6-8
啊我們這邊可以再加一個r 次的話	6-8
是dr	6-8
ok	6-8
這樣子	6-8
也就是說而我dr呢是根據剛才的r 的star 除以r	6-8
我們剛才是應該出現的r 次把它變成r star 次對不對	6-8
那這個就是我在這邊的good turing 的原則	6-8
就是一次變成一star r 次變成r star 次	6-8
那這個呢等於是等於是這個做一個改變	6-8
那我的這個這個基本上就是根據這個r 變成r star 這個原則	6-8
來做這個discount ratio	6-8
所以呢我們每一個都給它打一個折扣	6-8
打了折扣之後呢	6-8
那麼我真正就會少了多少次呢	6-8
那就是那就是變成是這個譬如說是r 是打了dr 折扣的話這個原來次數是n r	6-8
那我真正的是多少呢是n r 乘上一減掉	6-8
所以呢是n r 乘上	6-8
於是我就會變成這個r 次乘上n r	6-8
乘上一減掉dr	6-8
這些東西全部加起來的就是給了n 零對不對	6-8
譬如說我我這邊是譬譬如說我我這個一次變成現在只一次的打了一個折扣變成了零點九次的話	6-8
那我其實可以分多少給上面呢	6-8
是一乘以這個一減掉零點九	6-8
就是零點一	6-8
然後乘上n 的這些東西可以歸給它對不對	6-8
那麼兩次的乘以一減掉d 二	6-8
這個打的折扣剩下的呢	6-8
再乘以這麼多個event	6-8
這些東西可以歸給它	6-8
所以呢r 次乘以一減dr 乘以n r	6-8
這些東西可以歸給它	6-8
這樣子那所有的這些東西加起來就是我通通歸給它的	6-8
是多少呢	6-8
我歸給它的要等於n one	6-8
所以總共呢是n one 次	6-8
那這一點又是跟剛才是一樣的	6-8
這一點是剛跟剛才一樣我們剛才講的就是我總共把多少個次數歸給unseen 的呢	6-8
就是n one 次對不對	6-8
那把這個把n one 次歸給unseen 是的道理就是我們剛才講的	6-8
你總共只看到一次的	6-8
out off 所有的機率	6-8
好像剛才的十八分之三	6-8
就好像是unseen event 的機率	6-8
這樣來看的話我就把n one 歸給它吧	6-8
那我現在的做法呢就是	6-8
我只動從零到r 零為止我只動這些	6-8
那每一個呢打一個折扣	6-8
這打折扣的原則也跟good turing 是一樣的	6-8
就是這個原則	6-8
打折扣用這個原則	6-8
然後呢打折扣之後剩扣出來的那些東西呢就是一減掉dr 乘上這個嘛對不對	6-8
那麼就就一次的而言就是一次乘上一減掉dr	6-8
這個如果打打零點九的折扣那零點一的再乘上n one 的那些呢	6-8
給了它	6-8
這個如果打零點七的折扣的話呢那剩上的零點三的呢乘以它乘以它呢分給它等等	6-8
所以所有的這些東西加起來呢	6-8
就是我的要分給unseen 的就是n one	6-8
ok	6-8
所以呢那麼這邊講的這個這裡的原則其實都是剛才good turing 的基本的精神	6-8
就是我總共有n one 個event	6-8
分給unseen	6-8
n one 個event 分給unseen	6-8
那這n one 怎麼來	6-8
就是在一到r 零裡面的每一個分別去打折扣之後出來的	6-8
而這個折扣的原則是這個原則ok	6-8
好那這樣的話呢在這情形之下你其實是可以算得出來	6-8
因為我現在總共折扣的原則是這個比例	6-8
這個是你可以算得出來的	6-8
然後呢我的總數從一加到r 零的時候	6-8
就是r 等於一加到r 零的這個總數	6-8
也是確定的也是n one	6-8
所以given 這兩個條件given 這個總數是r one	6-8
以及每一個discount 的ratio 的原則是這個原則	6-8
given 這兩個條件你其實可以算得出來所有的東西	6-8
那就這些給了unseen	6-8
給了unseen 之後呢我們剛剛講	6-8
你這個unseen 的n 零你不應該是平分	6-8
因為unseen 裡面還是有高有低呀	6-8
那怎麼辦	6-8
我的distribution of count among unseen event 呢	6-8
是根據n 減一gram	6-8
就是next lower order 就是back off	6-8
就是我們剛剛講的back off 的原則	6-8
那你並不是全部平分給這n 零個	6-8
而是說那你看n 減一gram 怎樣	6-8
n 減一gram 高的就高低的就低	6-8
給他一個比例	6-8
那這樣的話呢就解決了我們剛才講的兩個問題	6-8
我們剛才講它的第一個問題就是n 減一就是你你那個unseen 並不是平分嘛	6-8
那它現在說我就是用n 減一gram 來分	6-8
所以這是解決第一個問題	6-8
第二個問題我們說機率最高的變成沒有了不通嘛	6-8
那它不會	6-8
就是r r 零以上的我都不動	6-8
所以這邊都不動	6-8
我只動中間的	6-8
那這樣這剛才講的兩大問題它都解決了	6-8
那這個呢就是所謂的katz	6-8
那我們可以舉一個例子來講一個這個的精神就是我們舉bigram 為例	6-8
那這個bigram 是怎樣的呢	6-8
就是我要算這個i 減一後面看到i 的機率	6-8
那你看到這個第一條式子就是說	6-8
超過r 零的就照原來的	6-8
就是這邊講的我超過r 零的這些呢	6-8
就照原來嘛不動嘛	6-8
就照原來	6-8
所以呢前面這個式子就是我們本來所講的求n gram 的式子	6-8
這個bigram 本來就是這樣了就是這兩個出現的次數除以那那一個出現的次數嘛	6-8
嗯所以這兩個就是原來的bigram 的式子	6-8
那然後呢如果是在r 零到一之間的話	6-8
一到r 零之間的話是什麼呢	6-8
就是我原來的次數乘上這個discount ratio	6-8
我本來應該算是一次的我現在變成是零點九次	6-8
所以呢我的原來的bigram 計算呢	6-8
要乘以零點九嘛	6-8
這個本來要出現兩次的現在變成要打一個折扣零點七的話	6-8
那我原來的bigram 就要乘上零點七嘛	6-8
所以就是乘上這個discount ratio	6-8
所以你看第二個呢在r 零到一之間嗯	6-8
一到r 零之間	6-8
就是第二個式子	6-8
那這後面這個這一堆就是原來的bigram	6-8
現在乘上這個discount ratio	6-8
ok	6-8
然後第三個式子就是說這個unseen event 怎麼辦	6-8
unseen event 我己經把n one 分給它了	6-8
那麼我這個時候用什麼來算	6-8
用它的n 減一gram 就是unit gram	6-8
所以呢我現在這個bigram 呢就是以unit gram 來計算	6-8
那麼我以unit gram 的高的就高低的就低	6-8
根據unit gram 來分這個分這些次數	6-8
那只是說呢這個分的比例常數你得要算一算就是了	6-8
所以這個東西的算法就是使得你的total count 等於原來你所看到的count	6-8
count 不改變	6-8
那這這幾個原則有的話你就可以算出這些東西來	6-8
那這就是用這個katz 來算bigram 的算法	6-8
啊那基本講起來的精神就是這樣	6-8
那詳細比較複雜課本裡面有啊	6-8
那我想是不管是這一本還是這一本都有	6-8
啊你看這本裡面也有講這本裡面也有講	6-8
大概都有說到	6-8
所以呢詳細怎麼算的那它會說得清楚	6-8
那我們就不多講	6-8
那麼你們大概了解這就是我們所講的smoothing	6-8
那課本裡面還會講不只這一種還會有別種	6-8
那這個嗯你自己看就好了我想基本精神都很像	6-8
我們在第二個習題裡會給你去train language model	6-8
那裡面你就會去也是一樣	6-8
有一個有一個現階段全世界的語音做語音的group 所用的軟體	6-8
叫做是這個sri 的language model	6-8
那那個language model 的的工具	6-8
它裡面就有所有的smoothing 的方法	6-8
那麼這些我們這邊講這些方法都在裡面	6-8
那麼你可以用那些來做	6-8
你就會知道做出來的結果會怎樣哦等等	6-8
那我們就不多講下去	6-8
你就自己看就行了	6-8
底下我們來講下一個topic	6-9
就是說language model 另外的一個常常常使用的很有效的方法就是所謂的class based	6-9
什麼叫做class based 呢	6-9
意思是說其實你真的一定要算這麼複雜的這種n gram 嗎	6-9
好像不盡然	6-9
因為明明很多個word 是屬於同一個class	6-9
所謂屬屬於同一個class 就是指它們在語意上或者在文法上其實是很像很像的	6-9
舉例來講譬如說john saw a dog	6-9
如果john 可以saw a dog 的話當然mary 也可以saw a dog	6-9
然後其他所有人的名字都一樣可以saw a dog	6-9
那這些人有區別嗎好像沒有區別	6-9
如果要你如果要算這個john 後面要接這個saw 的這個bigram 的話呢	6-9
只要你其中一個人算得出來	6-9
換誰都一樣嘛	6-9
就這樣一個想法	6-9
那麼如果這樣來想的話你其實是可以把很多東西	6-9
凡是屬於它們很像的	6-9
either 是語意上很像	6-9
or 是文法上很像都變成一個class	6-9
舉例來講你可以john saw a dog 當然也可以可以saw a cat 嘛	6-9
那麼cat 跟這個dog 好像沒有什麼區別嘛都一樣嘛	6-9
那麼因此呢好像可以放在一起哦	6-9
那你如果這樣子來想的話呢	6-9
那麼如果這些人可以幹嘛的話	6-9
那he 跟she 也可以嘛	6-9
如果he 跟she 可以的話my father 當然也可以my sister 當然也可以嘛	6-9
那麼可以是father 當然也可以是sister 啦對不對等等	6-9
那如果這樣來看的話當然我也可以drove a car	6-9
如果可以drove a car 的話當然也可以drove a bus 嘛對不對等等	6-9
所以這麼一來的話你可以把凡是相同的一群	6-9
變成一個所謂的class	6-9
如果變成一個class 的話呢	6-9
那我的n gram 就可以變成class based 的n gram	6-9
這所謂class based n gram 的意思	6-9
你現在就不需要去算這裡每一個word 這個word 後面這個這兩個word 後面接這個word 機率	6-9
你可以算這個是屬於那個word 屬於哪個class	6-9
如果你每一個word 都屬於一個class 的話呢你現在可以算這個class 的n gram	6-9
就是你如果前面看到這兩個class 的話	6-9
後面會接哪一個class	6-9
這個想法非常的單純啊很很容易想	6-9
但是呢你這個這個class 完了之後	6-9
這個這兩個class 之後接這個class 機率算出來之後呢	6-9
你應該還要算一下在這個class 裡面真的會看到這個word 的機率	6-9
換句話說這一堆人名	6-9
還有很常出現的跟很不常出現的	6-9
你如果john 是一個很常出現的話	6-9
換一個很不常出現的人名的話	6-9
他的這個可以不一樣嘛對不對	6-9
嗯那麼你你所以你可以再多一個這個	6-9
就是你看這個人名之後	6-9
在那個人名你看到這個人名之後	6-9
那人名裡面會看到他的機率是大還是小	6-9
你可以算可以算進去	6-9
嘛如果是這樣的話呢	6-9
我這樣子的class 的trigram	6-9
再乘上看到這個class 之後會看到這個人名的機率	6-9
這樣子這個東西呢就可以取代我原來的這個機率	6-9
那麼這麼一來的話	6-9
我的其實這個這就是所謂的class based language model	6-9
那class based language model 基本上它有smoothing 的效果	6-9
為什麼有smoothing 效果	6-9
那也是一樣把原來的unseen 都放在這裡嘛	6-9
你本來譬如說有一堆很很少人見的人名那些人名我根本不看到的	6-9
我都給他歸在這裡	6-9
那他也就有機率了嘛對不對	6-9
那麼我我我特別去算這些人名裡誰的機率高誰的機率低	6-9
那機率再低還是有機率嘛	6-9
所以這個就做到相當程度的smoothing 的效果	6-9
那如果這個可以做到smoothing 效果的話呢	6-9
那麼它跟我們剛才講的用lower order 啊lower order 就是我們講的back off	6-9
就是我們之前在這裡講的back off	6-9
就是用lower order	6-9
那它跟這個方法常常是互補的	6-9
也就是說你有的時候可以用back off 到lower order 的方式來算你的n gram	6-9
那有的時候其實你如果是可以是class 的話呢	6-9
我就用class 算也可以嘛	6-9
所以它跟lower order 這個是常常是一個互補的方法	6-9
常常是一個互補的方法	6-9
那麼也因為這樣子所以我class 的數目也可以大為降低	6-9
所謂class 數目大為降低的意思是說	6-9
你現在n gram 不要那麼多了因為class 很少可以少很多嘛	6-9
那麼以這個最常用以我們常習慣的中文為例	6-9
我們說中文的常用詞六萬個	6-9
我們中中文常用詞是六萬的話	6-9
你n trigram 有多少個機率	6-9
是六萬的三次方	6-9
你的trigram 有這麼多	6-9
然後你就要算這麼多個機率出來然後存起來	6-9
然後每一次去找這這麼多個	6-9
那如果我用這個方式的話中文的六萬個詞可以分成幾個class 呢	6-9
這個我們過去做的經驗呢大概在七百到一千五百之間就夠了	6-9
你你的分的class 大概分成七百個到一千五百個	6-9
大概夠了	6-9
以一千五百個為例你的trigram 變成這麼少嘛	6-9
這個比這個少很多啊	6-9
嗯那這就是指我的那我做起來什麼都方便了	6-9
這就是這個parameter 就大為減少	6-9
那在這個情形之下呢	6-9
可能最大的問題是那些word 放在一起變成一個class class	6-9
那這就是所謂的這個word clustering 的問題	6-9
你如何把什麼叫做語意上文法上相近這是什麼意思	6-9
哪些word 真的可以變成一個class	6-9
這變成一個真的困難的地方在這裡	6-9
那這裡面舉了講了兩個例子	6-9
那這個是limit domain 的例子	6-9
這個是假設我要做的事情是一個limit domain 的話	6-9
其實很容易	6-9
基本上就用人來做啦	6-9
舉例來講譬如說這是一個買飛機票的一個dialogue 系統	6-9
你就專門給人家打電話進去買飛機票	6-9
那那個不是一個旅行社的一個一個agent 而是一台電腦	6-9
那你打進去問說tell me all fights of united from taipei to los angeles	6-9
那你其實你可以想像所有的航空公司的名字變成一個class	6-9
所有的城市名字變成一個class 嘛	6-9
你這個用人做就好了根本不要去	6-9
那我我講的就是這些東西嘛	6-9
所以你只要把所有的城市名變成一個class	6-9
然後sunday monday tuesday 變成一個class 等等等等的話	6-9
你總共就這麼多個class 嘛	6-9
那這個時候呢你就直接用人手做這些class 之後	6-9
你你只要train 這些language model	6-9
就是這個這個class 在這些後面的機率	6-9
這個class 這這個class 接在from 接在to 後面的機率等等等等這樣就好了嘛	6-9
那這樣而且有一個好處就是new item 不用去train	6-9
直接加進去	6-9
今天有個新的航空公司長榮	6-9
那你就加進去就好了跟本就不用去train 嗯	6-9
你有一個新的航點飛到里斯本	6-9
ok 就加進去就好了	6-9
啊根本不要train	6-9
那麼你大不了就算一下里斯本在所有的city name 裡面它的機率是多少	6-9
最多多算一個這個機率	6-9
那這個機率可能是以航班的比例來算	6-9
或者以旅客的比例來算你可以加一個機率在這裡	6-9
那這樣的話就是這個機率那這樣就好了嘛	6-9
所以你基本用人手來做而且呢你都可以不用data	6-9
新的新的item 直接加進去就好了	6-9
這是limit domain 的情形	6-9
但是如果不是limit domain 的話呢而是general domain 的話那比較難了	6-9
你可以想我們我們這麼多個word	6-9
英文的常用詞word 的數目大概跟我們中文的六萬差不多同樣的order	6-9
你這麼多word 到底是誰跟誰是同一組	6-9
你不能全都用眼睛看嘛哦用人來看是可以看啦	6-9
但是你完全用用人又分不出來所以這個時候呢general domain怎麼做	6-9
那就要有一些所謂的這個word clustering 的方法	6-9
基本上是用這個這個這個這個啊data 以data driven 為主啦	6-9
你就是要用大量的data 去算	6-9
底下我們舉兩個例子	6-9
這個其實這種就是在general domain 裡面的這個general domain 裡裡面的這種這個大量的詞到底怎麼樣去做這個word clustering 是一個很有趣的問題	6-9
那麼在大概九零年代八零年代幾零年代是有非常多的研究在做這件事情	6-9
那麼確實他們做了不少非常好的language model 就是這樣class 的	6-9
那我們底下舉兩個例子這兩個例子應該是相當具有代表性的啊	6-10
那我不準備說詳細	6-10
都有一個reference 給你看	6-10
那麼我們大概說一下	6-10
那麼詳細的話我們你看這個reference	6-10
那這兩個可以算是這個九零年代初期八零年代末期相當具有代表性的兩個經典作品	6-10
你現在看起來這個reference 很早啦	6-10
這個是十四年前的	6-10
這個是再早十十七年前的所以這個都很很古老的	6-10
不過這些都是經典作品	6-10
那麼我們今天其實那些在用的那種clustering class based 相當好用	6-10
所以今天還有很多的很多的系統真的就是用這種class based language model	6-10
那他們所用的class 常常還是用這種方法在train 在train 出來的	6-10
或者這種方法衍申出來的更精緻的方法就是了	6-10
所以這些仍然是經典的是非常好用的經典哦	6-10
所以是很值得參考的所以我們還是把它列在這裡	6-10
那第一個例子是說	6-10
你就是把所有的word 一開始把所有的word 都當成一個 cluster	6-10
然後呢你每一次去找誰跟誰最像	6-10
把它黏起來	6-10
也就是說假設我有六萬個word	6-10
一開始每一個word 都自己是一個cluster	6-10
所以呢我有六萬個cluster	6-10
然後呢我每一個iteration 的時候我去找誰跟誰最像	6-10
它們應該可以黏起來變成一個class	6-10
那麼譬如說呢也許發現它跟它很像	6-10
可以黏成一個cluster	6-10
那什麼叫做像呢	6-10
那基本原則就是minimize over all perplexity	6-10
我原來用這個方式train 出來的language model perplexity 是多少	6-10
我現在如果把它們兩個黏成一個class	6-10
連成一個class 的話	6-10
我可以重新再做這樣子的language model 它的perplexity 是多少	6-10
那我們說過language model 我們是希望perplexity 要降要小嘛	6-10
所以看誰跟誰黏起來讓我的overall perplexity 降的最多的	6-10
那你如果這個iteration 發現是它跟它黏起來的話ok	6-10
我就把它跟它黏起來變成一個class	6-10
於是我現在的class 數目少一個了	6-10
那再下一次我可能發現是它跟它連起來	6-10
那麼它跟它連起來的時候呢我的perplexity 降的最多那我就把它跟它連起來	6-10
那再下一次我可能發現其實是這個時候呢再把它連起來	6-10
是降得最多的	6-10
以此類推你這樣的你可以想像這個是一個計算量非常大的一個一個程式	6-10
但是它是有效的啊	6-10
也就是說因為你你假設你有六六萬個詞	6-10
你兩兩去看它跟它黏起來是會變成多少	6-10
它跟它黏起來這個光是兩兩都算一次你就算就算不少嘛	6-10
那當然這裡面是有一些技巧的	6-10
你怎麼樣讓這個需要計算量降到最低這是是有學問的	6-10
如果有興趣就看這篇paper 啊	6-10
這個是後來所有的講到用這個class based language model 的時候幾乎都是site 這一篇	6-10
這是最重要的一篇reference	6-10
那你要用一些方法來讓這個計算量不至於太大	6-10
但即使那樣這仍然是一個很大的程式	6-10
我要用一個很大的training data	6-10
然後還要另外一個data 來算perplexity	6-10
然後呢我就是讓它不斷的跑	6-10
那計算量非常大然後最後我可以得到一個相當不錯的一一組class	6-10
你這個這個iteration 到什麼時候停止呢你有一個條件嘛	6-10
就是如果你的這個perplexity 降不下去了	6-10
你的perplexity 降不下去了你就停在那裡	6-10
你這個時候你就得到你的一組	6-10
那這是第一個example 所用的方法	6-10
第二個example 呢完全不一樣了	6-10
它是用我們上次說過的cart	6-10
也就是decision tree	6-10
你記得我們說過我把一個一大堆東西放在這裡之後呢	6-10
想辦法用一個question 把它分成兩個	6-10
這個原則是甚麼降低perplexity	6-10
然後呢再把它拆出來用一個question 再把它拆出來	6-10
這個decision tree 我們在五點零所說的	6-10
它用這個東西來做	6-10
所以這是所謂的tree based	6-10
那你現在要分的是什麼東西分的東西不一樣了我現在要分的東西是history	6-10
那你想每一個你可以看譬如說我現在講一一個word sequence 從w one 到w n	6-10
那它等於是每一個given 前面從w one 到i 減一之後	6-10
看到第i 個word 的機率i 一路乘下來的對不對	6-10
就是我從w one 開始看到w i 減一之後	6-10
那麼你看前面i 減一個	6-10
然後呢會看到第i 個的機率	6-10
然後我i 從一到n	6-10
這個就是這個的機率	6-10
那這裡面的一到i 減一就是所謂的history	6-10
就是w i 的history	6-10
就是h i	6-10
就是一到i 減一	6-10
你如果這樣來看的話我現在就可以把我的我有一個training data	6-10
我把這個training 這個語料庫裡面的所有的	6-10
所有的各種各樣的history 統統拿來	6-10
然後呢我嗯基本上應該是這樣講的我為每一個word	6-10
每一個word 都有一大堆它的history 放在這裡	6-10
然後去去把它拆出來等等	6-10
那麼我怎麼拆這個history 也是用一堆question	6-10
不過這個question 是什麼	6-10
是這些history 的question	6-10
舉例來講這個history 的question 會是什麼呢	6-10
不像我們之前所講的是我們之前在第五章講的說它左邊是子音還是母音	6-10
右邊是母音還是子音發右邊那個母音的時候嘴嘴巴是怎麼的甚麼甚麼那個那個的時候是為了要分做tri gram	6-10
所以看左邊是什麼右邊是什麼	6-10
左邊是母音嘴巴怎樣什麼什麼的	6-10
那現在呢那因為我現在是在算language model	6-10
是在是根據這個history 來的	6-10
所以我的question 變成我的history 裡面的question	6-10
譬如說history 裡面這個history 裡面有沒有哪一個字	6-10
裡面有沒有動詞	6-10
有沒有名詞	6-10
有沒有的	6-10
有沒有什麼什麼字take	6-10
你你可以你你可以想到所有的這一類的question	6-10
就都是指我的history 裡面有沒有什麼字有沒有哪一種詞類	6-10
然後它有沒有什麼什麼東西哦等等	6-10
那用這個方式有一大堆的question set	6-10
那一樣我也可以用它來我一樣算entropy	6-10
那根據entropy 降低來確定說我應該用哪一個question set 把它拆開來	6-10
於是最後我的變成一大的堆的history	6-10
那麼變成很像的一堆history	6-10
那每一history 很像那些東西呢變成一個class 等等	6-10
那這個方法的基本的精神你可以想像跟我們之前講的那個一樣	6-10
就是包括了文法跟統計兩種knowledge	6-10
所也就是一個是這個grammar 這個grammatical driven	6-10
用文法的因為你現在講它的history 裡有沒有那一個動詞	6-10
有沒有那一個名詞這個是在講文法嘛	6-10
你一方面是講文法一方面是在算算這個entropy 是在講這個data driven	6-10
所以你其實是這個文法跟data driven 這兩種通通都都算了	6-10
而且呢同時它包括了local 跟long distance relationship	6-10
這話的意思是說我們一般的n gram 都只有local 的relationship	6-10
什麼意思	6-10
這個n gram 永遠只算到前譬如說前面的n 減一個	6-10
跟後面的這一個	6-10
我n gram 算不到更遠的	6-10
所以n gram 永遠只是一個local relationship	6-10
而沒有long distance relationship	6-10
可是在真正的句子裡面long distance relationship 是永遠存在	6-10
而且重要的	6-10
我們舉一個例子來講	6-10
在不論中文跟英文任何一種語言都一樣	6-10
我們在講話的時候的語言顯然不是只靠local relation	6-10
而是有long distance	6-10
譬如說我們最簡最簡單的一個例子	6-10
洗了一個很舒服的澡	6-10
這個洗跟澡這是很有趣的	6-10
因為澡本身不是一個東西	6-10
你說吃了一個很吃了一個味道很很味道很甜的蘋果的話	6-10
那你你也是一樣吃跟蘋果	6-10
中間夾了很多很多東西	6-10
那你其實是應該是吃跟蘋果之間的有這個bigram 的關係	6-10
那麼你那個味道很好什麼東西	6-10
你你你這些東西呢插在中間	6-10
把吃跟蘋果中間會拉得很遠	6-10
那洗澡就更複雜了因為澡不是一個東西	6-10
那麼是一個詞是洗跟澡連起來的一個詞但是這個詞根本就切開了	6-10
那它們的關係是被拉到這麼遠	6-10
那你可以想像我們真正在講話的時候我們的很多很多句子裡面	6-10
真正你的bigram 應該train 的出來的bigram 關係可能是很遠的	6-10
在英文而言譬如說the boy walking on the street	6-10
decides to 幹嘛幹嘛幹嘛	6-10
那同樣的是the boy decides	6-10
那中間這些東西呢其實是跟decides 會在the street 後面嗎	6-10
好像沒什麼道理	6-10
它其實是在這後面的	6-10
這種就是所謂的long distance relationship	6-10
那你不管那一種語言我們在講話的時候都會有一大堆這種的long distance relationship	6-10
這種東西都是n gram 不會做的	6-10
n gram 做不出來的	6-10
那麼因此呢你其實是這是一種方法來做它因為如果你放在history 裡面的話	6-10
history 其實裡面就包括了long distance 都在history 裡面了	6-10
所以呢這是它的好處	6-10
那這個詳細的這個去看這個reference	6-10
我們這邊不多講	6-10
那我把這個reference 列在中間的這種情形是指說這個reference 是給你參考	6-10
然後做為這個其實是可以做期末報告題目的你如果有興趣的話	6-10
這種東西可以拿來做期末報告哦	6-10
那跟我寫在前面一個chapter 前面的那些reference 是不一樣	6-10
的如果我寫在chapter 的那裡的reference 的話	6-10
這種reference 的意思是說是你回去真的要唸的	6-10
啊期中考會考的	6-10
就是說你當天準備期中考的時候這種東西是要看的	6-10
但是呢你如果是寫在這中間的這種	6-10
是說考試不會考的	6-10
但是是很好的references	6-10
你如果考慮期末報告的話這種東西是可以做的	6-10
是這樣的意思	6-10
ok 好那所以這兩個我們不多說我們大概說到這裡	6-10
那以上大概我們把language model 裡面一些最重要的問題	6-10
從它的perplexity 到smoothing 這些東西class 我們大概都說到了	6-10
那啊底下我們要稍為講一些中文的一些狀況	6-10
那我們在這休息十分	6-10
ok	6-10
我們底下稍微講一點中文的狀況	6-10
那第一個呢就是class based language model 中文也一樣可以做這種事	6-10
我們中文也一樣可以把很多個詞兜在一起變成一個class	6-10
那可以怎麼兜法呢其實你的data driven 方法是不見得是剛才的那兩種	6-10
其實你馬上可以想出很多種來	6-10
剛才那兩個是最具代表性的就是了	6-10
最簡單的方法你可以想怎麼做	6-10
假設我有我有n 個詞	6-10
我有n 個word	6-10
我可以建一個matrix	6-10
這邊也是n 個word	6-10
以bigram 的精神來講的話呢	6-10
你可以數這個word 後面會接什麼word	6-10
譬如說後面接這個word 後面出現五十一次	6-10
接那個word 十二次接這邊都其他都是零次	6-10
這個二十六次等等等等等等	6-10
那麼這個word 呢又又他的一一堆	6-10
那這個word 呢在這邊出現了有二十六次在這邊有出現十一次	6-10
這邊有出現十一次	6-10
等等等等	6-10
那你會發現這個word 跟這個word 是不是很像	6-10
它們後面都會接這個word	6-10
都會接這個word	6-10
都會接這個word	6-10
那它跟它就很像了	6-10
等等	6-10
所以呢你只要做一個這樣子譬如說n 個word 跟n 個word 之間的table	6-10
這其實就是它們的bigram 這其實不是bigram	6-10
那你如果這樣來看的話呢	6-10
這其實這等於是它們的它們的feature vector	6-10
這個word 後面會接這些東西這是它的feature vector	6-10
這是它後面的feature vector	6-10
你如果這樣看成這樣的話呢這就是這些vector 你就可以做vq 嘛	6-10
我就可以去用vq 的演算法去做clustering	6-10
可以分成幾群那幾群大概後面接起來就很像	6-10
他們就可以等於是根據bigram 精神就可以把它們分群了	6-10
同理呢這樣子也可以啊	6-10
這表示這個word 前面會接什麼word	6-10
對不對	6-10
這個word 前面會接什麼word	6-10
如果它們像的話它們就會像嘛我就可以用這個來做	6-10
哦等等	6-10
那像這類從前我們都做過	6-10
這類都是可行的方法	6-10
你都會有效的把一些很像的詞兜在一起	6-10
啊	6-10
這這些都是簡單的做法	6-10
那當然這樣子的這一類的data driven 的data driven 的方法	6-10
它有它的弱最大的一個問題就是低頻詞很難做	6-10
因為你這個都是要靠頻率高的嘛	6-10
sil	6-10
如果是它本身真的是一個低頻的詞	6-10
很少見的罕用詞的話	6-10
low frequency word 不太容易做	6-10
那有什麼辦法呢	6-11
這是一個辦法就是用人工來加入	6-11
這裡這裡講的這個方法是我們在九零年代的時候曾經做過	6-11
效果非常好的一個word class	6-11
是這樣做的	6-11
那基本上是怎麼辦呢我們分成三個stage 來做這件事情	6-11
這三個stage 裡面第一個stage 是其實是用人工	6-11
那這是根據什麼呢	6-11
是根據所謂的pos feature 什麼是pos 呢pos 就是所謂的part of speech	6-11
也就是詞類	6-11
這是在語言學上的所謂的part of speech	6-11
就是我們說的詞類	6-11
你可以根據它的	6-11
分成詞類	6-11
譬如說動詞名詞介系詞形容詞	6-11
那你可以分到很細	6-11
那我們當時所採用的是根據中研院的一個group 它們做的	6-11
他們是由語言學家跟這些人在裡面一起做	6-11
他們把中文的詞分成一百九十八個詞類	6-11
為什麼會有這麼多	6-11
它分得很細	6-11
譬如說名詞它可以分成幾十種名詞啊	6-11
這個有生命的跟沒有生命的有生命裡面是人的跟不是人的	6-11
是人的裡面有是大人還是小孩的	6-11
啊是什麼	6-11
有生命的裡面有這個是不是人的是動物的還是植物的還是什麼	6-11
你可以這樣的分得很細	6-11
同樣呢這個	6-11
不是沒有生命的東西你也可以分成譬如說是建築物還是什麼東西	6-11
什麼你都可以這樣的分	6-11
所以它的名詞可以分將近一百種好像	6-11
分得很細很細	6-11
這是根據他的語意可以分得很細	6-11
同樣動詞也分了好幾十種	6-11
就是有動作有狀態的是人的動作還是生物的動作	6-11
還是什麼啊各種各樣	6-11
及物的不及物的有幾個動詞有幾個受詞你都可以這樣分	6-11
所以它可以分成幾所以這樣總共可以分成一百九十八類	6-11
那我們就根據它的一百九十九十八類呢它有一個詞典	6-11
每一個詞都說這個詞可以當成哪幾類	6-11
譬如說我們隨便舉一個例子譬如說組織	6-11
這是一個什麼	6-11
它可以當成名詞	6-11
這是一個organization	6-11
可以當成動詞to organize	6-11
對不對	6-11
可以當成它會有好幾種詞類	6-11
那詞類a 詞類b 詞類c 詞類d 詞類e 這樣構成	6-11
這個就代表這個詞的它的一種feature	6-11
那麼在中研院他們當時是因為他們每一個詞他都他有一個詞典	6-11
他們已經分好用語言學家他們去分好了	6-11
那所以我們就用就用這個當成第一第一個stage 的分法	6-11
就是我每一個我讓每一個每一個詞belong to 一個class	6-11
而那個class 呢是由一組詞類來組成的	6-11
譬如說假設這個詞是有這幾種詞類的話	6-11
那我就是就是這幾種詞類決定某一個class	6-11
那必須要同樣也有同樣也就是這幾種詞類的詞變成一個class	6-11
ok	6-11
所以呢這個就是講每一個word belong to 一個class 那個class 是由每一組詞類決定的	6-11
那這個詞類裡面包括這個syntactic 跟semantic 我們剛才講的語這個語意這個是語法啦	6-11
語法是說它及物的不及物的它後面要接什麼幾個受詞什麼東西這是講語法	6-11
這是講語意這個是有狀態的有動作的有什麼的	6-11
啊有生命的什麼這是這是semantic	6-11
你根據這些來分的	6-11
這樣子之後我把所有的詞先分分出第第一個class	6-11
到這個階段之後呢	6-11
所以這個階段是用人的知識來做的	6-11
那之後呢第二個階段是data driven	6-11
我現在根據它後面會接什麼前面會接什麼根據它的data driven 再分第二次	6-11
分到第二次之後己經分到很細了之後呢	6-11
第三次再merge 一次	6-11
因為很可能雖然在這邊的時候是根據詞類把它分開來了	6-11
但是最後其實它們其實是可以merge 在一起的我可以再把它merge 起來	6-11
這邊有一個簡單的例子就是譬如說汽車巴士火車飛機這些都是屬於交通工具	6-11
所以最後會變成一個class	6-11
相對於這個class 的呢這一堆動詞是屬於狀態的動詞	6-11
坐呀搭呀乘呀這屬於狀態的動詞	6-11
那這個呢是屬於去開它的去駕駛的這是屬於有動作的動詞	6-11
所以這兩種動詞我一開始講詞類的話	6-11
就分開來了	6-11
這個跟這個一開始我就就分到不同的地方去	6-11
然後最後呢它們譬如說這個呢會到這裡來那那這個呢會到這來	6-11
可是其實我最後發現它們後面都是接同樣的東西	6-11
我最後可以再merge 起來	6-11
等等	6-11
那這就是這個三個階段的做法	6-11
那這樣子得到一個非常好的一組word class 的class 的n gram	6-11
那就是我剛才說到中文的詞大概分到七百到一千五百就夠了	6-11
那就是這個	6-11
這樣大概這中間參數可以調你要調到七百到一千五百之間都不錯啊	6-11
那	6-11
這個是當時我們的一篇碩士論文你如果要查還是可以查得到	6-11
那	6-11
在這個情形最大的一個好處是什麼呢為什麼要用人來做	6-11
就是漢用詞低頻詞	6-11
因為低頻詞是你如果完全用data driven 的方式它因為它的頻率太低你做不做不好	6-11
它會分到亂七八糟的地方去	6-11
那怎麼辦呢我一開始因為根據它的人的知識分到那裡去之後	6-11
你怎樣後面data 很少它也這個八九不離十差不遠啊	6-11
人的知識分到那裡去之後	6-11
你怎樣後面data 很少它也這個八九不離十差不遠啊	6-11
所以所有的低頻詞漢用詞大概都會分到一個合理的地方去	6-11
然後那當然這個方法會變成both data driven 跟human knowledge	6-11
driven 你一方面是用了人的知識的作為第一階段的分群	6-11
之後後面再用data driven 的話你這兩者加起來這效果是不錯的	6-11
那這是一個簡單的例子	6-11
那後來這個方法有再進一步的版本就是我們把每一個詞的意思拆開來	6-12
因為我們很多詞是有好多意思	6-12
譬如說就這句話而言你知道這是一個單字詞	6-12
這個單字詞在這裡的意思跟這裡的意思是不一樣的	6-12
那麼因此到底哪一個詞歸哪裡	6-12
那麼在剛才的方法裡面的話ok	6-12
這個會這個哦這個是一個非常有非常豐富的詞類的詞	6-12
它可以是這個意思可以是這種詞類可以是這個詞類可以是這個詞類	6-12
有很多種	6-12
那它必須要有另外一個詞跟它完全一樣有那麼多的才跟它變成同一個class	6-12
這樣太複雜了	6-12
所以一個辦法是說	6-12
你就讓一個詞你如果它有不同的詞類的話呢就給它more than one class	6-12
那你乾脆就是就是這兩種通常是一樣的是	6-12
那這兩種是通常是在一起的	6-12
這些詞類是同樣的這些詞類都是同樣的詞會有的那我就乾脆那同時是它是它的話我乾脆把	6-12
這個詞在這裡也放在這裡讓它出現在兩邊	6-12
啊這是第二種方法	6-12
就是我的詞呢我之前的剛才那個法是每個詞只放在一個一個class 裡面	6-12
我現在可以讓它我現在可以讓它放在兩個以上的class 裡面所以同一個word 可以放在兩個以上的class 裡面	6-12
然後就那樣去做	6-12
可是這個時候做你的language model 要很難就很難train 了	6-12
為什麼	6-12
你要知道它是屬於那一個詞類	6-12
我們剛才是剛才是凡是有這個詞我就是就是照樣去train 了根本不管因為那個詞就是那個詞類	6-12
現在不是了	6-12
因為這個詞有的時候是這些詞類有的時候是這些詞類	6-12
那你怎樣train 它的language model	6-12
你要train n gram 的時候	6-12
對不對	6-12
這個後面這後面這兩個後面接這個	6-12
那你要你你現在變成要知道	6-12
你要train 你要知道說它是什麼詞類	6-12
它是什麼詞類你就知道這兩個詞後面會接什麼詞類	6-12
於是你的training data 裡面要知道每一個word 是什麼詞類才行	6-12
那就是所謂的tag corpus	6-12
這個tag 這個tag 這個字的意思是把這個詞類掛上去	6-12
那啊語言學界他們很早就做這件事	6-12
就是你要有一個自動掛詞類的方的程式	6-12
你在一篇文章裡面每一個詞屬於什麼詞類自動把它掛上去	6-12
那麼你要用這種掛好詞類的的文章去train	6-12
那他就會知道他什麼詞類	6-12
所以你就知道什麼詞類後面什麼詞類會接什麼詞類	6-12
那這樣子的話呢可以做	6-12
不過這個是工程比較浩大	6-12
因為你你的這個要把詞類掛進去才能train	6-12
但是這樣出來的language model 會好嘛	6-12
因為它把哪個詞類後面會接什麼詞類都己經算進去了	6-12
那這樣子的這個嗯	6-12
那我們說一下你怎樣掛詞類	6-12
掛詞類的辦法其實也是一樣	6-12
你第一個呢就是	6-12
所所謂的pos 的tagging 就是掛詞類	6-12
你就是為句子裡面的每一個詞	6-12
你就是為句子裡面的每一個詞都把它的詞類標上去那麼你標詞類的方法呢基本上就是有一個詞典	6-12
都把它的詞類標上去	6-12
那麼你標詞類的方法呢基本上就是有一個詞典	6-12
每一個詞會有那些詞類你都有了	6-12
有一個詞類的n gram	6-12
就是說你有一個詞類的n gram	6-12
什麼詞類跟什麼詞類後面會接什麼詞類	6-12
這是詞類的n gram	6-12
那麼通常因為詞類的總數有限	6-12
你的n gram n 可以比較大	6-12
我的word 有有六萬個所以我的trigram 沒辦法太大	6-12
可是如果我的詞類才兩百個嘛	6-12
我可以變成four gram five gram	6-12
那麼它們通常詞類的n gram 是做到	6-12
至少做five gram	6-12
因為你總共只有只有幾十個到上百個	6-12
所以不難做	6-12
你做到five gram 的話呢	6-12
你就有這個詞類的之間的關係	6-12
全部都有那就是所謂的pos n gram	6-12
你如果有這個pos n gram 的話	6-12
那麼這pos n gram 怎麼來是要用人先用人去標一次嘛	6-12
譬如說你拿一個database	6-12
是用人標好的詞類	6-12
你就可以train 詞類的n gram	6-12
然後呢你現在就可以做這個自動標詞類的工作	6-12
那你可以想像是這樣子	6-12
就是你現在有個詞典	6-12
這個詞	6-12
它有這個詞類這個詞類這個詞類	6-12
這個詞它有這個詞類這個詞類	6-12
這個詞它有這個詞類這個詞類這個詞類	6-12
你有一個這樣的詞典	6-12
這個詞典就是我這邊所謂的lexicon of words with possible pos	6-12
就是我都有這個詞典	6-12
我有這個詞典之後呢	6-12
我今天任何一個句子出來	6-12
這個詞可以是這三種詞類	6-12
這個是可以是這兩種詞類	6-12
這個是可以這三種詞類等等	6-12
那你可以想像我是一個這樣子的net work	6-12
那到底是哪一個呢	6-12
我用我的n gram 來算	6-12
我現在有的詞類的n gram	6-12
你就知道這個後面應該是	6-12
如果是它的話它後面接這個接這個	6-12
的機率是多少對不對	6-12
那如果是它的話呢它後面接這個	6-12
後面接這個後面接這個機率是多少	6-12
你這個都可以算嘛	6-12
你就就可以根據這個詞類的n gram 來算出來	6-12
這個是自動掛詞類的方去	6-12
就是根據這兩樣東西	6-12
就是一個是有標詞類的詞典	6-12
一個是這個啊詞類的n gram	6-12
你根據這個就可以把自動標詞類	6-12
你自動標詞類標好之後你就有一個很大的database	6-12
來可以做自動標詞類的的的標好詞類的語料庫	6-12
你就可以train 這種	6-12
好以上大概這個這個也是另外一篇碩士論文你如果有興趣的話可以查得到的	6-12
那我們要講一下就是說你	6-12
其實這個class based 跟word based language model 其實是可以整合的	6-12
那什麼意思呢	6-12
你基本上來講	6-12
這個word 以word 來做的n gram	6-12
跟以class 來做的n gram 哪一種比較好	6-12
其實是如果是真的是word 的你train 得好的話word 比較好啊	6-12
因為它比較精緻	6-12
那class base 是比較粗的	6-12
因為它把一大堆class 搞在一起嘛對不對	6-12
你可以想像	6-12
class 是比較粗的language model	6-12
因為你把一堆東西搞在一起	6-12
你想這個比較細緻還是這個比較細緻	6-12
當然是這個比較細緻啦	6-12
你如果這是哪一個word 跟哪一個word 之後會碰見	6-12
你如果這個train 得好的話	6-12
這個鐵定比這個好啊	6-12
這個是把一大堆word 跟一大堆word 跟一大堆word 放在一起了	6-12
所以這個是比較粗的	6-12
只是說這個的data 可能不夠你可能train 不好對不對	6-12
是data 不夠你train 不好的問題	6-12
所以這個其實這個class base 是有點像smoothing 的效果	6-12
train 不好的話不如用這個嘛	6-12
可是你如果train 得好的話這個顯然比這個好嘛	6-12
這是我們必須了解這一點	6-12
所以呢class 的不一定表示說一點會比word 好	6-12
應該是說word base 的是more precise	6-12
如果是常用詞	6-12
你的頻率夠高的話	6-12
你count 的數目對的話	6-12
其實是比較好的	6-12
只是說當你data 不夠的時候	6-12
你可能需要用class	6-12
所以呢比較好的辦法應該是把這兩者整合	6-12
你back off 到class	6-12
如果count 不夠的話	6-12
就是說你其實是可以做就是只要count 的數目夠高	6-12
我就用word 的的的n gram	6-12
數目不夠高我才用class	6-12
那這是一個比較最好的辦法	6-12
數目不夠高就用class	6-12
數目高我就用word	6-12
然後呢只要它的夠高夠高頻的word 就單獨一個class	6-12
夠高頻的那有很多很特你你知道在中文裡面有很多很特別的字	6-12
舉例來講把	6-12
這是一個非常有意思的一個字	6-12
在中文裡面	6-12
把什麼幹什麼這個由	6-12
那那那這是一非常高頻的詞它你你你要它跟誰在一起呢	6-12
不如它自己一個人在一起比較好嗯	6-12
那凡是這種很高頻的這個	6-12
那當然最高頻的是這個字嘛	6-12
那	6-12
這個可以用來做各種各樣的目的	6-12
啊	6-12
這個很漂亮的	6-12
坐在那裡的同學啊	6-12
那這個的意思都不一樣啊	6-13
譬如說坐在那裡的同學這個的是who	6-13
是一個介是一個這個是這個who sitting there	6-13
這是這是這個這個的是那個who 的意思哦等等	6-13
所以呢你可以想像就是如果說他是高頻到一個程度的話	6-13
就當成一個單單獨一個word 當成自己當成一個class 來做等等哦	6-13
這個是你可以事實上是可以把word base 跟這個class base 可以混在一起	6-13
好那再來我們再講一個問題就是在中文裡面有一個很大的問題就是	6-13
中文有字有詞啦	6-13
其實我們最早年開始做中文的時候並沒有了解到這一點	6-13
因為在英文裡面看到的文獻都是講words	6-13
對不對你的n gram 都是講words	6-13
都是w i 這個i 減二i 減一	6-13
都是在看這個東西	6-13
那這是什麼這是words 嘛	6-13
word 是什麼呢我們從小學的word 就是字嘛	6-13
所以我們在八零年代剛開始做的時候一直算的n gram 就是算字的n gram	6-13
就是字的n gram	6-13
電後面腦電腦後面接科腦科後面接技這樣子	6-13
一直算n 的字字的n gram 可以算	6-13
效果也有一定效果也不錯	6-13
但是後來才曉得	6-13
其實在words 在中文而言不是字是什麼是詞	6-13
那麼換句話說呢你其實應該是算這個n gram 是什麼是詞的關係	6-13
而應該不是字的關係	6-13
那麼以這邊為例的話你可以想像譬如說不後面接改	6-13
這實在這個這個bigram 沒有什麼道理	6-13
可是進步後面接改變這個bigram 就很有道理	6-13
ok	6-13
你這個做後面接方沒有什麼道理	6-13
這些字的bigram 字的trigram 都可以算的都有用的	6-13
可是你真的看的話就知道	6-13
字之間的relationship 是弱很多	6-13
因為不見得很有道理	6-13
可是詞中間的relationship 就很清楚	6-13
sil	6-13
那麼因此呢	6-13
really 這個n gram 應該是用詞來做會比用字來做要來得好	6-13
但是你如果要用詞來做有一個最大的問題哪裡是詞	6-13
sil	6-13
雖然我們從小就學造詞造句所以呢我們很知道什麼是詞	6-13
可以如果你仔細想一想的話呢我們其實中文的詞是一個not well defined 的東西	6-13
舉例來講我這邊舉的例子就是這個意思	6-13
就是說電腦是一個詞	6-13
科技是一個詞	6-13
的也是一個詞	6-13
但是科技的當然也是一個詞	6-13
電腦科技也是一個詞	6-13
電腦科技的也是一個詞	6-13
改變是一個詞	6-13
了是一個詞	6-13
改變了當然也是一個詞	6-13
工作是一個詞方式是一個詞工作方式當然也是一個詞	6-13
換句話說你給我一個句子	6-13
到底怎麼樣來切成哪幾個字是連起來是一個詞的	6-13
這是我們所謂的斷詞word segmentation	6-13
sil	6-13
斷詞	6-13
斷詞本身就不是一個有unique solution 的事情	6-13
它的solution 不是unique 的	6-13
那麼有不同的斷法	6-13
你兩個人來斷就斷得不一樣	6-13
sil	6-13
那麼因此呢到底什麼是一個詞不能確定	6-13
如果是這樣的話你的詞典其實不知道應該放哪些	6-13
所以在英文裡面是很容易	6-13
英文你只要是有一個有一個空白這就是一個word	6-13
有一個空白那就是一個word	6-13
那麼因此這些word 就該放在詞典裡面很簡單	6-13
可是中文不是	6-13
中文因為沒有空白	6-13
所以你不知道到底應該放哪	6-13
一個那當然有人想一些rule	6-13
sil	6-13
他說怎麼樣才可以	6-13
譬如說的單獨是一個word	6-13
不要跟科技的合在一起	6-13
等等	6-13
sil	6-13
科技的我一定要把它拆成科技跟的	6-13
但是呢那你電腦跟科技到底能不能合成一個詞呢這就是這個很多問題	6-13
所以呢你並不存在一個真正的lexicon 是大家都公認的	6-13
所以呢你的那個詞典到底應該怎麼做你我們一直在講的那個lexicon 本身就是一個問題	6-13
因為到底你們該什麼什麼知道	6-13
為什麼會這樣的基本原因是因為我們的每一個字這是叫做character	6-13
我們每一個字都是有意思的	6-13
不像英文的character 是這個叫做character	6-13
這個character 有沒有意思呢多半不見得	6-13
除了少數例外	6-13
sil	6-13
除了少數譬如說a 這個character 是有意思的	6-13
對不對	6-13
sil	6-13
你英文的話character 沒有意思所以呢word 才有意思所以word 很清楚	6-13
中文是因為每一個字自己有意思	6-13
所以它自己也有它的rule	6-13
那麼你可以想像中文的構詞是多麼有趣的一個問題	6-13
譬如說改變	6-13
其實改跟變各有它的意思	6-13
兜起來仍然是相當接近的兩個意思	6-13
啊	6-13
相當接近的一個意思	6-13
思想	6-13
思跟想都是有意思的字單字	6-13
不過它們合起來是一個意思	6-13
啊一個有關的	6-13
那其實在中文裡面的詞的意思跟它的component character 字的意思之間是有很有趣的關係	6-13
像這種是兩種	6-13
那還有相反的對不對	6-13
這個有有相似的有相反的譬如說歡喜	6-13
這兩個是意思相似的構成一個新的詞意思是跟它們都相像的	6-13
有相反的譬如說緩急對不對	6-13
那緩急到底是緩還是急	6-13
啊	6-13
這個這個是相反的構成的	6-13
那這個也有是類似而	6-13
譬如說大學	6-13
不是一個很大的學	6-13
但是它顯然是跟大有關係也跟學有關係	6-13
這是一個新的詞	6-13
那也有的詞是跟它的component word 沒有關係的	6-13
譬如說和尚	6-13
和尚跟和跟尚都沒有關係但是和尚是另外一個詞	6-13
對不對	6-13
光棍	6-13
光棍光有一個意思棍有一個意思這光棍到底跟光跟棍有沒有關係	6-13
好像有一點啊	6-13
這個所以你會有很複雜的東西所以你如果這樣子來看的話你就知道就是在一個句子裡面的	6-13
所以我剛才講就是說我們一開始做就是做字的n gram	6-13
唉它有work 的	6-13
字的gram n gram 也work 因為它字本來有意思嘛	6-13
但是字的n gram 比起詞的n gram 來哪一個好呢	6-13
詞的n gram 好	6-13
為什麼因為你可以想像是應該進步接改變嘛	6-13
工作接方式嘛	6-13
不是做接方嘛	6-13
對不對	6-13
工接作的話	6-13
譬如說字的n gram 為什麼會會很會不錯是因為這種詞的話它有非常強的n gram 譬如說工作	6-13
這個bigram 非常強因為這本來是一個詞	6-13
方式這個bigram 非常強嘛	6-13
所以雖然作後面接方好像不太通	6-13
但結果你還是work	6-13
sil	6-13
那麼那麼從這樣來看的話呢用	6-13
那你還有一點就是這邊講的就是我們中文的這個構詞非常的open	6-13
啊	6-13
是幾乎是unlimited 的詞不斷的出現	6-13
我們講過各種新詞多得不得了	6-13
這個電加腦變成電腦	6-13
啊	6-13
只有大概只有中文這麼聰明把把computer 叫做電腦	6-13
那麼人家只把computer 想成是一個會compute 的machine	6-13
只有	6-13
中文是很早就知道computer 不只是會compute	6-13
它就跟腦一樣啊	6-13
早在這個這個詞出現的多早	6-13
在三十年前就己經叫這個名字了	6-13
當時的電腦只會compute	6-13
但是為什麼會知道它是腦	6-13
這個這個名詞很了不起的	6-13
那麼	6-13
我們中文是很容易把長詞自動縮短成為短詞	6-13
台灣大學就叫台大	6-13
有人定義過嗎沒有	6-13
國家科學委員會叫做國科會有人定義過嗎沒有	6-13
啊	6-13
我我們自動就可以把它因為這是因為每一個字都有意思的關係	6-13
所以你就可以自動把它選擇最重要的字	6-13
意思最代表這個詞的字就把它拿來就兜起來了	6-13
你譬如說中統	6-13
有人定義過什麼是中統嗎沒有	6-13
可是你講中統就知道是什麼	6-13
sil	6-13
因為它己經把因為那個字本身有意思	6-13
它就把這些字有有它代表意思的最重要的那兩個字兜起來這就變成一個新詞	6-13
這是中文的特性	6-13
人可以隨便把人的名字拆開來	6-13
把他這個拆開來之後把它的title 塞到中間進去	6-13
啊	6-13
然後	6-13
這個除了短詞加起來變成長詞之外長哦這個是長詞可以縮短成為短詞	6-13
短詞可以加起來變成長詞	6-13
然後可以短詞加起來長詞之後再縮短譬如說北二高	6-13
是北部第二高速公路	6-13
很多北部第二然後高速公路	6-13
兜起來之後我再把它縮短就變成北二高	6-13
所以你很多這種你結構都不需要去define 而自動就會出現因為每個字都是有意思的	6-13
所以中文的詞其實是unlimited	6-13
你要做一個詞典是很難的	6-13
那麼因此呢我們就會發生一大堆的所謂的這個oov 的問題	6-13
在語音辨識裡面任何一國的語言都有所謂的oov 的問題	6-13
這是所謂的o out of vocabulary 的問題	6-13
什麼是oov	6-13
就是我們在一開始畫的那張地圖裡面需要一個詞典	6-13
sil	6-13
sil	6-13
我們在一點零裡面的那張最基本的圖	6-13
這張圖	6-13
在這張圖裡面的	6-13
就是要有一個lexicon	6-13
但是呢你永遠lexicon 永遠裝沒有辦法裝滿所有的詞	6-13
你一定會漏掉一些詞	6-13
嗯任何語言都一樣	6-13
那麼你如果那個詞不在你的詞典裡面你永遠不會辨識對	6-13
對不對	6-13
只要那個詞不在你的lexicon 裡面你就不會對了	6-13
而如果這樣子的話你可能前後都會錯因為你有n gram	6-13
所以你如果那個詞不在詞典裡面的話	6-13
那個詞鐵定不會對	6-13
這個word 如果不在lexicon 裡面這個word 鐵定不會對	6-13
那它不對的時候可能前後都會錯	6-13
因為是你靠n gram 做的	6-13
你就會錯	6-13
那這個問題就是所謂的oov 的問題就是out of vocabulary	6-13
就是你任何語言你都沒辦法知道exactly 多少詞才是那些詞是你是會用的因為你永遠會說到一些沒有的詞	6-13
那這就是所謂的oov problem	6-13
ok 那我們回到剛才的	6-13
這是這個我這個這是所謂的out out of vocabulary problem	6-13
只是說呢我們通常這個的算法是所說的oov rate	6-13
就是你一般的你的一個一篇文章裡面	6-13
你你會out of vocabulary 的詞的比例是多少	6-13
在英文啊什麼oov rate 是低的	6-13
也是會有oov 但是比較低	6-13
我們隨便舉個例來講譬如說你會說ibm	6-13
看你那個辭典有沒有把它放進去	6-13
如果沒有沒有的話這就是oov	6-13
等等	6-13
那最常出現的oov 就是這種	6-13
專有名詞嘛專有名詞很可能不在裡面	6-13
那麼但是你凡是碰到專有名詞不在裡面你那邊就會錯掉啊	6-13
那麼在中文而言oov rate 特別高	6-13
中文的這個oov rate 會特別高就是因為我們不斷的產生各種各樣的新詞	6-13
那麼因此呢	6-14
我們整個的整個的這個recognition 的方法是word based	6-14
在因為是在西方的語言英文為基礎的西方語言所發展出來的技術就我們剛才看的那張圖	6-14
一點零的那張圖	6-14
那張圖是以可以說是是word based approach	6-14
因為你的lexicon 就是用詞來做的	6-14
然後你的language model 就是n gram	6-14
什麼是n gram	6-14
是詞來做的n gram	6-14
所以你都是以詞為基礎來做的但是你現在如果詞搞不清楚的話	6-14
你整個都比較會有問題	6-14
這是中文裡面最常碰到的一個狀況	6-14
那麼這個時候呢	6-14
嗯	6-14
那還有一個問題就是你要會斷詞	6-14
你並不是隨便上網拿一大堆database 下來我就可以train language model 了	6-14
那你那樣能夠train 的就是字的language model	6-14
字的n gram 你還可以train	6-14
詞的n gram 你還不能train 你得要先會斷詞	6-14
那你的斷詞就是你要有一個程式去做斷詞的程式	6-14
那斷詞程式怎麼做	6-14
其實是不難	6-14
但是要做到好也是不容易的	6-14
你可以猜這個斷詞程式怎麼做	6-14
就是我有我有一個詞典	6-14
然後根據詞典做matching	6-14
這是最基本的原則	6-14
我有一個詞典告訴我這些詞兩字詞三字詞單字詞	6-14
我然後來做matching	6-14
我看到第一個詞的時候呢先去看所有的單字詞裡面有沒有它	6-14
如果它是一個單字詞的話那它可以是一個單字詞	6-14
可是呢我走到第二個詞去看的時候呢它是不是一個雙字詞	6-14
唉如果它有雙字詞的話呢長詞優先	6-14
如果這兩個是一個雙字詞的話呢我先考慮這可能是一個雙字詞	6-14
那就不要考慮它是單字詞了	6-14
那我再找到三字詞發現唉發現唉這個也是一個三字詞那三字詞又優先了	6-14
那我就考慮這個	6-14
這還有沒有四字詞沒有了	6-14
我從這裡再開始重新再來斷	6-14
發現了唉這邊這有一個雙字詞	6-14
再發現這邊有等等啊	6-14
你這第一個原則就是去做matching	6-14
然後長詞優先	6-14
然後你可以從從左邊切左邊match 過來	6-14
你可以從右邊match 過來	6-14
你可以兩邊來走	6-14
然後另外一個就是你可以用詞的n gram	6-14
sil	6-14
那到底是	6-14
你有的時候這樣子斷也可以這樣子斷也可以	6-14
那誰比較對	6-14
用n gram 來算	6-14
等等等等	6-14
這類的方法都可以做	6-14
這是所謂的這個斷詞的問題	6-14
那當然有的時候有ambiguity 當然有ambiguity	6-14
譬如說不知道理由	6-14
是知道跟理由各是一個詞呢	6-14
還是道理這個不知道理跟由	6-14
那憑什麼n gram 是一種solution 告訴我說知道理由	6-14
而不是道理跟由	6-14
啊等等	6-14
那那這這些就是那那這個就是所謂斷詞的問題那你第一個呢你要有一個自動斷詞的程式	6-14
這樣子我上網去抓一大堆training data 來之後	6-14
我斷了詞才能夠train 我的language model 如果是以詞來做的話	6-14
等等	6-14
那這是中文的一堆問題	6-14
在這個問題之下我們就可以講了	6-14
就是說那我可以用字來train n gram	6-14
我也可以用詞來train n gram	6-14
到底那個比較好	6-14
它們之間的關係是什麼	6-14
這邊是講用字所train 的n gram	6-14
這個上面是講用詞所train 的n gram	6-14
那基本上來講呢	6-14
這個	6-14
用字來train 的最大的好處就是你就避免了一個斷詞的問題了	6-14
詞的結構你也不管了啦	6-14
就是用把字去數字就好了嘛所以呢它這個這是最大的好處你沒有斷詞的問題	6-14
也沒有oov 的問題沒有oov 的問題	6-14
你反正就去把每一個字都都當成是一個單字	6-14
它就自動都有所有的	6-14
這都是一個一個word 就對了	6-14
你這樣就去算就是了	6-14
這是最大的好處	6-14
但是它有最大的壞處就是它顯然是比較weak	6-14
因為沒有word level information	6-14
就像我們剛才講的	6-14
你就只能把它看成是	6-14
不後面會接改	6-14
雖然沒什麼道理但是你就把它看成是這樣子	6-14
哦	6-14
這個腦後面接科你還以為是那個醫學院的腦科	6-14
等等	6-14
那麼做後面接方那不曉得為什麼不過反正有	6-14
你就是就是因為沒有word level 的沒有詞的level 的知識你只有字level 那你就用字level 去算	6-14
是可以但是你缺少一點東西	6-14
那所以它會比較weak	6-14
那還有一個很大的問題就是說	6-14
你可能需要higher order 到相當程度才能夠代表	6-14
為什麼因為譬如說你你這個trigram	6-14
是這兩個word 後面接這個word	6-14
但是這是三字詞這是兩字詞這是三字詞	6-14
所以你如果要handle 這個relation 的話你要幾個一二三四五六七八要八gram	6-14
你如果用字來做的話要八gram 才能夠做到這個relation	6-14
啊	6-14
你如果只是bigram trigram 是不太夠的	6-14
你哦你這個relation 是用字來講是應該要用八gram	6-14
那八gram 當然很難做了	6-14
所以你需要higher order n gram	6-14
才能夠做到像詞這樣子的關係這trigram	6-14
可是你要這麼n 如果這麼大的話是不容易做的	6-14
所以這個是用字來做的缺點	6-14
那用詞來做的話當然好處就是說詞就是真正的building block of sentence	6-14
我們造個句子其實是用詞造的嘛啊	6-14
就像我們剛才講的	6-14
這個句子顯然你不是用一個字一個字去拼的你是用一個詞一個詞去拼的	6-14
不管你心目中的詞的哪一個	6-14
那你是用詞造出來的	6-14
所以呢你你你造個句字的時候事實上是是真正是用word 用詞來做所以是是比較好的	6-14
而且你可以加更多的information	6-14
就以我們之前的所謂加更多的information 就以我們之前的例子來講	6-14
你就是可以把像它的詞類啦	6-14
什麼嗯	6-14
這些詞類的知識可以放進來這就是用詞做的好處用字就沒有了嘛	6-14
然後呢但是呢關鍵是說你有沒有一個好的詞典	6-14
你如果這個lexicon 如果不存在就沒有辦法了	6-14
那但是呢我們不太容易有一個很好的詞典	6-14
因為你不管做得多好的詞典總會漏掉一大堆詞	6-14
我們的這個oov rate 是非常高的	6-14
那麼你你不管你我的詞我們剛才講六萬詞你即使變成十六萬詞還是很多詞都沒有	6-14
二十六萬詞還是很多詞都沒有	6-14
你這是不太容易做	6-14
那當然還有一個問題就是你怎麼做斷詞	6-14
那麼我們剛才講一堆做斷詞的方法不表示這個就就很work	6-14
因為斷詞永遠有有一堆錯誤嘛	6-14
所以	6-14
那通常我們的做法就是說	6-14
你斷詞不可能期待百分之一百正確	6-14
但是就是用一個軟體來做斷詞的好處就是讓它consistent	6-14
如果它會把誰斷開來就它每一次都斷開就對了	6-14
譬如說科技的它一定會把的斷開來的話	6-14
你就的就會斷開來	6-14
你不能說有的時候的跟前面連在一起有的時候不連那這個就麻煩了	6-14
你就是要做consistent 的斷	6-14
不管它斷得對不對要consistent	6-14
用一個程式來做有一個好處就是它會做得consistent 的話呢有它還有一定的效果就是了	6-14
所以會有嚴重的oov 的問題	6-14
sil	6-14
那到底怎麼做比較好那麼	6-14
有一段時間大家都不知道怎麼做比較好	6-14
不過後來我們想了一個非常好的辦法turns out	6-14
這是一個簡單而有效的solution	6-14
那麼這個solution 用了十多年前我們就已經知道	6-14
那麼一直到今天仍然是一個非常好的solution	6-14
甚麼solution 呢就是我們講的你用詞來做	6-14
但是你把所有的字都當成單字詞也放進去	6-14
什麼意思	6-14
我有一個詞	6-14
六萬詞典六萬個	6-14
ok	6-14
我就把這六萬個詞的詞典	6-14
那我還有一大堆的oov 怎麼辦	6-14
我就把所有的字	6-14
我有八千個常用字當成單字詞放進去	6-14
於是我就有六萬八千個	6-14
我這裡面所有的詞都有	6-14
常所有的常用詞都有	6-14
所有的單字詞都有	6-14
那這樣之後呢	6-14
我somehow 可以handle oov 的問題	6-14
為什麼	6-14
譬如說只要是一個常用的oov	6-14
我可以解決	6-14
為什麼呢我們舉例來講譬如說呂秀蓮	6-14
這是一個oov 假設我沒有把她放在詞典裡面的話她是一個oov	6-14
但是呢我現在假設我從網路裡面download 一大堆data 來開始要train language model 的時候	6-14
我就用這個詞典來斷詞	6-14
我用這個詞典來斷詞的時候呢這個詞典裡面沒有呂秀蓮	6-14
但是這三個字都是單字詞	6-14
所以我就會把它斷成三個單字詞	6-14
這三個單字詞	6-14
如果這個這三個單字詞連在一起的這個呂秀蓮這個這個詞在這個網路上面出現得夠多次的話	6-14
它的bigram trigram	6-14
train 得夠強的話	6-14
它們自己就有就強了嘛	6-14
所以到時候我現在聽到一個呂	6-14
一個秀	6-14
一個蓮的時候	6-14
唉怎麼辦	6-14
我現在沒有一個雙字詞是呂秀沒有一個秀蓮的話嗯	6-14
那結果我就可能會把他變成這個是呂這是秀這是蓮	6-14
因為它們的trigram	6-14
它們bigram trigram 夠強結果出來就是呂秀蓮	6-14
這個在在多少年以前的當時這個做出來的時候人家很不相信	6-14
你說這個字沒有在詞典裡面	6-14
可是為什麼我說這三個這三個音的時候它會知道呢	6-14
不可能嘛	6-14
是可能的就是這樣做的啊	6-14
這是這個	6-14
只是說這個只能只只限於就是在database 裡面頻率夠高的高頻詞的話如果它夠高頻	6-14
即使它是oov 也會對	6-14
啊那就是因為這些單字詞是都變成一個單這些字都變成單所有的字都變成單字詞放在詞典裡面	6-14
所以我斷詞的時候就把它們當成一堆非常高頻的trigram 跟bigram 的單字詞	6-14
所以呢我就辨識都會正確	6-14
但是呢這個限於高頻你如果頻率不夠高的話	6-14
這個n gram 不夠強是出不來的就是了	6-14
這是講這句話講的意思就是就是這一招	6-14
那當然你也可以想說我是可以把這個我是可以把這個嗯同時做詞的做字的跟做class 的	6-14
真都都全部可以整合在一起也是做得到的啦啊	6-14
那會比較好了不不過有夠複雜就是了	6-14
sil	6-15
ok 那再來呢這還有另外一招就是說	6-15
如果說詞這麼難弄的話	6-15
我們也有一個辦法	6-15
是根本就不要去管什麼是詞了	6-15
我們就把凡是經常會出現在一起的pattern 當成一個詞	6-15
嘛這個我們稱之為second pattern	6-15
什麼是second pattern 呢就是幾個字經常出現在一起	6-15
不管它是不是詞我也不管詞的bound 詞的boundary 在哪裡我都不管	6-15
只要經常出現在一起的	6-15
我就當它是一個pattern	6-15
我就把這個pattern 放在lexicon 裡面	6-15
我這個lexicon 不要想成是詞典	6-15
就是把這堆pattern 放在一起這些東西就叫做lexicon	6-15
什麼叫做經常出現在一起的字把它兜在就把它放在lexicon 裡面呢	6-15
我們底下有一些例子你一看就了解了	6-15
就是經常出現一起的pattern	6-15
我們直接從網路新聞裡面把它抽出來	6-15
那它其實不是在詞典裡面的詞的	6-15
就兩字的pattern 而言你看這種兩字你就知道	6-15
啊	6-15
有些是oov 像這個oov 啦	6-15
這個只是一個oov 而已	6-15
但是你因為在那那段時間它新聞出現很多的話	6-15
它其實是可以當你你可以抓得出來	6-15
那多數是兩個單字詞	6-15
但是呢這兩個單字詞談常常會連在一起	6-15
那你就乾脆的連在一起了啊	6-15
那三字的pattern 呢常常是一個兩字詞跟一個單字詞	6-15
它們經常連在一起就會變成一個三字的pattern	6-15
當然oov 也是會這樣子可以抽得出來	6-15
那雙四字的話呢常常是兩個四字的pattern 常常是兩個雙字詞	6-15
譬如說兩個雙字詞	6-15
那它們這兩個雙字詞經常連在一起就乾脆變成一個四字詞算了嘛	6-15
sil	6-15
那這個觀念就是說我不不去管什麼是詞	6-15
我只是管我用我的database 去train	6-15
說哪些經常黏在一起的就給它黏在一起	6-15
黏在一起的時候呢就變成一個所謂的segment pattern	6-15
然後呢我就把這些pattern 存起來當成是lexicon	6-15
那這個好處呢你馬上想得到就是我就不管這個詞的到底哪裡是詞	6-15
而我可以完全就是你只要給我夠多的database	6-15
我就上網去抓我要的東西之後我就自己抽	6-15
那我就建一個我的詞典就對了	6-15
我根本不管那些那	6-15
怎麼抽這個pattern 這也有很多方法	6-15
不過我們用的最成最成功的簡單的方法就是我們之前講的那種	6-15
就是minimize perplexity	6-15
就是剛才講的跟剛才講的這個很像啊	6-15
這個	6-15
跟這個reference 講得很像就用它的方法	6-15
那所不同的是我現在是要連起來的	6-15
我現在是要連起來的怎樣呢就是說	6-15
剛才那個是說你要你任意兩兩的詞你都要去算它們能不能連	6-15
我現在不用	6-15
我現在只是相連的才要看它們能不能連嘛	6-15
所以譬如說我的網路上抓到的文章是這樣子的話	6-15
你一開始	6-15
每一個字自己是一個pattern	6-15
然後第二步開始哪兩個字可以連成一個pattern 呢	6-15
你去看這個能不能	6-15
這個能不能	6-15
對不對	6-15
這個能不能這個你你你這相鄰兩個去看它們能不能連起來	6-15
看這個能不能	6-15
那你怎麼看呢也是一樣	6-15
就是這個啊minimizing perplexity	6-15
就是我現在如果它們連成一個詞的話	6-15
連成一個pattern 的話	6-15
我的perplexity 有沒有降低	6-15
降低多少	6-15
它們兩個連起來的話呢我的perplexity 有沒有有沒有降低降低多少	6-15
我就用這個來算	6-15
那當然你不必去考慮說	6-15
這	6-15
個跟這個會不會連	6-15
它們現在不在一起你就不用管了嘛	6-15
所以跟剛才那個不一樣的地方是在這裡	6-15
我現在是有一個語料在這裡所以我就我不必管它跟它能不能黏這種問題不用考慮	6-15
那你只要看它跟旁邊能不能黏	6-15
如果它跟它己經黏起來的話	6-15
那下一步是這個東西跟左邊能不能黏	6-15
對不對	6-15
如果這個已經黏起來的話呢那再來要考慮的是它左邊要不要黏	6-15
右邊能不能黏	6-15
就這樣子	6-15
那你用這個方法來你都每一次都算	6-15
我的perplexity 是有增加還是減少	6-15
增加多少	6-15
用這個方法你可以得到一個minimum perplexity 的一個新的lexicon	6-15
那它不是我們平常所習慣的詞	6-15
但是裡面有很多很多我們的詞都自動抓得出來	6-15
那那譬如說專有名詞自動都會抓得出來	6-15
然後很多這種pattern 都自動可以抓出來你就得到一個新的詞典	6-15
你那個詞典就是我們所謂的所謂的segment pattern 的lexicon	6-16
那這個效果也不錯	6-16
那麼底下這一頁是在舉一個例子就是說	6-16
有了這個跟你沒有是有何不同	6-16
這是一篇新聞	6-16
那麼譬如說開車	6-16
這是一個pattern	6-16
但是其實應該開是一個單字詞車是一個單字詞	6-16
你這句話要辨識對的話	6-16
你如果那你必須是要有夠強的bigram trigram	6-16
才能夠把這個開跟這個車跟這個抓得出來	6-16
可是如果你現在開車是一個pattern 的話呢你只要有一個開的音有一個車的音的話	6-16
你自動就會把這個詞這個這個這個pattern 抓到	6-16
然後呢你那你就看它跟些的bigram 夠強trigram 夠強它就出來了嘛	6-16
所以基本上你如果是他們構成一個pattern 的話	6-16
是比沒有pattern 會辨識正確的機會是提高	6-16
那我們如果隨便拿網路上的新聞來比的話	6-16
大概有百分之二十八的是詞典裡面沒有的pattern	6-16
啊就是這種這種有百分之二十八可以抓出來是詞典裡面沒有的pattern	6-16
那這個是有助於辨識的	6-16
sil	6-16
那這個是用這個second pattern 來做的情形就是了	6-16
ok 好關於中文的我們說到這裡	6-16
這大概是這個language model 裡面是中文比較不一樣一點啊	7-1
有有有這些不同	7-1
好那麼	7-1
六點零我們說到這裡	7-1
休息十分鐘	7-1
我們底下開始進入七點零	7-1
ok 我們進入七點零	7-1
七點零是我們目前為止沒有真的去討論語音訊號長怎樣	7-1
到我們現在要來看語音訊號長怎樣	7-1
然後怎麼樣做front end processing	7-1
啊	7-1
那這一塊的位置	7-1
嗯	7-1
這一塊的內容比較複雜就是有各種各樣的東西	7-1
所以大概的reference 是包括這一本的前面這些這本的這些跟這本的這些	7-1
那這裡面是啊	7-1
我想第二個這部分呢是比較牽涉到各種語音各種不同的聲音它訊號會長怎樣等等等等的問題	7-1
稍微比較難看一點如果你覺得看不下去跳掉一些應該沒有什麼關係	7-1
啊	7-1
那這個比較重要這個比較重要就是了	7-1
啊	7-1
那那中間這個是比較比較啊嗯牽涉到很多不同東西就是了	7-1
那我之前提過一下就是說	7-1
我在每一個每一每一個chapter 的前面的這一堆reference 是基本上是我希望你嗯會去看的	7-1
也就是期中考的時候是會考到這些東西的	7-1
啊	7-1
那你可能會說啊這個我看到一堆看不懂怎麼辦	7-1
看不懂你就跳過去就好	7-1
sil	7-1
然後過一段時間你再回來一次會發現你看懂了多一點	7-1
就這樣子啊	7-1
那如果說是嗯	7-1
我再回來看我始終看不懂的話那那裡就是不會考的嘛	7-1
那我我們這門課因為修課的同學有很多不同的background	7-1
所以我們基本上只能用這種方式來來進行就對了	7-1
好那麼	7-1
嗯我們現在要講的是哪一塊呢我們如果回到這個剛才講的那一點零的那張圖的話你就知道	7-1
就是有這一張圖的話	7-1
那我們現在己經講過的是包括這邊	7-1
就是你從acoustic data 你從acoustic data 經過acoustic model training 來train 的這是什麼這是h m m	7-1
所以就是四點零跟五點零就是講這一塊	7-1
然後呢我們六點零講的是這一塊就是你怎麼樣用文字來train language model	7-1
sil	7-1
那我們七點零是講這一塊就是到底input 的語音訊號長怎樣	7-1
然後我們怎樣做front end processing	7-1
最後得到我要的那群feature	7-1
我們還差八點零是這一塊	7-1
sil	7-1
所以七點零的這一塊	7-1
跟八點零的這一塊講完的時候我們這張圖才算講完	7-1
那那個也就是我們期中考的範圍就是考到那裡	7-1
啊	7-1
所以大概還要兩三週才會講完這部分就對了	7-1
好那我們現在回到七點零	7-1
那麼語音訊號長怎樣呢那這個時候我們就得要看一點語音訊號的圖	7-1
那這個呢	7-1
這一類的語音訊號的圖在任何一本課本上都可以發現很多	7-1
那我沒有把它放在網頁上是因為這些因為都是不同的書上的	7-1
基本上都有它們的智智財權或者版權的問題如果如果放在網頁上都會有問題	7-1
所以我們就不放了	7-1
但是基本上呢你要看是你要找的是很容易找得到的	7-1
sil	7-1
那這個是什麼呢這是母音	7-1
e 嗯什麼a 這個啊哦等等	7-1
這些所有的母音的訊號長得像這樣	7-1
那麼你有沒有發現它們有什麼共同的特性	7-1
很容易看到的特性呢	7-1
就是它有週期	7-1
sil	7-1
所有的母音都是有明顯的週期性	7-1
你可以看到譬如說從這裡到這裡就是一個週期	7-1
啊	7-1
那它們不同只是因為一個週期裡面長得不一樣	7-1
這裡面如果長得這樣就變成ㄨ	7-1
對不對	7-1
如果長得這樣呢就變成ㄚ	7-1
長得這樣就變成ㄟ等等	7-1
這週期性是非常明顯的	7-1
這是語音訊號裡面一個很大的特點	7-1
但是呢並不是說所有的聲音都是有週期的	7-1
也有一堆是沒有週期的	7-1
那我們舉例來講呢	7-1
像這個子音	7-1
這裡面它其實是一個子音被插在兩個母音的中間	7-1
這裡要show 的是ㄒ這個音	7-1
那前面是ㄜ後面ㄚ	7-1
那前面這裡的週期性是非常明顯你看它是有週期性的	7-1
後面ㄚ也是非常清楚有週期性的	7-1
中間的這一堆是什麼呢	7-1
這一堆是ㄒ的音	7-1
那你可以看到它是沒有週期性的非常混亂	7-1
啊看起來跟white noise 是非常像的一些東西	7-1
而這個聲音呢跟前後之間呢是有一個transition	7-1
你看到它的這個這邊的ㄜ有週期性然後這個週期慢慢慢慢開始變化	7-1
到這邊還有週期性不過已經變得很厲害最後就變成這樣	7-1
這底下開始慢慢出現週期性最後變成這樣	7-1
中間這邊是一些transition	7-1
那這兩種的差別就是我們講的voice 跟unvoiced	7-1
那這就是所謂的濁音	7-1
我們在中文裡面翻作這個叫做濁音	7-1
這個叫做清音	7-1
那麼所謂的voiced 就是像這種週期性的	7-1
像剛才所有的母音都是這種週期性的	7-1
這是所謂的voiced	7-1
那麼所謂清音或者unvoiced 就是像這個	7-1
ㄒ這種呢是unvoiced	7-1
那並不是說子音母音這樣子分	7-1
母音都是voiced 沒錯	7-1
那麼子音並不完全都是unvoiced	7-1
這裡有另外一個例子	7-1
這個是這中間那個子音是v	7-1
這個v 的這個音v	7-1
那你看到它中間會變成這樣	7-1
它跟剛才不太一樣	7-1
它的週期性仍然很清楚的出現在這裡	7-1
這是非常清楚的週期性	7-1
那這個呢是子音但是它是屬於voiced	7-1
濁音的	7-1
它是voice 是濁音不是清音	7-1
嗯	7-1
那麼它的週期性非常清楚但是你也可以看到它的這個transition	7-1
從這邊的ㄜ的週期	7-1
轉到這邊v 的週期性是慢慢慢慢變下來最後變成這樣	7-1
你要變成底下那個啊呢也可以	7-1
它再慢慢再變過去變成這樣	7-1
嗯	7-1
那這個是這個另外一個例子	7-1
那我們真正的一段語音會長怎樣呢	7-1
底下這個例子就會看得很清楚	7-1
像這樣	7-1
這句話呢是	7-1
這句話是should we trace	7-1
那這句你可以看到前面是sh	7-1
這個全長是每一段長是一百個mini second 就是零點一秒	7-1
啊	7-1
這邊是sh	7-1
所以這個是unvoiced 的清音	7-1
是這種清音哦	7-1
然後呢would 的時候呢就變成一個濁音就是voiced	7-1
週期性非常明顯的	7-1
那d 這個d 的是一個子音	7-1
但是它很清楚是一個voiced	7-1
週期性仍然非常明顯的	7-1
sil	7-1
等等	7-1
we 這是e 是很清楚的這是一個週期性的這是voiced	7-1
trace 的時候呢這個tr	7-1
是一個unvoiced 的清音	7-1
那你要發tr 的時候你必須先把聲音停掉嘴巴關起來之後再打開	7-1
should we trace	7-1
你這個得再重新所以中間前面是有一段silence	7-1
之後從這邊開始是tr 的音	7-1
然後到這邊呢是a	7-1
最後再回到s	7-1
那這樣子的的一個這個這是一個非常typical 的一一句一個語音訊號就是長這樣	7-1
那你可以看到基本上就是分成兩大類	7-1
就是voice 跟unvoiced	7-1
那麼voiced 的呢那很清楚的就是以這個這個週期為它的最大的特色	7-1
然後呢大部分都是母音	7-1
但是也有voiced 的子音	7-1
像這個d 就是voiced 的子音	7-1
那unvoiced 的話呢清音的話呢像這種	7-1
sh 或者這個s 或者這個tr	7-1
那它們有一個共同的特性就是完全沒有週期性	7-1
然後看起來非常像white noise	7-1
的東西	7-1
那當然還有一個很明顯的特性你也可以看到就是	7-1
它比較小嘛	7-1
所有的這個這個基本上都這個母音的amplitude 會比較大嘛啊	7-1
或者voiced 的amplitude 會比較大嘛	7-1
那子音的amplitude 都會比較小像這個雖然是voiced 這個子這個amplitude 會比較小嘛	7-1
啊差不多是這樣子	7-1
好那有了這個了解之後那我們想可以說一下那麼這個我們真正的差別究竟是什麼	7-1
那真正的差別其實就是我們的聲帶有沒有在震動	7-1
那這個就涉及所有的聲音都是人的嘴巴說的	7-1
所以都是人的發聲系統才會發出這樣的聲音來	7-1
那你可以想像成是一個這樣子的一個這是我們的發聲系統	7-1
那這是我們的氣管	7-1
氣流從這邊進來	7-1
這邊是我的聲帶	7-1
或者聲門	7-1
我這邊聲帶在這邊震動之後進入口腔	7-1
這個時候depend on 你的其實我們發不同的音就是在這裡	7-1
因為你的不同的口型嘛你這ㄚㄛㄜㄟ	7-1
差別差在哪裡就是差在口型不同	7-1
那麼因此這邊不同結果給我出來一個不同的聲音	7-1
如果進去的叫做u 的t	7-1
出來是x 的t 的話	7-1
那所謂的這些voice 的聲音	7-1
為什麼voice 會有週期性	7-1
其實就是因為聲帶在震動	7-1
聲帶的震動的週期就是那個週期	7-1
也就是說這個u of t 呢是怎樣的	7-1
聲帶每震動一下的時候	7-1
聲門打開一次	7-1
有一堆氣出來	7-1
然後呢就關上了	7-1
待會再震動一次再一堆氣出來之後又關上了	7-1
因此呢我們的聲帶以某一種週期在震動	7-1
那麼它就會得到這樣子	7-1
這就是我們進去的u of t	7-1
那你凡是這一堆氣進去的時候	7-1
出來會變成怎樣呢經過這一堆你的口唇齒舌之間的關係	7-1
它會變成一個然後你下邊又出來一個	7-1
它就再來下一個	7-1
又來一個它就再來一個	7-1
那這個就是我的這就是x of t	7-1
於是你這邊看到的週期性就是這邊的週期	7-1
那這個週期就是所謂的pitch	7-1
這個pitch p 的話	7-1
也就是這邊的週期	7-1
也就是我聲帶以這個以這個頻率或者這個這個週期震動的話	7-1
每震動一下產生一個氣流	7-1
經過這邊就變成一一個週期的waveform	7-1
然後這邊再來下一個這邊就是下一個所以這邊就是這邊的週期	7-1
這個週期就是你這邊所看到的這個週期	7-1
那這樣子的這個震動的狀況其實你是很容易感覺到的	7-1
你只要把你的手指頭放在你的這喉嚨這邊	7-1
你發一個音的時候ㄚㄛ你可以感覺到麻麻的	7-1
它那邊在震動	7-1
那個震動就是就是這個	7-1
那你感覺得到的那個震動的那個頻率那個週期就是這個週期	7-1
那麼其實也就是這個週期	7-1
那這個關係當然你可以我們如果把它變成discrete time	7-1
這裡變成一個一個的sample	7-1
因為我們其實是以一個一個sample 在處理	7-1
那這就變成x 的n	7-1
那其實這邊也可以看得出來是一個一個的sample	7-1
那這就是u 的n	7-1
那麼在這個情形之下呢我們通常會用一個這樣子的input output 關係來來描述	7-1
這是u 的n	7-1
這個是x 的n	7-1
那這個un 到x n 的關係	7-1
其實就是從這個到這個的關係	7-1
或者是從這個到這個的關係	7-1
其實就是你這邊的唇齒舌之間的關係	7-1
就造成這個關係	7-1
那如果是這樣子看的話	7-1
那麼這個是voiced 的時候	7-1
會發生週期的原因	7-1
那這個這個字你如果是很喜歡音樂你就知道這個字是什麼就是我們音樂裡面講的音高	7-1
也就是你音的高低	7-1
其實就是我們稱之為的f zero	7-1
是什麼就是p 分之一	7-1
我的訊我的訊號裡面有一個所謂的f zero	7-1
就是這個週期的p 分之一	7-1
那這個東西就是我耳朵聽到的音高	7-1
那麼那這個影響最清楚的	7-1
就是我們的四聲	7-1
你知道我們中文的有聲調	7-1
巴拔把爸	7-1
的四聲是什麼	7-1
就是我那個f zero	7-1
剛才那邊畫的那那個f zero 就是p 分之一	7-1
as function of time t	7-1
你可以得到四種pattern	7-1
如果它大部分是平的這就是第一聲的巴	7-1
如果你得到這樣上去的話呢	7-1
這就是拔	7-1
那麼如果是把就是第三聲	7-1
如果是把你就會上來	7-1
第三聲有兩種啊	7-1
這叫做全這個叫做全上這叫做半上	7-1
那麼把	7-1
這是全上	7-1
你如果只唸把這就是半上前半上	7-1
那麼嗯那第四聲呢	7-1
第四第四聲是爸	7-1
第四聲	7-1
那些這其實就是f zero 相對於time t 的不同的pattern	7-1
就是我們這個音高的變化	7-1
第四第四聲是爸第四聲那些這其實就是f zero 相對於time t 的不同的pattern 就是我們這個音高的變化	7-1
那這是在在我們中文裡面的語音聽得最清楚的	7-1
那其實我們真正講一句話的時候不是這麼標準的pattern	7-1
而是很混亂的	7-1
譬如說可能是這樣子	7-1
那也就是說你除了基本上基本上比較像這個pattern 之外	7-1
它是會受到前後音的影響它都會動掉的哦	7-1
不是那麼漂亮的pattern	7-1
而是它都會受到前後音的影響	7-1
會動掉	7-1
然後它的它的level 也會動到	7-1
所以真正的你聽到的一個聲一句話的f zero as as function of t 的話呢	7-1
你會得到一種像這樣子的情形啊	7-1
那這個其實這樣的訊號週期的	7-1
然後它的這個f zero 這個現象其實是音樂裡面也是一樣的	7-1
你如果拿一個小提琴獨奏或都鋼琴獨奏的waveform 來看	7-1
也是這樣	7-1
那這個pitch 也就是那個的中央c 還是什麼	7-1
那個c d e f g 那個pitch	7-1
啊也就是這個	7-1
那麼這個是講voiced 的時候	7-1
那麼如果是輕音會怎樣呢	7-1
如果發輕音的話就是我進去的時候根本就沒有這麼	7-1
譬如說我發我發ㄚ你可以感覺到我這邊是麻麻的我確實是有一個頻率在震動	7-1
可是我發ㄒ	7-1
你發現我跟本就沒有動	7-1
換句話說它這個門根本是打開的	7-1
這個根本是打開的於是呢它就是一堆一堆氣在這邊通過	7-1
不像剛才是會關起來所以會一那樣子一個一個會關	7-1
現在沒有它就整串整串在那在這裡了	7-1
那這就變成u of t	7-1
那因此你如果取sample 還是一樣	7-1
它就是這樣一堆非常random 的在這裡	7-1
那就是u of n	7-1
在那個情形之下	7-1
當然經過了這個之後我這邊也變了	7-1
但是它還是這副長相	7-1
你會得到一堆這種東西	7-1
那這個就是我的x t	7-1
那你也可以取得一堆sample	7-1
不過就是長這樣子	7-1
那這個呢就是清音	7-1
那麼因此哪哪裡是濁音哪裡是清音你是很容易判斷的	7-1
那麼那你注意到我這邊現在畫的時候其實應該是這樣	7-1
就是中間會斷掉	7-1
斷掉的地方就是這個可能是中間可能是輕音	7-1
因為凡是如果碰到輕音的話	7-1
你沒有週期嘛	7-1
所以你求不出f zero 出來	7-1
所以你部分就是輕音這部分就是輕音	7-1
那你就就沒有f zero	7-1
那這個是濁音跟清音的情形	7-1
跟這個相關的另外一件事情我們順便說了就是	7-1
這個time 跟frequency	7-1
那麼這個我們修這門課的同學有不同的background 有的人可能非常熟悉有的人不熟悉所以我們簡單的解釋一下	7-1
那麼通常我們講一個signal	7-1
有兩種說法	7-1
一種是用x of t	7-1
或者它的x of n 來說	7-1
這就是time domain	7-1
或者是x n 變成它的sample	7-1
這是在time domain 上來描述這個signal	7-1
但是呢我其實還有另一種方式的表示法是在frequency domain 上	7-1
那我得到另外一堆	7-1
這個呢我們簡單的寫也許寫成x omega	7-1
這是我在frequency 上面的distribution	7-1
這是什麼意思呢	7-1
在這上面的任何一點譬如說這個是omega one 的話	7-1
那它所代表的是一個一的j omega t 的這樣子的一個訊號	7-1
那這是什麼呢你知道如果取實部的話	7-1
它不過就是一個cosine	7-1
我如果取它的實部	7-1
就是cosine 的omega t	7-1
這個是omega one 這是一的j omega one t	7-1
這個就是cosine omega one	7-1
所以基本它就是一個cosine	7-1
那那這上面的這個值	7-1
x 的omega one	7-1
它是一個虛是一個負數	7-1
它有一個amplitude 跟一個phase	7-1
所以呢如果這上面有一個a one e 的j phi one 這麼一個負數的值在這上面的話	7-1
意思是說	7-1
這個e 的j omega one t 這個東西它有一個這個coefficient	7-1
那麼它所代表的意思呢是	7-1
等於說我真正的訊號是這個	7-1
a one e 的j phi one	7-1
這個東西乘上e 的j omega one t	7-1
是	7-1
就是這個訊號e 的j omega one t 的這個component 它的大小是這個	7-1
所以應該是這樣	7-1
這兩個乘起來	7-1
那這個到底是什麼你如果要比較有感覺的話我就取實部就是了	7-1
那就是a one cosine 的omega one t 加上phi one	7-1
因此呢這個amplitude 跟這個phase 所代表的是這個amplitude 跟這個phase	7-1
那這個omega one 代表的是這個cosine	7-1
那你就就知道它所代表的意思其實是一個就是一個cosine	7-1
它的頻率是omega one	7-1
a one 代表它有多大	7-1
而phi one 代表零在那裡	7-1
它的前後的位置是phi one	7-1
那麼以此類推	7-1
我現在在另外一個地方有另外一點是omega two	7-1
那這上面的那一點呢是	7-1
它是a two e 的j phi two	7-1
那它也代表另外一個訊號是a two e 的j phi two	7-1
然後那個是那個東西	7-1
另外一個cosine	7-1
是另外一個cosine	7-1
那它的大小是a two	7-1
它相對零的位置是phi two 等等	7-1
那麼當我把這個訊號用這個方式來呈現的意思是說	7-1
我這裡的每一個都有這樣都代表了一個那樣子的cosine	7-1
那它們加起來就是這個	7-1
ok	7-1
所以呢我現在如果把這上面的每一個任何一個都代表一個這樣子的東西	7-1
然後我把這些東西全部加起來的話	7-1
就相當於那個	7-1
所以這個跟這個是同樣的同一個訊號的另外一個表示法	7-1
不過這個表示法是表示在frequency 上面	7-1
所以這個是我們所謂的frequency domain	7-1
因此呢我們是任何一個訊號同時有兩種表示法	7-1
是在time domain 上還是frequency domain 上	7-1
那你如果是用這個來呈現用這個discrete time 的n 來呈現取sample 的話其實也可以	7-1
那這邊也不過就是這個cosine 也是取sample 的cosine 嘛	7-1
不過就是這樣取sample 的cosine	7-1
這個cosine 也是另外一個取sample 的cosine	7-1
其他並無不同	7-1
因此呢這裡面我們說它是e 的j omega t	7-1
就會變成它是e 的j omega one n	7-1
就變成n 就是了	7-1
一樣的	7-1
那到時候就是這邊變成這個變t 變成n	7-1
這個t 變成n 就是了	7-1
那就變成這樣子	7-1
那就是我的取sample 的東西	7-1
取sample 的東西也一樣可以用這個來來呈現	7-1
那麼這兩者之間的關係	7-1
就是所謂的fourier transform	7-1
那如果我的不是用t 而是取sample 的話呢	7-1
就是加一個discrete	7-1
就是discrete 的fourier transform	7-1
那這個的演算法直接可以把這個算成這個的	7-1
那就是所謂的fft fast fourier transform	7-1
我有一個快速演算法可以做這個東西的關係	7-1
好如果了解這個東西的話呢我們其實是為了講我們剛才看的這些聲音會怎樣	7-1
不管你是voice 還是unvoiced	7-1
你如果去做這樣的一段聲音	7-1
或者是剛才的剛才的voiced	7-1
這裡有pitch	7-1
不管是這個還是這個	7-1
你都可以去做像剛才那邊所說的那種fourier transform	7-1
把它轉到frequency domain 去看的話	7-1
你得到的會是一堆	7-1
一堆這種東西	7-1
它毛毛的	7-1
哦	7-1
一堆混亂的毛在上面	7-1
但是你可以看到它是有清楚的這個這個peak 的形狀	7-1
這些個peak	7-1
這個上面可能非常亂	7-1
但是呢你可以看到很清楚的	7-1
在某一些frequency 它會高起來	7-1
這些frequency 呢就是我們所說的f one f two f 三f f 四	7-1
也就是你的聲音裡面的這些個頻率是最大的	7-1
因為這些個這些個頻率上面的的這個	7-1
啊	7-1
那些的頻率上面的內化的就是這個東西嘛	7-1
它上面這些cosine 它的是最大的	7-1
那這些東西是什麼呢	7-1
是我們所謂的formant frequency	7-1
那通常我們最最重最常會看就是這四個formant frequency	7-1
就是這四個最重要的formant 就是這四個peak 的位置	7-1
為什麼這個會這麼重要呢	7-1
我們舉一個很簡單的例子	7-1
這是語言學家早在幾十年前就發現的	7-1
譬如說你如果以f one 跟f two 來畫圖的話	7-1
拿這兩個frequency 來畫圖的話呢	7-1
你會發現	7-1
譬如說	7-1
ㄚ在這裡	7-1
ㄨ在這裡	7-1
e 在這裡	7-1
等等	7-1
我這是隨便亂畫的不過大概意思是這樣	7-1
就是說你不同的人唸ㄚ當然點不太一樣	7-1
你同一個人唸幾次也不太一樣	7-1
不過基本上大概在這一堆	7-1
如果ㄨ大概在這一堆e 大概在那一堆	7-1
也就是說你只要根據前兩個frequency 大概就可以分出來	7-1
它是哪一個母音啊等等	7-1
那這個說明	7-1
這些個	7-1
這些個這麼複雜的frequency 的變化裡面	7-1
這些個peak 的位置是非常重要的	7-1
這個聲音是什麼聲音的特性	7-1
這是所謂的formant frequency	7-1
好那如果有了這個的話	7-1
那我現在如果把剛才的一段聲音拿來看	7-1
一個很很長的一段聲音拿來看怎麼辦呢	7-1
我可以這樣子	7-1
就是我取裡面的一段去做一次fourier transform	7-1
得到一個這個	7-1
然後呢我待會再取這一段再做又得到一個	7-1
再取這一段又得到一個	7-1
那你可以看到我這個聲音不斷地隨著時間變化我的音在改變嘛	7-1
我音在改變我的frequency 也在改變	7-1
sil	7-1
你可以得到每一段每一段每一段它的frequency 上面的變化	7-1
那麼我可以把它拿來畫一個圖	7-1
這個橫軸是時間	7-1
縱軸是frequency	7-1
我第一個得到的這一個圖	7-1
把它用畫在這個上面	7-1
我用這個不同的顏色或者是黑白的強弱來呈現這個圖	7-1
第二個得到的我也畫在這上面	7-1
那我畫成一條	7-1
我也是用顏色或者黑白來呈現它	7-1
等等	7-1
那我這樣一條一條畫出來的話呢	7-1
我最後會得到一片	7-1
那這一片就說明說	7-1
當我在講這句話的時候	7-1
這個聲音一路變過來	7-1
它的frequency 其實一路在變化的情形	7-1
那或者說就是你在這上面隨便拿一條線來看的話呢	7-1
這一條線畫下來是一個這個東西	7-1
那你在這裡可以看到它的peak 在哪裡哦等等	7-1
那這個圖都是所謂的spectrum gram	7-1
那我們通常是	7-1
這個spectrum gram 是最清楚的看到time 跟frequency 這兩個dimension 上面聲音變化的聲音變化的情形	7-1
就是所謂的spectrum gram	7-1
好	7-1
那這樣我們可以看兩張圖	7-1
像這個就是我們剛才看的一個一段voice 的母音	7-1
這是voice 的母音你可以看到它有清楚的週期性從這裡到這裡到這裡	7-1
那這個voice 的母音如果我轉成frequent domain 就是我畫成這個這就是我那邊畫的那個東西啦	7-1
那你看到就是我這邊講的這個毛啦	7-1
上面一堆毛毛的	7-1
但除了毛毛之外你是可以看到這邊是有一個peak	7-1
這裡面這這這個pick 的位置就是f one	7-1
那麼然後這邊呢	7-1
應該是有一個f two 這裡有一個f 三這裡有一個f 四	7-1
sil	7-1
那這幾個就是我們這邊講的這個這四個formant frequency	7-1
等等	7-1
那你如果把這些東西	7-1
如果我把它整個的畫起來的話呢	7-1
就變成像	7-1
以我們剛才的那張那句話為例	7-1
這我們剛才講的那句should we trace	7-1
我們剛才畫得很長總共有好多段	7-1
但是我現在把它把它這個time scale 收得很緊就兜在一起了	7-1
sil	7-1
那你這時候看到的就是	7-1
這是should we trace	7-1
這是should we trace	7-1
啊	7-1
這是同樣這句話	7-1
那我把它兜得收得很緊的關係所以你其實看到的這裡從這裡到這裡就是一個週期	7-1
就是從這裡到這裡因為它畫得很緊的關係變成兩兩個尖的地方就兩條線	7-1
就是一個週期	7-1
sil	7-1
那這就是它的它的這邊就是voice 週期情形這是這是would 音這是e 的音這是a 的音	7-1
那這是sh 這是這個是s 等等	7-1
那你現在如果把這裡面的每一小段每一小段分別去做這個fourier transform	7-1
得到這個東西之後	7-1
畫成這個圖	7-1
就是上面這個	7-1
那你可以看到很清楚	7-1
這個sh	7-1
它的spectrum 是在這裡	7-1
對不對它的能量大概集中在這個地方	7-1
到了變成would 的時候變成是這一塊	7-1
然後呢這個we	7-1
we 本身是一個在變化中的聲音	7-1
所以你看到它的這個在變	7-1
然後這個是sh	7-1
那這個是a	7-1
然後這個是s	7-1
ok	7-1
那也許a 這裡看得最清楚	7-1
這就是f one f two f 三f 四	7-1
就是說它的這四個peak	7-1
這四個peak 是在隨著時間在變動	7-1
啊	7-1
f one f two f 三f 四這是看得最清楚的	7-1
那這個圖就是所謂的spectrum gram	7-1
那麼因此呢我們就是這個這個在早年的時候是很多人很仔細的研究這個東西之後	7-1
那麼發現事實上可以看這個圖就可以看出它在說什麼話	7-1
那這個學問就做spectrum gram reading	7-1
那有一個最厲害的人他就是只要看這個圖就知道你在講什麼	7-1
那這個	7-1
他只要看說這邊長怎樣這邊長怎樣就知道這是什麼音	7-1
這邊長怎樣這邊就知道什麼音	7-1
所以這樣他就知道你在講什麼話	7-1
這是所謂的spectrum gram reading	7-1
那我這裡有一張圖在講這個spectrum gram 可以更清楚一點的就是這一張圖	7-1
這一句話是he will allow a real lie	7-1
就是每一個這裡的每一個小段的音它都切出來	7-1
這是he 這是h 跟e will allow a real lie	7-1
那這個是它的time domain 的waveform	7-1
這也是收得很緊很緊所以你看起來不太像了哦	7-1
那這其實這兩條線的中間就是一個週期	7-1
收得很緊很緊	7-1
那在這個裡面呢你可以看到的	7-1
相對於這個的spectrum 就是下面這一段	7-1
的相對於這個就是上面這一段	7-1
這樣子	7-1
那上面這個呢就是它的spectrum gram	7-1
sil	7-1
所以呢也就是你把time domain 的waveform	7-1
就是那個time domain 的waveform	7-1
相對於它的frequent domain 的這個spectrum	7-1
兜起來就是這樣	7-1
那這四條白線就是f one f two f 三f 四	7-1
它的這四個formant frequency	7-1
那麼隨著時間在變化	7-1
那麼不同的音它就是不一樣了哦	7-1
就是我們講的其實你你你只你只要看到這四個f 在哪裡幾乎就己經發現它們不同的聲音就是在這邊不同就是了	7-1
sil	7-1
大概是這樣	7-1
ok	7-1
那這個呢	7-1
大概我們今天就是把這些圖都看過了之後	7-1
那你如果再去看我們剛才講的其實我們七點零最前面的就是講這些東西	7-1
啊ok 好我們今天就上到這	7-1
ok 喔我們上週在說的我們上週用view graph 有解釋一下用over head 我們有解釋一下這些東西嗯那基本上呢這個喔我們的語音訊號可以初分成為兩大類就是voice 跟unvoiced	7-1
那這個voice 跟unvoiced 我們說過呢其實你是可以根據他的這個的這個發生的狀況來分的	7-1
那麼如果說這個是進入聲從聲帶進入口腔內氣流叫做u 的t 那麼這個出來的叫做x 的t	7-1
或者呢是u 的n 出來叫做x 的n 的話呢那麼所謂的voice	7-1
我們說是因為這裡的聲帶這個地方他會他會震動他會一動一動的一開一開	7-1
所以呢你會看到他有有週期性的這樣一個一個每一次震動就是這樣動一下	7-1
那這個週期就是所謂的pitch	7-1
那這個的情形其實就反應在這邊那麼你每一每一鼓氣出來他就有一個每一鼓氣出來就有一個每一鼓氣出來他就有一個那麼因此在這邊也是週期性的	7-1
那這個就是我們在語音訊號上所看到的週期這個也就是pitch	7-1
那這個pitch 的倒數p 分之一就是我們所謂的f zero 也就是你聽到聲音的音高	7-1
那這樣的聲音就是所謂的voice	7-1
那麼另外還有一種就是所謂的unvoiced unvoiced 的話呢他進他進來的時候呢就會變成沒有這樣子因為聲帶根本就沒有關起來	7-1
就一鼓氣不斷的在裡面跑你就會變成一種這樣子的	7-1
那這鼓氣到底長怎樣呢就像是white noise 差不多那麼我們不太容易找出它有它有什麼什麼特徵它就是一片混亂	7-1
那這個時候呢通過了口腔之後我仍然是一片混亂	7-1
那麼只是說不一樣因為你仍然是因為口腔上面的不同這裡的不同而造成不同的聲音	7-1
那這就是x n	7-1
那麼一個很簡單的 可以了解就是說其實我們在說什麼音的時候這邊的影響不大這邊只是這樣的區別而已	7-1
真正差別在這裡	7-1
那麼或者說就是在這裡	7-1
換句話說呢你發的是嗚還是一還是哦還是阿是你的口腔的口型的不同而造成的	7-1
那麼並不是這邊造成的	7-1
嗚還是阿還是一都是都是這樣進去的	7-1
在這個地方而言並沒有不同不同在這裡那換句話說不同是在這裡	7-1
那麼也因此呢你烏還是阿還是一的不同是這一個週期裡面他會長的不一樣	7-1
它是會這樣還是會會另外一個這樣還是會這樣是這個問題而不是 它一個週期裡面會長的怎樣	7-1
是造成它是烏還是阿那是這裡面所造成的不是這裡面所造成的	7-1
那麼同理呢在這裡也是你是需還是輔還是斯是因為你的嘴型不同所造成的而不是這裡不同	7-1
這裡反正就一團氣就可以了	7-1
那麼是是這裡的不同造成他的不同	7-1
那麼在這樣的情形之下呢那我們還還說到另外一點就是你其實很容易判斷它是voice 還unvoiced	7-1
你只要用你的手指頭摸到你的喉嚨這邊你就可以感覺到你發音的時候有在麻的有在震動就是就是voice 沒有在震動就是unvoiced	7-1
因此你有的時候你也可以whisper 的話	7-1
你就是我就這樣子講話	7-1
你就會發現它都它都它都沒有在震動	7-1
所以這個時候你發的它全部都是unvoiced ok	7-1
那麼你如果如果從這點了解的話	7-1
那麼我們就可以想像真正的差別是在是在這個地方	7-1
那麼這個地方呢我們就可以把他用一個比較更直接的model 來model 它那麼我們就把它可以畫成一個這樣	7-1
這是engineer喜歡做的方法	7-1
就是把它想像成是一個這樣的東西	7-1
那這邊是出來的x n 這邊是進去的u n	7-1
那你如果這樣子看的話呢它就變成一個這個這個這個io 的關係	7-1
這是input 這是input 這是output	7-1
那麼這個就是我們所謂的vocal tract model	7-1
那麼等於是用一個model 來model 它	7-1
進去是u n 出來是y n	7-1
那麼那這個所謂的vocal tract	7-1
這個東西呢就是我們vocal tract 所謂的聲道就是指這一段從這裡到這裡這就是我們所謂的聲道	7-1
那麼事實上我們剛才講的意思是說你今天你講的是什麼聲音的差別是這裡的差別也就是這裡的差別	7-1
那前面的差別其實不大	7-1
前面只是一些是這樣還是這樣的區別而已	7-1
那麼我們可以做簡單的實驗	7-1
就是你如果有一個東西你如果讓它那麼後來人家就用這個來做model 嘛那就就這個圖	7-1
就就這個圖就這個model 所謂的source model 就是那既然這樣的話那我乾脆就這邊其實也弄一個generator	7-1
讓它產生這個東西	7-1
然後呢那這個那於於是我這個generator 等於是等於是在模擬後面這邊讓它generate 這種東西 然後呢你變成這個	7-1
當你這樣做的時後呢我們其實是可以人家做過很多實驗就是我同樣的這個東西我只要這邊改成阿還是嗚的口形的model 的話出來就是阿還是嗚並不受前面的影響	7-1
同樣你只要那一堆random noise 跑出來之後你前面是ㄕ還是ㄙ你也是在這邊影響就夠了那出來就是ㄕ還是ㄙ	7-1
那倒不受前面影響	7-1
所以前面其實很簡單只有這兩種	7-1
然後呢只有這個東西你要你要看妳的pitch 是多少決定你的音高	7-1
我們說過這個東西這個如果是週期越變越大的話表示它的它的f 零在變低表示說它是音調在向下掉等等那是在這裡	7-1
那這就是這邊就是這就是我們這邊所講的這個voice 跟unvoiced 然後呢這個pitch 跟音高跟聲道	7-1
那麼我們上週還說的另外一件事情就是frequency domain 的distribution	7-1
那這點我們這個已經提過	7-1
不過很簡單的repeat 一下	7-1
就是說呢你任何一個聲任何一個訊號都一樣	7-1
你除了在time domain 有它的這個是這是這這個是在在time domain有它的這個呈現的方式以外	7-1
那當然你也可以把它變成discrete 變成n 永遠是可以的	7-1
你只要在這邊取sample 就變成n了	7-1
那麼我們永遠有另外一種呈現的方式是在frequency domain 上	7-1
那麼這個東西的話呢你可以想像它是一堆frequency 所兜起來的	7-1
那麼這個上面的某一點譬如說是omega one 那這上面有一個值	7-1
這個值是一個complex variable	7-1
它是一個複數的值	7-1
那麼我們如果寫成a 一的j fi 的話	7-1
那麼它所代表的其實是一個e 的j omega one t	7-1
這個值代表的是一個這個東西	7-1
那它的大小是多少大小是這個就是a e 的j fi	7-1
那這個究竟是什麼呢	7-1
妳如果覺得不太容易有感覺的話我們就取它的實部那就是一個cosine	7-1
那你就了解他其實只是一個cosine而已	7-1
那麼這個a 代表的是它的大小它的amplitude	7-1
而這個phase 所代表的是它的時間的零在什麼地方	7-1
那麼這個phase你可以想像它可以在有零到二pi 裡面的變化	7-1
我們可以看這個cosine 是從真正的位子是在哪裡等等	7-1
那這樣的意思等於是說我現在把這個signal 我把它拆成一大堆的cosine	7-1
每一個omega 上面都有一個值都有一個cosine	7-1
那麼他們就是有不同的frequency 不同的phase 不同的amplitude	7-1
那他們這些東西加起來應該是構成他的	7-1
那這兩者之間的關係是所謂的fourier transform	7-1
那麼當我們是用discrete 來做的時候變成x n 時候也可以做	7-1
那這時候這邊也可以變成discrete 的一個一個	7-1
那這個如果這個也變成discrete 的一個一個的話我們通常這邊就變成k 軸	7-1
那也變成discrete	7-1
那這兩者之間的關係呢就是所謂的discrete fourier transform	7-1
也就是所謂的d f t	7-1
那麼因此呢我們所有的signal 我們都可以從time domain 來看也可以從frequency domain 來看	7-1
那如果是這樣子來看的話呢	7-1
你可以發現真正的我們拿一個語音訊號	7-1
不管是這種還是這種拿來這個作一個fourier transform 的話	7-1
你看到的都會是像一種某一種樣子	7-1
那麼基本的架構是這樣	7-1
那上面長很多毛就非常凌亂的毛長在上面高高低低的	7-1
但是呢你可以看到她有一個基本的這個這個結構	7-1
那這個基本結構裡面很重要的東西就是所謂的peak	7-1
這裡面幾個peak 的位子	7-1
這就是所謂的f one f two f 三	7-1
這是它的frequency domain	7-1
那不管是這種voice 還是這種unvoiced	7-1
你做transform 都會得到這種東西都會看到她的f one f two f 三這種東西	7-1
那這種東西呢其實是那麼根據他們的分析呢這個其實這個基本的結構我們說如果她有一個基本的結構是這樣的話	7-1
這個基本的結構其實是由這一塊來決定的	7-1
也就是你的口形長怎樣	7-1
它或者說妳的口形長怎樣或者說它是巫還是一還是阿其實是這些東西決定的	7-1
那這些東西其實就是我們所謂的formant frequency	7-1
或者formant structure 這些f one f two f 三就是我們所謂的formant frequency	7-1
那麼至於說上面那些毛	7-1
它高高低低的那堆很凌亂的毛	7-1
那其實就是由前面這些東西所造成的	7-1
那麼如果分析的話是可以發現你前面這些東西的的她的特性它的frequency	7-1
妳如果拿這個也去做frequency 的話呢	7-1
那它幾乎就是上面那堆毛的變化	7-1
就是在前面這些東西上面	7-1
而它的真正的這些高低呢	7-1
是是由後面的這一堆就是這個這塊聲帶或者說是vocal tract 所造成的	7-1
那麼在這樣情形之下呢我們所謂的formant structure 或者formant frequency 就是指這裡面的幾個peak	7-1
那麼我們說呢這幾個peak 其實是區別聲音裡面非常關鍵的部分	7-1
早在很幾十年以前當時的語音學家就已經發現	7-1
譬如說前面這兩個frequency f one 跟f two 是非常重要的	7-1
那那麼你如果拿f one 跟f two 來做圖的話	7-1
你會發現某一堆音就在這裡譬如說阿在這裡巫在這裡一在這裡或者怎樣	7-1
那基本上呢你不管什麼人發的阿基本上都在這一堆怎樣發的一都在那一堆你	7-1
可以根據這個f one 跟f two 就可以把他們區隔出來	7-1
那麼類似情形發生在各種聲音上都是這樣	7-1
那麼因此呢這些的peak 遠比其他的部分來的重要	7-1
也就是說你到底是f one 跟f two 的值f 三的這些值是多少	7-1
這個所帶的information 遠比這邊到底多大多小或者這邊這邊等等的information 要重要的多那這些東西就是所謂的formant frequency	7-1
好那麼那我們上週其實還有一件事情	7-1
就是說那你真正講一句話的時候是怎樣	7-1
呢我們真的說一句話是一大堆phone 接起來的	7-1
你說今天天氣很好前面是基後來是一這邊是因然後ㄊ一ㄣ	7-1
你中間的聲音不斷的是不同的phone 串起來的	7-1
那每一個phone 它有它自己的特性	7-1
所以呢你很可能這一段是voice 所以它有它的週期	7-1
然後這邊變成是unvoiced 它就會變成一堆unvoiced	7-1
然後呢這個之後呢又有另外一個voice 是聲音出來它於是就會變成是另外一個然後之後他可能又接到一堆unvoiced 她又變成一堆unvoiced	7-1
那我們上週有看過就是你真正的聲音就是長這樣的所以這是某一個phone 這是某一個phone 這是某一個phone 這是某一個phone 它從一路這樣接過去的	7-1
那這個是我們聲音裡面本來就是不斷地在變化	7-1
譬如說你可以想像這裡面這前面可能還是一個一個unvoiced	7-1
譬如說這是需這是巫這個是斯這個是阿這個是夫等等之類的	7-1
那麼你每一段其實可以看的出來那麼這一段是unvoiced 這段是voice 這段是unvoiced 這段是unvoiced 等等	7-1
那這是不同的聲音這樣串起來	7-1
所以呢那這個時候呢就會有喔你如果直接去做這個這個frequency domain 的話呢	7-1
那麼你如果把它整個的全部來一起來做這個東西會變成沒有意義	7-1
我們必須一小段一小段來看	7-1
那這個話的意思是說呢	7-1
在你如果去看一般的課本裡面講的這個這個喔嗯fourier transform 的話呢	7-1
它的數學式子應該是要它的數學式子應該是要	7-1
譬如說x of p e 的minus j omega t d t 積分積負無限大到正無限大	7-1
這是一般數學課本裡面的fourier transform	7-1
或者說是discrete 的s n e 的minus j omega n summation n 等於負無限大或者正無限大	7-1
那這是一般的課本裡面講的fourier transform	7-1
你如果把它你真的要算這個跟這個照說好像應該要這樣子算	7-1
可是你如果這樣子算的話就把它全部通通都加起來的話就把它從頭算到尾	7-1
你就會把你所有的聲音全部混在一起	7-1
把不同的母音不同的子音全部搞在一堆	7-1
那這樣其實是是就把你的所有聲音都混在一起了你其實看不出來真正的真正的變化	7-1
所以我們怎麼做的就是我們上週在下課的時候提到這個spectrum gram 那它不是這樣做它是每一次只取一段	7-1
譬如說我這一次取這個window 那在這裡面去做於是因此我這個summation 或者積分不是積整個只是積一小段	7-1
當我只是積一小段的時候呢這個時候才能夠清楚的呈現巫的這個音它裡面是怎樣的	7-1
那麼待會我再shift 一個window 慢慢看到由巫變成司那麼我這個window 慢慢shift 我可以看到這個spectrum 慢慢變化的情形	7-1
那我唯有這樣子在用一個很短的window 不斷的在每一小段去看她的spectrum 的變化	7-1
我才可以看到她每一個音ok 那個巫長的是這樣等一下變成阿的時候會長另外一個樣子那變成需又長另外一個樣子	7-1
你那樣才看的到一定要是在一個一個短的window 裡面才看得到的	7-1
那麼因為這樣子的關係所以呢我們真正在做這個的時候	7-1
我們不是用一般的課本裡面這樣子從頭加到尾或者從頭積到尾而是一個一個小的window 不斷的這樣shift 過來	7-1
那就是我們上週下課所看到所謂的spectrum gram	7-1
也就是說你看到的辨識變成是像一個像一個畫的很小就好了你每每一段看到的是一些嗯就像這種東西	7-1
那我的縱軸是frequency 或者omega 橫軸是time	7-1
那我等於是說我這邊有的時候寫f 有的時候寫omega 是因為不同的課本裡面有得有的喜歡用f 作單位有的喜歡用omega	7-1
那你知道他們的關係是omega 等於two pi f	7-1
那麼depends on 你要用omega 還是用f 其實都可以中間的關係差一個角度的關係	7-1
那麼這樣子的圖就是所謂的這個spectrum gram	7-1
也就是說你其實是在拿這每一小段每一小段分別去做每一小段每一小段分別去做這個fourier transform 的時候	7-1
你分別可以得到一個一個這樣子的代表那個音是巫還是由巫變成司還是什麼的時候那麼他們的spectrum	7-1
那麼因此呢你這裡每每一行其實就是一個那個的它的用顏色或者黑白高低來代表這個這邊的高低大小	7-1
那這樣的話你就可以的到那這樣子一塊一塊一片的聲音的變化	7-1
那麼這個時候呢這裡面的peak 你也可以發現這可能這就是f one 這就是f two 那麼他們隨著時間變化的情形那這樣子就是所謂的spectrum gram	7-1
好那這些東西大概是我們上週下課前在講到的東西	7-1
那麼有了這樣子之後呢那麼喔	7-1
一般而言我們最常看到的是這件事情就是所謂的這個source model	7-1
那這個model 最常用在所謂的low bit rate speech coding	7-1
那這塊呢其實就是今天各位打你的手機的電話的時候妳的聲音其實是經過一個這樣子的處理變成一個low bit rate speech coding	7-1
那這一塊我們平常在我們這門課裡面沒有不太多說所以我們就在這簡單的說一下	7-1
那麼換句話說呢你如果把這個我們發聲的這整個的講話的這一部分看成是一個像這樣子的model 的話	7-1
我們就可以想像成有一個model 來描述我們的口型的變化這就是所謂的vocal tract model	7-1
然後前面有一個generator generate 這個u n 那就是所謂的adaptator的generator	7-1
那這樣子的話呢妳u n 經過x n 出來x 就是我的聲音	7-1
那如果是這樣子的話呢那麼喔我就可以用這個model 裡面用一些參數來描述它	7-1
前面這個generator 參數是比較容易想像的	7-1
那麼照我們這邊講法其實就是兩件事情第一個你告訴它是voice 還是unvoiced	7-1
那麼如果是voice 的話你還要告訴他我的pitch 是多少	7-1
那你只要告訴他這兩件事情大概它就可以產生這個u n 就可以了	7-1
那麼vocal tract 的話呢這個比較複雜因為這個depend on 那你那個口型長怎麼樣	7-1
所以呢可能需要比較多的參數去描述它	7-1
那麼也許是要主要是要描述你其實真正的那個唇齒舌之間的各種關係	7-1
那如果把這點講清楚的話呢那你這個model 就可以出來了	7-1
那麼在你如果修過譬如說電機系的訊號與系統或者是資訊系的d c c 的話	7-1
你就知道那這個時候這這中間的一個簡單的描述方式是我這個中間這個vocal tract model 我可以用一個數學式子g n 來描述它	7-1
那如果是這樣子的話呢這樣它們的關係其實是x n 呢是u n 跟g n 的一個數學關係所謂的convolution	7-1
那如果你沒有修過並不了解這點其實也無所謂你只要知道這中間是有這麼一種數學關係是可以描述的	7-1
那麼我們就是用一個g n 的function 來描述這個vocal tract 然後他們的關係其實就某一種數學關係是所謂的convolution	7-1
那如果是這樣的話那其實也可以再進一步就是你這個x n 我可以變成它的frequency domain 描描述的方法就是x n	7-1
x 的omega 那這個東西就是相當於我們這邊講的這個我的這個x n 我可以用這個的x 的omega 來描述它	7-1
那x 就是這個東西那麼我如果那個x n 可以用這個x omega 來描述	7-1
同理u n 也可以用這個u 的omega 來描述	7-1
在這個情形之下他們之間的關係變成一個乘法其實這個g n 呢也可以用一個g 的omega 來描述那麼於是呢我就得到一個乘法的關係	7-1
那這也一樣如果你唸過這些東西你就知道那麼如果沒有唸過不知道也無所謂你只是了解說這中間其實都可以用frequency domain 的關係來描述的那我中間也是一個數學關係是g 的omega	7-1
那這個東西的transform 就是它的fourier transform就是它	7-1
那他們他們中間關係是一個乘法	7-1
你如果唸過z 的的表示法的話知道他們之前其實還有一個z 的表示法那我想這個比較喔知道就好不知道也無所謂	7-1
那這樣的model 有什麼好處一個最大的好處是用在所謂的我們剛才講的low bit rate speech coding	7-1
也就是譬如說你今天打打你的手機電話的時候那你知道你講的每一句話其實都要轉成一堆零跟一才能夠傳送出去	7-1
那要轉成零跟一最直接的想法其實就是每一個sample 都是一個real number	7-1
那麼每一個real number 你都可以變成一堆binary representation 用一堆零跟一去描述每一個點那這樣的話呢它其實就可以變成一堆零跟一傳出去了	7-1
只是說如果這樣做的話你呢需要很多個點需要很多個bits	7-1
所以呢你每一秒鐘要非常多的bits per second 這就是所謂的bit rate	7-1
 這個所謂的bit rate 就是bits per per second 你每一秒鐘的語音	7-1
每一秒鐘的語音到底要多少個bits 來傳送那麼很可能會需要非常多的bits 才能夠傳送一秒鐘的聲音	7-1
那這個時候呢當你打你的手機的時候呢你就知道你的手機的frequency 是非常珍貴的	7-1
它能夠送你你如果唸通訊語言你就知道它能夠送bits per second 是非常珍貴的	7-1
所以我們希望能夠用最低的bit rate 來傳送就是所謂的low bit rate	7-1
那如何做到這件事呢那麼很直接的想法是你如果這樣子來看一個一個sample 分別當成一個real number	7-1
然後用多少個bit 來送出去的話呢你是把它當成一個arbitrary 的reform 當成它是一個arbitrary 我根本不知道它有什麼特性	7-1
但是事實上如果是語音的話其實他不是arbitrary 因為我們都已經知道它已經長成這樣嗎它它不管怎樣它就是那個樣子它其實不是arbitrary	7-1
譬如說它不然就是長的這種unvoiced 的樣子不然就是長的那種週期性的它也就是就是一定一定的長相	7-1
的那其實意思是說我們的語音並不是一個arbitrary 的waveform 它是有一定的形狀的	7-1
因為它都是這樣子的model 它都是這種這種器官產生出來的所以呢它都是有一定的model 這個長相的	7-1
那麼或者你也可以說呢雖然都是因為它都是人的嘴巴說出來的雖然每個人嘴巴不一樣有的大有的小或者說每個人的舌頭不一樣有的大有的小不過事實上他們差別都不大所以出來的訊號都有一定的長相	7-1
那既然如此呢我們如果能夠做出一個好的model 來我只要用這些參數來描述它的舌頭嘴巴等等	7-1
用這些參數來描述它的聲帶等等的話那其實我做出來的聲音就是	7-1
那麼因此呢你真正的做法其實就是我變成說我只要我把這個model 變成一個程式放放在晶片裡面	7-1
那我其實當你要講一句話傳出去的時候我只要算我的那句話裡面它的這些參數各是什麼	7-1
當我在講這句話的時候我其實我在做聲音也是一個一個frame 或者一個一個window 算這個window 裡面我其實是voice 還是unvoiced 然後其實她的pitch 是多少	7-1
然後這個時候我的脣齒之間呢是一個怎麼樣的function 我如果把這個知道了的話我就把這些東西傳出去就夠了	7-1
所以我真正要digitize 然後要傳送的就是這些參數	7-1
那到了接收端的話呢妳接受到也就是這些參數但是因為這這這整個是一個程式寫在那個接收端那個手機的晶片裡面	7-1
所以呢它就只要把這些參數解出來去操去操作這兩個model 去一算算出來就得到你的聲音	7-1
因此呢其實你今天再用你的手機跟你的朋友打電話的時候你說的話進入的你手機做的第一件事情其實當然是取它的sample	7-1
當你變成這堆real number 之後不是他們直接把每一個real number 變成一堆bit 而是我們就會去判斷一個一個window 去算它這些個參數	7-1
當你算出這些參數之後我就直接把它把這些參數去digitize 然後傳送出去	7-1
那到了接受端呢他再把它解回來之後它有一個程式在裡面就把它算出來就得到你聽到的聲音	7-1
那如果是這樣的話呢最大的好處是我需要的參數很少而且變化很慢	7-1
也就是說在這麼一段這麼一個window 裡面的聲音其實我只要少數的參數	7-1
這邊所需要的參數並不那麼多那麼我就而且從這裡變成這裡搞不好沒有太大變化	7-1
如果我這一段都是同一個巫或者同一個司那可能變化並不太大所以它的參數也變化很慢那不像在這裡會變化的非常快	7-1
因此呢你原來在用如果每一個real number 分別用一個bit 來傳送的話我所需要的bit per second 是非常高的	7-1
可是變成這樣子之後我可以變的很低那這就是所謂的這個low bit rate speech coding	7-1
那麼那今天其實各位在打的妳的手機電話其實都是以這樣的一個原理在操做的	7-1
那這個是這個model 一個最重要的最重要的contribution	7-1
那麼對我們今天整個的這個我們的生活日常使用裡面用的最多的應該是這一塊	7-1
那這裡面到到我們這門課倒並不是以這個為重點所以我並沒有多說它你如果有興趣的話其實你去去找像low bit rate speech coding 這樣子的keyword 會找到非常多的文獻它都再講這些事情是怎麼做的等等	7-1
那我想我們這邊就大概把這些東西提到	7-2
好那有了這個了解之後	7-2
那麼底下這張圖其實就是在描述把剛才的這個model 說的再精確一點我們說這一塊叫做excitation generator 就是產生這個u n 的	7-2
這一塊呢所謂的vocal tract model 就是在model 我們的那個發那個聲音的那個口形的	7-2
那你如果仔細的畫的話呢那就是這邊的這兩塊那這個就是產生那個u n 的	7-2
那那個u n 呢我們說其實最簡單的解釋是其實他是unvoiced 還是voice	7-2
那麼因此呢你其實只要知道是我用一個bit 告訴他說現在這個window 是voice 還是unvoiced	7-2
如果是voice 我告訴他的pitch 是多少這樣就夠了	7-2
那你再多一個就是一個game 那麼這個amplitude 大小是多少	7-2
所以呢你只要這幾個參數那其實就可以產生我要的u n	7-2
那當然這個是早年最早的最粗的model 這樣子作	7-2
那今天你真的在打電話的時候不是用這個你用的是比這個精細很多的那這個只是我們從最基本的最基本的觀念來講的最最原始的最粗糙的model 是這樣子的不過這個觀念可能是最容易想像的	7-2
那麼口型這邊是比較複雜你如果真的要去描述脣齒之間的各種關係的話這東西是相當複雜的	7-2
不過後來人們發現其實可以簡化成為這個model	7-2
那這個model 這樣子寫用z 來寫可能有點頭大	7-2
那你如果不用z 來寫的話那麼喔其實那個是是一個非常簡單的數學式子	7-2
那其實你你如果是對於這個z 的表示法熟悉的人你一看就知道了	7-2
那你如果不熟不熟悉也沒有關係這個關係其實就是x n 減掉summation 的a k x 的n 減k k 等於一到p 等於u n 就是這樣一個數學式子	7-2
那那這樣一個數學式子用用z 來表示變成這樣子就是	7-2
那這個意思等於是說其實你只要把x n 的前面若干個點拿來然後跟u n 加起來就得到下一個點	7-2
阿所以呢你你其實你看這個這個這個這個加這個等於這個嘛	7-2
所以呢你其實只要給我前面這些個點	7-2
前面這些個點給我一個下一個u n 我就可以可以把下一個點加出來嘛所以呢其實是一個非常簡單的數學運算	7-2
那這樣的一個關係呢可以描述這個我們的這個u n 跟x n 之間的關係	7-2
而於是我現在的參數是什麼所謂的parameter 就是這堆a k	7-2
那這堆a k 是需要計算的是需要求的是沒有錯	7-2
所以呢你就是在給我這堆東西裡面呢我要從裡面算出這些a k 出來	7-2
也就是說你今天如果在講一段話的話我每一段聲音我通通都都經過一個演算法去求所謂的參數	7-2
什麼參數呢其實所謂的參數就是我們這邊講的這些包括它是voice unvoiced 它的pitch 它的a k 這些東西	7-2
那如果這些東西都算出來的話那我到了對方我只要把它東西把這些參數帶進去一跑跑出來就是我的聲音	7-2
那麼那這邊所講的其實就是我們剛才所說的因為這些東西其實就是我的vocal tract	7-2
所以它其實已經告訴我formal structure	7-2
也就是說我們說真正的formal structure 是就是像這個圖上面這個它在哪裡有formal structure 哪裡有peak 的這個長相就是所謂的formal structure	7-2
那這個formal structure 很很顯然是由這個決定的	7-2
也就是由這堆a k 決定	7-2
所以你你有這一堆a k 我其實是可以算的出來它的formal structure 是長怎樣	7-2
那這個事實上就是我們到底是阿還是巫還是一還是斯還是夫還是施那真正的關鍵就是這堆東西	7-2
那我們這個講的是一個非常粗的model 他只是一個good approximation 而已	7-2
當然我們今天用的是比它細的多不過呢這是一個最容易了解的一個圖所以我們用這個圖來講好	7-3
那底下這一頁其實大概沒有太多東西就剛才我們都已經說過了	7-3
那這邊只是在在畫一個也就跟我那邊剛剛畫的一樣的意思	7-3
那就是我們發聲的時候從氣氣是從這個肺在氣管過來	7-3
那這邊就開始就就有聲帶震動等等等等最後呢其實我們氣出來的東西一個可能是口腔一個可能是鼻腔	7-3
那麼因此呢我們如果真的要model 的話呢那麼這邊這是聲帶在這邊開關那麼然後你進來的時候我有一個口腔有一個鼻腔	7-3
那這兩個都有可能有氣出來	7-3
那在多數的時候我們發的聲音其實你如果感覺一下你就知道呢多數的時候你講話的時候聲音是從口腔出來這邊是沒有的絕大多數case	7-3
但是又有少數case 例外是反過來就是鼻音的時候	7-3
所謂的鼻音是我們這邊關起來沒有聲音	7-3
聲音是從這邊出來的那就變成鼻音	7-3
那你如果發一個嗯還是什麼你就會發現其實我的這邊嘴巴這邊是沒有氣的我已經關掉了那氣是從這邊出來的那就是鼻音	7-3
那底下這張圖是真正的去量出來的進入口進入這個氣管從口腔進從這個聲帶這邊進入口腔的氣流	7-3
我們剛才所畫的這個東西其實是一個比較誇張的畫法真正的是長的這樣的	7-3
就是說你這個就是我們講你你聲帶這個地方一開一關的妳震動那邊打開的時候他一一團氣它大概是長的像這個樣子的	7-3
就是我這邊畫的這個圖那真正的長相大概是這樣	7-3
那麼底下的這一張沒有什麼不同只是又重畫一次而已	7-3
所以呢妳如果看底下這個圖almost 就是我們剛才畫的上一頁的圖	7-3
這個我這邊進來的j 這個這個adaptation generator 可以有voice 跟unvoiced	7-3
那如果是voice 的話呢不過就是產生一堆這個這個嗯告訴他週期它就產生一堆那種這個週期性的signal 就是了	7-3
就像我們那邊所畫的這樣產生一堆這種東西就是了	7-3
那如果unvoiced 怎麼辦呢unvoiced 我們就就用一個random sequence generator 產生一堆random number 就好	7-3
因為我們說過這個地方其實你只要產生一堆generator 用那個random number generator 產產生一堆random number 就可以了	7-3
那這時候你只要這邊用的對你這邊用夫還是司還是雌這不同的口型你只要這個model 用的對出來就是你要的聲音	7-3
那倒並不care 這中間是什麼	7-3
那你也可以做這個很多實驗你說你把這一段跟這一段因為是random number generator 所以這一段跟這一段長得完全不一樣嘛可是你聽聽看他最後出來的你是是什麼音就是什麼音並不受到你前面是這個還是這個的影響	7-3
那麼換句話說你其實在unvoiced 時候你只要用一堆random number 就夠了	7-3
那麼其實進入口腔氣就是那樣的一堆random 的東西	7-3
那麼你是發的是什麼音是後面這塊的影響	7-3
因此呢我們在這裡只要作一個random number 的random sequence generator 就可以了	7-3
這也就是在我們上一頁的那個圖裡面也就是這樣子畫的	7-3
你看如果是unvoiced 的話就是random sequence generator	7-3
你只要產生一個random sequence 就好了	7-3
這個並不介意是哪一種random 都沒有什麼關係	7-3
那所以呢底下這一張圖almost 就是我們剛才的上面的這一張圖是完全一樣的了	7-4
那上面這一張圖是稍微複雜一點	7-4
那它把一些更多東東西呢弄進去	7-4
譬如說這邊多了一塊這個	7-4
這個是什麼呢就是當你聲帶在震動的時候其實那裡不是一個那麼單純的是稍微還有一點別的東西的	7-4
它用一個ma 用用一塊來model 那些東西那就是我們看到的這裡嗎它其實是這樣子的	7-4
那所以它還有一些有一些這個特別的長相跟變化等等	7-4
你如果要把那個也弄進去的話那就會變成這一塊	7-4
那還有一塊呢就是這個lip radiation	7-4
也就是說當你的聲音真的從嘴嘴巴出來的時候	7-4
你可以想像它在這個之前它是受到這邊脣齒各種的constrain 所以他在氣只能在這裡跑	7-4
你從嘴唇一出來它忽然變成沒有限制的一個free space 可以自由的完全的這個這個發散出去	7-4
所以這中間的變化其實是也是有另外一個嗯數學關係可以寫的	7-4
那就是底下的這一塊就是所謂的lip radiation filter	7-4
那我們現在只用一塊其實可以看成是一個combine	7-4
就是說把這三塊的效果這個才是vocal tract 這個是我們那個口型的	7-4
其實你要完整的講的話呢這個口型的在這裡前面有這個後面有這個	7-4
但是我們現在講的東西通常就是把這三個合成一個在這裡所以我們可以看的這麼簡單	7-4
其實是應該有這個效應這個效應跟這個效應合在一起的	7-4
也就是我們之前畫的這個這個數學式子或者是我這邊所寫的這個數學式子	7-4
那其實是已經把這三個效應畫在一起了那就只用這一個就只用這一個就可以了	7-4
那也就是這邊的這邊的意思那所謂的combine 就是把這三個合在一起了	7-4
好那以上我們大概是把這些聲音的這個基本的知識大概稍微說一下	7-4
那麼嗯因為我們喔要處理的東西是語音訊號我們必須要知道它長怎樣然後我們才知道我們後面為什麼要這麼做	7-4
那麼我們要了解那麼我們的聲音是這樣子一系列不同的phone 串起來的	7-4
每一個phone 它either 是voice 還是unvoiced 還是怎樣它都是不同的口型兜兜兜成的	7-4
所以呢它是不斷的在變化的	7-4
那麼在這個情形之下呢我們底下要做的事情必須要考慮到這些狀況我們才才知道為什麼我們是要這樣子作	7-4
那麼因此從這個以後我們現在底下要講的最主要的事情就是怎麼樣子做這個feature exaction 也就是其實就是我們所說的m f c c	7-5
所以在下來我們所說的其實就是要求m f c c	7-5
那麼我們在這裡會仔細的再講這裡面每一塊怎麼作	7-5
那這個我們在二點零已經說過一點	7-5
那麼他為什麼要這樣子作	7-5
那麼當我們有了在這裡講的這背景之後我們會比較了解為什麼m f c c 是這樣子做的	7-5
那我們先來說我們要抽抽這樣像m f c c 這種東西的話	7-5
那一個最主要的這個幾個考慮包括呢	7-5
第一個我們當然希望找一些參數他真的代表我們聲音裡面重要的部分	7-5
也就是說你如果光用眼睛去看的話	7-5
這些聲這個這東西變化萬千你實在無無法想像	7-5
那它們也就是說你如果直接從waveform上面看呢不容易看出來	7-5
我們如果用眼睛看是不容易判斷為什麼這一段是巫這一段是一那他們也就是說你如果直接從waveform上面可能不容易看出來他們的很多重要的東西在哪裡	7-5
那因此我們需要找一些feature	7-5
這些參數這我們這些參數我們希望它能夠真的描述聲音裡面一些重大的特性的一些參數這是所謂的feature	7-5
那這個到底是什麼呢其實我們不知道	7-5
那麼這是多少科學家一直在想而不容易找到答案的	7-5
那麼他們找比較容易想到的辦法是我們就去學人的耳朵是怎麼聽的	7-5
這是人的聽覺系統	7-5
那麼他們是怎麼聽的呢	7-5
那麼人就是這麼厲害	7-5
那麼不管誰說的阿我們一聽就知道它是阿不管誰說的是一我們就知道他是一我們根本不用看那個waveform也根本不要什麼我們一聽就知道了	7-5
人的耳朵為什麼這們厲害呢我們希望去學習人的耳朵是怎麼聽的	7-5
然後從那裡面找一個類似的model 來做這是基本m f c c 的想法其實是這樣來的	7-5
那麼第二個呢所謂robustness 就是我們希望他比較其實這個是很難做到我們只能說我們希望他比較robust	7-5
所謂比較robust 就是說因為我們的聲音是有很多外在環境的破壞的	7-5
包括雜訊的干擾包括你的聲音通過電話就是通過電話就經過channel 的傳送的變化	7-5
不同的人說的阿為什麼不同的人說的阿明明都不一樣為什麼我們聽起來都知道它是阿呢	7-5
然後這個transducer 譬如說是麥克風譬如說喇叭	7-5
那這一些個東西的每一每一支麥克風的都不一樣妳同樣的一個聲音經過兩支不同的麥克風出來的東西就是不一樣了	7-5
為什麼我們人聽還是一樣的聲音呢	7-5
那麼就是我們希望能夠僅可能比較robust 一點	7-5
那再一個呢就是dynamic 就我們的聲音裡面顯然有很多dynamic 的特性那我們希望能夠抓到	7-5
所以dynamic 是指說它是它是在變化的	7-5
因為我們不斷的我們不是只發一個阿不是只發一個音	7-5
我們是把很多很短的phone 把它串起來	7-5
所以我們聲音不斷在變的而且這個變一定是連續的	7-5
因為這是一種口型那是一種口型這是一種口型	7-5
那麼他們中間是連續變過來所以是一個time variant 隨著時間連續在變化的	7-5
那麼因此呢他們在時間上變化的情形應該也是一個重要的	7-5
所以這些是一個一些主要的考慮	7-6
那麼底下我們來看我們要說的m f c c	7-6
那這個圖其實是我們裡面的所有東西我們大概在二點零都已經說過了哦	7-6
那我們現在再重新先再來revisit 一次那麼這個時候我們再來想一下裡面為什麼是這樣	7-6
那麼這裡面呢我們說m f c c 是我們今天最普遍使用的	7-6
那它基本上呢公認是有一個合理的accuracy	7-6
我的計算量也相當的小是一個合理的正確率跟合理的計算量	7-6
那麼大概在不同的不同的application 不同的task 大概都不錯	7-6
那麼其實我們今天真正在實驗裡面在研究裡面用的是比這個多很多種啦	7-6
feature 本身是一個很重要的研究題材那有千千萬萬種不同的feature	7-6
不過m f c c 始終是少數這個最使用最普遍而效果算是大家公認不管你是辨識什麼	7-6
不管是阿拉伯文還是伊朗文不管是阿還是巫還是一不管是在這個電話的還是雜訊的還是不管是什麼基本上大概都還不錯	7-6
那麼它的這個整個的計算的過程這個我們大部分都已經說過喔	7-6
就是我譬如果我我我底下整個七點零在講的是這個所以我們稍微用看一下它的這個符號	7-6
假設我原始的聲音叫做x n 這就是我最開始的這個x n	7-6
我做的第一件事情是做pre emphasis 把它x n pron	7-6
變成x n pron 之後我開始取window	7-6
那這個window 就是我們這邊講的這一個一個	7-6
那這個window 我們可以以它的某一個時間點譬如說他的這點作為t 的話	7-6
那麼這是t 這是t 加一這是t 加二t 加三等等	7-6
這是t 加二這是t 加三如果這樣來看的話呢我就是以這個t 作為我的index	7-6
因此呢我經過window 之後我就得到一個x t 的n	7-6
那這個時候等於是說	7-6
以這個為例的話呢這是x t 的n 的話我其實就是得到這一段	7-6
所以你得到的是這一段外面就沒有了	7-6
那這樣子的話我就得到這個window 的這一段這就是x t 的n 然後這個時候我拿來做所謂的d f t	7-6
d f t 就是我們這邊講的discrete fourier transform	7-6
那麼做了這個d f t 之後我就把這個x t 的n 轉到這邊來	7-6
但是因為這是discrete 所以他得到的並不是一個連續的而是discrete	7-6
所以我的index 變成k 那我就得到這個這個index 為k 的這邊個這邊的一系列的點	7-6
那其實就是我的x t 的k	7-6
那麼這時候我開始在上面作所謂的mel filter bank	7-6
那你如記得的話是一系列的三角形的	7-6
我們二點零講過就一個一個三角形	7-6
那麼等於說我在每一個三角形取它所cover 的這一些點等等	7-6
那這樣子話呢我每我這個時候得到我就把它叫做y t 的m	7-6
這個t 還是這個t 就是這個window	7-6
我每一個window 得到一個嗎這個t 也在這裡嗎	7-6
所以我每一個window 得到一個這個這個就變成我們的這個x t 的k	7-6
我這個東西現在橫軸現在叫做k 因為現在變成discrete 的點變成一個個的sample 它也是sample 那我變成x t 的k	7-6
然後我在這上面取所以呢這個t 仍然代表我這個window 裡面的東西	7-6
那我現在這個window 裡面的東西呢這個m 呢這個m 是我這個這個filter 的index	7-6
第一個三角形是m 等於一第二是m 等於二第三是m 等於三等等	7-6
那這樣子的話呢我得到一系列filter 的output 是這裡	7-6
那這個時候我再取絕對值平方取log 那麼這個時候我們稱稱為他的pron	7-6
這個時候我再做inverse d f t 轉回來得到的這個東西就是我們所謂的m f c c	7-6
它還是對某一個t 而言的	7-6
那這個時候得到的另外一堆index 叫做j 這我們後面會再說到	7-6
那通常到這裡的時候呢這就是我們所謂的m f c c	7-6
但很多時候我們同時把energy 也拿來這個energy 就是那一個frame 的energy 或者那一個window 的energy	7-6
你可以把這一段這一段所對應的energy 全部加起來那這邊這樣你所得到的就是e t	7-6
它也是對時間t 的那個frame 而言的	7-6
那這個時候呢我們通常譬如說這邊可能取十二個或者十三個再加這一個的話呢大概是十三個或者十四個就是這邊的前面十三個或者十四個	7-6
這時候我們可以做微分一次微分跟兩次微分就得到另外的這樣我總共得到三組	7-6
如果一組是十三的話那三組就是三十九維我們講的三十九維的參數	7-6
就是y 的t /*這句也消失了*/	7-6
那這一塊的裡面我們底下現在就是針對裡面的每一塊單獨來看他怎麼做的他為什麼那樣子作等等	7-6
那我們先看最前面的pre emphasis	7-7
那這點倒是我們簡單我們之前也已經說過了那現在再說一次而已	7-7
pre emphasis 只是把我原來的訊號x n 變成一個新的x pron 的n 用的非常簡單的數學式子就是這樣	7-7
那這個把它變成這個就是用這個數學式子也就是我們這邊所說的把x n 變成x pron 的n 的這個pre emphasis	7-7
那中間的這個動作很簡單就是這樣一個數學式子如果寫成z 的話寫成這樣	7-7
那它的意思是什麼呢我們之前說過那個a 是非常接近於一的值零點九六零點九七零點九八這種值	7-7
當你a 非常接近於一的那麼一個值的時候它的效果是像一個這樣子的像這個零點九五像這種這種東西	7-7
那它是在高頻的部分會變的比較大	7-7
所以它是一個所謂的high pass filter	7-7
那麼所謂的高頻變的比較大的意思呢我們在這一頁有說我為什麼做這件事	7-7
那基本上對voice 的section 而言	7-7
你可以看到其實我們聲音裡面有voice 有unvoiced	7-7
但是voice 是一個非常關鍵的部分你如果voice 沒弄對當然就不對了所以voice 是所有的母音都是voice voice 非常重要的	7-7
voice 有個特性呢就是它是這個有一個slow 會慢慢掉下去	7-7
也就是說它的這個format structure 基本上是有一個斜率的這樣下來的	7-7
你越高頻它就越低	7-7
這個斜率基本上呢roughly 大概是多少呢二十d b per decade	7-7
也就是說你每十倍的frequency 就掉二十個d b	7-7
你如果從這個一百h z 到一千h z 大概會降二十的d b	7-7
一k 到十k 大概又會掉二十個d b	7-7
它幾本上是voice 聲音都是會這樣子掉下來的	7-7
那這個原因為什麼呢	7-8
應該是我們發生了有一些生理的特性就是這樣	7-8
那麼因此呢我們人的發聲系統不管是在這邊或者在這邊不管是在這邊或者在這邊總之我們就是會變成我們產生的就是會變成一個這樣子的斜下來的	7-8
因此越高頻的部分越微弱越不清楚	7-8
那麼因此呢也就是說這個high frequency 的的format 呢是這個它有它的安培會小的多比起low frequency format 來	7-8
也就是說你到了f 三f 四的時候會會變的比較微弱那麼比起f 一f 二來	7-8
因此呢你的我們做這一塊的目的就是把它拉上去	7-8
也就是說你如果本來是這樣的話	7-8
那我希望把它拉到上面來讓它可以我的這個原來的format format structure 我希望不要改變它	7-8
但是我把它拉上去	7-8
把它高頻讓讓它能夠這個到f 三f 四的時候可以差不多一樣高	7-8
那這樣的話呢我比較容易抓到所有重要的東西這是一個最基本的原因	7-8
也就是說我把這些higher frequency 把它拉高	7-8
可以得到比較清楚的所有的format	7-8
那當然也有另外一個說法就是其實人的聽覺對對於一個kilo h z 以上的那些頻率其實是相當sensitive	7-8
那麼既然我們人會非常sensitive 的話	7-8
那那邊這麼弱為什麼會sensitive 不知道	7-8
那我們就把它拉高一點那麼那麼讓我們的比較sensitive 的的感覺能夠在這裡能夠感覺的出來	7-8
那這個就是我們這邊所說的這個pre emphasis 他在高頻拉高的這個基本的一個說法	7-8
那這個其實一個更簡單的說法也許是說有人做這個實驗之後發現欸辨識效果就是比較好嗎	7-8
那麼如果不不是辨識效果比較好就不做了嗎	7-8
就是因為不管怎麼樣做這個都是會比較好的所以呢後來大家都都做了	7-8
就是了好這是第一個然後第二個再下來呢我們要看的就是我為什麼要取window	7-9
取window 的基本的原理其實我們剛才已經說了	7-9
就是因為我的一段話是很多不同的phone 串起來的	7-9
那我really 要知道每一個phone 在幹什麼這個是巫這是斯這是阿每一個phone 在幹什麼我必須要知道	7-9
那我如果不取window 的話我做的東西都是像這樣的	7-9
那也就是我們一般在課本裡面所學的這個fourier transform 或者是這個discrete fourier transform	7-9
都是這種東西都是從負無限大加到正無限大或者是這樣子加的	7-9
也就是把它全部等於是把他所有聲音全部平均掉了	7-9
但是在我們這裡是不能這樣子做的因為我們的語音訊號顯然是每一小段是什麼音每一小段是什麼phone 才是最重要的	7-9
你如果都這樣做掉就沒有了	7-9
所以呢我們必須要取一小段好那這樣的一小段的話就是我們所謂的short time fourier analyses 或者是	7-9
這個也就是說我們的聲音是非常的這個none stationary	7-9
就是我我們的聲音的訊號是它的特性隨著時間在改變這個是這一個音這是這一個音當然不一樣嗎	7-9
它不斷的在改變的所以呢那你怎麼辦你只能夠取一個小一個小的window 來看	7-9
在這個小window 裡面你可能可以假設它它是某一種特性	7-9
因為它就是巫它就是巫它就是這樣這邊它就是阿它就是那樣那麼因此我可以在那一小段裡面可以看他的特性	7-9
那因此我就必須要作這樣子的short time 的這個fourier analysis	7-9
那麼因此我們也可以說我們這個語音是語音的訊號是非常的none stationary 他不是隨著時間變化的	7-9
但是我們可以假設它是在這個這個很短的時間裡面假設它是stationary 我們才可以做這一件事情就是所謂的short time stationary	7-9
換句話說這上面這句話講的意思是說我們一般課本裡面這樣講是假設那個signal 本身有某一種特性	7-9
他那個特性並不並沒有隨時間在改變	7-9
如果那個signal 本身沒有隨時間在改變的話那麼我可以從頭到到尾整個積分積起來或整個加起來作一個	7-9
那你如果每一段有不同的特性的話其實就不能這樣子做了	7-9
那麼因此呢我們的聲音不是他這邊所謂的stationary 我們的是none stationary 的	7-9
我們的聲音是none stationary 特性	7-9
所以呢不能夠把它想像成是stationary	7-9
然後可以然後再用這種方法來做其實是不對的	7-9
因此呢我們必須要做個假設說其實這個stationary 呢出現在哪裡出現在很小一段裡面	7-9
所以呢我如果只取這麼一個window 的話呢我可以說它是stationary	7-9
在這個情形下我可以在這裡面作	7-9
那因此我就是做這個short time 的stationary	7-9
假設因此我就是在做這個short time 的fourier analysis 那就是這段話在講的意思	7-9
那因為這樣子的關係我就必須要做window	7-9
那這時候就第一個問題window 到底要多長	7-9
顯然不能太長也不能太短要有一個一定的長度	7-9
那為什麼不能太長你如果太長的話把好幾個phone 都放進來那又失去意義的了	7-9
你你只要長到對不對包含好幾個phone 就沒有意義了你顯然是要短到應該只有在一個phone 裡面	7-9
那你只有在慢慢shift 過來的時候你會cover 到兩個phone	7-9
那這個時候這個window 其實是代表的就是由這個phone 轉成這個phone 的時候它有什麼變化	7-9
那慢慢再轉過來最後到下一個phone	7-9
你只能夠這麼長不能夠太長	7-9
當然你也不能太短太短的話可能短到你不能抓到裡面的特性那也就沒有意義了	7-9
所以一定一定要不長不短那差不多是在這個range	7-9
然後第二個呢你要隔多遠shift 一次	7-9
就是從這裡跳到這裡的時候到底要跳多遠這就是所謂的frame shift	7-9
那你也可以想像我們第一個不能跳太遠	7-9
如果隔的太遠的話這個跟這個已經完全不一樣了	7-9
那這個時候呢顯然中間少了他們變化過去的情形	7-9
所以你不能跳太遠	7-9
但是也不能跳太近跳太近的話就是我的計算量太大了	7-9
你每一個都要算的話計算量會會太大所以呢我顯然是要選一個不遠不近的這是所謂的frame shift	7-9
那這個frame shift 就決定了我的frame rate	7-9
也就是每一秒鐘要幾個frame	7-9
那這個幾個frame 就是幾個window 我們有的時候稱frame 有的時候稱window 是指同樣的東西	7-9
那這個就是我們在做window 的時候要做的事情	7-9
那麼底下呢我們就要進一步來說明你做了window 之後到底發生什麼現象	7-10
那其實是對聲音做了相當多的一些個喔作用在上面	7-10
我們這個喔休息十分鐘好了	7-10
好	7-10
我們接下去要講的就是 你做了window 之後會怎樣	7-10
那你可以想到的是說我們其實中間要算這些東西	7-10
要做這些個 嗯 m f c c 的時候	7-10
要做這種我們是非常非常 careful 它的 frequency domain	7-10
也就是說這些東西	7-10
因為這些東西是真正幫助我們決定是什麼音的是這這裡是它的 format structure	7-10
也就是它的 frequency domain	7-10
這個這是影響非常大的	7-10
那也是為什麼譬如說我們要做 pre emphasis	7-10
也是為了要把他這個東西修的更容易處理	7-10
所以 frequency domain 是非常的重要的	7-10
那當我們做了這個 window 之後在 frequency domain 有什麼影響	7-10
那就是我們這邊要說的	7-10
那麼事實上你做了 window 之後	7-10
無可避免的對他做了相當大的破壞	7-10
為什麼說做了window 會得到相當大的破壞呢	7-10
你可以想像譬如說	7-10
我的我原來的 signal 是這樣	7-10
然後我是用這一堆	7-10
那這一堆是一堆 cosine	7-10
對不對我們說過	7-10
這個其實是一堆 cosine 加起來的	7-10
他是這樣這樣	7-10
等等	7-10
很多東西 這些東西 整個兜起來變成這個東西	7-10
可是當我做了一個 window 之後	7-10
譬如說呢 我從這裡 開始	7-10
但是呢我到這邊就沒有了	7-10
這後面就沒有了	7-10
那麼我等於是把這後面這些東西都切掉了	7-10
那你想會怎樣	7-10
我原來的這堆東西	7-10
原來的這堆東西 cosine 兜起來是這個的話	7-10
我現在 我要有什麼辦法可以讓它們兜起來變成 中間是一樣的但是外面會沒有	7-10
這個是一個很大的變化	7-10
我這邊要動很多東西很顯然 這個地方都被破壞了	7-10
這個地方我顯然都要動掉	7-10
我都要動掉很多東西之後才有辦法使得它加起來結果是 中間不變外面要變成零	7-10
ok	7-10
那 於是你就可以猜的出來是中間受了很多變化	7-10
我這個地方 整個都 亂掉	7-10
那 這個整個都亂掉之後我變成是跑出很多不同的東西出來	7-10
然後也 整個動掉很多東西之後才有可能 讓他們都兜起來 會變成中間不變 而外面呢會變成零	7-10
那這個過程呢	7-10
我們從數學關係來講的話呢	7-10
就是 你乘上一個 window	7-10
那你再乘上一個 window 在 frequency domain 而言呢	7-10
就是在做了一個 convolution 的關係	7-10
阿那 giving 這個 這個數學關係我們就這樣講	7-10
那麼你如果是唸過這些東西你就了解他是什麼意思	7-10
如果你沒唸過 嗯 不了解也無所謂	7-10
那基本上 我們知道有這麼回事	7-10
也就是說 我們剛剛講它真正實質 實質的影響 就是 整個都會被動掉	7-10
為什麼會被動掉因為我要把它 我現在要重新變成 他們加成起來要變成中間一樣	7-10
但是外面是零	7-10
那這個情形就使得我動掉很多東西	7-10
那這個動的情形呢 如果用數學來講的話	7-10
在 time domain 我這個 window 就是原來的 x pron 乘上一個window	7-10
可是在frequency domain 的話我把他們都分別去做transform	7-10
到frequency domain 去看	7-10
他們通通都到這個domain 來看的話	7-10
那他們就會變成	7-10
他的 transform 跟這個window 的transform	7-10
再做一個convolution 才是我的東西	7-10
那這個convolution 變化呢就是我們這邊講的他整個全部都動掉了	7-10
那動掉些什麼呢	7-10
那麼基本來講	7-10
我們如果來看	7-10
他是一個convolution 的關係	7-10
那麼這個window 的 transform 是什麼呢	7-10
我們就看一個最簡單的rectangular 的window	7-10
那麼 rectangular 的window 長怎樣	7-10
畫一個簡單的圖來說的話	7-10
那麼你如果熟悉就知道	7-10
不熟悉的話	7-10
你	7-10
了解一下就好	7-10
那麼它的transform 呢是一個 所謂的 sink	7-10
這種東西	7-10
那這種東西呢 它其實他的 他這個兩邊的這個這個東西很快就變的很小	7-10
那 我們不太容易真的畫出來他是怎樣所以我們	7-10
通常把它畫成一個 一個這個 畫成一個這個圖	7-10
這個呢 就是 這個其實畫的就是這個東西	7-10
但是我現在是用log 來畫	7-10
所以這邊是取db就就是這邊是取log 了	7-10
取log 之後呢	7-10
我其實是這個地方等於是這個	7-10
那它的 這是它的第一個side lobe 下來	7-10
這是第二個	7-10
它是一個這樣子的	7-10
有一個 有	7-10
它是一個這樣子的	7-10
那麼這個什麼意思呢 我我我我取取log 之後	7-10
這個變成零	7-10
然後呢這個變成這個這個log scale 在下來	7-10
那麼當我掉在第一個零的時候它其實是在取	7-10
如果這個是 喔 這個transform 的話	7-10
它其實是在取這個然後取log	7-10
取它的絕對值然後取log	7-10
之後得到這個圖	7-10
那麼因此呢	7-10
我在 這個地方掉下來的時候它就這樣掉下來	7-10
掉到零的時候應該是掉到負無限大	7-10
因為取log 是會掉到負無限大嘛	7-10
然後再來這個的話就會跑出第二個出來	7-10
然後呢這個會在 這個是指對應到第二個	7-10
那這是指對應到第三個	7-10
這樣子	7-10
那麼 其中的這一 第一個呢我們叫做main lobe	7-10
這個東西是我們所謂的	7-10
就是這邊寫的這個main lobe	7-10
那另外的這一些呢就是我們所謂的side lobe	7-10
就是後面這些東西	7-10
這些東西是我們所謂的side lobe	7-10
就是這些這些side lobe	7-10
那所以呢你你	7-10
這個圖其實跟上面沒有什麼不同只是因為 你你真的要畫上面這個的時候	7-10
你會變的很小很小	7-10
不太容易看喔	7-10
重畫一個	7-10
它是一個是一個這樣子的	7-10
那麼你很快就變的非常小	7-10
不太容易才看的 不太容易看的出來啦	7-10
因此我們通常用用db來畫	7-10
這樣的話呢 比較容易看的出來它一個一個	7-10
這就是它所謂的side lobe 一個一個side lobe 一個很清楚的在這裡	7-10
那就是我們這邊所畫的這一張圖的這個rectangular 這個	7-10
就是 這是這是它的main lobe	7-10
然後這是第一個side lobe 第二個side lobe 這樣一直下來	7-10
那這個是用log scale 來畫比較看的清楚	7-10
得到這個 這 這個情形	7-10
那麼這個	7-10
那麼因此呢你你如果你的window 的transform 長的像這樣	7-10
這個是 我的 這是這兩個之間的fourier transform	7-10
喔	7-10
就是你取一個window	7-10
我取一個window 相當於是取 這個 譬如說我從零到l 減一	7-10
這中間是一 外面是零	7-10
我如果取一個 這樣子的長度是l 的window	7-10
中間是一外面是零的	7-10
他的transform 是長的像這樣或者說是這樣	7-10
那它有一 它有一個main lobe 然後有一堆side lobe	7-10
那麼於是所謂的我的這個東西跟這個window 去做convolution 什麼意思	7-10
是我原來這裡面有一個這個	7-10
我們說原來是這樣的一個	7-10
這個東西去跟他去做一個convolution	7-10
那這個convolution process 你如果了解的話你可以想像它的意思就是我這邊所寫的	7-10
那你如果不容易了解也就 聽一聽就好	7-10
那基本上呢main lobe 的的的結果是什麼呢	7-10
你這個東西去跟他convolve 的結果呢	7-10
就是它把它通通磨	7-10
就是我們這邊所畫的意思	7-10
它的效果是 就是把它都磨開來了	7-10
變成一片	7-10
都把它磨開	7-10
來所以呢 它就會 不再是原來那麼精 那麼精確的	7-10
會跑出一堆亂七八糟的東西出來	7-10
這是 這是main lobe 的效果	7-10
這個這個main lobe	7-10
跟他做convolution	7-10
就會把它都弄亂	7-10
那就是這邊所講它會sprayed out the y 的 frequency range	7-10
你原來是有一個很清楚的一個 譬如說你本來有一個format	7-10
本來是一個這樣子的	7-10
你會會會被他整個磨開來	7-10
變成一片	7-10
因此呢 會會使得你這個local frequency resolution in formant allocation 造成困難	7-10
你本來這個地方有一個formant	7-10
非常 清楚可以看到譬如說這是f 三在這裡的	7-10
可是你這麼一搞就把它弄亂了	7-10
把它整個磨磨開來	7-10
所以呢你就不容易抓到它的formant frequency 了	7-10
這個是main lobe 所造成的效果	7-10
那side lobe 更糟糕	7-10
因為它會把遠距離的 遠距離frequency 攪和在一起	7-10
這些side lobe 的convolution 造成什麼效應	7-10
可以把這個不同的frequency 搬來搬去 哼	7-10
你如果了解這個convolution 效應你就知道	7-10
你可以把這個搬來 這個搬到這邊去 喔	7-10
把他這個搞來搞去之後	7-10
所以會弄得更複雜	7-10
這個東西會更散	7-10
因為你把這邊搬到這邊來 這邊搬到這邊去	7-10
所以會弄弄得更更亂一團	7-10
這是side lobe 效應	7-10
那麼因此呢不管是main lobe 還是side lobe 都是一個很不好的結果	7-10
因此你所得到的雖然我只是乘上一個window	7-10
說穿了很簡單我只是乘上window 我只是為了要取這一塊嗎	7-10
我不要把別的音弄進來我不要別的phone	7-10
我只要取這一段	7-10
講起來很簡單 其實我做的事情是把它整個都弄亂了	7-10
那簡單的解釋就是我們剛才講的	7-10
因為你要中間還要一樣	7-10
外面又要變成零	7-10
就把它整個都弄亂了	7-10
那用數學來講	7-10
就是我們這邊講的	7-10
convolve with 這個window 的transform	7-10
那它長的這樣的結果	7-10
就使得我的main lobe 跟side lobe	7-10
都有一堆效應	7-10
使得我整個都變的很糟糕	7-10
那這個就是我們平常取的window 不得不做	7-10
但是你做了之後	7-10
我們必須了解我們取了window 之後會變成這樣的情形	7-10
那因為是這樣的關係呢 所以呢	7-10
但是反過來你又不能不做	7-10
因為你如果不做的話是不行的啦	7-10
喔我們說過非作不可嗎 阿	7-10
但是你你你想要沒有這個效應除非你那個window 等於一for all n	7-10
你除非你得到一個window 等於一for all n 的話	7-10
那全部都是 那那個時候就沒有不會有這種變化可是那樣你就抓不到你的這一段了	7-10
你就變成全部不行了嗎	7-10
那麼因此呢 我就我們這邊講的就是這個意思	7-10
所以呢你沒有辦法做這件事情說	7-10
我讓window 等於一for all n	7-10
我必須取一段	7-10
你一取一段之後	7-10
譬如說用rectangle 就會就會產生這一堆問題	7-10
那因此呢我們這個window design 的問題就是說	7-10
我希望main lobe 越窄越好	7-10
side lobe 越低越好	7-10
我希望這個side lobe 越低越好	7-11
如果這個side lobe 可以壓到很低的話	7-11
使得我們剛才講的這個side lobe 的問題	7-11
這個他就搬來搬去這個問題	7-11
可以效應可以降低	7-11
所以我希望把這個side lobe 僅可能壓低	7-11
反過來呢我希望這個main lobe 越窄越好	7-11
main lobe 越窄的話也是使得我這個本來這個磨平的這個效應呢 可以可以比較小	7-11
所以呢我的目標就是希望main lobe 越窄越好	7-11
side lobe 越低越好	7-11
但是呢這個是做不到的	7-11
就是說你沒有辦法同時做這兩件事	7-11
你如果把這個side lobe 壓低的話呢通常main lobe 就會變寬	7-11
你如果把main lobe 變窄的話side lobe 就會高	7-11
那為什麼會這樣我想這個也不在我們這裡解釋	7-11
那你如果去修相關的課	7-11
會學到這些事情	7-11
不過我們這裡我們只是這樣說一下就是了	7-11
就是說你要同時得到這兩個的話是無法得兼的	7-11
所以呢中間存在一個trade off	7-11
那我們今天最常用的window 呢是哪一個呢	7-11
是hamming	7-11
hamming 跟這個有點不一樣	7-11
那我們之前也說過hamming 是我把它兩邊都壓低了	7-11
把它變成一個cosine	7-11
我把它變成一個 一個這樣子的cosine	7-11
那變成這樣cosine 的話在這上面效應是什麼呢	7-11
是比較成功的把side lobe 都壓低了	7-11
所以我比較不會造成這些frequency 搬來搬去胡搞一陣的現象	7-11
可是我無可避免的win 這個main lobe 會變的更寬	7-11
所以它其實他也會磨的更利害	7-11
那這個效應就是在我們這個上一頁的圖的這裡	7-11
我這邊畫的另外一個就是hamming 阿	7-11
那你看到從上面這個是rectangular	7-11
底下這個是hamming	7-11
那它明顯的把side lobe 壓低很多	7-11
從這裡壓到這裡	7-11
這壓低多少呢你如果看這邊大概幾乎壓低二十個db	7-11
二十個db 是一百倍的意思嗎	7-11
所以呢基本上是把它side lobe壓的非常低	7-11
這是從rectangular 走上的window 的 走上hamming 的成功的地方	7-11
但是無可避免的我的main lobe 是變寬了	7-11
是變寬了的	7-11
所以呢它有有好處也有壞處	7-11
但是這個好處是很大的	7-11
就是你看到side lobe 降低了二十db 阿	7-11
那麼因此呢我的這個有一些效應是減少	7-11
雖然有一些效應是無法克服的	7-11
那這個就是我們選擇這個hamming 的情形	7-11
那hamming 的這個式子我想我們從前就已經看過了 阿 這就是就是這個一個cosine	7-11
那麼關於這一點呢其實我們在講二點零的時候曾經說過另外一個說法	7-11
其實也是同一件事	7-11
我們當時講	7-11
從從這個rectangular 變成hamming 我們有另外一個說法	7-11
就是他在time domain 上的說法	7-11
那其實意思是一樣的	7-11
是講同一件事	7-11
我們當時的說法是這樣說的	7-11
就是你的訊號譬如說我的這樣子	7-11
我現在如果我取一個window	7-11
取這樣的話	7-11
取這樣的一個window 的時候	7-11
我就說應該長一點	7-11
取取取這裡好了	7-11
取這樣一個window	7-11
那等一下我如果shift 一下到這來	7-11
會發生什麼情形呢	7-11
我這邊丟掉了很大的東西	7-11
這邊只收進來很小的東西	7-11
所以這個變化會很大	7-11
但其實我這個語音訊號從這裡從這個window 到下個window 有變化那麼大嗎 沒有	7-11
它明明是非常穩定的	7-11
對不對	7-11
從你可以看到從這個邊其實是非常穩定的東西	7-11
但是你從這邊變到這邊的時候呢 你這邊就會 可能丟掉很很大的東西 這邊只加進來很小的東西	7-11
使得我那個值會變化很大	7-11
是不合理的	7-11
那為了避免這個現象呢我的辦法呢就是ok 我變成是一個cosine 的形狀	7-11
那他兩邊被weight 的很	7-11
如果是這樣的話呢你可以看的出來我	7-11
這個時候	7-11
我就讓兩 兩邊的這個 這邊會丟掉 這邊會增加 這個變化呢	7-11
都被weight 很低	7-11
我強調是中間	7-11
那這個時候我就比較不會影響	7-11
那這個說法其實跟我們剛剛講的這說法是一致的	7-11
那只是這個是一個time domain 的說法你也許比較容易想像	7-11
那我當我從這個變成這個的時候因為我把兩邊都壓低了	7-11
所以它的影響很小	7-11
那這個效應其實就是我這邊的side lobe 的效應降低了	7-11
我side lobe 就不會發生那些現象	7-11
那就就比較好一點	7-11
這個是我們選擇hamming window 的一個原因	7-11
所以這邊講的是	7-11
講這個window 的process	7-11
有了window 之後呢再下一步是幹麻呢 我們回過頭來看一下	7-11
我window 完了之後我現在就得到這一段了	7-12
我得到這一段了	7-12
我先來開始做discrete fourier transform	7-12
那我們知道我transform 的時候中間會跑 因為取了window 會跑出一 一些	7-12
亂的現象	7-12
不過不管怎樣呢我們得到了一系列的sample	7-12
就是這些個	7-12
這些一個一個東西	7-12
那這些東西就是我們的這邊所寫的這個x t 的k	7-12
我現在是在k 的軸上	7-12
那麼我會得到一些這樣子的	7-12
這些東西這就是x t 的 嘶 喔 大寫吧	7-12
他是一個大寫的x t 的k	7-12
那這個橫 我們說橫軸的k 其實就是frequency	7-12
然後呢那大寫表示我現在是在frequency domain 上面	7-12
那 我是一個一個的	7-12
仍然是取它一個一個的sample 因為我的d f t 算出來的就是一個 一個一個的sample	7-12
那這個時候呢	7-12
我們現在來看我後面要做的事情	7-12
其實就是這個mel filter bank	7-12
那這點其實我們之前也已經說過	7-12
我們現在重新看一次	7-12
第一件事情我們要說的是	7-12
你因為我的signal 是real 的signal	7-12
你如果去了 了解這個d f t 的話	7-12
就知道	7-12
不了解也沒關係	7-12
我得到的東西應該是會是對稱的	7-12
他是symmetry	7-12
所以會變成只有一半有效就行了	7-12
換句話說你譬如說原來是l	7-12
我這個window 這是l 是window 的長度譬如說五百一十二	7-12
我這個window 的長度從這邊的這個五百一十二點	7-12
我做d f t 之後	7-12
還是五百一十二點	7-12
但是其實因為它是左右對稱	7-12
我只要兩百五十六點就夠了	7-12
所以變成二分之l	7-12
ok	7-12
所以我這邊只要兩百五十六點	7-12
二分之l 就好了	7-12
而這兩百五十六點上面我來做這些mel scale 的這個filter	7-12
那一個個的三角形我們把它畫在這裡你就看的出來它的意思	7-12
那麼depends on	7-12
譬如說就就這個三角形而言	7-12
你可以想像是這幾個值	7-12
就是這幾個值	7-12
是被他所cover 的	7-12
那麼分別乘上這些個三角形的的weight	7-12
然後加起來得到一個值	7-12
那我就等於假設是取這一塊	7-12
喔 等等喔	7-12
那麼我這一個個三角形就分別去取每一塊每一塊	7-12
分別就把這些個點乘進去	7-12
用這些三角形的weight 乘進去之後加起來得到一個值	7-12
那這點我們在之前就已經說過	7-12
那其實是在模擬	7-12
人為什麼可以聽其實我們始終不懂	7-12
那麼但基本上你可以想成是在frequency domain 上面	7-12
那一組聽覺神經就是管這堆frequency	7-12
那另外一組聽覺神經就是管另外一堆frequency	7-12
他們會overlap	7-12
他們會overlap	7-12
但是呢顯然overlap 沒關係	7-12
那他們就是這麼做的	7-12
那怎麼辦呢我們就用成這樣子的三角形	7-12
那這一點呢就是在我們底下這一 這邊所講的意思	7-12
喔	7-12
就是說 當你有一個 一堆複雜的聲音 在某一個frequency bandwidth 裡面的時候	7-12
你可 你的耳朵其實分不出來	7-12
這就是指這一堆 嗯	7-12
譬如說這一堆	7-12
這一堆frequency	7-12
這一堆這一堆frequency 裡面的	7-12
譬如說有的聲音 有有一個這個frequency 跟有一個frequency 的話其實我耳朵分不出來	7-12
因為我是同一組 聽覺神經在那邊做這件事	7-12
所以你如果在這一堆裡面的話這個跟這個其實我是分不出來的	7-12
這是這句話在講的意思	7-12
這是講我們聽覺的現象	7-12
你在一個complex sum 的某一個 在某一個certain bandwidth 裡面的時候	7-12
它我其實是我我分不出來的	7-12
可是呢你如果是在另外一個的話你就分的出來了	7-12
譬如說如果一個frequency 是在是在這裡	7-12
另外一個frequency 是在這裡的話呢	7-12
這兩個我反而分的出來	7-12
為什麼會這樣因為這個是	7-12
它在聽這個是它在聽	7-12
所以結果我在不同的地方我反而分的出來	7-12
嗯	7-12
那其實說穿了就是我們這邊講的意思	7-12
就是說我其實是一堆一堆的	7-12
這一堆在聽	7-12
然後這一堆在聽這一堆	7-12
然後這一堆在聽這一堆	7-12
它就是這樣子	7-12
它是一堆一堆在聽的 喔	7-12
那所以第二句話的意思就是說你如果是在兩堆裡面的話你聽的出來他們的不同	7-12
那這樣的每一堆呢在他們講的這個聽覺裡面他們稱為critical bank	7-12
所謂的critical bank 就是指這樣的一堆東西叫做一個critical bank	7-12
那因此呢我們現在的 現在用的這個三角形的這種的這個filter bank 呢	7-12
其實是在模擬那麼一個現象	7-12
那麼如果是這樣的話那我們現在再來說另外一件事情	7-12
就是 這我們之前講過的 就是	7-12
你在 一個kilo h z 以下它是uniform 一個一個	7-12
但是在以上的時候它會變成什麼呢 會變成一個log scale	7-12
越變越大	7-12
它是一個log scale	7-12
那麼為什麼會這樣	7-12
那麼當然這個也是我們聽覺就是這樣的	7-12
那我們可以用很多別的佐證來說這件事	7-13
譬如說我們人聽的音高本來就是log 的 frequency	7-13
我想這個是 喔 我們之前大概也說過	7-13
那麼你可能在別的地方 也聽過這個 常識	7-13
一個最容易想像就是你學音樂的時候	7-13
do ra mi f a so la si do	7-13
你知道它的這個 do 跟ra 之間的距離 ra 跟mi 之間的距離都是一樣的	7-13
這叫做全音	7-13
mi 跟far 之間的距離是半音	7-13
是他們的一半	7-13
然後so 跟la 又是全音	7-13
中間距離又是一樣的	7-13
到si 跟do 又變成半音	7-13
是他們之間的一半	7-13
這是什麼距離	7-13
這個就是log 的frequency	7-13
或者說那個pitch 的p 分之一	7-13
這個東西	7-13
也就是說你 你如果把這個pitch	7-13
你音樂的你	7-13
這個如果是交響樂很複雜你看不出來那個音樂非常複雜	7-13
你如果是單一的樂器譬如說是鋼琴	7-13
或者小提琴	7-13
你你去把它的wave form 拿出來也是長的這樣	7-13
也是這個樣子的	7-13
然後他也有週期那個週期就是pitch	7-13
就是音高	7-13
但其實是什麼音高你如果把那個pitch 的那個音也是這樣一個peak	7-13
你把這個pitch 的p 分之一	7-13
也就是我們講的f zero	7-13
你把這個東西取log 之後	7-13
就是exactly 這樣的scale	7-13
你在那上面會看到do 跟ra 的距離 跟ra 跟mi 的距離是一樣的	7-13
mi 跟far 的距離是一半都是這樣子的	7-13
那麼你知道從do 變成do 的時候是exactly 這個東西變成兩倍	7-13
它就變成這樣子	7-13
那麼因此呢 所謂變成兩倍的意思呢	7-13
你可以想像就是	7-13
如果講它的一個cosine 的話	7-13
這是一個這是一倍這是兩倍嘛	7-13
這樣的嘛	7-13
所以一個一個是do 一個就是do 嘛	7-13
那麼也就是說呢	7-13
我們本來所謂do ra mi f a so la si 這個音階其實就是把這個這個range 平分出來的	7-13
但是我平分在什麼平分在log 的frequency 上平分	7-13
所以我們人的聽覺本來就是	7-13
這個human percipient 對於音高的感覺本來就是跟log of frequency 成正比	7-13
那麼這可能是一個簡單的原因說	7-13
我們為什麼在在這個在這個這個地方	7-13
我們後面這個這些三角形是以log scale 去分分配	7-13
那麼越到後面是一直都以log 來來來分配的原因其實就是	7-13
那也可以說其實你分析這些個critical bank	7-13
他本來就是以log 的方式	7-13
接近log 的方式來來呈現的	7-13
那就是這邊所講的意思了啊	7-13
那這另外我們也可以這樣講就是	7-13
這個這是另外一個理由就是說呢low frequency 本來就是這個important role in human ear	7-13
我們剛才看到最重要的是f one 跟f two	7-13
f one 跟f two 是最明顯的區分所有的音的 是f one 是在log frequency 這邊	7-13
那也因為這樣所以呢我們在我們說在low frequency 的時候呢我們就給它uniform	7-13
就是在一kilo h z 以下就是用很精密的一個一個	7-13
對	7-13
嗯	7-13
我們就讓它很精密的一個一個三角形	7-13
這樣子	7-13
那	7-13
為什麼要那麼精密	7-13
那是因為在一個kilo h z 以下是最重要的部分	7-13
我們希望做的很精密嘛	7-13
那然後呢	7-13
那麼為什麼它的每一個 到後來三角形越來越大	7-13
那這也是這個critical bank 本身它的bandwidth 就是跟他有跟他的center frequency 有關	7-13
也就是說這個東西越到高頻就是會越大	7-13
它的寬度就是越到高頻會越來越大	7-13
那他跟他的center frequency 就是它所位置那個frequency 是有一個正比的關係	7-13
大概在一個有一定的比例之內	7-13
所以當你到了高頻的時候	7-13
它就是就是會變大的	7-13
所以呢我們這些都是都是這些個filter 會長成這樣的原因	7-13
ok 好	7-13
那這個是我們說的這個這一塊	7-13
就是我做這個mel filter bank	7-13
那於是呢我的出來變成是y 的t 的m	7-14
那麼這個y 的t 的m 就是指第m 個filter	7-14
所以呢如果第一個filter 是m 等於一	7-14
第二個是m 等於二 第三個是m 等於三等等	7-14
那我每一個做出來得到一個	7-14
那就是我的y	7-14
t 呢仍然是我這個window 的index	7-14
那麼當我得到這個之後下一步幹什麼就是取絕對值平方然後取log	7-14
關於這點呢也是有理由的	7-14
那就是寫在底下	7-14
就是我們這邊所說的	7-15
當你這個假設這是m 就 總共是 這個橫軸是這個filter 的index 小m	7-15
m 總 這個filter 總共是大概是大m 個	7-15
大m 個可能是二十幾 嗯	7-15
不同的人作法不一樣大概從二十三到二十六到三十大概	7-15
那這個數字大概也就是我們一般講的這個聽覺的這個critical bank 的數目	7-15
大概是 二十三到二十六 二十七	7-15
差不多這樣的數字	7-15
那你每一個得到的一個值之後呢	7-15
你把它絕對值平方取log 就變成pron	7-15
然後我們要做inverse d f t	7-15
為什麼絕對值平方取log 呢	7-15
那這邊也有一堆理由	7-15
你第一個呢所謂的絕對值就是把phase 丟掉了	7-15
因為你本來的每一個東西 都是有這個有這個的	7-15
那有有這個amplitude 有這個phase 所謂phase 就是這個東西	7-15
那這個phase 的影響是什麼	7-15
就是我們說那個cosine 嘛	7-15
那個cosine 它的零在哪個位子	7-15
就是這個phase	7-15
那不同的phase 讓我這個cosine 可以前後移動	7-15
但是我現在如果取絕對值的話是怎樣	7-15
取絕對值就是把這個拿掉只保留a	7-15
這個沒有了嘛	7-15
所以就是把這個phase 丟掉了	7-15
phase 為什麼可以丟掉呢	7-15
那 我們的了解就是一般人的聽覺就是不太聽phase 的	7-15
我們人的耳朵就是不太聽phase 的	7-15
那這一點呢你很容易做個實驗就是	7-15
你如果把phase 丟掉	7-15
你 放一個random 的phase 進去的話	7-15
你再transform 回來你聽不出有何區別	7-15
這很容易做的實驗 就是你如果拿某一段聲音	7-15
我做了transform 之後	7-15
我把這個phase 全部丟掉	7-15
把random 的number 放進來	7-15
那這個phase 完全random 之後	7-15
其實就是我這邊的每一個都前後都都動掉了	7-15
我再加回去再inverse transform 回去	7-15
我得再把他加回來這個這邊長的完全不一樣了	7-15
但是我耳朵聽起來幾乎是相同的	7-15
那這是我們的聽覺一個很奇怪的現象	7-15
喔	7-15
喔	7-15
這個如何解釋不曉得 但是這是一個大家所公 共同知道的一個現象	7-15
我們也就是說當你把它的transform 過來的時候	7-15
本來你每一個frequency component 上面都有一個phase	7-15
他告訴我那個cosine 的位子在哪裡	7-15
但是我可把他這個完全丟掉之後完全加random 的number 進去	7-15
等於把他每一個都全部的前後都全部動掉	7-15
我再加回去	7-15
那這邊當然就完全不一樣了	7-15
因為你這邊每一個component 都全部都動掉 再加回去這就完全不一樣了	7-15
可是你耳朵聽起來是完全一樣的	7-15
那就表示說我們好像不聽這些東西	7-15
既然我們不聽呢 不如把他拿掉	7-15
那這個是一個簡單 簡單的解釋	7-15
為什麼把這個東西只取這個	7-15
喔 不是完全聽不到	7-15
所以呢 他們今天有不少的研究 其實是怎麼樣把這個phase 好好的處理	7-15
那是有幫助的	7-15
不過呢大多數情形我們所了解是不太聽這個	7-15
所以呢我們就可以把這個phase 拿掉	7-15
那然後呢 為什麼要平方	7-15
這裡還有一個平方喔 喔	7-15
除了把phase 拿掉之外我還要平方	7-15
平方也是因為我們的聽覺	7-15
基本上是sensitivity 是 跟energy 有關	7-15
也就是說 你如果 這個變成 兩倍的話	7-15
你耳朵聽起來是四倍的四倍的強度 阿	7-15
也就是說我們耳朵聽的 是它的 能 是它的energy	7-15
不是它的amplitude	7-15
這個變成一半的時候我們耳朵聽起來是只有四分之一的強度	7-15
那麼因此我們聽的是它的 是它的energy	7-15
然後為什麼要取log	7-15
還有取log 取log 有沒有道理呢	7-15
取log 的道理也是有的	7-15
log 就是把dynamic range 壓小	7-15
那他本身就是一個壓小的目的其實就是可以怎麼樣呢	7-15
可以less sensitive to variation	7-15
那這個是 本來就是我們的 也是我們聽覺裡面一個自然的現象	7-15
也就是說	7-15
你知道log 是怎樣的log 是這樣的	7-15
因此呢你如果這邊變的很大的話呢	7-15
這邊變的沒有那麼大	7-15
他在 也就是 當你的 當你range 小的時候 這邊比較像是linear vari variation 是差不多的	7-15
比較像是linear relation	7-15
可你如果變大的話呢	7-15
它會被壓的比較小	7-15
ok 它不是這樣上去而是被壓下來的 嗯	7-15
所以log 本身是有這樣的特性 這個特性有它的道理	7-15
就是說當你如果有一個很 如果那麼大的話	7-15
八成是一些雜訊阿 或者什麼	7-15
那那些東西的話我們不要聽那麼大 我們就把它壓小了	7-15
所以這個是一個 也是在我們的這個human hearing 自動發生的一個現象	7-15
雖然那個現象不見得exactly 是log	7-15
但是這用log 來做卻是more or less 描述這個現象	7-15
那它呢 是可以這個 使得我們這個抽的feature 可以less sensitive to variation	7-15
你一些大的noise 進來的時候會會被壓小	7-15
那因此我們也一樣做這個log	7-15
是有這個原因的	7-15
那做這個log 還有一個好處寫在底下	7-15
不過這個我們後 後面再一起來說	7-15
好 那麼這個是講這個 做這個 絕對值	7-15
取 把phase 丟掉平方取log	7-15
當我得到這些東西之後呢 我現在就可以做inverse transform	7-15
我 做到這邊之後我現在再transform 回去到這邊來	7-16
那這是inverse discrete fourier transform	7-16
那你本來transform 回來應該是得到相同點數	7-16
你這邊有m 個點	7-16
這邊應該得到了 得到m 個點才對	7-16
那我們通常會丟掉最後的這些點	7-16
保留比較少的點	7-16
這個為什麼我們底下也會解釋	7-16
那是為什麼我們後來這邊只有十三個嘛	7-16
十二或者十三個	7-16
那本來這邊就是二 二十三或者二十六個	7-16
二十多個為什麼變成十多個	7-16
其實我們丟掉了一些 阿	7-16
那丟掉的原因我們待會會解釋	7-16
那同樣呢 我原來這個地方的是 frequency	7-16
我再transform 回來是什麼呢	7-16
那個是 這我們這邊叫做j	7-16
叫做quefrency	7-16
那這個東西其實 阿 是一個很奇怪的東西	7-16
那麼你可以想像我如果沒有做這個log	7-16
沒有做這個的話	7-16
其實transform 回去就是time domain	7-16
就是你本來是transform 過來inverse transform 回去	7-16
應該就是time 才對	7-16
但是因為我現在在上面做了一堆奇怪的東西我又取了絕對值又取了log 又平方什麼東西	7-16
之後這個東西已經不是原來的frequency domain 的東西了	7-16
是一種 而且你看 我是這樣子一個一個 嗯 是另外一種東西	7-16
所以transform 回回去的時候呢	7-16
這個其實不是原來那個time	7-16
是另外一種東西很像time 但不是time 的東西	7-16
那當時的 發明這個的人	7-16
它就自己取個名子他說我不知道如何取它	7-16
我就把frequency 的四個字母倒過來 阿	7-16
其實這個就是原來的frequency	7-16
這個f r e q u e	7-16
把這兩個字母倒過來	7-16
他就叫做quefrency	7-16
那它只是這樣的意思	7-16
那這樣過來之後的這個東西呢	7-16
那他也取另外一個名子	7-16
那本來的這個frequency domain 這個東西我們有一個名子叫做spectrum	7-16
這個字spectrum	7-16
我們中文通常翻做頻譜	7-16
就是指我的訊號在頻率上分佈的情形	7-16
這是所謂的頻譜	7-16
那當然你這樣轉回去的時候 這又不曉得是什麼東西	7-16
它已經取了一個很奇怪的名子叫做quefrency 了	7-16
那這東西它也取了一個名子呢	7-16
它也一樣 就是把它就叫做 cepstrum	7-16
那這所謂的cepstrum 也也不過就是這前面這四個字把它倒過來	7-16
他把前面這四個字母倒過來就變成cepstrum	7-16
那是這個字的由來就是這樣子	7-16
那麼 喔 所以我們現在所謂的m f c c	7-16
你如果回憶起來我們當初講m f c c 它的全名是什麼	7-16
就是mel frequency cepstral coefficient	7-16
mel frequency cepstral coefficient	7-16
這是它的全名	7-16
這是m f c c 的全名	7-16
那麼這個 這個字是什麼東西	7-16
這個字就是這個字	7-16
那它只是 他只是把他字母倒過來而已	7-16
那這個東西 當時在十多年前	7-16
我們在想這個字應該怎麼翻成中文	7-16
因為spectrum 叫做頻譜嘛	7-16
那這個字應該怎麼翻	7-16
喔 不知道	7-16
阿那曾經有人建議我們就叫他譜頻	7-16
我們就叫他譜頻	7-16
那這樣的話呢就跟他的原意好像很接近 嗯	7-16
那 那 不過當時也有另外一個人有有另外一個翻法叫做倒頻譜	7-16
它加個倒字阿	7-16
前面加一個倒	7-16
那意思是一樣我們把它倒過來就是了啦 阿	7-16
那你就為什麼叫做倒其實也就是因為它是 它是字母倒過來的就是了 阿	7-16
那麼 不過後來好像用倒頻譜的名的的比較多就是了	7-16
所以所以呢 所謂倒頻譜也就是指這個cepstrum	7-16
也就是指這個m f c c 啦 嗯	7-16
這個解釋一下這個名子的由來	7-16
好 那底下我們現在來說我現在為什麼要做這個inverse d f t 呢	7-16
就是做這件事	7-16
那麼基本上呢這個 基本上我們在做的事情是inverse 的d f t	7-16
但是其實呢	7-16
這個inverse d f t 呢會變成一個discrete cosine transform	7-16
那麼為什麼呢 因為我的log power spectrum 本身是real 而且symmetric 嗯	7-16
那關於這一點我想細節我們不說	7-16
你如果有學相關的數學那些東西的話就知道他這裡面講的意思	7-16
如果沒有學不了解 也就無所謂阿	7-16
我們就是了了解這這件事就是了	7-16
就是說我這邊做的其實是d inverse d f t	7-16
就是這一個inverse discrete for fourier transform	7-16
就是這個這個轉回去的轉回去的這個過程	7-16
但是我其實因為我現在這上面所做的東西	7-16
其實是 不是這樣子畫的random 的	7-16
而是它有一定的特性的	7-16
這個特性就是我們這邊說的他是 real 而且是symmetric	7-16
在這兩個特性之下那個inverse d f t 呢	7-16
會變成一個discrete cosine transform	7-16
什麼是discrete cosine transform 呢	7-16
跟fourier transform 是很像的	7-16
只是我的basis 就直接是cosine 嗯	7-16
也就是說我我們這邊講的時候我這邊的每一個東西	7-16
都是用 我這邊的每一個	7-16
我都都說它其實所代表的是	7-16
一的j omega one t 等等	7-16
都是這個東西	7-16
然後我的transform 呢 也都是把他寫成	7-16
譬如說x of t	7-16
e 的minus j omega t d t 我的	7-16
積分都是這樣積的	7-16
或者說我的summation 都是我剛才寫的這邊的啦 哈	7-16
就是x n	7-16
e 的minus j omega n	7-16
summation over n	7-16
就是這種東西	7-16
那基本上呢我都是以e 的j 這種東西	7-16
作為我的基本的basis 來做這些事情的	7-16
那cosine transform 唯一的不同	7-16
只是這些東西我都變成cosine 就對了	7-16
那其實我們說過這些東西你如如果取實部就就是cosine 嘛	7-16
那它就 根本不 這邊就 這邊就直接改成cosine 了	7-16
我這邊就直接改成cosine	7-16
我如果直接把cosine 放進去的話	7-16
這種東西就是所謂的cosine transform	7-16
那cosine transform 跟這個 跟這個原來的這個fourier transform 之間	7-16
是有非常密切的一堆關係的	7-16
那我想不在我們這門課要說	7-16
你如果有興趣的話在相關的文獻相關的課本修相關的課會學到	7-16
那我們這邊不講我這邊只說這句話就是說呢	7-16
阿 你去 你去查paper 可以查的到	7-16
它可以證明	7-16
這個cosine transform 有個很大的特性	7-16
就是可以把這個 這個中間的correlation 降到最低	7-16
使得你得到的最後得到的東西是highly un correlate	7-16
那麼換句話說你如果做了這個cosine transform 的 的話	7-16
你所得到的這些	7-16
最後這些東西他們彼此之間 的 幾乎是沒有太多的correlation	7-16
它們幾乎都是 非常independent 的component	7-16
那這樣有什麼好處	7-16
這樣最大的好處是	7-16
我到後來我的那個gaussian 那個gaussian 裡面	7-16
我都可以用對角線的matrix	7-16
這是指什麼	7-16
你記得我們的 我們的hidden markov model 裡面 h m m 裡面	7-16
譬如說 這個state 裡面是怎樣的	7-16
我們說是一堆gaussian 對不對	7-16
譬如說這是一個gaussian 這是一個gaussian 這是一個gaussian	7-16
我用一堆gaussian 來來描述說	7-16
當我的訊號在這個state 裡面的時候它的distribution 是這樣的	7-16
那這每一個gaussian 你如果去寫的話	7-16
它是會有什麼東西	7-16
e 的minus 什麼東西	7-16
那這裡面會有這個 譬如說 這一類的東西	7-16
那它有一個covariance matrix	7-16
我現在這個東西是三十九維	7-16
所以應該是要有一個三十九維乘三十九維的covariance matrix	7-16
那 那個matrix 非常複雜	7-16
那在它的對角線上	7-16
是相當於每一個自己的variance	7-16
但是這外面的每一個點呢是他們各個component 彼此之間的correlation	7-16
那麼我現在如果說這些東西都變成highly un correlate 的話呢	7-16
那他們等於說你可以想像	7-16
我大概比較可以假設他們是zero	7-16
我如果假設他們是zero 的話呢	7-16
那就簡單很多 我這邊都是零	7-16
於是我這個matrix 只有三十九個參數	7-16
你否則要三十九的平方的參數就很多很多 喔	7-16
那事實上我們後後來真的在做的時候	7-16
在很多的情況之下	7-16
我們都會做這個假設	7-16
說 它的每一個gaussian 的這個covariance matrix	7-16
我們都說它是 我們都說它是diagonal	7-16
你就讓它是假設外面是零	7-16
那這個假設為什麼可以成立	7-16
就是因為我這這邊是做了d d c t	7-16
等於是經過了這個d c t 之後	7-16
我們比較可以做這個假設	7-16
因為它transform 出來的東西比較是un correlate 喔	7-16
所以呢我們可以 可以做這個假設說他們是un correlate	7-16
所以這些東西呢就可以假設它是零	7-16
在很多時候是可以這樣做的	7-16
當然在某些情形你如果要做特別精細的話	7-16
你可能還是不能做這個假設	7-16
你還是要讓他這個所有的維都跑進去	7-16
但是很多時候我們是這樣做這個假設	7-16
好那這個是 這個 阿 阿 這個 我們做這個 這個 dis 這個d c t 的其中一個原因	7-16
那還有這個地方我想有一點寫錯	7-16
喔 因為我們其實應該是 你如果看這個圖就知道	7-16
我們應該是這個y t 的m	7-16
取絕對值平方 取log	7-16
再做這個cosine transform	7-16
所以呢看起來這裡應該是	7-16
取絕對值平方應該是掉了一個平方	7-16
有了平方之後再取log	7-16
之後這個東西	7-16
乘上一堆cosine 加起來	7-16
這個就是cosine transform	7-16
就是我現在把這個東西改成cosine 嘛	7-16
就這個東西乘上一堆cosine 加起來嘛	7-16
所以應該是 這個地方應該是有一平方的漏掉了	7-16
ok 那麼底下的這件事情	7-16
是跟我們前面的 連 連在一起的	7-16
我們跟前面這件事情連在一起來解釋	7-16
我我們休息十分鐘好了喔	7-16
喔 我說一下我們的這個期中考的schedule 喔	7-16
我們現在今天是四月	7-16
今天是四月十八	7-16
下週是二十五	7-16
再下週是五月 二號 九號 十六號	7-16
我們期中考的範圍是到第八點零考完	7-16
那麼我估計大概今天可以把七點零講完所以下週大概可以把八點零講完	7-16
所以大概是 期中考的範圍大概到這裡	7-16
是大概講完	7-16
所以呢過一週兩週	7-16
合理的期中考時間可能是在九或者十六	7-16
那我的我的這個plane 是在	7-16
這一週 因為這一週我出國	7-16
那麼如果在我出國的時候考期中考	7-16
可能是最不需要補課的一個狀況	7-16
喔 所以呢我現在的plane 是	7-16
就是我們在今天把七點零講完下週把八點零講完	7-16
這個時候我們把basic 全部結束	7-16
這是我們期期中考範圍到這裡為止	7-16
ok	7-16
我們現在來解釋剛才這邊沒有說的一個就是這個嗯	7-16
嗯在做log 的時候我們底下這一段話在說的東西跟	7-16
這邊的我底下也有這段話	7-16
那這兩段話在講的意思是什麼呢	7-16
那就是我們剛才在這邊所講的啊已經擦掉了	7-16
就是我們的這個	7-16
你可以想像成是	7-16
這個你可以想像成是一個進去是u n	7-16
然後出來是x n	7-16
那這是我的vocal tract 我的發聲的這個脣齒口型的變變化我們叫做g n	7-16
那其實x n 呢是u n 跟g n 的convolution 的一個數學關係是這樣子的	7-16
那麼這個意思你等你可以想像呢是說	7-16
我們所聽到的每一個聲音	7-16
是有兩個部分的現象	7-16
一個部分的現象是這個excitation 裡面發生的	7-16
一個現象是在這個g n 裡面發生的	7-16
那麼那麼這個g n 裡面的東西呢等於是在描述我的口型的那些東西	7-16
而excitation 是在描述我我進去的這個氣流的	7-16
那這兩種現象呢在我們的聲音裡面其實最後是混在一團	7-16
對不對	7-16
經過這個運算混成一團	7-16
在time domain 上得到一堆像這樣子	7-16
或者說frequency 的到一堆像這樣子	7-16
那都是把它們這兩個混在一起了	7-16
那麼你如果是這樣子來看的話	7-16
那麼在time domain 上是一個這就是u n 跟g n	7-16
就是這個是這個excitation 的氣流	7-16
跟這個g n 是我的那個那個喔vocal tract	7-16
那這兩個是混在一起變成我的訊號	7-16
所以我的訊號是這兩個混在一起的	7-16
那這個時候你如果從frequency domain 來看的話呢	7-16
這兩個會變成相乘的意思	7-16
如果他在frequency domain 上面是用這個來呈現	7-16
它是用這個來呈現的話它也可以用這個來呈現	7-16
他們是相乘的關係	7-16
在frequency 上面是相乘的話呢	7-16
妳取了絕對值還是相乘	7-16
取了log 最後會變成相加	7-16
那這個意思是說呢	7-16
你如果是看在這個地方的話	7-16
我其實是有取絕對值有在取log	7-16
所以雖然在原來的訊號裡面他們是有這個convolution 的關係	7-16
到了這邊之後呢其實是已經變成相加的關係了	7-16
所以呢在這裡的時候其實他們已經是一個相加的關係	7-16
那這個時候呢雖然是相加不過還是兩個混在一起	7-16
加法仍然是混在一起的	7-16
所以你可以想像呢	7-16
就是兩種東西一種是描述我的口型的	7-16
一種是描述我的excitation 的	7-16
這兩種東西呢是加在一起	7-16
加在一起之後譬如說一個是這種東西另外一個可能是這種東西	7-16
它可能會還是加在上面	7-16
但是基本上呢	7-16
雖然是加在一起可是呢	7-16
我如果是看他們變化的速度的話呢	7-16
一個是vocal tract 變的很慢	7-16
而excitation 是變化的很快的	7-16
那麼也就是說呢你如果在看他的這個譬如說你以這個unvoiced 這個為例	7-16
這個是變化的很快的他的這一瞬間跟這這不一樣的東西的	7-16
或者說你看這個的話呢其實他的音高一會高它是有變化的是蠻快的	7-16
所以基本上來講他們是變化快的東西	7-16
而vocal tract 呢是變化很慢的東西	7-16
因此呢你這個時候如果做個inverse d f t 的話會怎樣呢	7-16
會把它們換到兩個兩段去	7-16
喔	7-16
也就是說我如果回到剛才的那一張來看的話	7-16
這裡	7-16
我我做inverse d f t 回到這個quefrency 的這個domain 的時候	7-16
這是那個j 就是quefrency 的那個domain 的時候呢	7-16
那基本上在這上面的時候他們是加在一起的還是加的啦	7-16
就是雖然一個變化慢一個變化快還是加在一起的	7-16
可是經過這裡之後呢	7-16
一個變化快一個變化慢就會跑到兩個不同的區段來	7-16
就是這個就是在我的這個j	7-16
就是這個quefrency 的domain 上面	7-16
在這個上面的話呢那麼	7-16
你變化比較快的一堆在這裡	7-16
變化比較慢的一堆在這裡	7-16
那麼其實呢	7-16
這堆呢跟這堆因此可以拆開來	7-16
那其中呢你可以想像成	7-16
這一堆的information 就到了這邊	7-16
ok	7-16
然後呢這上面的這些information	7-16
就這樣的意思	7-16
sil	7-16
那麼	7-16
當我這邊所畫的是把它畫在frequency domain 上	7-16
那其實呢我們是從是從這個domain 轉過來的嗎喔	7-16
那基本上在這個上面的時候我們講在這上面的時候其實這兩種現象仍然是加乘在一起的	7-16
但其實加乘在一起的時候他們其實一個是快一個是慢的	7-16
所以呢當我transform 的時候	7-16
其實會把快的transform 到比較高的j 比較大的地方	7-16
那麼這個慢的會transform 到j 比較低的地方	7-16
所以會變成這個兩段	7-16
基本上他們就somehow 雖然還是這兩個加在一起啦	7-16
你可以看成是還是加在一起不過就是說一個是在這邊嗎一個是這邊這邊比較比較沒有	7-16
一個是在這邊這邊比較沒有	7-16
好你是是是這兩個東西	7-16
還是都是整段啦還是相加的啦	7-16
只是說一個比較都在前面那一段那這邊沒有	7-16
一個是比較在這一段後面沒有	7-16
因此你這樣transform 回來的時候其實是這兩個是幾乎可以拆開來	7-16
那麼這是為什麼你在我們在這裡後來我們說	7-16
我這邊是取m 這邊只取j	7-16
這邊本來是二十多個二十多個filter 二十三到二十七個filter	7-16
可是這邊我只取十二十三個為什麼	7-16
其實就是在做這件事	7-16
我就取這個就夠了我這個就不要了	7-16
因為我們現在要的是這個formant structure	7-16
我們真正要分辨的是在這個formant structure 我要要的是這個東西	7-16
這個東西就是被這些毛我們講這上面這些毛搞的很亂	7-16
所以你不太容易抓的到它的formant 在哪裡	7-16
他這個毛非常亂	7-16
所以呢你不太知道他那個peak 在什麼位置	7-16
但是呢你如果其實peak 在這裡譬如說	7-16
那你如果把這個毛拿掉的話	7-16
就毛在這裡我把這個毛拿掉了之後	7-16
剩下這個東西的話	7-16
就得到裡面這個	7-16
就得到這個	7-16
你反而清楚知道ok 這個f 這個你的f f f 三在這裡或者什麼	7-16
sil	7-16
那等於是這樣的意思	7-16
所以呢	7-16
我們大當這裡這裡做inverse d f t transform 的時候	7-16
是有這個原因就是為什麼我只取前面後面不要了	7-16
那其實後面因為後面more or less 就是那堆毛	7-16
那麼我其實可以把它丟掉後前面反而清楚	7-16
我可以得到一個比較清楚的東西	7-16
那這個解釋我們剛才講的就是說你中間為什麼取log	7-16
這邊是在解釋為什麼取log	7-16
取log 是有這個好處就是取log 之後可以讓這兩個東西由convolution 變成加法	7-16
那麼convolution 是把整個的把整個東西完全混在一起	7-16
可是變成加法之後	7-16
是有可能像我們這邊所畫的這個情形	7-16
就是你只要他在不同的區段的話其實是拆的開來的	7-16
sil	7-16
這個就變成這樣子於是我拆開來	7-16
所以他這個有一個log 除了我們前面這些原因之外還有這個原因是把	7-16
乘法變成加法	7-16
把convolution 變成加法	7-16
那麼這邊講的也就是這件事情所以你可以把interference of excitation on formant formant structure 把他拿掉	7-16
也就是說我要的其實是這個formant structure 就是我們這個紅色的	7-16
這個紅色的這個東西	7-16
那你這堆藍色的東西毛呢你可以看成是一堆interference	7-16
是我的這個excitation 在上面的那一些破壞	7-16
我可以把他拿掉	7-16
那這樣子話呢我就可以比較清楚的得到我所要的東西喔	7-16
這個是這邊講的意思	7-16
好	7-16
有了這些之後我們現在m f c c 就出來了	7-16
就是回到剛才這張圖	7-16
就是回到剛才這張圖	7-16
那這樣子我這邊得到這個y t 的j 就是我的m f c c 了	7-16
那我們很多時候	7-16
同時也取energy	7-16
就是把那一個frame 的把這個window 的所有的energy	7-17
就是把那一個frame 的把這個window 的所有的energy 加起來就是平方相加的這個energy 也拿來	7-17
所以呢這個變成如果這是十三個加一就是十四個	7-17
或者這個是十二個加一就是十三個	7-17
或者這個是十二個加一就是十三個	7-17
那這就是我們的前面的這一組	7-17
然後我可以做一次微分跟兩次微分	7-17
那底下我們來說這個微分	7-17
微分我們之前做很簡單的解釋是說他們相減相減	7-17
這樣也可以	7-17
sil	7-17
那我們現在這個就是每一個frame 的十三維	7-17
那這個就是我剛才的那個quefrency 的j	7-17
所以呢就等於說是我們剛才這裡的零到十二嗎	7-17
對不對零到十二嗎這樣就十三維	7-17
那這個這個軸零到十二這個是quefrency 這個j 的軸	7-17
就是這邊的這個軸這零到十二	7-17
所以呢我一個frame	7-17
這裡如果我我符號沒有統一喔符號應該是用t 才對	7-17
所以這個是t	7-17
t 加一t 加二t 減一	7-17
也就是說我是在時間t 的時候取的這個取的這個window	7-17
那如果這是t t 加一t 加二的話呢	7-17
我每一個t 的時候我得到那十三維	7-17
那這樣呢就是我的這些東西	7-17
那這個時候	7-17
我怎麼做這個微分	7-17
我們之前的說法是說ok	7-17
它減它得到它	7-17
它減它得到它對不對	7-17
它減它得到它	7-17
它減它得到它	7-17
這樣做當然是可以的	7-17
這樣做出來也不錯	7-17
不過後來有人發現其實有更好的方法	7-17
那麼我們今天多半都是用這個式子	7-17
那這個式子是什麼呢我們來看一下	7-17
它是把前後的二p 加一個點	7-17
來做一次得到一個	7-17
以p 等於一而言的話就是前後三個	7-17
所以這邊所話的就是p 等於一個時候	7-17
就是你把這是t t 減一跟t 加一的這三個合起來	7-17
做一個這一個	7-17
那它怎麼做的呢是你看他分別乘上t 減m	7-17
乘上m	7-17
相加然後除以m 的平方	7-17
那這是什麼呢	7-17
你可以想像這個是我的這個喔m	7-17
這是零	7-17
負一嗯零正一正二負一負二	7-17
m 的平方呢就是零一四一四	7-17
所以呢如果說p 等於一的話	7-17
就是只取前後三個	7-17
p 等於一的話這是t	7-17
t 減一t 加一	7-17
所以就是t 減m t 加m m 從負p 到正p 嗎	7-17
對不對	7-17
所以如果m 如果p 等於一的話呢就是從	7-17
從這個t t 減一t 加一這三個	7-17
這三個裡面呢那就是取這三個	7-17
那麼分子是什麼呢	7-17
分子你看就知道是t 加一減掉t 減一	7-17
對不對它減它嗎	7-17
所以呢等於是這兩個相減的差	7-17
除以二	7-17
對不對	7-17
就是就是這個式子	7-17
它乘以正一然後呢	7-17
t 加一乘以正一t 減一乘以負一其實就是它減它嗎	7-17
就是它減它然後除以這個平方相加就是二了	7-17
所以呢就變成是它減它除以二得到它	7-17
然後呢它減它除以二得到它	7-17
等那當然p 不一定是一p 等於二如果p 等於二的話呢就是就這這樣子	7-17
如果p 等於二的話呢就變成是	7-17
它減它然後呢還有呢它減它然後呢它減它	7-17
對不對	7-17
所以呢就變成是它減它	7-17
然後呢它減它要乘以二	7-17
所以呢就會變成這個二	7-17
然後除以多少呢除以十嗎	7-17
那這些是什麼東西呢	7-17
那這些公式其實是來自linear regression	7-17
linear regression 你一定聽過的	7-17
那就是說假設我今天有一堆點	7-17
如果這堆點是x i y i 的話	7-17
我希望找一條直線	7-17
這條直線是y 等於a x 加b	7-17
使得他跟他們之間的距離最接近	7-17
這個所有所有的distance	7-17
所有的不是真的垂直距離而是他們y 的距離最接近	7-17
也就是說我要讓這個喔	7-17
a x i 加上b 減掉y i 的平方等於minimum	7-17
ok	7-17
就是我我讓這個這些個點跟y 的距離	7-17
讓他們的這個東西可以可以最接近的這條線的斜率	7-17
這條線的斜率就是這個公式	7-17
那當然我現在的時候是有一點不太一樣	7-17
就是我現在這些點	7-17
不是任意的x i y i	7-17
而是我的橫軸是整數	7-17
這個是譬如說這是t t 加一	7-17
t 加二	7-17
所以呢我這邊橫軸是整數	7-17
只是縱軸是比較不同的值	7-17
那我譬如說如果是那邊的p 等於二的話	7-17
意思是前後五個點	7-17
我把前後五個點拿來找一條直線來讓它們距離最近	7-17
那這條直線的斜率就是最後算出來的這個東西	7-17
那公式就是這個公式	7-17
那所以跟這個稍微有點不一樣的我現在y i 不是任意的	7-17
y i 是整嗯x i 不是任意的	7-17
x i 是整數	7-17
x i 是整數	7-17
那在這個情形之下我們來做這個linear regression 的話呢	7-17
我得到是這個式子	7-17
那麼因此呢他這個比比我們原來說的它減它得到它要來的更精細一點	7-17
等於是我用前後的前後的三個點五個點或者七個點	7-17
來得得到一條這個這個這個直線的來趨近他	7-17
然後那條直線的斜率	7-17
那當然這個意思是跟作微分是差不多的	7-17
那當有人用這個方法做的時候發現這個效果比我們原來講的這個兩個相減得到它要來的好	7-17
所以後來多數人要做得精細一點	7-17
都用這個方法來做喔	7-17
這就是我們作微分的方法	7-17
那麼因此呢你可以看到說是在這個case 在這裡畫的就是p 等於正負一的話	7-17
p 等於一的話	7-17
這兩這三個做起來得到這一點	7-17
那這三個做起來得到這一點	7-17
對不對這三個做起來得到這一點	7-17
等等等等	7-17
那這樣這十三維就得到第二個十三維	7-17
那我這邊可以再來一次	7-17
這個公式你看是完全相同的公式	7-17
所以呢我這邊再來一次就得到再下來十三維	7-17
那這樣的話我就全部三十九維都出來了	7-17
這個是我們的這個微分的做法	7-17
那這裡講的大概是差不多的事情	7-17
就是喔你你為什麼要做這個微分	7-18
我要有dynamic characteristics	7-18
也就是說我們我們本來你的聲音不是一個一個discrete 這個ㄚ這個ㄧ這個ㄨ不是discrete	7-18
我們本來就是在連續的變化的	7-18
你的嘴型不會在一瞬間變成另外一個	7-18
你是連續變化所以這中間變化的這個time variation	7-18
time rate of change	7-18
是非常重要的information	7-18
那你問題你這個p 要怎麼選	7-18
p 不能太大不能太小	7-18
那這個意思也是你可以想像的到的	7-18
如果p 太小的話	7-18
你就沒有真的抓到它的斜率了	7-18
對不對	7-18
你如果p p 太小的話	7-18
你只取三個點的時候	7-18
這個斜率是不太穩的你可能抓不太對只要有一點不對就不對了	7-18
所以當然要長一點比較好	7-18
喔可是你如果p 太長當然也不對	7-18
因為你如果太長其實你已經變成另外一個state	7-18
應該是不同的了	7-18
嗯所以呢你不能太太長也不好	7-18
你會把不同的state 東西弄進來	7-18
太短也不好你會抓不對	7-18
所以呢你這個p 要有好的選擇	7-18
那這個時候呢其實做這個delta 還有另外一個好處	7-18
就是我們底下所說的	7-18
你可以把convolutional noise 消掉	7-18
那這話怎麼講呢	7-18
這話其實是跟我們之前講的另外一件事情是很像的	7-18
因為我們很可能碰到這個狀況	7-18
就是我我我對麥克風說的話	7-18
說的話之後我不是立刻拿去做recognition	7-18
而是經過怎麼樣的處理之後傳送出去了	7-18
到了對方收進來之後	7-18
我才去做recognition	7-18
譬如說這個可能是server	7-18
我其實是在手機這端講完話之後在那邊才做recognition	7-18
那中間經過這個過程話我的聲音已經破掉了	7-18
這個是x n	7-18
你這邊進來是x n	7-18
可是你到這邊的時候已經變成y n 了	7-18
y n 是什麼	7-18
是他跟某一個東西的convolution	7-18
也就是說你如果中間經過某一些process 的話	7-18
我的訊號可能已經改變了	7-18
被破壞了	7-18
其實我們通過麥克風本身	7-18
麥克風也有一個破壞的的process 也好比有一個這個東西一樣	7-18
也是妳得到已經不是真實的原來的聲音已經被破壞了	7-18
你如果中間再經過別的process	7-18
再傳送什麼	7-18
都是被破壞了之後	7-18
那麼所有的這些破壞我們都可以用一個用用這東西來描述它	7-18
就是它經過某一種數學上的convolution	7-18
變成另外一個了	7-18
那如果是這樣的話會怎樣	7-18
用我們剛才的這個地方你就會看到	7-18
其實	7-18
你如果是經過某一種convolution	7-18
我最後得到的這個m f c c 是什麼	7-18
是相加的	7-18
因為我這裡面的convolution 我在m s m s 中間呢是在frequency domain 是相乘的	7-18
然後我又取了log 變成相加的所以最後是相加的	7-18
因此呢	7-18
你如果是這樣來看這邊的問題的話	7-18
你如果y n 是是這兩個的convolution 的話	7-18
我經過了m f c c 的處理之後	7-18
其實這邊你可以想像是某某某一種x 的m f c c 跟g 的m f c c 是相加的	7-18
那麼得到y 的m f c c	7-18
ok	7-18
那麼因此呢	7-18
這個m f c c 是有這個好處	7-18
就是把convolution 的這種複雜的東西呢轉成一個加法	7-18
當你變成一個加法的時候有一個好處就是像我們這邊講的	7-18
我這邊如果取取斜率	7-18
這邊是在取斜率我們現在講的微分是在取斜率	7-18
那取斜率的話就自動把那個加的東西拿掉	7-18
如果你加的東西差不多一樣的話	7-18
那你可以假設成是說這個	7-18
譬如說我講了那一句話在我講了那一句話裡面這東西是差不多的喔	7-18
如果說我是經過某一個電話的程序到server 這端去的話	7-18
那也許在我講那一句話的時間裡面這個沒什麼改變	7-18
所以這個東西是一樣的	7-18
如果是這樣的話	7-18
那我其實通通都加那個東西我現在寫成h	7-18
都是一樣的h 的話	7-18
我在作微分的過程就是把它消掉了	7-18
所以呢這個channel effect h 就自動消掉在delta 的程序裡面	7-18
那你也可以說是	7-18
我在取這個斜率好了	7-18
你如果都加了一個h 的話我只是把它整個向上搬嗎對不對	7-18
我如果都加了一個h 的話只是把它整個一起向上搬動	7-18
所以這個斜率是相同的嗎	7-18
也就是說如果這些點它所受到的那個破壞的那個h 是差不多的話	7-18
在一個很短的時間裡面沒有太大改變的話	7-18
這個加上去的東西在這裡是整個一起加的	7-18
因此呢我如果算斜率的話呢是不受影響的	7-18
所以算斜率這種就把這一部分拿掉了	7-18
那麼有了這個好處之後我們就可以了解說你這個m f c c 裡面如果多了一堆dc part 都可以自動消掉在這裡	7-18
那麼也就是說在剛才這張圖來看我們這種共三十九維	7-18
三十九維這上面這十三維是會被這種東西破壞的	7-18
這種東西是破壞是會把它破壞	7-18
可是到了這十三維跟這十三維的時候	7-18
這些h 這些g 都自動沒有了	7-18
因為我都消掉了	7-18
所以這個是它用這個的另外一個好處	7-18
好我們現在剛才以上我們把這個所有的m f c c 的東西都講完了	7-18
我們應該是把這張圖裡面的每一塊都走了一次喔	7-18
就是這張圖的這裡面的每一塊大概都走了一次	7-18
那我們現在最後一頁講的是另外一件事情	7-19
這也是這個front end process 非常重要的就是end point detection	7-19
那所謂的end point detection	7-19
是說我們其實在說話的時候有非常豐富的時段是沒在說話的	7-19
也就是說我講話鐵定是這段時間在講之後呢這段沒有在講	7-19
這段在講這段沒有在講	7-19
那麼我沒有在講的時候是怎樣呢	7-19
中間一樣會有訊號的	7-19
為什麼就是noise	7-19
只是說呢你這邊有的時候我的noise 繼續加上來就是了	7-19
也就是說我們在講話的時候	7-19
如果說我的我的voice 是這樣子的話	7-19
我沒有講話的時候呢我一樣有雜訊	7-19
這個雜訊在這邊繼續加上去	7-19
我們其實碰到是一個這樣的狀況	7-19
那麼	7-19
那所謂的這個end point detection 其實就是在你要知道哪裡是真的你的聲音哪裡是雜訊	7-19
那就recognition 而言	7-19
當然我們不能夠把不應該把這堆noise 當成是聲音拿去跑h m m 的話你跑出來就不對了	7-19
所以我們當然要知道把這個是在哪裡	7-19
那其他的原那其他的事情也是一樣	7-19
譬如說你打電話你的手機打電話的時候	7-19
你知道打電話時候是這樣子的	7-19
你講話的時候	7-19
你講話的時候	7-19
對方應該是聽的	7-19
除非他跟你吵架	7-19
然後呢	7-19
對方會講話呢應該是在你講完停下來的時候	7-19
這個時候你是聽的	7-19
然後等到你在什麼在在他講完後你才會在講	7-19
所以呢如你如果是打電話的話應該是一個這樣的情形	7-19
這兩個signal 是大概有超過一半的時間是silent	7-19
那麼因此呢我們要降低我傳送的bit rate 很重要一件事情就是我要知道哪裡是在講話	7-19
然後我就送這一段	7-19
這段我就不要送了	7-19
我不但節省bit rate 而且我沒有不要送noise	7-19
那像這些都一樣需要做這件事情就所謂的end point detection	7-19
那這有另外一個名子就是所謂的voice 就是vad 就是這個voice activity detection	7-19
你要在一堆noise 所充滿noise 環境裡面你要知道哪裡真的是voice 哪裡不是	7-19
那哪裡是真的有voice activity 哪裡沒有	7-19
喔	7-19
這就我們這邊要說的事情就是所謂的end point detection	7-19
那這些並不好做	7-19
那麼一直到今天仍然是一個很重要的研究課題因為一直沒有標準的答案	7-19
沒有很好的solution 尤其如果你背景雜訊很厲害的時候	7-19
你的在在街上在什麼很吵雜的地方的話這非常難做的事情	7-19
那麼怎麼辦	7-19
基本上可以想像有人想簡單的辦法就是push to talk 或者是push and hold to talk	7-19
一個簡單的辦法就是你讓你的東西有一個有一個按鍵	7-19
你一按之後開始講	7-19
那它就知道那算是聲音嗎	7-19
然後呢我講完再按一下	7-19
表示是關表示我不講了	7-19
sil	7-19
那這是用人手來幫助解決的辦法	7-19
那另外的辦法就是push and hold 就是你講的時候我按下去講完了放下來	7-19
那這個其實是很簡單但是有效的辦法	7-19
雖然讓user 很麻煩	7-19
那你知道這個我們今天這個電話打電話的時候這個這個這個語音信箱	7-19
據說從前一開始你要做語音信箱的時候有很大的困難	7-19
就是你你把你錄音錄進去的時候人家不曉得從哪裡開始	7-19
所以不曉的是哪一段	7-19
那過了很久有人很聰明他說	7-19
嗶一聲開始	7-19
那個那個從嗶一聲開始錄音那個嗶一聲就是告訴你end point 在哪裡	7-19
那從那個以後就語音信箱就可以解決了	7-19
那我們這裡也是一樣那那那個那個point 怎麼怎麼辦	7-19
那當然最好的辦法是讓它可以自動去聽嘛	7-19
所謂continued listening 就是你繼續在錄音但是我隨時自動判斷這裡開始	7-19
sil	7-19
如果可以自動判斷是最好啦	7-19
那怎麼做自動判斷呢我們這邊底下講的是一些自動判斷常用的方法	7-19
最簡單的就是energy threshold	7-19
我就是取energy	7-19
那假設有語音的話energy 比較大	7-19
沒有語音的話energy 比較小	7-19
noise 比較小所以我就可以就是用一個window 算那個win window 的energy	7-19
然後那個window 不斷的移動過來	7-19
那我這個可能得到一個window 在我得到這個energy 在這邊的時候很小	7-19
當我window cover 到語音的時候就會慢慢變大	7-19
然後呢可能會比較大一點	7-19
如果是這樣的話我可以定一個threshold	7-19
超過這個threshold 就算是語音哦	7-19
這是一個最簡單的辦法來做	7-19
就是energy threshold	7-19
但是呢這個常常呢就是說到底哪裡是一個哪裡是哪裡是是這個threshold 呢	7-19
你可能是需要adaptive	7-19
因為這個顯然是跟背景雜訊有關	7-19
我要在這個有一開機還沒開始講話的時候先錄當時的背景雜訊來計算一下那麼應該以哪個threshold 為準比較好	7-19
那這個辦法也有它明顯的弱點就是當我的背景雜訊變的很強	7-19
當然很可能背景雜訊很強跟這個一樣強你就沒有辦法分辨了	7-19
所以這是最簡單但是效果不見得好的方法	7-19
那不管怎樣這裡面有兩種可能的error	7-19
一種是false accepts	7-19
一種是false rejection	7-19
什麼是false accepts 呢	7-19
就是	7-19
我們有兩種一種是false accepts	7-19
一種是false rejection	7-19
false accepts 是說	7-19
明明不是聲音而我以為是	7-19
譬如說我把它切成我把它切成從這裡開始	7-19
我就把這一堆不是語音的我把它accept 成為語音了	7-19
這就是所謂的false accepts	7-19
那false rejection 是反過來	7-19
是說明明是語音的你把以為切掉了以為這個是noise	7-19
那這個呢就是false rejection	7-19
這是兩種不同的error	7-19
那常常這兩者不容易得兼	7-19
那麼你如果這個把threshold 定的高的話	7-19
你很可能是會把聲音切掉	7-19
就是產生這種情形	7-19
你把threshold 定的低的話呢很可能會把noise 收進來就是這種情形	7-19
所以呢常常不容易得兼	7-19
那這時候怎麼辦呢	7-19
我們這邊講的就是說基本上我們是覺得false accepts 比較沒有關係	7-19
因為你如果把它弄進來的話我還有辦法把他拿掉	7-19
可是你如果這邊把它切掉就沒有了	7-19
所以這兩種error 在這裡是不效果是不同的	7-19
你把聲音切掉就沒有了你那個就一定沒有了你就一定是發生error 了	7-19
可你如是把noise 弄近來你也許還有別的方法再來處理這塊	7-19
所以呢就是false accepts 還有辦法來救回來	7-19
可是false rejection 就不行	7-19
因此呢我們基本上會希望呢是這個發生這個不要發生	7-19
所以我的rejection rate 低一點	7-19
我就盡量的這個這個threshold 低一點	7-19
那這樣的話呢我我盡量讓他這個發生而這個不要發生	7-19
這它的一個基本原則	7-19
那這個時候這個怎麼辦	7-19
一個最常用的辦法就是你加一個model	7-19
叫做silence 或者noise	7-19
就是說你的本來的每一個word 都有一個model	7-19
你還有一個model 叫做silence	7-19
那麼這個時候你就可以想像假設它是它是這裡面講的三個字的話講的三個word	7-19
這是一個model 這是一個這是這三個word 其實你就把它當成前面還有一個silence	7-19
所以呢我就可以讓這個這個我用這個noise 或者silence train 一個也是一個h m m	7-19
那你就train 一個這個model 來	7-19
然後呢那麼讓它也是變成一個可能的word 可以接在word 的前面或者後面	7-19
那這是一個常用的辦法	7-19
那這個通常可以handle 一部分這樣子的問題	7-19
那底下的這個呢是在是在課本上它有一個他講的一個方法那蠻好的喔	7-19
不過這個這個在在這裡我這裡的reference 的這個的九點三的就是這個	7-19
這個就是我剛才講的這個	7-19
那它有一個方法	7-19
那麼我們簡單解釋一下就是它當成兩個state	7-19
就好比是h m m 只有兩個state 一樣	7-19
這兩個state 就是一個是speech 一個是noise	7-19
那你每一個frame 可以去算它是屬於這個還是屬於這個	7-19
如果現在是speech 的話呢下一個frame 可能還是下一個frame 可能還是	7-19
但是一講完我就會跳到silence 去	7-19
是silence 我可以繼續記住silence 然後我會跳回來	7-19
這樣就變成兩個state 的h m m	7-19
你就可以去跑它的viterbi 你一樣跑viterbi	7-19
那你就可以分出來對不對	7-19
你就是譬如說在這一段裡面它就是在這一個state 裡面跑	7-19
這邊就跳到另外一個state 去	7-19
這邊在跳另外一個state 過來	7-19
你就兩個state 的viterbi	7-19
你可以讓他這樣子來跑	7-19
那這個時候呢你這兩個state 分別都可以train 它的gaussian	7-19
用一堆gaussian mixture	7-19
就是一堆gaussian function	7-19
用我們平常的做法一樣	7-19
你都可以train 出	7-19
你現在只有兩個state	7-19
就是只有兩個state	7-19
只不過他們各自都可以回到他們自己來	7-19
然後它可以回來	7-19
這兩個state	7-19
那每一個state 呢基本上都是一堆gaussian	7-19
一樣的你就可以train 一堆一堆gaussian	7-19
那這是另外一堆gaussian	7-19
那那這些gaussian 裡面的的用什麼用energy 啦log energy delta energy 這些都這一類的東西當它的feature	7-19
你就可以train 這種東西	7-19
然後你可以調裡面的parameter	7-19
這些mean variance 都可以調	7-19
然後你就可以看到	7-19
他們跳來跳去	7-19
用這樣來來切這是一個比較比較複雜但是相當不錯的方法	7-19
你如果要看詳細在那個我剛剛講的那個課本那裡	7-19
ok 好我們今天上到這裡	7-19
那麼喔底下有這個就是說嗯這個是我們去年語言所有一有一位語言學家它提供我一堆網站	7-20
因為那個時候我們我們說這個	7-20
那些關於語音波形啊什麼那些因為都不能放在網站上	7-20
它就提供我一些這些網站都是跟語音學訊號波形這些有關的	7-20
你如果有興趣可以去去那邊看	7-20
這個這作為reference 參考	7-20
ok 好	7-20
這樣我們這一部分講到這裡	7-20
ok 喔 我們今天要講的八點零就是這裡要講的最後一塊還沒有詳細說的	8-1
就是這個我們一直在還一直圍繞著這一塊在說	8-1
那麼我們上週的七點零講的是這塊就是 frontend processing 包括怎麼求這些 feature	8-1
六點零講的是 language model 是這一塊	8-1
然後呃五點零四點零講的是 h m m 是這一塊	8-1
那現剩下就最後這一塊那這一塊其實並不容易	8-1
因為你現在得把所有的這些東西全部整合你包括要把這些的 feature vector 這些個 h m m 跟 language model lexicon 全部要整合所以呢這個地方其實是相當複雜的一塊	8-1
那就是我們今天八點零要說的事	8-1
那麼這個我們有另外一張圖其實是一樣的意思那麼在二點零裡面	8-1
對那麼我們那時候在說的事情是這件事也就是說當你的acoustic model 給你一堆h m m	8-1
language model 給你一堆n gram 之後	8-1
你這個search 在做什麼事情你可以想像成是given given 一個這個sequence of feature vector x	8-1
你希望找一個word sequence w 那能夠maximize 這個機率也就是 m a p 的principle	8-1
那這個機率呢我們可以把它變成這樣子因此呢你要算這個機率是用h m m 算的這個機率是language model 算的	8-1
那你要找一個word sequence 這兩個乘起來最大	8-1
這個也這個也就剛才那個圖這樣講起來好像很容易其實並不容易	8-1
為什麼你只要想我這個word sequence 每一個word 假設我有六萬個word 的話辭典裡面有六萬個可以用的word	8-1
那麼你這邊有每一個都有六萬個可能	8-1
所以呢你一開始的第一個你的第一個word 就有六萬個可能第二個word 又有六萬個可能每一個word 都有六萬個可能所以呢你其實就有六萬的r 次方種	8-1
那其實應該講你的第一個phone 就有假設你有六十個phone 的話第一個phone 就有六十個可能第二個phone 又有六十個可能所以你如果要把所有的通通都這樣子來做一次的話來maximize 的話其實是非常難做到的事	8-1
那也就是我們現在八點零要說的那麼在八點零裡面我們倒底用什麼辦法來做這個search 這就是所謂的search process	8-1
好那我們現在來看八點零那麼這裡所說的呃這個其實是recognition 的一個一個chord element 因為你如果沒有這塊的話你其實是沒有辦法真的執行	8-1
那麼在所有課本裡面都會講這一段大概相當於這本課本的這些跟這些或者這些	8-1
我這邊都寫or 意思是說你只要選擇一個你覺得喜歡看的就可以了	8-1
那麼這裡面呃講得最完整的可能是這個	8-1
那我這邊講的大概也是從它裡面我大概也是以它為這個主要的reference 來講的我主要是從它來做的	8-1
但是呢它講的比我講的多很多所以我只是從它裡面抽一些少部分的我來講而已喔	8-1
那它的缺點是說它寫得多而不見得那麼好看	8-1
那寫得淺而比較好看的大概是這一個這是寫得淺而比較好看的	8-1
那這個呢它講得其實相當不錯只是說它畢竟不是一本教科書它只是他的演講稿所以呢稍微難讀一點就是了	8-1
不過這三個我想你只要選任何一個來讀都可以	8-1
四十一篇paper 哦是算寫得非常好的完整而詳細的一篇paper 把這邊所有東西都講得很清楚	8-1
呃缺點是說這篇不好念就是了相當長而且不好念但是你如果認真讀這是相當好的一篇	8-1
那五的這個倒是不是我們這邊講的search	8-1
而是我底下要講的這個我們一開始先講一個古代的東西	8-1
就是所謂的dynamic time wrapping d t w	8-1
那這個東西是在還沒有h m m 之前最成功的技術	8-1
那麼其實到今天仍然有它的價值所以我們稍微提一下	8-1
那它的觀念其實也就是在做search	8-1
所以呢我們可以可以把它看成是這個search 的基本精神可以可以從這邊來看	8-1
那麼因此呢我們會先講一下這個東西	8-1
那這個是現在用得比較少了但是其實呢相當值得學一學它們的觀念所以我們稍微說一下	8-1
那這個的reference 是我們剛才的這個	8-1
那這本書是比較比較早的書九三年的書所以我們現在不太reference 它因為它講的很多東西其實之前已經不太用了	8-1
但是它的這段其實講得非常好因為那個年代其實他們花了很大功夫做這些東西喔	8-1
在還沒有h m m 他們做做得不錯的所以你如果要了解詳細的話是可以reference 這一個	8-1
好那麼我們現在說什麼是d t w	8-1
我們說過這個是在pre 我所謂的pre h m m 的時代也就是說在古代h m m 還沒有成熟之前	8-1
當時已經有非常多的語音辨識的研究跟技術	8-1
當時用的最成功的方法就是這一個	8-1
那這個這個方法even 今天仍然很好	8-1
如果你只是做small vocabulary 跟isolate word recognition 的話	8-1
那也就是說呢今天仍然有一些其實有相當多的isolate word small vocabulary 的應用是用它做的	8-1
那麼因為用它比用h m m 簡單如果你是一個這麼簡單的problem 的話	8-1
那舉個例子來講你如果去看玩具有的玩具是可以聽聲音的	8-1
你說這個這個turn to the right 它就向右邊轉你turn to the left 它向左邊轉那 sing a song 它就唱個歌它能夠聽少數的字的這種就是isolate word small vocabulary	8-1
那它怎麼做的一片晶片在裡面	8-1
它那片晶片其實裡面做的是這個因為在這種狀況之下其實這個比h m m 要簡單	8-1
但是呢你如果複雜一點的話呢它就輸給h m m 了	8-1
當你的字彙很大的時候我們說過h m m 最大的好處是它可以把小的串成大的	8-1
所以你只要有有phone 的model 就可以串成word 的model word 的model 就可以串成sentence model h m m 的好處是可以直接串起來	8-1
那它這個就沒有這個好處所以你要連起來不是不能連連起來困難比較多而且比較做不太那麼好	8-1
那麼因此呢他要做continuous speech 就比較難它比較適合做isolate	8-1
另外呢它大概vocabulary 不容易大也因為這樣就不容易大喔那麼等等這是它的缺點	8-1
但是即使如果只是做這種小問題的話	8-1
它到今天仍然是跟h m m 一樣好	8-1
大概比起正確率是完全相同的是一樣好的	8-1
那它的基本精神是什麼	8-1
就是找一條optimal path 來match 兩個template with different length	8-1
這個講different length 有點簡單化了一點其實不只是different length 而是different sequence of events	8-1
嗯我們舉個例子來講譬如說san francisco	8-1
你要辨識一個san francisco 它可能這麼長	8-1
等一下你再念一次san francisco 的時候呢san francisco 就只有這麼短	8-1
那麼你你怎麼怎麼辨識這中間的的東西呢	8-1
那麼第一個它們長度本來就不一樣你誰跟誰去比呢	8-1
而且你要講這裡面的譬如說某一段音斯它在這裡它在這裡	8-1
它也比較長它也比較短那你怎麼知道它跟它比呢	8-1
另外一個可san francisco 的這個可在這裡那它在這裡	8-1
那你怎麼知道它在這裡它應該它跟它跟它比呢	8-1
那那如果你這個都不能知道的話呢我怎麼比呢	8-1
那我們知道今天h m m 怎麼做這件事情	8-1
h m m 做這件事情是是我們用的方法是就是它的state transition	8-1
因為這些都是random 的	8-1
對不對所以它可以state 在這邊比較長也可以跳下去	8-1
所以每一個state 的的都是可長可短h m m 是用這種方式來handle 這個問題	8-1
那它這裡怎麼辦呢它就是想辦法去找一個optimal path 來match 它們	8-1
那麼我們舉例來講假設你這個叫做reference 這叫做test pattern	8-1
那我的 reference pattern 在這裡假設是比較長	8-1
test pattern 在這裡比較短	8-1
那怎麼辦呢那你可以想像的是我如果拿裡面的某一個參數來說	8-1
假設某一個parameter p one 或者p k 它這邊的值是這樣子的	8-1
那那個p k 在這邊的值呢是這樣子的	8-1
那你大概可以可以想像那麼很可能應該這一點是對應到這一點的這一點呢是對應到這一點的那麼這一點應該是對應到這一點的	8-1
同理呢起點對應到起點終點對應到終點	8-1
那你真正要找的應應該是這麼一條path	8-1
我如果可以找到那麼一條path 那條path 告訴我哪一點對應到哪一點哪一點對應到哪一點哪一點對應到哪一點	8-1
那如果是這樣的話呢我現在就可以在這延著這一條path 來比對	8-1
那這個時候就知道它應該對應到它來比它應該對應到它來比等等	8-1
喔那這個就是這個d t w 的基本精神就是這件事就是找一個optimal path mention to template with different length	8-1
那麼我剛剛講不一定是different length 而且還包括這個這個different distribution of events	8-1
那這個event 在這裡這個在這裡你要對應到它們各自應該在哪裡等等	8-1
好那麼這個基本精神是這樣真的要做的時候其實是有非常多種方法	8-1
所以我們剛才看到在還沒有h m m 的年代呢他們下了相當大的功夫喔所以你詳細你要看的話是看這一這一段	8-1
喔那我這邊大概簡單的說一下我們舉一個例子這只是一個example	8-1
不見得是最好的但是呢我們稍微看一個例子看它是怎麼做的	8-1
那基本上呢你第一件事情就是我的一個是我就是我那邊畫的一個是reference 一個是test 喔	8-1
所以一個是test template 是y 然後reference template 是x 那我怎麼樣去去這個調它們呢	8-1
那麼這個是這個x 是x i i 等於一到m	8-1
所以呢譬如說這個是x one x two 一直到x m	8-1
這是我的這個喔reference 這是我的reference template	8-1
然後我的test template 是 y j j 等於一到n	8-1
喔所以這個軸是i 然後這個軸是j	8-1
y j 呢這是y one y two 一直到y 的n	8-1
那當然 m 跟n 是沒有理由會一樣它們是兩個不同的integer	8-1
那我現在要把它們來做剛才講的那件事情找那條path	8-1
怎麼找呢我現在在這個方法裡面這只是它有諸多方法裡面的一種在這個方法裡面它說呢我先想辦法	8-1
把它們都對應到一個共同的長度l 去	8-1
那麼因此我都做一個mapping 讓它們對應到l	8-1
換句話說呢我不論是這個test template 還是這個reference template 我都重新畫一根軸	8-1
那這根軸呢變成l	8-1
這邊變成一二到l	8-1
這個軸呢叫做這個x f x 的i 對應到m	8-1
所以我這邊就變成一個 m 軸這個是一個m 軸	8-1
那麼我重新把它調成長度是l 這個index 叫做m	8-1
那麼以別於剛才那個叫做 i 的現在是m	8-1
那同理呢y 這邊我也重新做一根軸我也是一到l	8-1
那這軸叫做n 以別於剛才的那個j	8-1
那這樣子之後呢這就是我這邊寫的這個	8-1
我有一個這個function 把i 對應到m j 對應到n 那m 跟n 都變成只有長度相同都是l	8-1
那這個時候當然你有一些條件你這個mapping function 怎麼定義呢就是這邊所講的那一對應到一最後都對應到l 對不對	8-1
我一都對應到一一都對應到一那這邊的話呢m 要對應到l 這邊的話呢n 要對應到l 那就是這一排所謂的n point constrain	8-1
之後呢所謂的monotonic constraint 是說呢這樣子	8-1
那這個意思是說當然你現在這個這個m 不見得等於l 嘛你可能是要拉長或者縮短嘛	8-1
那你如果要拉長或者縮短的話	8-1
你可以想像會變成這一點對應到這一點這一點對應到這一點這一點對應到這一點是可能的我中間丟掉一些這是可以的	8-1
但是呢不可以說是這個對應到這個之後這點給我對應回來這個不可以	8-1
好那就是所謂的monotonic constraint 那你看這個意思就是這個意思喔	8-1
就是你你如果是你你不能這個你可以往前對應過去如果它比較長的話你要丟掉一些東西嘛	8-1
反過來如果它比較短的話你也可以上面丟掉一些東西嘛阿你可以這樣子嘛這個都可以嗯	8-1
但是呢你不能說這樣子嘛這個是不行的嘛就是就是就這樣的意思喔	8-1
所以這個是monotonic constraints	8-1
那有了這個之後我現在都都對應好了然後呢它最主要的動作就是底下這一步	8-1
那其實所有的這個我們說這個做法千變萬化有很多種不過基本精神就是這個這是一樣的	8-1
那這個的意思其實跟我們在跟我們在嗯四點零講的viterbi algorithm almost 是完全相同的觀念	8-1
你如果回想一下我們那時候講的viterbi 是怎樣的	8-1
如果還清楚的話我們現在說這個就非常簡單因為跟那個是一樣的意思	8-1
你記得viterbi 是怎樣的嗎	8-1
我今天如果有一個這個在時間t 要在某個state i 上面我定義一個東西叫做delta t 的i	8-1
那是指說我在時間t 走到i 的時候呢這個的某一條path	8-1
那條path 走到這裡的時候我的分數是最高的那個的分數	8-1
那如果說我現在這點每一個都求出來了的話	8-1
那麼下一下一排的t 加一呢我就只要去算這些東西裡面倒底是誰過來的就好然後一路走往前走這就是viterbi	8-1
那你如果這點還記得的話我們這邊講的其實almost 是相同的事情	8-1
那麼他這邊的 d 的m n 是個cumulate 我這邊漏寫一個字哦應該是minimum distance up to m n 應該有個minimum 這是minimum distance	8-1
那麼換句話說呢我現在如果我走到走到某一點的 n m	8-1
m 在這裡n 在這裡的這一點是m n	8-1
那有一個分數叫做d	8-1
那它是到這裡為止的所有的path 裡面分數最低的	8-1
嗯分數因為它現在是在算distance 要找一條minimum distance 所以它現在是算distance 所以是minimum	8-1
我們那邊 viterbi 我們因為那邊是在算機率所以是要maximum 這邊是算minimum	8-1
也就是說這個時候我我的d m n 呢就是我走到這邊為止的一個minimum distance 的 path 某一條	8-1
是我的我的這個分數最低的一條	8-1
那如果這點知道的話呢那麼我的這個嗯現在是怎樣呢我現在要算一條新的m n 是對所有的m plum n plum	8-1
那我們可以假設這點是m plum 這點是n plum 這個是m plum 這個是n plum 的話	8-1
那我現在要算一個新的點	8-1
這一點呢是m 跟n 這點是m n	8-1
那同樣的這點可以來自很多點	8-1
它可以從這點這樣過來也可以從另外一點這樣子過去也可以從另外一點這樣子過來	8-1
那每一點呢都相對於它有另外一條optimal path 走過去	8-1
那看看是誰加到這邊是minimum distance	8-1
那這個精神跟我們那邊講的viterbi 是完全一樣的	8-1
所不同的是在viterbi 的時候我們已經變成是一排一排這樣一行一行向前走	8-1
在這裡的時候現在講的這裡呢它並沒有規定要一行一行走	8-1
你可以從前面的任何一點往前面走就這樣不同	8-1
ok 所以呢我現在在這些個m plum n plum	8-1
我都有走到那裡為止的minimum distance path 跟它的minimum 的分數就是accumulate minimum distance	8-1
那然後呢那我的到這邊怎麼算呢就是看從誰過來分數最低嘛是minimum	8-1
那這時候我就要算從m plum n plum 走到m n 的要多加多少就是多加這個	8-1
那多加的這個是什麼呢這個東西你可以看它其實就是從m plum n plum 走到m n 的所有的多增加的這個分數	8-1
從n plum m plum 走到m不管是從這個還是這個我都可以過來嘛那就看哪一點過來的分數加起來最少	8-1
那這裡面又要稍微算複雜一點	8-1
那就是說因為我現在呢我現在從這個n 這邊走到這邊我的每一點呢我都要先由剛才那個function 對應回去	8-1
所以這個f x 跟f y 的inverse 就是這些mapping 的先對應回去到 i j	8-1
因為在i j 可以算它的distance measure 在原來的d t w 的frame work 裡面最重要就是	8-1
你任何一個frame 的x 跟任何frame 的y 你可以算他們的distance 所以它就去算這個distance	8-1
但這個distance 定義在這個上面呢	8-1
所以你現在是要把這個n plum 我現在看看這點是什麼那這點跟這點是什麼	8-1
然後你拿這個的x 跟這個的y 去做去求它們的distance	8-1
好所以呢那也就是我們這邊所說的	8-1
你從這裡m plum n plum 這 plum 這邊呢你先inverse 回去得到在原來的i 跟j 軸上面各是哪個地方	8-1
然後拿這個的x 跟這個的y 才能夠算它的distance	8-1
那你算distance 的時候你還要算另外一個東西	8-1
這個所謂的w 的delta i delta j 是什麼呢是 weight for different types of move	8-1
什麼意思呢你從這邊或者這一點或者這一點往那邊走可以有很多種走法	8-1
你可以想像的是我可以這樣子我可以這樣子走我可以這樣子走我也可能是這樣子走那我可也等等哦	8-1
那你每一種你可以讓它們分數不一樣	8-1
你可以定義如果這個是一步的話這個是一步這個可能是一點五步或者這個是什麼你可以定義它們的weight 不同好	8-1
那這就是這邊所說的這個w i w j	8-1
也就是你這個地方到底向這邊走的是w i 嘛這個走的是w j 嘛	8-1
那你看w i w j 你看是走走多少看你是走哪一種move 那你可以有不同的weight	8-1
那這樣的話呢你就可以把從你的m plum n plum 走到n m 那中間一路這個走好幾步這樣走到這邊那麼這中間的通通加起來	8-1
但是weighted by 這個這個weighting factor 那這樣子之後呢你就可以那這個精神這樣做的話呢你這個精神就跟我們那邊講的viterbi 是完全一樣的	8-1
所以呢你可以假設一開始的那一點這點很容易開始	8-1
然後呢你每一次要往上走的話呢你就是看倒底哪一個哪從哪一點走過來是這變成一個iteration 嘛你可以看出來這就是一個iteration	8-1
那這個式子跟我們viterbi 的那個iteration 是完全一樣的	8-1
那你每一次你只要看說這個前面走到這邊了我下一步走到這邊的話應該從哪裡過來分數是最低的	8-1
喔跟那邊的情形是完全一樣所不同的是我們剛才講viterbi 的話我是進步到一排一排一排變成一排一排走下來	8-1
它這邊沒有它這個你往上面走都可以	8-1
那這個的精神你基本上可以看成是就是所謂的dynamic programming	8-1
那你如果清楚的話知道它是什麼那麼如果不清楚的話呢我底下幾章在這裡有有一個在講什麼是dynamic programming	8-1
它的基本精神就是說replace the problem by a smaller sub problem 然後formulate 一個iterative procedure	8-1
你看它就是在做這件事我們講的viterbi 也是這樣子	8-1
就是你本身你要viterbi 是想要找一條optimal path 這個很難找啊你怎麼找這個optimal path	8-1
那我其實是把它reduce 成為一個簡單的多的一個sub problem 是smaller sub problem	8-1
在這個problem 裡面我只是要算這個iteration	8-1
假設我已經有optimal path 到這裡了	8-1
假設我已經有optimum path 到這裡了那我現在看的是	8-1
下一個在這裡的話應該從哪來	8-1
我只要做好這一點就行了這是一個smaller sub problem	8-1
我有了這個smaller sub problem 之後呢我就變成一個iteration 就把就可以把它做出來	8-1
那這個意思就等於是dynamic programming 的基本精神就是這樣	8-1
那我們的viterbi 其實就是在做這麼一件事	8-1
那同理這邊在做的這個也是這件事	8-1
那我的sub problem 也是一樣	8-1
如果我已經有了這些個點的minimum distance path 到這裡的話	8-1
那下一步怎麼辦我就看誰走的最近我只要做這個這一步就好了那這一步就是所謂的smaller sub problem	8-1
好那如果是這樣來看的話呢那其實大概這就是d t w 的基本精神	8-1
雖然詳細做不一定要這樣子做啦這只是這裡面這裡寫的只是諸多的方法裡面的一種而已喔	8-1
那這裡面呢其實還有很多別的譬如說呢有所謂的local constraint 跟global constraint	8-1
什麼叫local constraint 呢就是我們剛才說ok 你可以這樣子走可以這樣子走但是你可能規定說我我一次最多不能走多少步	8-1
這邊最多不能走多少步然後遠到多少程度就不能走了對不對我一次只能夠跳多這一類的就是所謂的local constraint 你可以做下各種規定	8-1
那global constraint 呢是說我們通常會規定說這邊有一個限制吧在這個範圍之外不能走	8-1
你就是說你如果走到這裡的話是有點問題走到這邊的話好像是說這整個的聲音聲音都歸給它一點點	8-1
你走到這邊來就它這麼一點點要要涵蓋所有這不太通所以你應該是最底下是不能走	8-1
同理最這邊也不能走好你如果在這邊也不通的	8-1
所以呢你會有一個global constraint 說我整個的我大概在哪個範圍之內才可以走	8-1
好這是所謂的global constraint 等等	8-1
那這些就構成所謂的d t w	8-1
我們說這個是沒有h m m 之前的古代最成功的方法	8-1
那它有很多當然它有它的缺點	8-1
一個缺點就是缺好缺少一個training 的方法你這會怎麼train	8-1
那麼其實它們怎麼train 的呢它們就是說你今天如果有譬如說san francisco	8-1
你念個二十次的話有san francisco 有長有短怎麼辦你就用這個方法把它們都warp 到一個長度	8-1
對不對你如果有二十個二十個san francisco 的training data 你就拿其中的一個當reference	8-1
其它的通通都都跟它對應成相同長度用這個方法得到一條path	8-1
然後就把以那條path 為準把它們全部平均起來變成一個做為reference	8-1
他們就當當是這樣做的也因為這樣所以它本身沒有一個統計模型	8-1
它本身欠缺統計模型所以它比較這個這個pattern 比較train 不好	8-1
那當然還有一個大問題就是它不是它沒有辦法做這個continuous speech	8-1
可以做做不太好你怎麼把它連起來你怎麼把一堆pattern 連起來你還要能夠找這條path	8-1
那變成有點難做他們當時其實都做了那但是做的不如h m m 做得好就是了	8-1
ok 關於這個d w d t w 我們說到這裡	8-1
那麼它基本上你可以想像它是一個它是就是我們剛剛講的這個dynamic programming 的一個具體的實現的做法	8-1
那麼這樣子的在search 過程其實跟我們講的viterbi 是一樣的	8-2
那其實viterbi 就是我們底下要講的我們底下要講我整個的這個problem 怎麼做其實也是用viterbi 哦	8-2
所以你再看下去的話我們底下這邊最後後來做的其實都是viterbi 我們都在都在做viterbi	8-2
那麼viterbi 的精神也就是這個dynamic programming	8-2
那這底下是在說呢其實就是這這個剛才我已經講過了	8-2
就是我們在在講四點零的時候講這個basic problem two	8-2
那個那個viterbi 我們就可以拿來做isolate work recognition	8-2
那其實那也就是一個最基本的search algorithm	8-2
那麼我們底下要說的就是這樣子的一個search algorithm	8-2
那這個基本的基本的問題就是我們剛才講的	8-2
你一開始其實我的每一個我一開始每一個phone 都有可能的	8-2
你可以想像我現在進來一個一段聲音的話	8-2
這難度在哪裡進來這段聲音的話	8-2
這是第一個 phone 第一個phone 假設我有六十個可能的phone 這第一個phone 六十個都可能第二個phone 也六十個都可能這些phone 都是六十個都有可能的	8-2
那你要知道倒底是什麼你必需一一都去找的話不得了	8-2
然後呢這些phone 可以構成某一個word 這個phone 可以構成某一個word 這個word 六萬個都有可能對不對	8-2
你每一個都有可能所以呢你怎麼樣從頭去找這件事情是一件非常難做的事	8-2
那麼比起我們之前講的這個這個要難很多就是了我們這個是蠻簡單的	8-2
你現在如果這樣想這是非常難的問題那我們底下先來看幾個例子	8-2
這都滿簡單的就是呃digit stream recognition	8-2
假設我只是辨識digit	8-2
辨識digit 的話呢假設就是零到九	8-2
我有個digit stream 不過我不知道有幾個所以呢就是譬如說三二一五一八七	8-2
那麼你每一個你第一個呢不知道是幾不過就是十個裡面的一個然後	8-2
這個呢也是十個裡面的一個你每一次就只有十個裡面的一個就是了	8-2
那你三二一五一八七這個也不是那麼容易辨識	8-2
那你想想看是怎麼做的呢	8-2
那第一個問題就是說我在這個情形裡面因為只有digit 只有digit 所以呢我沒有辭典	8-2
呃並沒有規定說哪個number 後面要接哪個 number 對不對	8-2
那麼因此呢我們沒有number 之間的constraint 沒有辭典	8-2
並沒有誰跟誰連起來是某一個word 沒有這回事兒同樣也沒有language model	8-2
所以三二一後面會接什麼是沒有n gram 的所以也沒有language model	8-2
那這時候怎麼辦你可以想像情形是這樣	8-2
就是我零到九任何一個數字都可能而任何數字講完的時候後面又可以接所有的可能	8-2
那這個情形我們如果畫h m m 的話呢這是零有零的model 一有一的model 到九有九的 model	8-2
每一個model 走完都可以從頭再走但是你不曉得從頭走幾次因為你不曉得它有幾個digit	8-2
那這樣的問題在中文的數字比英文的數字難一點	8-2
那麼難不少應該講因此你如果做中文的digit recognition digit stream recognition 正確率都會比英文的來的低一點	8-2
為什麼中文比較難呢因為中文很多音是不容易分辨的	8-2
譬如說五五五五五五倒底是幾個五我們人耳朵聽得出來是幾個五機器是很難分別是幾個五喔	8-2
然後呢一一一一是幾個一那我們還有七七的話呢是七還是七一七一跟七是非常像的對不對	8-2
甚至於七一一那麼這倒底是幾個這是很難分辨的	8-2
那麼那我們還有這個譬如說這個六跟九六跟九是非常像的	8-2
這個二跟五其實也是蠻像的那因此呢你有很多這種問題這個在英文是沒有的	8-2
你的英文six five 還是seven 還是nine 這非常清楚它們都不一樣喔	8-2
因為它們都是這個前面有子音後面有子音什麼的所以這很清楚	8-2
但是中文的話這個地方是是難度比較高的	8-2
那這個怎麼做那麼最直接的做法就是像這樣子	8-2
你可以想像呢就是就跟我我們就跑 viterbi 然後呢假設零的這個這個其實就是我們在做的viterbi 的那個的那個圖那	8-2
就就就跟這邊畫的一樣就這張圖	8-2
那你如果是零的話我有零的model 在這裡一有一的model 在這裡這是某一個零的model 或者一的model	8-2
那你就在這邊走一個那你在這邊走一個零或者一的model 就是這裡等於一個平面嘛	8-2
等於這個平面上走一次走一個零跟一的path 那這樣就是這一個平面	8-2
但是呢我一開始是零到九九個都有可能嘛	8-2
所以我就有九個平面我等於是有九個model 在這裡我一開始的這九個平面都在這上面走	8-2
所不同的是說呢我走到底的時候我立刻可以跳到任何一個	8-2
因為你譬如說一走完之後下面又可以從頭走嘛	8-2
一走完之後我沒有理由接一後面是零到九都可以接	8-2
所以你凡是走到頂的時候你立刻可以跳下來可以接到任何一個零到九的繼續走	8-2
所以呢我就變成有有這樣的九個九個平面然後呢嗯十個平面	8-2
我這樣一路每一個平面我都向上走凡是走到頂的時候我就可以立刻跳下來我就可以立刻跳下來又可以從這裡面的任何一個接下去走	8-2
那就是我們這邊講的switch to the first state of the next model at the end of the previous model	8-2
你前面一個走完你就可以switch 又從頭開始走	8-2
那這樣呢因為我不知道到底有幾個數字	8-2
那在我們而言問題很多就因為你五五五不曉得是三個五還是二個五還是一個五	8-2
那因此你並不知道到底有幾個數字你只好一路這樣走	8-2
那麼這樣走的話呢我等於是在一個三d 的一個 grid 上面在那裡在那邊走	8-2
這張圖跟這張圖意思是完全一樣的喔只是你用不同的方法來畫而已	8-2
這是一樣我先在這個平面上走走到頂的時候	8-2
我跳下來到任何一個平面又從頭開始走走到頂的時候再跳下來再走這樣子這樣我總共走n 次喔	8-2
那這樣的話我可能會造成什麼呢	8-2
就是會造成substitution deletion 跟insertion	8-2
這是我們通常講continuous speech 最大的問題就是這三個嘛	8-2
那所謂substitution 就是說你一當成是七這是substitution	8-2
那麼這個deletion 呢是說他明明說是七一但是我以為是只有一個七把那個一丟掉了這就是deletion	8-2
那麼insertion 是明明是一個七我當成七一了就是多了一個一出來這就是insertion	8-2
那這是我們講的三大type 的error	8-2
那這個等於是一個前菜你大概可以想像我們想的問題像這樣一些問題	8-2
不過這個算是簡單的因為我們總共只有十個digit 每一次就是十個你選一個而已	8-2
那底下的這張呢講的是呃這個其實是稍微容易一點就是假設有幾個digit 我已經知道了	8-3
譬如說我已經知道這是四個四個digit 三二一八我知道四個所以我就是走四次嘛	8-3
那如果是這樣的話呢我其實就是做四層	8-3
第一層也是零到九有九個平面第二層也是零到九有九個平面嗯等等我做四層	8-3
然後在第一層走完的時候一樣的我立刻跳到第二層的頭任何一個都可以接	8-3
每一層走完我都可以接到下一層的所有的頭這樣子	8-3
那所不同因為我我知道它是四個所以我走完四層之後一定要走完	8-3
到這邊沒走完的都不算我到這邊一定要走完	8-3
那這種東西呢叫做所謂的level building 那這就是一個一個level 就是假設我知道是四個的話我就是四個level	8-3
哦所以number of level 呢就是number of digits 那在這個情形之下呢你是	8-3
這句話我們剛才講了就是你你當這個last state of the previous model 走完的時候你	8-3
就自動跳到first state of the next model	8-3
在這個時候你可以從跳到任何一個那這樣走法你走到最後你也可以算算哪條path 分數最高嘛	8-3
那這個也是一個比較複雜的一個viterbi 我們這一這一頁跟上一頁這兩個都是viterbi	8-3
你一樣跑viterbi 只是說你這個比較稍微多一點稍微複雜一點而已	8-3
那在這種情形之下它的好處就是沒有insertion 沒有deletion 為什麼沒有了因為我已經知道有四個了	8-3
我知道有四個最後要走完所以辨識出來就是四個	8-3
那這個時候只會算錯不會算只會算錯不會算多了一個少了一個這個是這個情形	8-3
好那有了這些個基本的想法之後我們底下來看我們真正要講的問題是這一個	8-3
這一個就是我們剛剛講的你一開始的話你不知道是哪一個phone	8-3
然後我並不知道每一個phone 都有所有的六十個可能	8-3
然後哪些phone 可以串成一個word	8-3
你基本上可以想成我有六我每一個都有六萬個word	8-3
那這樣一路我還不曉得到底有多少個phone 我也不曉得倒底有多少個word 喔	8-3
那麼因此呢這個問題是非常複雜然後如何來做其實很不容易	8-4
那我們這邊講的是一個最基本的做法是這樣子	8-4
那最上面這個回到我們一開始所說的map principal	8-4
那這就是我們剛才講的喔就是在二點零裡面就說到	8-4
我今天的problem 是什麼problem 進來一個x 一個sequence of sequence of feature vector x	8-4
我要找一個w 就是這個word sequence w 使得這個機率最大	8-4
那個這個機率就是a posterior probability 所以這個就是m a p 的principal 這就是我們二點零所說的	8-4
那然後我們說這個怎麼這麼這個怎麼做呢	8-4
這個機率我們不會算我們就把它倒過來這是bayes theorem 把它倒過來	8-4
倒過來之後這個可以不要看	8-4
因為我們是要在在不同的w 裡面去找機率最大那一個	8-4
這個都是相同的對任何w 都一樣所以這個可以不要看我就變成算是maximum 上面這兩個相乘	8-4
上面這兩個相乘呢這一個就是這個就是h m m 所算的這個就是n gram 所算的	8-4
那基本上我們就要maximize 這個東西	8-4
那這個是沒什麼問題這個跟我們之前講的是完全一樣的	8-4
那問題只是說這個怎麼做這個幾乎不能做	8-4
因為就是我們剛才講假設你這個word 總數有六萬的話	8-4
你那個word sequence 有 r 個word 就有這麼多個word sequence	8-4
你不可能為每一個word sequence 都去算這件事	8-4
所以這個其實這個式子其實是不能不能解的	8-4
那麼我們底下就在講怎麼解這個怎麼做這件事	8-4
那為什麼它不能解第一個你可以想到這個其實你還不能做這個東西因為這個是什麼	8-4
這個是我有無限多個word sequence 我有無限多個state sequence	8-4
那麼我們舉例來講假設就就用這個就用這個來看	8-4
假設你的那個你的那個word sequence w	8-4
有一個state sequence 在這裡	8-4
那這個是時間t 你這邊好比我得到的就是x	8-4
就是我我的我這邊所輸入的我輸入的這個這個聲音的sequence feature vector sequence 在這裡	8-4
就是這個橫軸在時間上面	8-4
然後我的word sequence w 可以串成一個大的這個hidden markov model 在這裡	8-4
但這裡面呢有無限多個path 都可以走啊有非常多的path 你每一條都可以走啊	8-4
那麼你的機率這個機率應該是什麼呢	8-4
這個機率應該是這個機率就是	8-4
如果是任何一個state sequence 的話它的機率是多少	8-4
然後把所有state sequence 全部加起來應該是這樣	8-4
所以這個機率呢照說就是這個	8-4
那這個式子是什麼其實就是我們只前講的basic problem one 你如果還記得的話	8-4
我們當時寫成這樣就是在做這件事這是我們的basic problem one	8-4
那麼你要從也就是說呢你所謂的你要在這個model 裡面	8-4
你在這個model 那個那個時候我們把這個叫做lambda 叫做這個這個model	8-4
你要在這個 model 裡面看到這個o o 就是這個東西我們那時候叫做o	8-4
你要在這裡看到這個o 的話你必需要把所有可能的path 統統都加起來那就是要加這個哦	8-4
那這個非常非常大的一個加法這個做起來就會累死人	8-4
那麼因此呢光是算這個就會這麼就要那麼難算	8-4
所以呢怎麼辦那麼我們通常不這樣做我們就做一個 approximation	8-4
我們就說呢ok 照說你這個跟這個相乘去maximize	8-4
那這個又要等於這個嘛所以你就變成是這個完全一樣這只是把這個代過來	8-4
我就變成要這個東西我要用這個來算我要把所有的state sequence q 統統加起來	8-4
然後再乘上這個language model 然後再來看maximize	8-4
如果這樣的話做死人了	8-4
所以怎麼辦呢我們就做個假設	8-4
做個 approximation 說這個呢我們就不要那麼做了啦	8-4
我們就選一個這裡面看哪一條path 最大我就拿那個就好了	8-4
那這個的意思是其實跟我們在講四點零viterbi 我們就說過這件事	8-4
我們當時講的是這麼一件事其實跟這個精神是完全一樣的	8-4
我們當時說如果你要做isolate word recognition	8-4
假設我有這是零的model 這是一的model 二的model k 的model 一直到九的model	8-4
進來一個聲音我怎麼知道它是零還是一	8-4
那我就是在算某一個聲音o given lambda k 看誰最大	8-4
最大的那一個的k 就是我的答案	8-4
對不對我這是零的model 一的model 到九的model	8-4
進來一個六我怎麼知道它是六呢	8-4
我就把這個六放到每一個model 裡面去然後看誰最大最大的那個就是六k 等於六的時候會最大答案就是	8-4
六但是呢我們也可以做另外一件事情是	8-4
那這個是什麼這個就是我們的basic problem one	8-4
要算這個東西就是我們講的basic problem one	8-4
這個solution 就是所謂的 forward algorithm	8-4
但是呢我們也可以不做這個我們可以做另外一個	8-4
就是什麼就是跑viterbi	8-4
如果跑viterbi 的話你可以跑完的時候可以得到一個optimum 的probability given 某一個lambda k	8-4
然後你看誰最大	8-4
那這個東西是跑什麼這是我們的basic problem two	8-4
我們的solution 是viterbi algorithm	8-4
那這個的意思是說我跟這個有何不同	8-4
這個是我對每一個我進來一個六的時候六的所有的path 我都算進去了	8-4
也就是我們剛才講的我進來一個六的時候如果這是六的model 這是六的聲音的話呢	8-4
它所有的path 我全部都算進去了我的forward algorithm 就是把所有的全部加起來	8-4
那這個才是真正的那個機率那麼因此這個機率我得到就是這個那這個就是forward algorithm	8-4
basic problem one 得到forward algorithm 那這樣是一個正確的答案	8-4
那我如果跑viterbi 的話那我我那是左邊的那個	8-4
我如果跑右邊這個的話變成說我沒有真的去加全部的我只去找誰最大	8-4
我找到說ok 是它最大	8-4
最後走最後走到底的時候呢是它最大	8-4
那我就以最大的那個分數為準來算那麼其它我就不看了啦	8-4
那些path 因為基本上最大的那個機率大概已經告訴我誰最大了	8-4
所以呢我就只看最大的那條那條path 的機率誰最大就好	8-4
那麼因此呢我就變成我跑viterbi 找最大的那條path 那個機率然後看誰最大就好	8-4
那這兩件事情是完全不一樣的	8-4
這兩個機率是不同的	8-4
這個是把所有的path 的機率全部加起來	8-4
這個是我只找最大的那一條就好了	8-4
但是我現在重要的不是在算機率是算誰最大	8-4
那turns out 這兩個常常是相同的	8-4
就是最大的那個model 常常是同一個	8-4
最大的常是同一個	8-4
所以呢我只是在找我只是在找誰最大而已我並不是真的要算機率	8-4
所以你要算這個也可以算這個也可以	8-4
那麼如果那個model 六那個聲音真的是六的話它的最大那條path 大概就是在六裡面最大	8-4
跟你這邊把它全部算出來之後它大概也是它最大	8-4
所以呢最大的常常是同一個所以呢我就算這個跟算這個是同樣答案通通是相同的	8-4
所以當你要做isolate word recognition 的時候你用這個forward algorithm 算這個還是用viterbi 來算這個	8-4
其實沒什麼不同答案大概差不多	8-4
那你如果是是在當時去看這兩個演算法的話你也會發現	8-4
他們也沒什麼這個演算法所需要的計算量大概也差不多	8-4
好像沒有理由要選哪一個因為計算起來大概差不多你去看這兩個algorithm 大概也差不多	8-4
可是到了這裡的時候就不一樣了	8-4
我們這裡講的事情是跟那個一樣的喔這件事情從這個到這個其實就是從這個到這個就是從這個到這個	8-4
那這個是把所有的path 都算進去這個我只找最大那一條	8-4
那就是這邊講的這個這個事情那麼在這邊的時候對不對	8-4
那在這邊的時候我是要把所有的 path 全部加進去	8-4
這邊全部這個summation over 所有的state sequence q 的這件事	8-4
就相當於那邊的算那個東西就是forward algorithm 要做的事情就是所有的path 全部都要算的	8-4
而這邊的話呢就相當於viterbi 我只做一條path 我只做一條path 喔	8-4
那麼那麼這個情形就是我們剛剛講的那麼其實是完全不一樣的機率但是因為我只是要找最大的那一個	8-4
對最大的那一個而言大概是一樣的通常是差不多的所以我這樣的話省了很多事兒	8-4
因此呢我們在講isolate word 的時候從這個變成這個好像沒什麼道理因為這兩個計算量差不多	8-4
在isolate word 的時候這個跟這個反正也差不多沒有什麼不同	8-4
可是你如果像現在在這裡是一個大字彙有六萬個word 又是continuous speech	8-4
在這個情形之下的話這兩個就差別很大了	8-4
那麼因此呢我這邊就每一次只要算一條path 這個所有path 都要算那麼	8-4
那麼因此呢我們就把它簡化成為只算一條	8-4
當你簡化成為只算一條的時候其實就是簡回簡化到回到所謂的viterbi	8-4
那麼因此呢我們就回到viterbi 那麼你唯有只算一條只算最大那一條你才可能用viterbi	8-4
那才有才有辦法解出來但是因此我們底下講的都是用viterbi	8-4
那如果用viterbi 的話呢我們必需了解一點就是我們這邊講的都是一個sub optimum 的approach	8-4
什麼是sub optimum 也就是說我其實我是做了這個假設的這個假設其實不見得正確嘛	8-4
所以真正的機率是這一個我已經省我已經省掉成為這樣子了	8-4
所以這個不見得真的能夠得到我的optimum	8-4
所以我這個只是一個sub optimum	8-4
因此我這邊做的其實只是一個sub optimum 的做法	8-4
好那麼這樣子之下呢那麼我們怎麼做這件事	8-4
我現在要跑viterbi 的話呢我還是一樣	8-4
那麼這個我們剛剛也講過了就是跟我們剛剛講viterbi 的精神完全相同	8-4
我這個呃我就是formulate 一個簡單的sub problem	8-4
然後呢變成一個algorithm 變成一個iterative algorithm 這樣一路一路這樣子算過去	8-4
那麼在這個時候如果這樣算就會發現我是一個時間一個時間一個一個frame 這樣算過來的	8-4
那是為什麼他叫做所謂的time synchronous 或者frame synchronous 的意思也	8-4
就是best score at time t update from all states at t 減一	8-4
viterbi 精神就是這樣子嘛我在前面一個state 的時候的所有東西去算下一個得到一個optimal path 一個optimal 點在下一個	8-4
所以呢這就是viterbi 的做法	8-4
就是一個time 或者叫做time synchronous 或者frame synchronous viterbi search	8-4
那真正的難題在哪裡難題在底下	8-4
那你可以想像其實我現在並不是跑一個model	8-4
我不是跑一個model 把我我分別每一個去跑viterbi 那沒什麼問題現在不是的	8-4
我現在有六萬個word 而且每一個每一個word 都有六萬種可能它們是連起來的	8-4
那怎麼辦呢那最基本的這個想法就是你要有一個tree lexicon 來做為我的working structure	8-4
什麼是tree lexicon 呢就是這裡畫一個很小的例子	8-4
就是我把它的每一個音拿來建一個tree	8-4
譬如說如果是第一個音是斯後面如果是a 的話呢就變成say	8-4
如果後面接p 接e e 後面再接咳就變speak	8-4
後面這邊如果接c h 就是speech 嗯這個接這變成 spell 等等等等	8-4
那我如果有個辭典有六萬個word 我就可以如果每一個word 都告訴我它是哪些音拼起來的	8-4
我的lexicon 本來就是這件事嘛lexicon 就是lexicon 就是一個辭典裡面有所有的word 然後每一個word 都告訴我它是哪些音拼起來的	8-4
我就寫一個程式把那些建成一個這樣的tree	8-4
建成一個這樣的tree之後這裡的每一個arc 是一個hidden markov model	8-4
譬如說每一個arc 這裡是一個phone 嘛哦那麼這是一個h m m 這是一個h m m 這是一個h m m 因此我這些 h m m 是連起來的	8-4
然後你走走每走到任何一個leaf note 的地方呢就是一個word	8-4
ok 那這樣的話我構成一個tree 所謂的tree lexicon 就變成一個這樣子	8-4
那我一路走走到底的時候呢就是一個 word	8-4
那這個時候我的viterbi 怎麼走從頭開始走	8-4
那麼從頭開始走的時候呢跟我們這邊所畫的圖是一樣的唯一不同的是我現在是一個tree 不是只有一條不是只有一條而是一個tree	8-4
因此會變成怎樣呢會變成這樣我們舉個例子來講	8-4
假設我走到這裡之後分叉成為兩個的話	8-4
那其實相當於我這個平面相當於我這個平面走到這邊的時候就拆成兩個平面	8-4
對不對所以呢假設我在走viterbi 的時候	8-4
我我我這是一個很大的這變成一個tree 的h m m 了對不對	8-4
我這裡的每一每一每一個arm 其實是每一個arc 是一個h m m 那這些h m m 全部串起來變成很大的tree 嘛	8-4
那所以你如果是這樣子的話呢它就變成說是在這邊的時候它可以走這個也可以走這個平面	8-4
那麼因此呢我現在在這裡走的時候呢譬如說我這條path 走到這裡的時候是可以往這邊走	8-4
也可以在上面這條走走這邊對不對	8-4
我就可以可以這樣走嘛	8-4
那同理呢我到這邊的時候我又拆開啦	8-4
我又拆成拆成兩個	8-4
我這邊又拆開了所以走到這邊的時候呢我可以再拆開來	8-4
我這邊又拆成兩個對不對	8-4
於是我這邊又得到這一個跟這一個等於是這樣子嘛等等呢	8-4
因此你可以想像我們現在在講的事情是一個很大的h m m 的state 所構成的一個很大的一個tree	8-4
它我我我每這裡每一個小arc 可以看成一個小的h m m	8-4
但是呢我整個變成一個那麼大的我的viterbi 在上面走的話呢在這邊就開始兩邊都我本來這邊都可以走但是現在呢我到這邊就變成又可以拆分開來走這邊又可以分開來走等等等等	8-4
所以走到這邊的時候呢我又可以往這邊走跟往後面對不對	8-4
喔那就就就這樣子一直往下走的話呢那你現在的這個可以走的這個路就非常非常多了	8-4
那在這個情形之下你就可以想像為什麼要用這個viterbi	8-4
因為你如果要去算所有的state sequence 不得了	8-4
那因此呢我就我就每一次我在每一個時間上我就只算機率最大的那一條嘛對不對	8-4
我就只算即使是這樣都已經很累了因為我其實我在在每一條的時候我其實都要把它全部算出來啊對不對	8-4
按照我這邊來講我在每一個時間t 我要把全部的都算完才能算下一個嘛	8-4
那這邊已經多到不得了了所以你光是這樣走已經很累很累了	8-4
那那這是為什麼我們一定一定要用這個approximation 然後做這個viterbi 而沒有辦法再做這件事情	8-4
那這樣子當你變成一個tree 有什麼好處	8-4
變成一個tree 的最大好處應該就是這句話就是你的search processes for a segment of utterance through some common units 的話呢就可以share	8-4
什麼意思我們說我我不知道第一個word 是什麼word 它有六萬個可能	8-4
但是呢如果它是它是speak 還是speech 其實只在最後不一樣而已	8-4
前面一路走過來都是一樣的	8-4
所以呢你不需要去跑一次viterbi 去跑一個speak 的word 的viterbi	8-4
再跑一次speech 的不需要跑兩次	8-4
其實你從頭一路走你只要跑斯的跑這個音的跑這個音的到這裡為止	8-4
它們都可以share 同樣一條path 同樣的分數只有到最後才拆開來ok	8-4
那麼因此呢我到前面的這一長段跑的viterbi 就它們就可以全部都share 那就這句話的意思	8-4
那麼因此呢我這中間譬如說這個一開始當然你可以想像	8-4
假設我有六十個phone 的話這一開始理論上應該有六十個path	8-4
那這邊下去又應該有六十個path 但是事實上也沒那麼多	8-4
因為這個這個tree lexicon 告訴我說哪些音才可以構成一個word	8-4
所以呢要後面有這個word 它才會接它	8-4
譬如說s p e 之後不是接所有的音因為只有你有這些字的才會接	8-4
所以到這裡的時候呢你到這邊就接k 變成speak 接c h 變成speech 或者再接哪些音會變後面有字才有啊	8-4
所以其實有另外的好處我這邊沒有寫在這裡的就是	8-4
應該說就是這個search space reduced by the constraint given by the words in the lexicon	8-4
也就是說這個這個辭典這個辭典裡面有六萬個word	8-4
那其實這六萬個word 已經告訴我說哪些音	8-4
如果前面這三個音下來的話它不是接所有的音	8-4
只有後面有哪些字的時候它才會接那些音	8-4
那你不是所有的音都要都要找了你就只要找這些有字的音就好了	8-4
因為我現在每走我走的每一步都是因為後面有字的關係	8-4
喔所以完全根據有哪些word 來找	8-4
所以呢它把我的不是每一每一段都可以用所有的phone	8-4
所以我的search space 是reduce 因為我的辭典裡面的word 給我這些constraint	8-4
那當然這裡還有一點就是我們這邊呃沒有說的應該講這樣講比較簡化一點	8-4
這是一個phone 這是一個phone 每一個phone 它有一個h m m 然後把它串成這些word	8-4
那實際上呢我們說這個可能是一個tri phone	8-4
喔這些可能是tri phone 也一樣啊	8-4
那你如果說我的我的辭典我是用tri phone 來做的話呢	8-4
我的辭典裡面每一個word 告訴我是哪些tri phone 連起來的	8-4
那我就變成把這些tri phone 建成一個tree	8-4
那這個基本上這個這個是完全一樣那其實如果是tried phone 建成一個tree 的話那它有更多的constrain	8-4
因為如果這是哪一個tri phone 已經確定它後面會接什麼對不對	8-4
tri phone 是已經知道後面會接什麼的	8-4
所以呢這個雖然多了很多但是我後面接什麼是確定的喔	8-4
所以你真正用的可能是一個tried phone 的tree 那我們這邊沒有畫就是了我們這邊是假設是一個普通的phone 就是了	8-4
那如果是這樣做的話呢那麼會怎樣	8-4
底下這句話意思是說same tree copy reproduce at the each leaf note	8-4
那這個意思是說我真正跑起來會怎樣呢	8-4
我跑起來會變成這樣這麼大一個tree 跑死了才跑一個word 哦	8-4
你有一個很大的tree 跑他最後有一堆一堆word 有六萬個word	8-4
這是這是一個lexicon tree	8-4
你你跑一個這麼大的六萬個word 的串的所有的它的phone unit 串的這麼大的一個tree 的 h m m	8-4
你跑完的時候呢我理論上我六萬個word 都跑出來	8-4
所以我就知道這六萬個word 裡面是它的分數是多少是它的分數是多少每一個都有一個viterbi 分數就是這一個	8-4
不過這才是一個word 哦那這這後面又可以接嘛	8-4
譬如說就這個而言它又可以接另外一個tree 後面又有六萬個word	8-4
那這個後面也接另外一個tree 後面又有六萬個word 這個後面又有又有另外一個tree 等等等等	8-4
那這就是所謂的tree copy	8-4
那麼因此呢你可以想像的是這所謂的tree copy 就是你這個又又做同一個tree	8-4
那麼也就是說你的我們說第一個word 有六萬種可能第二個word 又有六萬種可能但是其實是什麼是六萬乘六萬了嘛對不對	8-4
是接它的也有六萬個可能接它的也有六萬個可能這接這六萬個都有六萬個可能嘛喔	8-4
所以呢你這個是第二個word 就變這樣子那第三個word 呢這裡面的每一個都要再接一個tree 一個一個tree 嘛對不對	8-4
所以你光是這樣想這個search 仍然是非常龐大	8-4
那這個就是我們講基本上你如果要這樣做的話就變成這樣子	8-4
那這個仍然是非常複雜的問題所以我們需要把它把它做得比較有效率一點	8-4
這就是底下要講的事情ok 我們在這休息十分	8-4
ok 我們接下去	8-4
我們剛才在講的情形就是	8-4
我的第一個word 就是一顆tree	8-4
這顆tree 就後面就有六萬個	8-4
第一個word 我就就這個這個非常大的tree 的hidden markov model	8-4
那麼走到最後有六萬個	8-4
然後之後呢那每一個走完之後應該都要再接一個	8-4
都要再接一個	8-4
所以你就接很多	8-4
這是第二個word 就有六第二個word 其實就有六萬個這個tree 在這裡喔等等	8-4
那在這個情形之下我整個的viterbi 會變成非常複雜的viterbi	8-4
不是我們原來單獨的這一個	8-4
不是單不是單獨這這一個這樣子走而已	8-4
而變成是我一路這樣子散開來一路散開來變成很大很大	8-4
那為了要讓這個比較清楚得來講我們怎麼來做這件事情	8-4
所以底下呢用用一些這個符號把它們specify 清楚它們是什麼	8-4
這講起來是蠻複雜的	8-4
不過事實上其實你如果對於我們原來所講的那個viterbi 了解的話其實是一樣的	8-4
好那我們第一個現在現在比較複雜所以我們一堆東西	8-4
第一個呢就是我要define 清楚我這是什麼這個是什麼	8-4
這個其實就是我們原來viterbi 裡面所說的那個東西	8-4
只不過現在變複雜了所以符號變多了而已	8-4
我們原來viterbi 不是這樣	8-4
當我走到時間t 在state i 的時候	8-4
我這邊所define 的一個東西叫做delta t 的i	8-4
就是走到這裡為止的一條有最高機率的path	8-4
它的機率就是這個東西	8-4
那這個東西其實就是我現在的這個東西	8-4
是一樣的東西	8-4
這個東西其實就是這個	8-4
只不過我現在的東西變複雜了所以我現在東西變一堆符號喔	8-4
那這個意思還是一樣	8-4
那我現在變成是怎樣呢你可以看到是	8-4
我在我在時間t 的時候	8-4
在state q k 啊 q t of the word w	8-4
那當我走過來走到這段是一個word w	8-4
譬如說這裡我是一堆state	8-4
那麼這一堆這個這是一個word w	8-4
那這個word w 是是我們剛才在這個一路這樣跑下來的中間的某一段嘛	8-4
譬如說我在這裡的時候從這裡跑到這裡的這條是一個word w 對不對	8-4
因為這個我從這個tree 的頭一直走到tree 的尾的時候這是某一個word	8-4
如果到這邊發現走到這邊發現這個word 是w 的話就表示說這整個path 是w	8-4
那這個w 中間會有一堆 是一堆phone 一堆state 走起來的	8-4
那這個呢就是這個	8-4
如果這樣的話呢我在時間t 的時候	8-4
停在某一個時間t 的時候停在某一個state	8-4
這個state 叫做q t	8-4
這個q t 是屬於word w 的到這裡為 止的這個分數	8-4
那麼我一路走過來	8-4
走到這裡的這個分數	8-4
就是這邊的這個 d	8-4
這個d 的這個什麼t q t w 就是這個東西	8-4
ok 其實是一樣的	8-4
那為為什麼這個w 那這邊還有什麼因為我從頭走過來嘛	8-4
我可能從從前面走走第一個tree 走第二個tree 這樣走走到這邊	8-4
這是某一個w 嘛喔	8-4
所以這是走到路的這個path 整個path 中間的某一個w 而言	8-4
在這個w 裡面的某一個state q t	8-4
我在時間t 的時候我剛好停在這裡的這個分數喔	8-4
那麼就是所以這個其實是跟跟這個意思是一樣	8-4
只是我現在比較複雜而已	8-4
那所謂的 object function 其實也就是這個東西	8-4
也就是這個一路走過來最高的機率就是所謂object function	8-4
那我就是要optimize 這個東西	8-4
我要maximize 它嘛	8-4
那麼這個呃那是什麼是best partial path any at time t is state q t for the word w	8-4
也就是說這邊有有千千萬萬個path 走過來	8-4
但是我現在講的是到這裡為止	8-4
最好的那一個	8-4
那就相當於viterbi 裡面到這裡為止最好的那一個這樣的意思	8-4
那我都是以某一個時間來算的	8-4
所以呢是時間t 的時候來算的	8-4
那你記得viterbi 裡面很重要的一件事情是做什麼back track	8-4
因為你每一次得到這個之後下一個的時候呢	8-4
我下一個state 當t 加一的時候	8-4
它可以從這上面的任何一點跳過來	8-4
depends on 誰過來的那條path 最大	8-4
那你很可能發現結果是從這一點跳到這邊的時候機率是最大的	8-4
所以結果呢	8-4
你要把到這裡為止最好的path 就變成是這一條了	8-4
對不對這是viterbi 的基本精神嘛	8-4
所以呢你你要算t 加一的時候	8-4
你得要在算從 t 的時候所有的state 都會過來的	8-4
然後看誰最大	8-4
最後最大的是這一條的話呢	8-4
你到這邊為止最大的就變成這一條了不是它了	8-4
那因此呢我我到這邊的時候我一定要記得說	8-4
哦剛才是從這裡來的	8-4
所以這是所謂的back 這是back track 的那個pointer	8-4
我要我要說ok 它是從這兒來的	8-4
這樣我一路要記得它的前一個是哪裡	8-4
那這個是viterbi 裡面很重要一件事情你要能夠記得	8-4
你剛才從你的最佳這點是從哪裡來的	8-4
這是所謂back track pointer	8-4
那我這邊也一樣也要	8-4
那就用這個符號來代表	8-4
就是說你如果是在你的這個這個你的partial path	8-4
你你的這個best partial path end 在time t in state q t for word w 跟剛才一樣	8-4
那你如果現在是在這裡的話	8-4
那你要算剛才是哪裡來的	8-4
剛才如果是在t 減一的時候	8-4
如果是這裡來的話	8-4
那你要把這個記下來說哦剛才是從這裡來的	8-4
那個那記下來的這件事情就是就是這邊的這個h	8-4
好我就記就是就是這個back track pointer	8-4
所以這個事情跟我們講的viterbi 是完全一樣的	8-4
只是說我現在要這個變得很複雜而已	8-4
好當這個沒問題之後呢	8-4
底下的這兩件事情其實說穿了也很簡單	8-4
也跟viterbi 這邊所想的事情是完全一樣的	8-4
那只是呢我現在變得複雜了	8-4
那我們要弄清楚	8-4
現在有兩種狀況	8-4
一個叫做 intra word transition	8-4
一個叫inter word transition	8-4
什麼是intra word 就是在一個word 裡面	8-4
也就是在一個tree 裡面	8-4
這是h m m only	8-4
沒有language model	8-4
當我在這裡面走的時候	8-4
當我在這一棵 tree 裡面走的時候	8-4
我是在這裡面這條路上走	8-4
那這條路其實你可以想像	8-4
最後就是一個word	8-4
是這六萬個word 裡面的一個word	8-4
那在這個word 上面走的話呢	8-4
我其實是走一個相當長的h m m 而已嘛	8-4
所以呢它是在其實就是在走h m m 的viterbi	8-4
因此呢這裡面的東西	8-4
跟這裡面這個東西	8-4
其實就是我們原來講的viterbi 這件事情是一樣的	8-4
這是h m m only	8-4
那在這個是後沒有 language model 的事情發生因為它是在算一個word	8-4
那底下呢	8-4
什麼是inter word transition 呢	8-4
是在當你這個走完我走下一個的時候	8-4
當你這個走完我要再走下一個的時候	8-4
那你你從這個跳到下一個去的時候呢	8-4
那這個時候我我是在一個h m m 跳到另外h m m	8-4
這時候中間有什麼有language model 分數要加進來	8-4
所以呢所以呢底下是所謂inter word	8-4
是這個這個時候是language model 分數要加進來	8-4
但是我我不是在h m m 裡面是在h m m 的外面	8-4
所以呢其實很簡單就是上面是跑h m m 沒有language model	8-4
底下是跑language model 沒有h m m	8-4
好那我們分別看一下這兩個情形	8-4
那就這個而言	8-4
它在說的事情其實我們先說 h m m only	8-4
也就是在intra word 裡面	8-4
也就是在一棵tree 裡面	8-4
一棵 tree 裡面的某一條path 上面的h m m	8-4
你在上面走的時候其實這個走的事情	8-4
就是在這邊走這段word	8-4
就是在走這段嘛	8-4
那走這段的情形跟這邊是完全一樣的	8-4
跟我們從前說的其實是完全一樣的	8-4
所以這個式子其實也就是我們從前講的那個式子	8-4
只不過現在看起來比較複雜一點而已	8-4
你看我要算時間t 在q t	8-4
時間t 在q t 怎麼算	8-4
我就先算t 減一的嘛	8-4
算算這個在t 減一的時候掉在q 的t 減一的時候	8-4
在這個word 裡面我現在都在這個word 裡面嘛	8-4
我是intra word 在這個word 裡面	8-4
所以呢我在t 減一的時候	8-4
我是在q 的t 減一state	8-4
對不對所以呢我在這個t 減一的時候	8-4
還是在word w 裡面的q 的t 減一的state	8-4
在這個q 的t t 減一的時候	8-4
在q 的t 減一的那個 state	8-4
上面我也有一個最高的分數	8-4
就是那個分數然後呢再加上跳過來的分數	8-4
跳過來分數有兩個	8-4
在我們當時講viterbi 的時候呢	8-4
那兩個是什麼一個是a i j	8-4
一個是b j 的o t	8-4
你如果記得我們是有這兩個東西	8-4
a i j 告訴我從這個跳到這個機率是多少	8-4
那b j 的o t 是我現在要把現在把這個新的vector	8-4
放進來放到這個state 來ok	8-4
所以就是這兩個分數	8-4
那這兩個其實就是這裡的這兩個	8-4
就是這個東西就分成這兩個這兩個就是這個	8-4
所以呢對不對就也就是說你現在從這裡的時候	8-4
我現在要算如果說這個是 t 這個是t 加一的這是t 這是t 減一的話	8-4
我現在在t 的分數是要所有的t 減一的	8-4
都有可能跳過來看誰最大	8-4
所以所有的t 減一跳過來有有兩種	8-4
一個是a i j	8-4
每一個跳過來都有個a i j	8-4
一個呢是我要把這個b 把這個o t	8-4
放到這個新的state 裡面去	8-4
這是這是state j b j 的o t	8-4
我要把這個放進來	8-4
那現在這兩個機率其實也就是我們這邊講的這兩個機率	8-4
所以你看到譬如說這個是什麼這就是a i j 嘛	8-4
這就是在word w 裡面	8-4
然後我從q t 減一跳到q t 的機率	8-4
所以這個東西其實就是a i j	8-4
就是從這邊跳到這邊的機率就是a i j	8-4
那這個是什麼呢	8-4
這個其實就是b j 的o t	8-4
因為你你你現在就是這個啊	8-4
它就是我在這個word 裡面那麼我現在是這個o t	8-4
我看到的這個第t 個 feature vector	8-4
第t 個vector 掉在q t 的機率	8-4
那其實就是這個東西喔	8-4
那你現在把這兩個它現在是寫log 用加的啦	8-4
那意思是一樣啦就是你這兩個加起來的這個	8-4
就是說就是這個這一項嘛	8-4
就是我從t q t 減一在時間從t 減一到t 的時候	8-4
我state 從t 減一到 t 的時候	8-4
那麼我要加進去的機率是這個	8-4
那然後呢因此呢我現在在算t 的時候	8-4
就是把t 減一的所有可能的q t 減一通通加起來嗯通通看起來誰最大	8-4
就跟這邊是一樣的嘛	8-4
你你現在要算t 的時候	8-4
你就把t 減一的看看是從這過來的還是從這過來從這過來看看是誰的最大	8-4
你就算那一個最大的那嘛	8-4
那那這邊其實也是完全一樣啊我現在就是把這個嗯我現在d 的t	8-4
q t 的話呢是什麼呢	8-4
就是在t 減一的時候的所有可能的q t 減一	8-4
那這邊所有可能的q t 減一在這裡	8-4
這邊所有可能在這裡就就是等於這邊的所有可能一樣的	8-4
這邊所有可能在這裡	8-4
那所有q t 減一都有一個最大的最佳的分數	8-4
再加上跳過來的時候可能的	8-4
然後加起來之後呢我在所有的q t 減一裡面看誰最大	8-4
然後知道ok 我就是從那裡過來	8-4
所以跟這個是完全一樣的情形	8-4
然後呢我現在就找到那個最大之後我就把我就得到一個下一條path	8-4
就最大的path	8-4
因此呢這邊所說的事情	8-4
跟我們原來所說的 h m m 是完全一樣的喔	8-4
只是現在整個整個變成複雜而已	8-4
其實精神是完全相同的	8-4
那底下這個式子只是在說	8-4
那我要做這個back track	8-4
我要記得從哪過來	8-4
所以剛才的maximum	8-4
倒底是誰是q t 減一	8-4
剛才是從哪一個過來的我要記得	8-4
我就把那個剛才過來那個記下來	8-4
這就是我的back track pointer	8-4
那麼說明這個我的q bar	8-4
就是指我的t 減一的時候如果現在t t 在q t 的話	8-4
那麼t 減一是從哪個state 是在哪個state	8-4
我把那個記下來	8-4
就是剛才那個裡面的maximum 的q t 減一是哪一個	8-4
把它記下來	8-4
然後然後把它放在那個back track pointer 裡面	8-4
於是我就記得剛才是從這樣過來的	8-4
好就這樣子而已	8-4
所以這個說穿了沒什麼特別符號變複雜而已	8-4
那麼其實講的就是這件事情	8-4
就是這件事情是完全一樣的	8-4
這是intra word transition	8-4
所以這個沒什麼不同	8-4
跟我們之前講的一樣	8-4
不同的是底下這個	8-4
因為我現在還會從一棵tree 接掉接到下一棵tree 去	8-4
當我從這棵tree 接到下一棵tree 的時候會怎樣呢	8-4
那就是做了一個inter word 的transition	8-4
從這個word 跳到下一個word	8-4
那這個時候呢我們假設說	8-4
現在這個word 走完了叫做v	8-4
v 是一個word	8-4
然後呢q f 的v 是它的final state	8-4
畫清楚一點	8-4
我現在走完了這一個這個word 叫做v	8-4
它有好多state	8-4
然後它一直到最後	8-4
這是它的最後一個state	8-4
這是它的q f 的v	8-4
就是 v 的這個word	8-4
q f v 是v 的word 的這個final state	8-4
然後我現在要從這裡開始接下一棵tree	8-4
怎麼接法呢	8-4
我先增加一個空的state 叫做q	8-4
q 是一個空的state	8-4
沒有裡面沒有任何東西只是為了接方便起見	8-4
為了是要接底下這棵tree	8-4
所以q 底下呢就接底下這棵tree 出來	8-4
那這棵tree 底下會有會有這個六萬個word	8-4
所以呢我現在的這個那如果後面這個後面這個word 呢	8-4
叫做w 好	8-4
所以呢我現在這裡的這裡有六萬個word	8-4
不過每一個word 我們都叫它w	8-4
所以呢這個我這個v	8-4
v 現在這個word 走完了	8-4
到了最後final state	8-4
了那我現在要開始接下一個tree 了	8-4
那下個 tree 有六萬個可能我們叫它是w	8-4
那麼開始的時候我有一個pseudo initial state	8-4
這是一個空的state 只是為了串接方便起見叫做q	8-4
那麼如果是這樣的話呢	8-4
好我現在就有一個空的 state 在這裡	8-4
這是我的 v 的final state 接下去	8-4
那麼因此呢我現在這個這個低的這個分數啊我就先給它跳到這裡面	8-4
跳到q 裡面來了	8-4
是也就是說當我這個t 到這邊	8-4
如果我們在這裡我們說是這邊是我的word v	8-4
這邊是我的v word v word v	8-4
走完的時候呢	8-4
是在t 的時候	8-4
我我這個這個最後這個state呢就是我的final state	8-4
所以這個state呢 就是q f 的v	8-4
那麼這個時候呢	8-4
我增加一個空的state 是 q	8-4
所以這邊我有一個空的state 是q	8-4
我仍然在時間t 的時候就給它走過來	8-4
ok 我在時間t 的時候我讓它	8-4
在這個空的state 裡面然後開始要往下接	8-4
那這個時候我就是在在這個呃時間t 從q 開始要走這個word w 了	8-4
那麼這個時候呢我這分數怎麼算	8-4
我先算我時間同樣的時間t	8-4
是我同樣的時間t 走到v 的final state 之後	8-4
當時的分數	8-4
然後現在要加language model 的分數	8-4
那麼這裡其實在這個寫錯了這要改一下喔	8-4
這個language model 分數你我們這邊是假設就是只假設language model 我們做bi gram 就好	8-4
其實那tri gram 更複雜啦喔	8-4
你想一想就知道tri gram 是怎樣的	8-4
不過我們現在只講bi gram	8-4
bi gram 的話呢這個很直覺的以為是這個	8-4
因為我現在v 後面要接w 嘛	8-4
所以我現在就是given 這個 v 後面接w 有個bi gram	8-4
其實這個寫錯了	8-4
我們應該是要看前面的	8-4
也就是前面的這一個	8-4
這個是u	8-4
如果是這樣的話呢那個機率應該是	8-4
probability 的這個v given u	8-4
是u 後面接v 的bi gram	8-4
不是 v 後面接w 的bi gram	8-4
這寫錯了喔	8-4
為什麼	8-4
因為其實你走到這邊為止的時候	8-4
你只知道我這條路上到這邊是v 而已	8-4
我後面w 還不知道了還沒開始走	8-4
所以w 有六萬個可能	8-4
這邊有六萬個可能的w	8-4
那你不可能把這六萬個bi gram 統統加上來	8-4
那就變成有六萬個分數了這不太可能的	8-4
所以這裡其實你不太可能知道那個w 是多少	8-4
我w 還沒開始走嘛	8-4
w 還沒開始走我我沒有這六萬個word 的不知道是誰我我如果真的要這個機率的話我有六萬個是不可能加	8-4
的而是應該是走到這邊走完的時候v 知道了	8-4
因為我走到最後才知道是哪一個word 嘛	8-4
我一路在找	8-4
一路在找這個最佳的path 對不對	8-4
所以我一路走過來走到最後才知道我這個word 是v	8-4
當我知道這個word 是v 的時候我可以把這個u 後面接v 的bi gram 加進來喔	8-4
所以你剛才在這邊你v 的bi gram 沒有加進來因為我不知道是什麼我我一路找嘛	8-4
我一路找不到最後不知道它是哪一個word嘛	8-4
所以呢我找到最後的時候才知道它是v	8-4
這個時候我是把u 後面的v 加進來	8-4
ok 所以這個地方是應該是這個u 後面接v	8-4
是這個的bi gram	8-4
不是v 後面接w 的bi gram	8-4
ok 那這個意思是什麼	8-4
這個意思跟剛才這邊是一樣的	8-4
也就是說我們剛才是說我從這邊過來	8-4
它可以從前面一個時間可以可以從任何地方過來	8-4
那我這邊其實也是一樣	8-4
你可以想成我在時間t 的時候其實這邊有六萬個word	8-4
我這個一路散開來的時候	8-4
在時間t 走完的時候其實有譬如說有三百個word	8-4
我這邊有一個v one	8-4
這邊有一個v two	8-4
那這邊有一個v 三	8-4
v one v two v 三都在這個時間t 的時候結束	8-4
它們都可以跳到這個q	8-4
你可以想是這樣ok	8-4
那也就是說我們這個圖現在已經不夠畫了	8-4
這個圖現在不夠畫了因為我其實這個不是一個 one d 的	8-4
這邊是我們這邊所畫的這個tree 嘛	8-4
這邊是一個tree 的結構你長上去的時候很多啦	8-4
所以呢當你到這邊的時候	8-4
這邊譬如說你在時間t 結束的時間t 所結束的word	8-4
其實有v one v 不是只有一個v	8-4
有v one v two v 三	8-4
都在最後結束分數都在分別是在它那個path 裡面最高的	8-4
那它們都在時間t 結束	8-4
所以呢我這邊其實有有好幾個	8-4
有好幾個	8-4
那這個v one v two v 三都在時間t 的時候結束	8-4
那因此我現在要跳到這個q 來	8-4
準備接下一個word 的時候呢	8-4
我可以有好多個可以從這個跳過去也可以從這個跳過去也可以從這個跳過去	8-4
那麼因此呢我先要看它是從哪一個v 跳過來的	8-4
那就是這件事	8-4
那精神跟這邊講的是完全一樣嘛	8-4
我現在只是說是要看它是從這個word	8-4
還是從v one v two v 三的哪一個的的最後的那個final state	8-4
會跳到這個q	8-4
來它的分數才是最高的	8-4
所以呢我就分別把所有的這些我這邊有六萬個word 在這邊結束	8-4
那有的早一點有的晚一點	8-4
你可以假設在這個時間t 的時候	8-4
有三百個word 在這邊結束	8-4
在 t 加一呢又有五百個word 在這邊結束了	8-4
t 加二又有一千個word 在這邊結束等等都可能	8-4
那麼因此呢你在每一個時間都在做這件事	8-4
就是whenever 你的word 走完的時候	8-4
你word 走完的時候	8-4
你就把那個 language model 那個的word 加進bi gram 加進去之後	8-4
然後我要看到底是哪一個word	8-4
會跳到那個q 分數是最高的	8-4
我就選那一個	8-4
那這個精神跟這邊是完全一樣的喔	8-4
所以呢我現在就是每一個v 走完的時候的分數	8-4
加上那個v 接在那個前面的 u 後面的language model 分數加進去	8-4
然後看誰的v 最大	8-4
我就從那個跳過去	8-4
那麼因此呢我這樣就得到這個那這個是相對於這個	8-4
只不過我現在是從是從這個h m m 跳到下一個h m m	8-4
或者說從這個tree	8-4
跳到下一個tree 的時候的的這個	8-4
跟剛才是在裡面走不同的地方在這裡而已	8-4
那這樣子我知道是誰最大之後呢我也一樣在這裡	8-4
我把那個最大的記下來	8-4
所以就把剛才那個maximum	8-4
所以這個也寫錯了喔	8-4
這個也是應該是這個應該也是這個u 後面接v 的bi gram	8-4
這個也是寫錯了	8-4
就把剛才這個maximum 誰最大記下來	8-4
最大的那個就是我的前一個對不對	8-4
所以呢如果是這個v bar 才是最大的	8-4
我們現在v one v two v 三	8-4
都都在這邊結束後我現在看到的這個	8-4
是看到現在是最大是這個	8-4
那麼因此我就應該把它的最後最後state 接記記下來	8-4
所以我就知道它是從這樣過來的	8-4
於是我現在就知道ok 它是從這樣過來的	8-4
於是它是從這樣過來的	8-4
於是呢我後面開始接下一個tree	8-4
那麼因此我現在就把它的這個v bar記下來	8-4
做為我的所以我的那個v bar 的最後那個state	8-4
就做為我的這個back point	8-4
back 這個back track pointer	8-4
那麼於是呢那就這這是兩種transition	8-4
只要這兩種繼續操作	8-4
那我就可以一路走下去	8-4
ok 一路走下去是可以	8-4
不過這個這個還是大的不得了	8-4
所以我們要有一些辦法來簡化它	8-4
有很多種方法來簡化它因為現在這個search 你可以想像非常大	8-4
這是我們所謂的search	8-4
那怎麼簡化它呢一個最簡單的辦法就是所謂的beam search	8-4
beam search 意思是說在每一個時間t 我只保留一個sub set	8-4
of 最可能的path	8-4
其它都丟掉	8-4
你可以想像我從一開始走	8-4
它很快就長很多很多很多	8-4
長那麼多之後你簡直沒辦法處理	8-4
所以呢最簡單的辦法就是做beam search	8-4
舉例來講define 一個beam width l	8-4
我們通當講譬如l 是三百或者六百或者二百	8-4
也就是說我我我很快走過來這邊就很多很多了	8-4
那我就只保留分數最高的	8-4
那二百個還是六百個path	8-4
其它全部丟掉	8-4
那我一路走的時候呢我一路在算分數最高的那個path	8-4
之後我保留六百個譬如說	8-4
其它全部丟掉	8-4
那這樣我才有辦法往前走	8-4
那當然如果這樣走的話就表示這不是已經不是一個optimum 的了	8-4
喔這又是一個喔這又是一個是個approximation	8-4
因為你可以想像	8-4
分數最高的path 不見得從頭到尾一定分數最高嘛喔	8-4
這個龜兔賽跑的原理嘛	8-4
期中考考最好的人期末考不一定最好嘛	8-4
所以你如果一開始就把ok 期中考裡面考最好的十個人留下來其它通通殺掉的話	8-4
那到最後其實可能最好的被你殺掉了對不對	8-4
這裡也是一樣的	8-4
你這個這個一路跑過來的時候	8-4
所以你的這個這個beam width 如果保留的越大是比較好	8-4
但是你的計計算量立刻就會大很多嘛	8-4
所以這個就是怎麼選擇這個問題	8-4
通常我們兩種簡單的辦法	8-4
一種是保留一個就定義一個beam width	8-4
譬如說你就是每在每一個時間點t 上面	8-4
我永遠只keep 前六百名或前三百名	8-4
等等那這樣的話讓我的計算量不會太大	8-4
第二種我就定義一個threshold	8-4
凡是的我分數比最高分少那個threshold 之內的我都保留	8-4
不管多少個	8-4
那有的時候這裡有一百個有的時候這裡有一千個	8-4
我反正是是這個在這個threshold 之內的我都保留喔	8-4
這兩種基本上這都是我保留一個beam	8-4
然後呢我就在beam 裡面走	8-4
那我自然就已經把可能的optimum 丟掉是可能的	8-4
所以你這樣子得到不見得是最佳的	8-4
但是是接近就是了	8-4
那這是最簡單最常用的這個reducing search space 的方法	8-4
當然還有很多進一步的方法我想我們這邊就不說	8-4
你如果有興趣去看讀相關的reference 就會覺得講很多種方法	8-4
因為這個其實是一個關鍵性的問題那麼有一堆研究如何做	8-4
那麼一個例子就是ok 你也可以從acoustic model 從acoustic 的h m m 的分數裡面去看	8-4
哪一些地方h m m 看起來它比較好比較不好把它丟掉	8-4
從language model 來看	8-4
那麼哪些應該丟掉什麼這這都有	8-4
那麼另外一個非常標準的做法就是所謂的multi pass 的search	8-4
也就是說我至少分成兩個pass	8-4
那這個意思是什麼呢	8-4
就是說喔應該是講說我在我先有第一個path	8-4
用比較簡單的knowledge	8-4
簡單的constraint	8-4
我就得到一個比較簡單的比較小的search space	8-4
在第二個裡面再做複雜的	8-4
嗯這話怎麼講呢	8-4
最簡單的想法就是說譬如說tri phone	8-4
tri phone 太複雜了	8-4
我前面就只做一mono phone	8-4
我我我一開始我我我不要做那個	8-4
我不要那這裡面我tree我這個lexicon 也可以有兩種嘛	8-4
一種是phone 的	8-4
一種是tri phone 的	8-4
tri phone 數目多很多所以會複雜	8-4
我就我先不要用tri phone 我就先用這個單獨的phone 做	8-4
那這樣就比較簡單	8-4
我就可以做第一個pass	8-4
或者譬如說這個language model 那裡呢	8-4
你可以想像我們這邊只講bi gram	8-4
是因為tri gram 複雜哦	8-4
我如果tri gram 的話	8-4
我走到這裡的時候	8-4
我不但要把這個bi gram 加進來	8-4
還要把這個tri gram 加進來	8-4
那我每一次都要都要再再算一個bi gram 算一個tri gram 是會複雜	8-4
那我也可以說我在我在第一個path 的時候我只做bi gram	8-4
後面呢才做tri gram	8-4
或者說我甚至於language model 我在前面不做	8-4
我我 language model 到後面才做等等	8-4
那因此呢我的第一個path	8-4
就可以比較簡單一點	8-4
那第一個path 的的出來結果呢	8-4
我們把它做成一個word graph	8-4
或者一個n best list	8-4
什麼意思呢	8-4
所謂的一個word graph 就是所有可能的word	8-4
可能性比較高的分數比較高的word	8-4
把它的時間點通通記下來	8-4
就構成一個 word graph	8-4
這是時間點	8-4
所以呢它譬如說譬如說到這個時間為止	8-4
從這一點到這一點	8-4
是可能是w one 是某一個word	8-4
到這一點也可能是 w two 是這個word	8-4
那這邊呢可能有另外一個word 是w 三	8-4
那這邊可能有另外一個word 是w 四	8-4
那這邊可能有另外一個word 是w 五	8-4
這可能有另外一個word w 六	8-4
那這邊可能有另外一個word 是w 七	8-4
ok 所以呢我從這個時間點到這個時間點的話呢	8-4
我可能是這樣子	8-4
這個可能是w word w one 這個可能是w two	8-4
它也許是w two 的前面一半喔	8-4
那那它也許到w two 也許不是對也許是w 四的前面一半喔等等	8-4
那麼因此我就把所有可能的word 它的時間點的起點終點通通記下來	8-4
它就可以構成一個graph	8-4
那這個graph 呢其實你給我一句話我可以先把這個graph	8-4
找出來當我這個graph 找出來之後呢	8-4
那其實它告訴我我現在只要在這上面找就好了	8-4
它 either 是一三五	8-4
或者是二五	8-4
或者是一六	8-4
或者是四七等等	8-4
那搞不好這邊還有一個	8-4
譬如說這可能也是一個	8-4
這個w 八	8-4
於是也可能是一三八七對不對	8-4
那麼因此呢你就在這裡面去看喔	8-4
那麼如果這樣的意思是說我的第一個pass	8-4
基本上做法還是跟剛才一樣這樣子做	8-4
但是呢我我只用比較簡單的東西	8-4
譬如說我只用這個這個我不要用tri phone 我只用單獨的一個phone	8-4
我不要用tri gram 我只用 bi gram	8-4
什麼的話	8-4
我也可以這樣走	8-4
這個程式稍微簡單一點然後我就取最可能的分數最高的word	8-4
哪裡是可能分數最高的word	8-4
那你可以想像因為我現在六萬個word	8-4
有的早一點結束有的晚一點結束	8-4
有的早結束有的晚結束就是我們這邊所畫的就是	8-4
譬如說w one 在這邊就結束了	8-4
w two 到這邊才結束	8-4
w 四要到這兒才結束對不對	8-4
我就把這裡面分數最高的word 保留下來	8-4
就構成一個 word graph	8-4
那麼這個東西我底下就只要在這上面算就好了	8-4
那麼因此呢我這個複雜的東西	8-4
在後面算	8-4
那麼我這個時候我這個再把複雜的	8-4
那也等於是說我我這個很複雜的這個這個tree 後面接這麼多tree 後面接這麼那這個東西呢我就把它reduce 成為變成只有那樣子	8-4
不但是變成只有那樣子而且它不會發散	8-4
而是最後會reduce 到一點	8-4
不一定是一點啦你這邊可能也有也有不只一個	8-4
但是譬如說這邊還有一個w 九	8-4
但是基本上你不不會一直這樣越長越大越長越大	8-4
你你你可以限制它就這麼大	8-4
ok 於是呢我真正的複雜的tri gram 啦	8-4
或者tri phone 啦什麼這個複雜的東西	8-4
我只在這上面算	8-4
那這個search space	8-4
比原來那個要小很多很多那個太大了	8-4
那個大到無法算所以我就先我先用一些簡單的就是less knowledge 或者less constraint	8-4
用一些簡單的辦法	8-4
把那個大的tree	8-4
那個太大的那個那個那個 tree	8-4
reduce 到變成一個小的graph	8-4
然後呢我現在把這個東西	8-4
在這上面才做詳細的	8-4
那這是我們通常稱為re scoring	8-4
你現在再把你的詳細的你的tri gram tri phone	8-4
分數詳細去算	8-4
那剛才因為只用簡單的所以你那個分數不太對	8-4
我現在可以把詳細重算一次分數	8-4
所以叫做re scoring	8-4
那之後呢你可能會發現這上面雖然有這麼多種可能	8-4
其實最可能的是這條	8-4
譬如說是w two 接w 八接w 九	8-4
可能這條才是你的答案譬如說這樣子	8-4
那你就可以在 word graph 上面找出來	8-4
那這是所謂的multi pass search 的基本觀念	8-4
那當然這樣做的時候基本上你前面的這塊第一個path	8-4
所謂的這個word graph generation	8-4
其實跟那剛才那個是一樣的	8-4
只是簡單一點	8-4
我用比較簡單的knowledge 用比較簡單的constraint	8-4
譬如說我只用我我不要用tri phone 我不要用tri gram	8-4
等等我簡單一點就其實是一樣的	8-4
然後我就是保留最重要分數最高的word	8-4
譬如說在這個時間點結束是以它最高	8-4
或者你也可以再保留一個啦對不對你可以再保留	8-4
你保留若干個這個時間點結束的分數最高的	8-4
然後你在這個時間點你把它保留你這樣一路這樣你會得到一堆	8-4
那你就把它們構成一個graph	8-4
那如果是這樣子的話很可能我們可以把它畫成	8-4
這樣子這是w 十	8-4
那它們都n end 都在同一點	8-4
然後後面都可以接這些等等	8-4
那這就是所謂的 word graph	8-4
那你有了word graph 之後在word graph 上面	8-4
再用比較詳細的再重跑一次	8-4
re score 這些所有的path 之後	8-4
你算哪一條path 分數最高等等	8-4
那這是這個所謂用word graph 的方法	8-4
那麼n best list 是相同的意思	8-4
只是說呢它沒有做成這樣子的word graph	8-4
而是直接把前一百名譬如說這個n best 就是n 就是這個這個前n 個名次分數最高的word list	8-4
全部把它保留下來	8-4
那麼舉例來講在這個case 的話你就可能	8-4
就這個case 的話你可能想像的就是譬如說	8-4
一三五這是一個	8-4
w 一 w 三 w 五	8-4
這是一個一三五	8-4
那麼二二五也是一個w 二 w 五這也是一個	8-4
那麼w 四九也是一個喔等等等等	8-4
那你如果沒有把這個word graph 建起來	8-4
只是說把分數最高的一些word 的word sequence 把它通通都留下來	8-4
譬如保留前一百名或者保留前二百名或者前五十名	8-4
那就所謂的這個n 等於一百或者五十或者二百的 n best	8-4
那你就把這個list 留下來之後我重新在這上面算分數	8-4
那你可以想像這兩種那一個好呢	8-4
這個是比較精簡啦	8-4
這個可以把它們這個其實包含的東西比這個還豐富	8-4
這個只告訴我說一後面接三三後面接五	8-4
那這個其實告訴我說	8-4
一是在什麼時候結束	8-4
三在什麼時候開始	8-4
三是在什麼時候結束後面五等等	8-4
所以呢我我其實三後面還可以接八接九什麼	8-4
它都都在這邊都呈現了	8-4
所以這是一個比較精簡的描述的方法	8-4
你這樣保留一個這個word graph 的效果	8-4
會比這個好	8-4
但是這個簡單	8-4
你這個這個呢你就是把剛才一路找過來的你第一個path 也是用比較簡單的方法來做	8-4
但是我一路走過來之後我就把前一百名留下來	8-4
得到一個一百的list 喔	8-4
那這就是所謂的n best list	8-4
那這兩種方法都可以	8-4
我這上面舉的這兩個例子在說明這個這個n best list	8-4
是不如這個 word graph 來的有效喔	8-4
那像這個例子呢你常常前幾名是只差一點點	8-4
i’ll tell you what i think 還是 why i think 還是when i think	8-4
只是這個地方不對不曉得是哪一個	8-4
其它都一樣	8-4
那你如果是做保留這個n best list	8-4
就會發現常常譬如說前五名	8-4
都一樣只差一個字	8-4
那你保留這個呢你全部重算有點浪費嘛	8-4
其實你應該把它變成一個word graph	8-4
那這邊都一樣只有這個地方不同	8-4
對不對只有最後這個地方不同	8-4
那這樣子的話你的這個嗯比較有效的使用空間跟這個資訊	8-4
那所以呢這個這個這個是	8-4
word graph 這是n best list	8-4
那不管怎樣你都是這樣	8-4
所以呢我的真正的效果呢就是	8-4
我de cup 這個de couple 本來的這個複雜的search problem into a simpler process	8-4
對不對就是說我我現在就是把我整個的做的話這個太複雜了	8-4
所以呢我可以把它拆成兩半	8-4
第一半用比較簡單的東西	8-4
the first primary by acoustic scores	8-4
或者是the second by language 這也是一種辦法	8-4
我language 在在第二個做哦等等	8-4
或者這個是一個例子	8-4
這底下也是一個例子	8-4
那基本上我就是把它 de couple 成為兩個stage	8-4
或者可以更多	8-4
所謂的multi pass 不一定兩個啦你還可以第三個啦	8-4
你可以在在這邊之後	8-4
我還不做決定	8-4
我在這邊之後呢我可以這個弄一個比較複雜的word graph	8-4
在這邊再做一次re scoring 把它簡化成一個再簡單一點的再做第三次也可以哦看你要怎麼做	8-4
所以你可以分成不只一個path	8-4
那這樣的話呢就每一個path 都比較簡單	8-4
那我的search space 只有在第一個的時候很大	8-4
後面就算動縮小	8-4
縮小之後	8-4
我再做精緻的	8-4
那這是一個常用的方法好	8-4
那再下來的這一些呢是是另外一招	8-4
這個也是使用很多的	8-4
那這一招其實就是所謂的heuristic search	8-4
就是我們底下要說的	8-4
heuristic 跟這個a star	8-4
那heuristic 跟a star 呢這個基本上是a i 裡面搬來的喔	8-4
那麼各位之中如果你修a i 的課的話就講一大堆這種東西就很清楚了	8-4
那我們這邊呢稍微提一下喔	8-4
那等於就是把a i 裡面的heuristic search 搬來	8-4
那這個是一個非常有效的方法	8-4
那麼也是我們常用的喔	8-4
那我們在這裡休息十分鐘好了	8-4
ok 我們接下去	8-5
我們剛才在講的情形就是	8-5
我的第一個word 就是一顆tree	8-5
這顆tree 就後面就有六萬個	8-5
第一個word 我就就這個這個非常大的tree 的hidden markov model	8-5
那麼走到最後有六萬個	8-5
然後之後呢那每一個走完之後應該都要再接一個	8-5
都要再接一個	8-5
所以你就接很多	8-5
這是第二個word 就有六第二個word 其實就有六萬個這個tree 在這裡喔等等	8-5
那在這個情形之下我整個的viterbi 會變成非常複雜的viterbi	8-5
不是我們原來單獨的這一個	8-5
不是單不是單獨這這一個這樣子走而已	8-5
而變成是我一路這樣子散開來一路散開來變成很大很大	8-5
那為了要讓這個比較清楚得來講我們怎麼來做這件事情	8-5
所以底下呢用用一些這個符號把它們specify 清楚它們是什麼	8-5
這講起來是蠻複雜的	8-5
不過事實上其實你如果對於我們原來所講的那個viterbi 了解的話其實是一樣的	8-5
好那我們第一個現在現在比較複雜所以我們一堆東西	8-5
第一個呢就是我要define 清楚我這是什麼這個是什麼	8-5
這個其實就是我們原來viterbi 裡面所說的那個東西	8-5
只不過現在變複雜了所以符號變多了而已	8-5
我們原來viterbi 不是這樣	8-5
當我走到時間t 在state i 的時候	8-5
我這邊所define 的一個東西叫做delta t 的i	8-5
就是走到這裡為止的一條有最高機率的path	8-5
它的機率就是這個東西	8-5
那這個東西其實就是我現在的這個東西	8-5
是一樣的東西	8-5
這個東西其實就是這個	8-5
只不過我現在的東西變複雜了所以我現在東西變一堆符號喔	8-5
那這個意思還是一樣	8-5
那我現在變成是怎樣呢你可以看到是	8-5
我在我在時間t 的時候	8-5
在state q k 啊 q t of the word w	8-5
那當我走過來走到這段是一個word w	8-5
譬如說這裡我是一堆state	8-5
那麼這一堆這個這是一個word w	8-5
那這個word w 是是我們剛才在這個一路這樣跑下來的中間的某一段嘛	8-5
譬如說我在這裡的時候從這裡跑到這裡的這條是一個word w 對不對	8-5
因為這個我從這個tree 的頭一直走到tree 的尾的時候這是某一個word	8-5
如果到這邊發現走到這邊發現這個word 是w 的話就表示說這整個path 是w	8-5
那這個w 中間會有一堆 是一堆phone 一堆state 走起來的	8-5
那這個呢就是這個	8-5
如果這樣的話呢我在時間t 的時候	8-5
停在某一個時間t 的時候停在某一個state	8-5
這個state 叫做q t	8-5
這個q t 是屬於word w 的到這裡為 止的這個分數	8-5
那麼我一路走過來	8-5
走到這裡的這個分數	8-5
就是這邊的這個 d	8-5
這個d 的這個什麼t q t w 就是這個東西	8-5
ok 其實是一樣的	8-5
那為為什麼這個w 那這邊還有什麼因為我從頭走過來嘛	8-5
我可能從從前面走走第一個tree 走第二個tree 這樣走走到這邊	8-5
這是某一個w 嘛喔	8-5
所以這是走到路的這個path 整個path 中間的某一個w 而言	8-5
在這個w 裡面的某一個state q t	8-5
我在時間t 的時候我剛好停在這裡的這個分數喔	8-5
那麼就是所以這個其實是跟跟這個意思是一樣	8-5
只是我現在比較複雜而已	8-5
那所謂的 object function 其實也就是這個東西	8-5
也就是這個一路走過來最高的機率就是所謂object function	8-5
那我就是要optimize 這個東西	8-5
我要maximize 它嘛	8-5
那麼這個呃那是什麼是best partial path any at time t is state q t for the word w	8-5
也就是說這邊有有千千萬萬個path 走過來	8-5
但是我現在講的是到這裡為止	8-5
最好的那一個	8-5
那就相當於viterbi 裡面到這裡為止最好的那一個這樣的意思	8-5
那我都是以某一個時間來算的	8-5
所以呢是時間t 的時候來算的	8-5
那你記得viterbi 裡面很重要的一件事情是做什麼back track	8-5
因為你每一次得到這個之後下一個的時候呢	8-5
我下一個state 當t 加一的時候	8-5
它可以從這上面的任何一點跳過來	8-5
depends on 誰過來的那條path 最大	8-5
那你很可能發現結果是從這一點跳到這邊的時候機率是最大的	8-5
所以結果呢	8-5
你要把到這裡為止最好的path 就變成是這一條了	8-5
對不對這是viterbi 的基本精神嘛	8-5
所以呢你你要算t 加一的時候	8-5
你得要在算從 t 的時候所有的state 都會過來的	8-5
然後看誰最大	8-5
最後最大的是這一條的話呢	8-5
你到這邊為止最大的就變成這一條了不是它了	8-5
那因此呢我我到這邊的時候我一定要記得說	8-5
哦剛才是從這裡來的	8-5
所以這是所謂的back 這是back track 的那個pointer	8-5
我要我要說ok 它是從這兒來的	8-5
這樣我一路要記得它的前一個是哪裡	8-5
那這個是viterbi 裡面很重要一件事情你要能夠記得	8-5
你剛才從你的最佳這點是從哪裡來的	8-5
這是所謂back track pointer	8-5
那我這邊也一樣也要	8-5
那就用這個符號來代表	8-5
就是說你如果是在你的這個這個你的partial path	8-5
你你的這個best partial path end 在time t in state q t for word w 跟剛才一樣	8-5
那你如果現在是在這裡的話	8-5
那你要算剛才是哪裡來的	8-5
剛才如果是在t 減一的時候	8-5
如果是這裡來的話	8-5
那你要把這個記下來說哦剛才是從這裡來的	8-5
那個那記下來的這件事情就是就是這邊的這個h	8-5
好我就記就是就是這個back track pointer	8-5
所以這個事情跟我們講的viterbi 是完全一樣的	8-5
只是說我現在要這個變得很複雜而已	8-5
好當這個沒問題之後呢	8-5
底下的這兩件事情其實說穿了也很簡單	8-5
也跟viterbi 這邊所想的事情是完全一樣的	8-5
那只是呢我現在變得複雜了	8-5
那我們要弄清楚	8-5
現在有兩種狀況	8-5
一個叫做 intra word transition	8-5
一個叫inter word transition	8-5
什麼是intra word 就是在一個word 裡面	8-5
也就是在一個tree 裡面	8-5
這是h m m only	8-5
沒有language model	8-5
當我在這裡面走的時候	8-5
當我在這一棵 tree 裡面走的時候	8-5
我是在這裡面這條路上走	8-5
那這條路其實你可以想像	8-5
最後就是一個word	8-5
是這六萬個word 裡面的一個word	8-5
那在這個word 上面走的話呢	8-5
我其實是走一個相當長的h m m 而已嘛	8-5
所以呢它是在其實就是在走h m m 的viterbi	8-5
因此呢這裡面的東西	8-5
跟這裡面這個東西	8-5
其實就是我們原來講的viterbi 這件事情是一樣的	8-5
這是h m m only	8-5
那在這個是後沒有 language model 的事情發生因為它是在算一個word	8-5
那底下呢	8-5
什麼是inter word transition 呢	8-5
是在當你這個走完我走下一個的時候	8-5
當你這個走完我要再走下一個的時候	8-5
那你你從這個跳到下一個去的時候呢	8-5
那這個時候我我是在一個h m m 跳到另外h m m	8-5
這時候中間有什麼有language model 分數要加進來	8-5
所以呢所以呢底下是所謂inter word	8-5
是這個這個時候是language model 分數要加進來	8-5
但是我我不是在h m m 裡面是在h m m 的外面	8-5
所以呢其實很簡單就是上面是跑h m m 沒有language model	8-5
底下是跑language model 沒有h m m	8-5
好那我們分別看一下這兩個情形	8-5
那就這個而言	8-5
它在說的事情其實我們先說 h m m only	8-5
也就是在intra word 裡面	8-5
也就是在一棵tree 裡面	8-5
一棵 tree 裡面的某一條path 上面的h m m	8-5
你在上面走的時候其實這個走的事情	8-5
就是在這邊走這段word	8-5
就是在走這段嘛	8-5
那走這段的情形跟這邊是完全一樣的	8-5
跟我們從前說的其實是完全一樣的	8-5
所以這個式子其實也就是我們從前講的那個式子	8-5
只不過現在看起來比較複雜一點而已	8-5
你看我要算時間t 在q t	8-5
時間t 在q t 怎麼算	8-5
我就先算t 減一的嘛	8-5
算算這個在t 減一的時候掉在q 的t 減一的時候	8-5
在這個word 裡面我現在都在這個word 裡面嘛	8-5
我是intra word 在這個word 裡面	8-5
所以呢我在t 減一的時候	8-5
我是在q 的t 減一state	8-5
對不對所以呢我在這個t 減一的時候	8-5
還是在word w 裡面的q 的t 減一的state	8-5
在這個q 的t t 減一的時候	8-5
在q 的t 減一的那個 state	8-5
上面我也有一個最高的分數	8-5
就是那個分數然後呢再加上跳過來的分數	8-5
跳過來分數有兩個	8-5
在我們當時講viterbi 的時候呢	8-5
那兩個是什麼一個是a i j	8-5
一個是b j 的o t	8-5
你如果記得我們是有這兩個東西	8-5
a i j 告訴我從這個跳到這個機率是多少	8-5
那b j 的o t 是我現在要把現在把這個新的vector	8-5
放進來放到這個state 來ok	8-5
所以就是這兩個分數	8-5
那這兩個其實就是這裡的這兩個	8-5
就是這個東西就分成這兩個這兩個就是這個	8-5
所以呢對不對就也就是說你現在從這裡的時候	8-5
我現在要算如果說這個是 t 這個是t 加一的這是t 這是t 減一的話	8-5
我現在在t 的分數是要所有的t 減一的	8-5
都有可能跳過來看誰最大	8-5
所以所有的t 減一跳過來有有兩種	8-5
一個是a i j	8-5
每一個跳過來都有個a i j	8-5
一個呢是我要把這個b 把這個o t	8-5
放到這個新的state 裡面去	8-5
這是這是state j b j 的o t	8-5
我要把這個放進來	8-5
那現在這兩個機率其實也就是我們這邊講的這兩個機率	8-5
所以你看到譬如說這個是什麼這就是a i j 嘛	8-5
這就是在word w 裡面	8-5
然後我從q t 減一跳到q t 的機率	8-5
所以這個東西其實就是a i j	8-5
就是從這邊跳到這邊的機率就是a i j	8-5
那這個是什麼呢	8-5
這個其實就是b j 的o t	8-5
因為你你你現在就是這個啊	8-5
它就是我在這個word 裡面那麼我現在是這個o t	8-5
我看到的這個第t 個 feature vector	8-5
第t 個vector 掉在q t 的機率	8-5
那其實就是這個東西喔	8-5
那你現在把這兩個它現在是寫log 用加的啦	8-5
那意思是一樣啦就是你這兩個加起來的這個	8-5
就是說就是這個這一項嘛	8-5
就是我從t q t 減一在時間從t 減一到t 的時候	8-5
我state 從t 減一到 t 的時候	8-5
那麼我要加進去的機率是這個	8-5
那然後呢因此呢我現在在算t 的時候	8-5
就是把t 減一的所有可能的q t 減一通通加起來嗯通通看起來誰最大	8-5
就跟這邊是一樣的嘛	8-5
你你現在要算t 的時候	8-5
你就把t 減一的看看是從這過來的還是從這過來從這過來看看是誰的最大	8-5
你就算那一個最大的那嘛	8-5
那那這邊其實也是完全一樣啊我現在就是把這個嗯我現在d 的t	8-5
q t 的話呢是什麼呢	8-5
就是在t 減一的時候的所有可能的q t 減一	8-5
那這邊所有可能的q t 減一在這裡	8-5
這邊所有可能在這裡就就是等於這邊的所有可能一樣的	8-5
這邊所有可能在這裡	8-5
那所有q t 減一都有一個最大的最佳的分數	8-5
再加上跳過來的時候可能的	8-5
然後加起來之後呢我在所有的q t 減一裡面看誰最大	8-5
然後知道ok 我就是從那裡過來	8-5
所以跟這個是完全一樣的情形	8-5
然後呢我現在就找到那個最大之後我就把我就得到一個下一條path	8-5
就最大的path	8-5
因此呢這邊所說的事情	8-5
跟我們原來所說的 h m m 是完全一樣的喔	8-5
只是現在整個整個變成複雜而已	8-5
其實精神是完全相同的	8-5
那底下這個式子只是在說	8-5
那我要做這個back track	8-5
我要記得從哪過來	8-5
所以剛才的maximum	8-5
倒底是誰是q t 減一	8-5
剛才是從哪一個過來的我要記得	8-5
我就把那個剛才過來那個記下來	8-5
這就是我的back track pointer	8-5
那麼說明這個我的q bar	8-5
就是指我的t 減一的時候如果現在t t 在q t 的話	8-5
那麼t 減一是從哪個state 是在哪個state	8-5
我把那個記下來	8-5
就是剛才那個裡面的maximum 的q t 減一是哪一個	8-5
把它記下來	8-5
然後然後把它放在那個back track pointer 裡面	8-5
於是我就記得剛才是從這樣過來的	8-5
好就這樣子而已	8-5
所以這個說穿了沒什麼特別符號變複雜而已	8-5
那麼其實講的就是這件事情	8-5
就是這件事情是完全一樣的	8-5
這是intra word transition	8-5
所以這個沒什麼不同	8-5
跟我們之前講的一樣	8-5
不同的是底下這個	8-5
因為我現在還會從一棵tree 接掉接到下一棵tree 去	8-5
當我從這棵tree 接到下一棵tree 的時候會怎樣呢	8-5
那就是做了一個inter word 的transition	8-5
從這個word 跳到下一個word	8-5
那這個時候呢我們假設說	8-5
現在這個word 走完了叫做v	8-5
v 是一個word	8-5
然後呢q f 的v 是它的final state	8-5
畫清楚一點	8-5
我現在走完了這一個這個word 叫做v	8-5
它有好多state	8-5
然後它一直到最後	8-5
這是它的最後一個state	8-5
這是它的q f 的v	8-5
就是 v 的這個word	8-5
q f v 是v 的word 的這個final state	8-5
然後我現在要從這裡開始接下一棵tree	8-5
怎麼接法呢	8-5
我先增加一個空的state 叫做q	8-5
q 是一個空的state	8-5
沒有裡面沒有任何東西只是為了接方便起見	8-5
為了是要接底下這棵tree	8-5
所以q 底下呢就接底下這棵tree 出來	8-5
那這棵tree 底下會有會有這個六萬個word	8-5
所以呢我現在的這個那如果後面這個後面這個word 呢	8-5
叫做w 好	8-5
所以呢我現在這裡的這裡有六萬個word	8-5
不過每一個word 我們都叫它w	8-5
所以呢這個我這個v	8-5
v 現在這個word 走完了	8-5
到了最後final state	8-5
了那我現在要開始接下一個tree 了	8-5
那下個 tree 有六萬個可能我們叫它是w	8-5
那麼開始的時候我有一個pseudo initial state	8-5
這是一個空的state 只是為了串接方便起見叫做q	8-5
那麼如果是這樣的話呢	8-5
好我現在就有一個空的 state 在這裡	8-5
這是我的 v 的final state 接下去	8-5
那麼因此呢我現在這個這個低的這個分數啊我就先給它跳到這裡面	8-5
跳到q 裡面來了	8-5
是也就是說當我這個t 到這邊	8-5
如果我們在這裡我們說是這邊是我的word v	8-5
這邊是我的v word v word v	8-5
走完的時候呢	8-5
是在t 的時候	8-5
我我這個這個最後這個state呢就是我的final state	8-5
所以這個state呢 就是q f 的v	8-5
那麼這個時候呢	8-5
我增加一個空的state 是 q	8-5
所以這邊我有一個空的state 是q	8-5
我仍然在時間t 的時候就給它走過來	8-5
ok 我在時間t 的時候我讓它	8-5
在這個空的state 裡面然後開始要往下接	8-5
那這個時候我就是在在這個呃時間t 從q 開始要走這個word w 了	8-5
那麼這個時候呢我這分數怎麼算	8-5
我先算我時間同樣的時間t	8-5
是我同樣的時間t 走到v 的final state 之後	8-5
當時的分數	8-5
然後現在要加language model 的分數	8-5
那麼這裡其實在這個寫錯了這要改一下喔	8-5
這個language model 分數你我們這邊是假設就是只假設language model 我們做bi gram 就好	8-5
其實那tri gram 更複雜啦喔	8-5
你想一想就知道tri gram 是怎樣的	8-5
不過我們現在只講bi gram	8-5
bi gram 的話呢這個很直覺的以為是這個	8-5
因為我現在v 後面要接w 嘛	8-5
所以我現在就是given 這個 v 後面接w 有個bi gram	8-5
其實這個寫錯了	8-5
我們應該是要看前面的	8-5
也就是前面的這一個	8-5
這個是u	8-5
如果是這樣的話呢那個機率應該是	8-5
probability 的這個v given u	8-5
是u 後面接v 的bi gram	8-5
不是 v 後面接w 的bi gram	8-5
這寫錯了喔	8-5
為什麼	8-5
因為其實你走到這邊為止的時候	8-5
你只知道我這條路上到這邊是v 而已	8-5
我後面w 還不知道了還沒開始走	8-5
所以w 有六萬個可能	8-5
這邊有六萬個可能的w	8-5
那你不可能把這六萬個bi gram 統統加上來	8-5
那就變成有六萬個分數了這不太可能的	8-5
所以這裡其實你不太可能知道那個w 是多少	8-5
我w 還沒開始走嘛	8-5
w 還沒開始走我我沒有這六萬個word 的不知道是誰我我如果真的要這個機率的話我有六萬個是不可能加	8-5
的而是應該是走到這邊走完的時候v 知道了	8-5
因為我走到最後才知道是哪一個word 嘛	8-5
我一路在找	8-5
一路在找這個最佳的path 對不對	8-5
所以我一路走過來走到最後才知道我這個word 是v	8-5
當我知道這個word 是v 的時候我可以把這個u 後面接v 的bi gram 加進來喔	8-5
所以你剛才在這邊你v 的bi gram 沒有加進來因為我不知道是什麼我我一路找嘛	8-5
我一路找不到最後不知道它是哪一個word嘛	8-5
所以呢我找到最後的時候才知道它是v	8-5
這個時候我是把u 後面的v 加進來	8-5
ok 所以這個地方是應該是這個u 後面接v	8-5
是這個的bi gram	8-5
不是v 後面接w 的bi gram	8-5
ok 那這個意思是什麼	8-5
這個意思跟剛才這邊是一樣的	8-5
也就是說我們剛才是說我從這邊過來	8-5
它可以從前面一個時間可以可以從任何地方過來	8-5
那我這邊其實也是一樣	8-5
你可以想成我在時間t 的時候其實這邊有六萬個word	8-5
我這個一路散開來的時候	8-5
在時間t 走完的時候其實有譬如說有三百個word	8-5
我這邊有一個v one	8-5
這邊有一個v two	8-5
那這邊有一個v 三	8-5
v one v two v 三都在這個時間t 的時候結束	8-5
它們都可以跳到這個q	8-5
你可以想是這樣ok	8-5
那也就是說我們這個圖現在已經不夠畫了	8-5
這個圖現在不夠畫了因為我其實這個不是一個 one d 的	8-5
這邊是我們這邊所畫的這個tree 嘛	8-5
這邊是一個tree 的結構你長上去的時候很多啦	8-5
所以呢當你到這邊的時候	8-5
這邊譬如說你在時間t 結束的時間t 所結束的word	8-5
其實有v one v 不是只有一個v	8-5
有v one v two v 三	8-5
都在最後結束分數都在分別是在它那個path 裡面最高的	8-5
那它們都在時間t 結束	8-5
所以呢我這邊其實有有好幾個	8-5
有好幾個	8-5
那這個v one v two v 三都在時間t 的時候結束	8-5
那因此我現在要跳到這個q 來	8-5
準備接下一個word 的時候呢	8-5
我可以有好多個可以從這個跳過去也可以從這個跳過去也可以從這個跳過去	8-5
那麼因此呢我先要看它是從哪一個v 跳過來的	8-5
那就是這件事	8-5
那精神跟這邊講的是完全一樣嘛	8-5
我現在只是說是要看它是從這個word	8-5
還是從v one v two v 三的哪一個的的最後的那個final state	8-5
會跳到這個q	8-5
來它的分數才是最高的	8-5
所以呢我就分別把所有的這些我這邊有六萬個word 在這邊結束	8-5
那有的早一點有的晚一點	8-5
你可以假設在這個時間t 的時候	8-5
有三百個word 在這邊結束	8-5
在 t 加一呢又有五百個word 在這邊結束了	8-5
t 加二又有一千個word 在這邊結束等等都可能	8-5
那麼因此呢你在每一個時間都在做這件事	8-5
就是whenever 你的word 走完的時候	8-5
你word 走完的時候	8-5
你就把那個 language model 那個的word 加進bi gram 加進去之後	8-5
然後我要看到底是哪一個word	8-5
會跳到那個q 分數是最高的	8-5
我就選那一個	8-5
那這個精神跟這邊是完全一樣的喔	8-5
所以呢我現在就是每一個v 走完的時候的分數	8-5
加上那個v 接在那個前面的 u 後面的language model 分數加進去	8-5
然後看誰的v 最大	8-5
我就從那個跳過去	8-5
那麼因此呢我這樣就得到這個那這個是相對於這個	8-5
只不過我現在是從是從這個h m m 跳到下一個h m m	8-5
或者說從這個tree	8-5
跳到下一個tree 的時候的的這個	8-5
跟剛才是在裡面走不同的地方在這裡而已	8-5
那這樣子我知道是誰最大之後呢我也一樣在這裡	8-5
我把那個最大的記下來	8-5
所以就把剛才那個maximum	8-5
所以這個也寫錯了喔	8-5
這個也是應該是這個應該也是這個u 後面接v 的bi gram	8-5
這個也是寫錯了	8-5
就把剛才這個maximum 誰最大記下來	8-5
最大的那個就是我的前一個對不對	8-5
所以呢如果是這個v bar 才是最大的	8-5
我們現在v one v two v 三	8-5
都都在這邊結束後我現在看到的這個	8-5
是看到現在是最大是這個	8-5
那麼因此我就應該把它的最後最後state 接記記下來	8-5
所以我就知道它是從這樣過來的	8-5
於是我現在就知道ok 它是從這樣過來的	8-5
於是它是從這樣過來的	8-5
於是呢我後面開始接下一個tree	8-5
那麼因此我現在就把它的這個v bar記下來	8-5
做為我的所以我的那個v bar 的最後那個state	8-5
就做為我的這個back point	8-5
back 這個back track pointer	8-5
那麼於是呢那就這這是兩種transition	8-5
只要這兩種繼續操作	8-5
那我就可以一路走下去	8-5
ok 一路走下去是可以	8-6
不過這個這個還是大的不得了	8-6
所以我們要有一些辦法來簡化它	8-6
有很多種方法來簡化它因為現在這個search 你可以想像非常大	8-6
這是我們所謂的search	8-6
那怎麼簡化它呢一個最簡單的辦法就是所謂的beam search	8-6
beam search 意思是說在每一個時間t 我只保留一個sub set	8-6
of 最可能的path	8-6
其它都丟掉	8-6
你可以想像我從一開始走	8-6
它很快就長很多很多很多	8-6
長那麼多之後你簡直沒辦法處理	8-6
所以呢最簡單的辦法就是做beam search	8-6
舉例來講define 一個beam width l	8-6
我們通當講譬如l 是三百或者六百或者二百	8-6
也就是說我我我很快走過來這邊就很多很多了	8-6
那我就只保留分數最高的	8-6
那二百個還是六百個path	8-6
其它全部丟掉	8-6
那我一路走的時候呢我一路在算分數最高的那個path	8-6
之後我保留六百個譬如說	8-6
其它全部丟掉	8-6
那這樣我才有辦法往前走	8-6
那當然如果這樣走的話就表示這不是已經不是一個optimum 的了	8-6
喔這又是一個喔這又是一個是個approximation	8-6
因為你可以想像	8-6
分數最高的path 不見得從頭到尾一定分數最高嘛喔	8-6
這個龜兔賽跑的原理嘛	8-6
期中考考最好的人期末考不一定最好嘛	8-6
所以你如果一開始就把ok 期中考裡面考最好的十個人留下來其它通通殺掉的話	8-6
那到最後其實可能最好的被你殺掉了對不對	8-6
這裡也是一樣的	8-6
你這個這個一路跑過來的時候	8-6
所以你的這個這個beam width 如果保留的越大是比較好	8-6
但是你的計計算量立刻就會大很多嘛	8-6
所以這個就是怎麼選擇這個問題	8-6
通常我們兩種簡單的辦法	8-6
一種是保留一個就定義一個beam width	8-6
譬如說你就是每在每一個時間點t 上面	8-6
我永遠只keep 前六百名或前三百名	8-6
等等那這樣的話讓我的計算量不會太大	8-6
第二種我就定義一個threshold	8-6
凡是的我分數比最高分少那個threshold 之內的我都保留	8-6
不管多少個	8-6
那有的時候這裡有一百個有的時候這裡有一千個	8-6
我反正是是這個在這個threshold 之內的我都保留喔	8-6
這兩種基本上這都是我保留一個beam	8-6
然後呢我就在beam 裡面走	8-6
那我自然就已經把可能的optimum 丟掉是可能的	8-6
所以你這樣子得到不見得是最佳的	8-6
但是是接近就是了	8-6
那這是最簡單最常用的這個reducing search space 的方法	8-6
當然還有很多進一步的方法我想我們這邊就不說	8-6
你如果有興趣去看讀相關的reference 就會覺得講很多種方法	8-6
因為這個其實是一個關鍵性的問題那麼有一堆研究如何做	8-6
那麼一個例子就是ok 你也可以從acoustic model 從acoustic 的h m m 的分數裡面去看	8-6
哪一些地方h m m 看起來它比較好比較不好把它丟掉	8-6
從language model 來看	8-6
那麼哪些應該丟掉什麼這這都有	8-6
那麼另外一個非常標準的做法就是所謂的multi pass 的search	8-6
也就是說我至少分成兩個pass	8-6
那這個意思是什麼呢	8-6
就是說喔應該是講說我在我先有第一個path	8-6
用比較簡單的knowledge	8-6
簡單的constraint	8-6
我就得到一個比較簡單的比較小的search space	8-6
在第二個裡面再做複雜的	8-6
嗯這話怎麼講呢	8-6
最簡單的想法就是說譬如說tri phone	8-6
tri phone 太複雜了	8-6
我前面就只做一mono phone	8-6
我我我一開始我我我不要做那個	8-6
我不要那這裡面我tree我這個lexicon 也可以有兩種嘛	8-6
一種是phone 的	8-6
一種是tri phone 的	8-6
tri phone 數目多很多所以會複雜	8-6
我就我先不要用tri phone 我就先用這個單獨的phone 做	8-6
那這樣就比較簡單	8-6
我就可以做第一個pass	8-6
或者譬如說這個language model 那裡呢	8-6
你可以想像我們這邊只講bi gram	8-6
是因為tri gram 複雜哦	8-6
我如果tri gram 的話	8-6
我走到這裡的時候	8-6
我不但要把這個bi gram 加進來	8-6
還要把這個tri gram 加進來	8-6
那我每一次都要都要再再算一個bi gram 算一個tri gram 是會複雜	8-6
那我也可以說我在我在第一個path 的時候我只做bi gram	8-6
後面呢才做tri gram	8-6
或者說我甚至於language model 我在前面不做	8-6
我我 language model 到後面才做等等	8-6
那因此呢我的第一個path	8-6
就可以比較簡單一點	8-6
那第一個path 的的出來結果呢	8-6
我們把它做成一個word graph	8-6
或者一個n best list	8-6
什麼意思呢	8-6
所謂的一個word graph 就是所有可能的word	8-6
可能性比較高的分數比較高的word	8-6
把它的時間點通通記下來	8-6
就構成一個 word graph	8-6
這是時間點	8-6
所以呢它譬如說譬如說到這個時間為止	8-6
從這一點到這一點	8-6
是可能是w one 是某一個word	8-6
到這一點也可能是 w two 是這個word	8-6
那這邊呢可能有另外一個word 是w 三	8-6
那這邊可能有另外一個word 是w 四	8-6
那這邊可能有另外一個word 是w 五	8-6
這可能有另外一個word w 六	8-6
那這邊可能有另外一個word 是w 七	8-6
ok 所以呢我從這個時間點到這個時間點的話呢	8-6
我可能是這樣子	8-6
這個可能是w word w one 這個可能是w two	8-6
它也許是w two 的前面一半喔	8-6
那那它也許到w two 也許不是對也許是w 四的前面一半喔等等	8-6
那麼因此我就把所有可能的word 它的時間點的起點終點通通記下來	8-6
它就可以構成一個graph	8-6
那這個graph 呢其實你給我一句話我可以先把這個graph	8-6
找出來當我這個graph 找出來之後呢	8-6
那其實它告訴我我現在只要在這上面找就好了	8-6
它 either 是一三五	8-6
或者是二五	8-6
或者是一六	8-6
或者是四七等等	8-6
那搞不好這邊還有一個	8-6
譬如說這可能也是一個	8-6
這個w 八	8-6
於是也可能是一三八七對不對	8-6
那麼因此呢你就在這裡面去看喔	8-6
那麼如果這樣的意思是說我的第一個pass	8-6
基本上做法還是跟剛才一樣這樣子做	8-6
但是呢我我只用比較簡單的東西	8-6
譬如說我只用這個這個我不要用tri phone 我只用單獨的一個phone	8-6
我不要用tri gram 我只用 bi gram	8-6
什麼的話	8-6
我也可以這樣走	8-6
這個程式稍微簡單一點然後我就取最可能的分數最高的word	8-6
哪裡是可能分數最高的word	8-6
那你可以想像因為我現在六萬個word	8-6
有的早一點結束有的晚一點結束	8-6
有的早結束有的晚結束就是我們這邊所畫的就是	8-6
譬如說w one 在這邊就結束了	8-6
w two 到這邊才結束	8-6
w 四要到這兒才結束對不對	8-6
我就把這裡面分數最高的word 保留下來	8-6
就構成一個 word graph	8-6
那麼這個東西我底下就只要在這上面算就好了	8-6
那麼因此呢我這個複雜的東西	8-6
在後面算	8-6
那麼我這個時候我這個再把複雜的	8-6
那也等於是說我我這個很複雜的這個這個tree 後面接這麼多tree 後面接這麼那這個東西呢我就把它reduce 成為變成只有那樣子	8-6
不但是變成只有那樣子而且它不會發散	8-6
而是最後會reduce 到一點	8-6
不一定是一點啦你這邊可能也有也有不只一個	8-6
但是譬如說這邊還有一個w 九	8-6
但是基本上你不不會一直這樣越長越大越長越大	8-6
你你你可以限制它就這麼大	8-6
ok 於是呢我真正的複雜的tri gram 啦	8-6
或者tri phone 啦什麼這個複雜的東西	8-6
我只在這上面算	8-6
那這個search space	8-6
比原來那個要小很多很多那個太大了	8-6
那個大到無法算所以我就先我先用一些簡單的就是less knowledge 或者less constraint	8-6
用一些簡單的辦法	8-6
把那個大的tree	8-6
那個太大的那個那個那個 tree	8-6
reduce 到變成一個小的graph	8-6
然後呢我現在把這個東西	8-6
在這上面才做詳細的	8-6
那這是我們通常稱為re scoring	8-6
你現在再把你的詳細的你的tri gram tri phone	8-6
分數詳細去算	8-6
那剛才因為只用簡單的所以你那個分數不太對	8-6
我現在可以把詳細重算一次分數	8-6
所以叫做re scoring	8-6
那之後呢你可能會發現這上面雖然有這麼多種可能	8-6
其實最可能的是這條	8-6
譬如說是w two 接w 八接w 九	8-6
可能這條才是你的答案譬如說這樣子	8-6
那你就可以在 word graph 上面找出來	8-6
那這是所謂的multi pass search 的基本觀念	8-6
那當然這樣做的時候基本上你前面的這塊第一個path	8-6
所謂的這個word graph generation	8-6
其實跟那剛才那個是一樣的	8-6
只是簡單一點	8-6
我用比較簡單的knowledge 用比較簡單的constraint	8-6
譬如說我只用我我不要用tri phone 我不要用tri gram	8-6
等等我簡單一點就其實是一樣的	8-6
然後我就是保留最重要分數最高的word	8-6
譬如說在這個時間點結束是以它最高	8-6
或者你也可以再保留一個啦對不對你可以再保留	8-6
你保留若干個這個時間點結束的分數最高的	8-6
然後你在這個時間點你把它保留你這樣一路這樣你會得到一堆	8-6
那你就把它們構成一個graph	8-6
那如果是這樣子的話很可能我們可以把它畫成	8-6
這樣子這是w 十	8-6
那它們都n end 都在同一點	8-6
然後後面都可以接這些等等	8-6
那這就是所謂的 word graph	8-6
那你有了word graph 之後在word graph 上面	8-6
再用比較詳細的再重跑一次	8-6
re score 這些所有的path 之後	8-6
你算哪一條path 分數最高等等	8-6
那這是這個所謂用word graph 的方法	8-6
那麼n best list 是相同的意思	8-6
只是說呢它沒有做成這樣子的word graph	8-6
而是直接把前一百名譬如說這個n best 就是n 就是這個這個前n 個名次分數最高的word list	8-6
全部把它保留下來	8-6
那麼舉例來講在這個case 的話你就可能	8-6
就這個case 的話你可能想像的就是譬如說	8-6
一三五這是一個	8-6
w 一 w 三 w 五	8-6
這是一個一三五	8-6
那麼二二五也是一個w 二 w 五這也是一個	8-6
那麼w 四九也是一個喔等等等等	8-6
那你如果沒有把這個word graph 建起來	8-6
只是說把分數最高的一些word 的word sequence 把它通通都留下來	8-6
譬如保留前一百名或者保留前二百名或者前五十名	8-6
那就所謂的這個n 等於一百或者五十或者二百的 n best	8-6
那你就把這個list 留下來之後我重新在這上面算分數	8-6
那你可以想像這兩種那一個好呢	8-6
這個是比較精簡啦	8-6
這個可以把它們這個其實包含的東西比這個還豐富	8-6
這個只告訴我說一後面接三三後面接五	8-6
那這個其實告訴我說	8-6
一是在什麼時候結束	8-6
三在什麼時候開始	8-6
三是在什麼時候結束後面五等等	8-6
所以呢我我其實三後面還可以接八接九什麼	8-6
它都都在這邊都呈現了	8-6
所以這是一個比較精簡的描述的方法	8-6
你這樣保留一個這個word graph 的效果	8-6
會比這個好	8-6
但是這個簡單	8-6
你這個這個呢你就是把剛才一路找過來的你第一個path 也是用比較簡單的方法來做	8-6
但是我一路走過來之後我就把前一百名留下來	8-6
得到一個一百的list 喔	8-6
那這就是所謂的n best list	8-6
那這兩種方法都可以	8-6
我這上面舉的這兩個例子在說明這個這個n best list	8-6
是不如這個 word graph 來的有效喔	8-6
那像這個例子呢你常常前幾名是只差一點點	8-6
i’ll tell you what i think 還是 why i think 還是when i think	8-6
只是這個地方不對不曉得是哪一個	8-6
其它都一樣	8-6
那你如果是做保留這個n best list	8-6
就會發現常常譬如說前五名	8-6
都一樣只差一個字	8-6
那你保留這個呢你全部重算有點浪費嘛	8-6
其實你應該把它變成一個word graph	8-6
那這邊都一樣只有這個地方不同	8-6
對不對只有最後這個地方不同	8-6
那這樣子的話你的這個嗯比較有效的使用空間跟這個資訊	8-6
那所以呢這個這個這個是	8-6
word graph 這是n best list	8-6
那不管怎樣你都是這樣	8-6
所以呢我的真正的效果呢就是	8-6
我de cup 這個de couple 本來的這個複雜的search problem into a simpler process	8-6
對不對就是說我我現在就是把我整個的做的話這個太複雜了	8-6
所以呢我可以把它拆成兩半	8-6
第一半用比較簡單的東西	8-6
the first primary by acoustic scores	8-6
或者是the second by language 這也是一種辦法	8-6
我language 在在第二個做哦等等	8-6
或者這個是一個例子	8-6
這底下也是一個例子	8-6
那基本上我就是把它 de couple 成為兩個stage	8-6
或者可以更多	8-6
所謂的multi pass 不一定兩個啦你還可以第三個啦	8-6
你可以在在這邊之後	8-6
我還不做決定	8-6
我在這邊之後呢我可以這個弄一個比較複雜的word graph	8-6
在這邊再做一次re scoring 把它簡化成一個再簡單一點的再做第三次也可以哦看你要怎麼做	8-6
所以你可以分成不只一個path	8-6
那這樣的話呢就每一個path 都比較簡單	8-6
那我的search space 只有在第一個的時候很大	8-6
後面就算動縮小	8-6
縮小之後	8-6
我再做精緻的	8-6
那這是一個常用的方法好	8-6
那再下來的這一些呢是是另外一招	8-7
這個也是使用很多的	8-7
那這一招其實就是所謂的heuristic search	8-7
就是我們底下要說的	8-7
heuristic 跟這個a star	8-7
那heuristic 跟a star 呢這個基本上是a i 裡面搬來的喔	8-7
那麼各位之中如果你修a i 的課的話就講一大堆這種東西就很清楚了	8-7
那我們這邊呢稍微提一下喔	8-7
那等於就是把a i 裡面的heuristic search 搬來	8-7
那這個是一個非常有效的方法	8-7
那麼也是我們常用的喔	8-7
那我們在這裡休息十分鐘好了	8-7
ok 我們接下來這段基本上是	8-7
a i 或者別的相關課你可能學過喔	8-7
如果你沒學過我們很快說一下	8-7
如果學過我們就是複習一下喔	8-7
那麼這是這個通常人家把這個search problem 看成是一個譬如說像這樣的一個city travel problem	8-7
也就是說你如果要從city s 做為你的starting city	8-7
g 是你的goal city	8-7
你要從s 走到g	8-7
那麼你有一個map 你只知道說從s 呢可以走到a 跟b	8-7
它是三公里它是兩公里	8-7
等等	8-7
你一路走下去到底走哪一條才是minimum distance path 呢	8-7
那如果你已經有一張map 你從頭這樣去加你當然可以找得出來	8-7
otherwise 這個變成一個很複雜的問題	8-7
那你可以想像這個這個minimum path 的這個minimum distance path 這個問題呢	8-7
可以看成是一個tree 的結構	8-7
當你變成一個tree 的時候就跟我們剛才講的就比較像了喔比較像這樣子	8-7
那怎麼變成一個tree 呢就是ok 你s 呢可以走a 可以走b	8-7
如果走a 的話呢又可以走 b 跟c	8-7
那走b 的話呢又可以走a 跟d 等等	8-7
那你就是把所有可能的path 通通接上去	8-7
你可以這樣找出來變成一個tree	8-7
然後呢你如果這樣走的話呢各是幾公里一路可以加進來	8-7
譬如說走a 跟c 的話你就變成六公里了	8-7
再走到e 的話就變成九公里等等都可以這樣子做	8-7
那即使你變成一個tree 你需要有好的方法才能夠找到比較好的路	8-7
舉例來講這幾個方法都不算是好方法	8-7
所謂的這個depth first	8-7
就是說你一你凡是碰到你可以選擇的時候	8-7
你就任意arbitrarily 選擇一條	8-7
然後就向前走	8-7
你隨便找一條向前走隨便找一條向前走如果這樣子的話呢當然不見得最好	8-7
你只是最快走到最深的地方去而已	8-7
那反過來呢breath first 這個這個breath first 的話呢是說	8-7
我凡是碰到可以選的時候	8-7
我就把這層的所有的note 看誰看誰比較譬如說這裡的話二跟三那二比較近我就先走二	8-7
那當然不見得走二就是好啦對不對哦等等	8-7
所以呢你每一次就是把這same level 的都去看一次到底誰比較好	8-7
不過這也不見得比較好所以這是breath first	8-7
這都不見得好	8-7
那這種呢都有共同的問題就是所謂blind search	8-7
就是你沒有真的知道它們那麼誰是怎麼樣才是最好的	8-7
或者說你其實沒有一個sense about where the goal is	8-7
就是說你並不知道你的goal 在哪裡	8-7
你只是在那裡一路找而已	8-7
那怎麼樣比較好呢	8-7
所謂的heuristic search	8-7
所謂heuristic search 的一個簡單的這個這個解釋是說呢	8-7
我現在假設說我要從這裡走到這裡	8-7
雖然我現在我沒有辦法把整個地圖畫出來看的話呢	8-7
我至少呢假設假設這個是台北一零一	8-7
它有一個高樓	8-7
從遠方就一路都看得到它	8-7
因此呢我就可以每到一個地方的時候我就都可以估計一下	8-7
用目測的	8-7
目測一下說它的直線距離是多少	8-7
那我就用那個當做所謂的heuristic	8-7
於是呢我就可以做這件事	8-7
就是在我我在每一次可以選的時候	8-7
我就算一下走這個跟走這個倒底哪一個比較好呢	8-7
除了走到這邊的距離是所謂的g n	8-7
所以g n 是distance up to note n	8-7
所以假設我從這邊是要走a 還是走b 呢我就看這個是g n	8-7
是三這個b 是二之外呢	8-7
我還在看假設我從a 來看的話	8-7
大概估計是多少	8-7
那這裡有一個假設我們舉例來講這是一零一的高樓	8-7
所以你可以用目測的測它的直線距離	8-7
那假設從這邊測過去的是八點五	8-7
我這邊用這個顏色寫的就是假設這邊預測是八點五	8-7
那這邊的話呢從這邊估計一下呢是十點三	8-7
那這就是h n	8-7
h n 就是heuristic estimate for the remaining distance up to g	8-7
所以呢於是你你如果這個時候選a 跟選b 就有兩種選法啦	8-7
如果a 的話呢我是這邊是要三	8-7
這邊到這邊呢估計是八點五	8-7
所以加起來是十一點五	8-7
這邊是要二	8-7
這邊到這邊估計是十點三	8-7
所以加起來是十二點三	8-7
那它比較近啊	8-7
所以我就選擇它	8-7
那麼因此呢我這個就開始選擇a	8-7
那這個時候呢因為這是三這是八點五	8-7
所以我的估計是十一點五	8-7
所以呢我在每一次可以選的時候	8-7
我就把這兩個選的東西我都算一次這個分數	8-7
其中一個是我知道的	8-7
一個是我不知道的但是我做一個estimate	8-7
那麼然後呢我用根據這個estimate 來選	8-7
那麼等到到了a 之後我現在再看的話呢可以走b	8-7
可以走c	8-7
走c 的話這邊是三	8-7
這邊是五點七	8-7
那這樣子的話呢接下來是八點七再加三是十一點七	8-7
可是我如果走b 的話是十點三	8-7
還要再加四	8-7
還要再加三	8-7
是十七點三	8-7
那這個遠得多了	8-7
所以我就說c	8-7
所以呢a 之後我就選c	8-7
這個時候我的估計是十一點七	8-7
是因為這是三六再加五點七是十一點七	8-7
那那當我走了這個的時候我就到了到了這個c 了	8-7
c 沒有什麼好選的因為只有這個就走到e	8-7
這個就是這個再加三	8-7
那到了e 的時候呢我這個時候再來看的話呢	8-7
你這邊是九	8-7
這邊是二點八	8-7
所以就是十一點八	8-7
那等等那這樣下去的話呢這樣我一路走下去這條path 就是非常接近optimum 的一條path	8-7
那我其實一路估計的其實差不了多少真正的距離是十二	8-7
我一路估計的是十一點幾是很接近的	8-7
在這個例子而言	8-7
那這樣的方式這就是所謂的heuristic 這個heuristic search	8-7
那有另外一個名字所謂best first search	8-7
我就一路在做估計	8-7
然後一路呢找best	8-7
那有另外一個名字叫做heuristic pruning	8-7
你知道pruning 的意思就是砍樹	8-7
就這個tree 太大了我就一路把它砍掉	8-7
那我們之前講的這個也叫做pruning喔	8-7
所謂的pruning 都是這個樹太大我要把這個樹砍小的意思	8-7
那就這個example 而言	8-7
我們比較像是在我們比較像是在因為是在走這個travel	8-7
所以是要找 minimum distance	8-7
跟我們講的不太一樣	8-8
那我們講的應該是底下這個case	8-8
我們把它另外做個題目	8-8
這是算這個最高分數	8-8
因為我們其實都是算最高分數	8-8
而且呢我們其實最終目標不是一個	8-8
而是你只要走到任何一個word 都可以	8-8
所以呢比較像這樣	8-8
這也可以就是說我現在如果從a 開始走	8-8
那每走一步會得到一個分數	8-8
每走一步得到一個分數	8-8
那麼走到最後走到底就都可以	8-8
沒有一個固定的	8-8
不像剛才	8-8
剛才這是有一個固定的goal 在那裡	8-8
我這邊可以是不固定的	8-8
然後呢我要看哪一個分數最高	8-8
那這個problem 是完全一樣喔	8-8
那麼你怎麼做	8-8
你這邊a 如果下面可以選b c d 的話我就把 b c d 都列出來	8-8
那這個時候呢我的我可以先算走到b c d 各得幾分	8-8
四三二	8-8
然後呢在 b c d 的時候各去估計一下	8-8
我如果走到底的話可以得幾分	8-8
這就是h n	8-8
就是這個estimate value	8-8
然後把它加起來	8-8
那這就是我的估計	8-8
然後根據我的估計來看的話呢	8-8
嗯c 最高	8-8
我就選c 等等喔	8-8
所以呢這個意思是一樣的	8-8
就是我每每一次當我有選有得選的時候	8-8
我都去看走到這邊的話我知道得多少分之外呢	8-8
後面我不知道但我想辦法估一個值	8-8
然後呢我用這二個去相加	8-8
然後呢去看我要這個來選	8-8
那這樣子的選法呢	8-9
這個這在a i 的課本裡面他們他們有一套說法	8-9
那麼這個說法就是嗯第一個它有一個叫做所謂的admissibility	8-9
所謂的admissibility 是說有某一種search 的方法是admissible	8-9
如果保証你找的第一個solution 就是optimum	8-9
只要optimum solution 存在	8-9
只要optimum solution 存在你照那個algorithm 去做	8-9
你找到的那一個第一個就是optimum	8-9
這個叫做admissible	8-9
那當然很多時候你的你的algorithm 不是 admissible	8-9
那當然我們會希望	8-9
它是admissible	8-9
那麼舉例來講	8-9
我們剛剛講的beam search 顯然就不是因為	8-9
beam search 我已經很可能把那些個嗯optimum 都已經丟掉了	8-9
所以beam search 顯然就不是	8-9
它只是一個practically engineering solution	8-9
它適合engineering 的方法來得到答案而已	8-9
它不見得是	8-9
喔它顯然不是	8-9
那	8-9
嗯	8-9
我們比較希望我們的方法是admissible	8-9
那麼也就是說只要存在一個optimum solution 我就照這個方法去找的我的第一個solution 就是	8-9
有沒有條件呢有的	8-9
這個這個這是這個一個定理	8-9
在課本裡面有	8-9
那a i 課本裡面也有這個到處都有	8-9
那我們這邊並沒有打算要去說它如果你有要了解它的 exactly 的意思	8-9
或者它怎麼証明的	8-9
課本都查的到	8-9
不過講起來很簡單就是說	8-9
以我們剛才那個case 而言你如果是要最高分數的話	8-9
你可以証明它是admissible	8-9
你的條件就是我的所有的都是高估	8-9
那h n 是什麼	8-9
h n 是真正的	8-9
我真正的從這裡到那裡	8-9
真正從這裡到那裡會得幾分是真正的	8-9
那h n 的star 呢是你的估計值	8-9
那麼	8-9
這個	8-9
定理是說呢你如果是一個最高分的problem 你要找最高分我們剛才像剛才這個就是我要找最高分看走到哪裡最高分	8-9
你如果是要找最高分的話	8-9
你其實就是	8-9
你所有的都是高估	8-9
你只要所有的都是高估	8-9
你就會得到	8-9
那麼就就它就是admissible	8-9
反過來如果是minimum 的problem 的話	8-9
像我們前面那個	8-9
是一個 minimum distance 的problem	8-9
你要minimum	8-9
的話呢那反過來就是要低估	8-9
你如果所有的都是低估的話	8-9
它就是admissible	8-9
那	8-9
這個	8-9
詳細的証明我們就不講	8-9
那凡是符合這個條件的admissible 的話呢我們就說它叫做a star search	8-9
喔這個a star 這個是a i 裡面的名詞	8-9
那	8-9
那麼因此我們會prefer	8-9
這種a star	8-9
那麼它的基本的做法	8-9
那那這樣來的我們這個這樣一來這個heuristic	8-9
這些heuristic search 就有道理啦	8-9
喔	8-9
我們剛才講好像沒什麼道理因為你你憑什麼估記	8-9
你憑什麼去估這個東西呢	8-9
那現在有一個很簡單的原則就是你要高估或者低估	8-9
你如果要minimum distance 就是要每一次都低估	8-9
然後你如果有 maximum score 就是你每一次都要高估	8-9
你如果就是高估或低估的話就可以保証你是	8-9
a star	8-9
嗯這叫做所謂的a star search	8-9
那這個做法就是我們剛剛已經講過就是每一次	8-9
凡是你有得選的時候	8-9
你就把所有的可以選的都算一次	8-9
算的時候就是包括剛才講的這兩個嘛	8-9
就是	8-9
已經知道走到這邊會有幾	8-9
的分數以及我高估或者低估的那個分數	8-9
把它加起來	8-9
然後你把它列出來看看誰誰是你要的	8-9
你就照那個來選	8-9
你如果是這樣子的話	8-9
所以呢譬如說這邊講你如果是要最高分數的 problem 的	8-9
話	8-9
你就是用高估的	8-9
然後呢你每一次可以選的時候就選那個最高的	8-9
那這樣的話呢你這個就是一個a star	8-9
然後你就是可以得到admissible 的的答案	8-9
好	8-9
那我們大致這樣講那做語音的時候怎麼做	8-9
做語音的時候是一樣的情形	8-9
你可以想像的情形是說	8-9
其實我們是在做一個跟剛才一樣的	8-9
這個multi multi pass 的search	8-9
但是呢我在第二個pass 的時候我現在可以用a star	8-9
因為我在我在第一個pass 走過來的時候譬如說	8-9
我已經建好這個tree 了	8-9
tree 上已經有很多東西了	8-9
我可以用這個tree 上的知識	8-9
來來估計	8-9
來高估	8-9
所以我第二次重走的時候呢	8-9
我就可以走a star	8-9
我就可以算說	8-9
我走這個的話後面估計是多少	8-9
如果我走這個的話後面估計是多少	8-9
我走這個的話後面估計是多少等等	8-9
那就等於說我是在這個	8-9
所以呢我的這個第二步的時候呢我可以用 a star 的方式來做	8-9
來make sure 我走的是admissible 的	8-9
然後我會得到optimum	8-9
至少在在那個word graph 裡面是optimum	8-9
喔	8-9
等等	8-9
雖然我一開始建的時候可能我有把最好的丟掉是有可能的	8-9
那這個是這我們用a star 來做那一塊基本上是這樣子	8-9
那你怎麼用	8-9
這個這個tree 這個word graph 上面的東西來估計	8-9
做這a這a star 要估計啊	8-9
要做高估啊	8-9
那	8-9
你怎麼怎麼個高估法呢	8-9
那我們這邊舉兩個例子很多方法都可以用	8-9
那我們舉這兩個兩個例子	8-9
那第一個例子呢就是說	8-9
你你估計時間	8-9
你先估計這個 average score per frame	8-9
那舉例來講呢我可以有training data	8-9
那麼我可以算這個東西	8-9
這是什麼呢	8-9
這是o i j 就是我一堆observation	8-9
從frame i 到frame j	8-9
然後呢	8-9
q i j 是它的一個state sequence	8-9
然後中間呢有多長	8-9
因此呢這個就是	8-9
我如果有一段signal	8-9
這是從time frame i 到time frame j	8-9
那麼這一段呢	8-9
它假設走某一個state sequence	8-9
走過來	8-9
那麼這個呢就是o i j	8-9
這個呢就是q i j	8-9
那我如果是	8-9
假設是given 這一個state sequence 的話呢	8-9
它的總分數是多少	8-9
那麼總分數是多少呢我除以j 減i 加一就總共多少 frame 嘛	8-9
那我大概可以估計這個每一個frame 每一個frame 分數是多少	8-9
那這樣的話呢我現在如果有夠多的training data	8-9
我可以用這個來統計	8-9
我大概可以算出來平均每一個frame	8-9
每一個frame 它的分數	8-9
平均是多少maximum 是多少minimum 是多少等等	8-9
這些都知道了	8-9
然後呢	8-9
因為我剛才在b第一個pass 已經走過一次	8-9
所以我就知道	8-9
每一點到最後的時間還有多少	8-9
我就用這個時間來算	8-9
這就是t 減t	8-9
大t 就是	8-9
最後的	8-9
那你在時間t 的時候	8-9
那你可以估計我這邊還剩下多少個frame	8-9
對不對因為我前面已經走過一次我的first pass 已經走過一次	8-9
我已經知道到到大t 會結束 大t 是多少我已經知道了	8-9
因此我現在到這個node 的話呢我其實我已經知道	8-9
還剩下多少時間	8-9
然後我可以估計平均每一個frame	8-9
會有多少時間minimum 是多少時間maximum 多少時間等等	8-9
我可以用這個來來算	8-9
我每一點估計到最後大概有多少時間	8-9
那這樣的話我就可以得到我的這個heuristic 分數	8-9
用那個來估	8-9
那我現在是要高估	8-9
那我就我要make sure 我找的是maximum 我就高估好	8-9
等等	8-9
那第二個例子是差不多的情形就是你先在現用week constraint	8-9
可以得到它的分數	8-9
那那個可以拿來當做heuristics 用	8-9
就是說你現在我剛才假設我只用	8-9
我沒有用tri phone 我只用最簡單的phone model	8-9
我也可以得到它們的acoustics 分數	8-9
我沒有用tri gram 我只用bi gram 我也可以得到它們的	8-9
linguistic language model 分數	8-9
這些分數都可以拿來做estimate嘛	8-9
所以我可以用第一個pass 所得到的比較粗的	8-9
東西	8-9
所以呢這個這個first path 得到的那些比較粗的week constraint 的分數	8-9
拿來估計我的heuristic	8-9
也可以啊	8-9
那其實這個估計的會比剛才那個還更準一點啦	8-9
喔因為我這個可以把那些分數算進去	8-9
那這樣的話呢我這個都是這類的分法	8-9
那這樣的話我們的這個之前講的這個這個multi pass 的方法呢	8-9
其實我到第二個階段的時候我就可以做a star	8-9
那這個也是一個常用的方法	8-9
好那到這裡呢我們八點零講完了	8-9
那麼嗯或者說是我們的這個所有的basic 到這裡都講完了	8-9
那我們來先來說一下期中考	8-9
今天是四月今天是四月二十五	8-9
下週是五月二號	8-9
五月九號五月十六號	8-9
期中考範圍到這為止	8-9
所以呢這個我上次提過合理的考試時間可能是兩週以後	8-9
但是呢我希望排在十六號	8-9
原因是那週我出國	8-9
那所以那週考試的話我們不影響進度	8-9
就可以大概可以至少可以等於是補了一次課一樣啦	8-9
那所以我們十六號考試沒問題吧	8-9
好 ok 那所以我們期中考是嗯五月十六號	8-9
我們的考試時間兩小時	8-9
也就是十點	8-9
我們十點十分到十二點十分好了好不好	8-9
我們就是考最後的考後面的一百二十分鐘	8-9
這是所以就是十六號那一週我出國我們就是考期中考	8-9
十點十分到十二點十分	8-9
考試範圍是到八點零為止	8-9
那麼我會在下週把上一次的考古題發給各位	8-9
那你就比較容易知道我會怎麼考	8-9
那嗯這個期中考不會難	8-9
因為並沒有要為難各位喔	8-9
那麼期中考的目的其實最簡單的就是兩件事	8-9
第一個就是make sure 大家有在念書喔	8-9
那我了解這個我們台大同學的這個最主要的問題就是如果不考試的話你就不會念的嘛	8-9
所以呢我們就是需要考試來make sure 你有念書就是了	8-9
然後當然第二個目的是我們得要有個分數喔	8-9
要有分數才能夠才能夠算成績嘛喔	8-9
喔只是這樣原因而已	8-9
所以期中考不會難考	8-9
喔但是你要念喔	8-9
要念什麼	8-9
除了你不是光是上課講這些東西而已	8-9
我每一個每一個八點零七點每一個點零的地方前面都會有它的reference	8-9
這些基本上我大概都會說到	8-9
喔譬如說我剛才講這個時候我就會說這三個裡面你選一個嘛	8-9
有or 就是你選一個	8-9
要念其中一個	8-9
嗯然後呢那這個我就沒有說你一定要念啊我就說這個是一個很好的reference	8-9
那這個我也沒有說你要念啊喔	8-9
這我就說這個是屬於那個古代的方法裡面的很好的reference	8-9
所以這兩個應該是不會考的譬如說	8-9
我都會再講到	8-9
所以呢那但是呢我如果有講說什麼地方要念你要念哦不然那個地方會考到	8-9
那然後我們這門課因為修課同學background 差異很大	8-9
我們從大三一直到博士班	8-9
從資工的到電子的到什麼都有	8-9
所以呢我們基本上哦你可以假設就是這我之前也講過就是	8-9
你如果會覺得沒有辦法看下去你就跳過去	8-9
你繼續往下看	8-9
但是當你看了很多之後你可以回過去再看	8-9
你可能發現你原來你看不下去的地方你再看就看懂了因為你看了後面的東西	8-9
喔所以你always 可以再跳回去看前面的	8-9
如果你跳回去看仍然看不懂的話	8-9
那那個地方不會考啦喔	8-9
那基本上就是這樣喔	8-9
所以我想我會考的部分應該是不會depends on 你的某個專業的background 的	8-9
所以呢我想這個是這個期中考的部份	8-9
然後呢我們的習題還有第二題會在下週給你	8-9
但是交習題時間會在期中考以後哦	8-9
所以第二題習題是做language model	8-9
就是train n gram	8-9
哦等等	8-9
因為你的第一題是acoustic model 嘛哦	8-9
第二題是language model train n gram 嘛	8-9
會在下週給你	8-9
然後在考後交	8-9
那之後的話你現在就等到期中考考完之後	8-9
你只要把第二題習題交完你只剩下一件事	8-9
就是期末報告	8-9
喔那所以呢	8-9
那這個我們從下週以後開始我們就在講後面的了	8-9
我們這個fundamental 到這裡結束	8-9
那那我下週開始我用跳的	8-9
所以呢喔我下週會先直接講這個這個十一點零跟十二點零	8-9
然後會往下走	8-9
那麼我九點零跟十點零	8-9
屬於另外一些個理論比較多的東西	8-9
我覺得留到比較晚一點再說	8-9
那從十從這個十一點零以後都是各個研究領域的相關的適合給你做報告的題材	8-9
喔所以呢我儘可能提早先講這些個各個研究領域的部分	8-9
那麼讓你提早接觸這些東西	8-9
這樣你期中考一考完你就可以開始思考你期這個期末報告可以做什麼	8-9
那在這個裡這裡以後我講的方法就會跟這邊都不一樣了	8-9
因為到這裡八點零為止我們是在講basic 所以我每樣東西講得很慢	8-9
那從十一點零開始	8-9
那其實我每一個都只講它是什麼然後觀念是什麼	8-9
我就跳下去了	8-9
哦就不斷用跳的	8-9
所以從這後面開始會用比較快的跳的方式來進行	8-9
那麼跟這邊的我們講的basic 是不太一樣的喔	8-9
好這個是講這個我們之後的進行的情形	8-9
那麼所以呢底下我們可以稍微開始一點點十一點零	8-9
那麼十一點零我們在講的是speaker	8-9
從這裡開始我剛才講我們現在就講一樣一樣東西	8-9
我們每一樣東西就會給你reference	8-9
然後這個嗯我每一樣都只講一下它的基本精神就往下跳了喔	8-9
那我們十一點零是講不同的speaker 聲音不一樣的問題	8-9
那你可以想像我們到目前為止沒有考慮不同speaker 會怎樣	8-9
但其實每一個speaker 聲音是不一樣的	8-9
我們說ok	8-9
這堆是嗚	8-9
這堆是啊	8-9
這是一個很粗的說法	8-9
如果單獨一個speaker 的話	8-9
一個人它的嗚是會這樣子	8-9
有一個distribution 啊會有一個distribution	8-9
但是你如果一群人的話它的嗚顯然會比較大	8-9
那它的啊也顯然會比較大	8-9
於是就會overlap喔	8-9
那因此呢你你這個不同的人顯然就會有很多不同的問題跑出來喔	8-9
那麼那麼我們要解決這個不同的speaker 的問題	8-9
那我們底下會說一些重要的方法	8-9
那我每每一樣重要的東西我們就會列個reference 給你喔	8-9
那這些都是我們後面你如果要做期末報告的很好的起點	8-9
那我們先說一下這裡的主要的problem 是什麼	8-9
那麼基本上來講最好的當然是speaker dependent	8-9
也就是說我只為一個speaker 來train	8-9
只用你的聲音train 你的系統	8-9
這個顯然是正確率最高的	8-9
只是說呢我們需要大量的data	8-9
那你的聲音需要train 所有的tri phone	8-9
對不對需要train 所有東西	8-9
所以呢你這個這個本身就已經很大了嘛	8-9
那麼在早年做語音研究的時候人家都以為這樣是最好的方法這樣正確率最高	8-9
不過到後來就知道這個是不可行	8-9
為什麼不可行	8-9
就是因為需要的training data 太大了	8-9
而天下的user 是天下最懶惰的一群人	8-9
如果你要叫他先發夠多的聲音去train 的話就沒有人要發	8-9
所以呢天下最懶惰的一群人就是user	8-9
所以就不要靠user 的話是不可能的	8-9
所以最後我們就知道這個solution 是不通的	8-9
那後來就有人想說那這樣子嘛我們做multi speaker	8-9
譬如說同一個家庭的人聲音都比較像	8-9
這個兄弟姐妹都很像	8-9
所以呢你就可以把他們train 成一個model	8-9
那是不錯啦喔	8-9
你這樣的話是因此呢你如果本來要用十個小時的data	8-9
你兄弟姐妹四個人所以呢	8-9
你每個人用二點五小時嘛哦	8-9
那這個是稍微有點好處但是幫助不大	8-9
這個沒有太多用	8-9
所以這個後來也放棄	8-9
那最理想是什麼 speaker independent 嘛	8-9
那就是我用譬如說找一千個人	8-9
來五百個男生五百個女生	8-9
那我這樣的話我就可以train 所有的聲音都在裡面了	8-9
那這樣子的話呢就這個這個每一個人其實這一千個每一個人可能只要三十分鐘的data	8-9
我就可以那這樣我希望可以good for all speakers	8-9
但是這個turns out 也不那麼成功就是因為	8-9
我的accuracy 一定會比較低	8-9
就是我們剛才講這個情形嘛	8-9
你如果一個人的時候我可能啊跟嗚可以分得開來	8-9
可是當你可能有一千個人的時候呢每一個人的嗚不太一樣	8-9
所以它的distribution 就會變大	8-9
每一個人啊都不一樣就會變大所以顯然就會overlap	8-9
於是就不容易分得開來嘛喔	8-9
所以呢這個時候呢speaker independent	8-9
顯然是我們都希望的	8-9
但是它的缺點就是accuracy 一定會低一點	8-9
那麼今天其實我們所有的語音系統	8-9
幾乎都是這個	8-9
但是呢它都是會差一點喔	8-9
就是我們幾乎已經沒有沒有這個speaker dependent 的case	8-9
或者這個幾乎是沒有了	8-9
但是呢大概都是朝這個在做	8-9
但是都不會太好	8-9
那怎麼辦	8-9
那最好的solution 我們今天所知道的最好是這個	8-9
就是speaker adaptation	8-9
就是用這個speaker independent 開始	8-9
但是呢它去學speaker 的聲音	8-9
你儘量學	8-9
讓你說最少的話我就學會你	8-9
所以譬如說呢你一開始是一個speaker independent model	8-9
所以我一開始的時候我我輸入的時候它的正確率稍微低一點	8-9
這個也有也有技術你怎麼樣做得比較好	8-9
但是不會特別好	8-9
那這個時候呢	8-9
當你開始對它講三句話講五句話之後	8-9
它就馬上學進去	8-9
它就開始知道你的聲音是怎樣的	8-9
它就會開始知道說喔你的嗚其實是這一群	8-9
你的啊其實是這一群	8-9
如果它有辦法幫你把根據你講的那少數的幾句話	8-9
就幫你把你的嗚由這堆收縮到這兒來	8-9
啊由這堆收縮到這兒來的話呢	8-9
欸你就分開來了嘛	8-9
所以呢你就可以這個用這個limit quantity 的data 用少量的data	8-9
我們稱之為adaptation 的data	8-9
於是呢你就可以學你的聲音	8-9
於是你就可以正確率可以馬上提高	8-9
那這個觀念是技術上是可以做得到	8-9
實際上也是可行	8-9
也就是說我們今天所看到所有的系統幾乎是這一種	8-9
也就是基本上它是speaker independent	8-9
但是你如果跟它講話講的講的話他就越學越快喔	8-9
那就是所謂的speaker adaptation	8-9
那我們這個十一點零主要就是講這個speaker adaptation	8-9
那怎麼樣來讓系統讓這些model 學你的聲音學得最快	8-9
那這裡面可我們可以分成supervised 跟unsupervised 兩種	8-9
那所謂的這個 supervised 是說你所輸入的東西假設是知道的	8-9
譬如說這個一開始系統先先跟你講幾句話	8-9
先叫你說個什麼	8-9
那其實你就照它說	8-9
所以你說的話是它已經知道的	8-9
因此呢它完全知道你的這些你輸入的這個聲音裡面	8-9
這是什麼phone 這是什麼它完全知道	8-9
因此它就可以用這個phone 去train 這個	8-9
用這個phone 去 train 這個等等	8-9
它都知道那這是所謂supervised	8-9
但是這個比較不太好的地方就是在於你user 一開始得要回答系統一堆問題	8-9
喔user 可能不喜歡	8-9
那user 喜歡可能我就開始我就跟他講我要講的嘛	8-9
如果是那樣的話呢	8-9
就表示user 一開始說的我系統已經不知道他在講什麼了	8-9
所以呢怎麼辦呢	8-9
也就是我的 text 你輸入的聲音的文字它其實不知道	8-9
那這個時候呢	8-9
你知道一開始就是用用speaker independent model 去做辨識	8-9
所以我一開始不知道這你在說什麼但是我就用我這個model 去辨識	8-9
它可能是這個它可能是這個	8-9
那我就用它去train 它跟它去train 它	8-9
那這樣當然會有錯	8-9
有錯所以呢可能是你要你如果可以iteratively	8-9
perform 會比較好	8-9
也就是說	8-9
我現在你講的第一句話我雖然不知道是什麼但是我就用我的原來的speaker independent model 去辨識一下	8-9
說這個可能是這個	8-9
這個可能是這個	8-9
所以我用它來調它用它來調它	8-9
當我把這些都調過之後這個比較好了	8-9
我再來辨識一次	8-9
我可能會辨識比較準嘛	8-9
等等我可以用 iterative 方式	8-9
來做那這是所謂unsupervised	8-9
那我們今天比較prefer 是這一種	8-9
這樣你系統 user 可以跟系統直接講話	8-9
然後呢你就可以進步	8-9
但是這個就是說這個有技術嘛	8-9
那再來呢我們可以分成batch 跟 incremental online 的區別	8-9
所謂batch 就是你輸入一堆然後它一起幫你幫你調	8-9
incremental 就是你一步一步跟它講它一路調	8-9
也就是說舉例來講譬如說你你當你講了前三句話的時候	8-9
它就根據你的前三句話調一次	8-9
你又講了三句話的話呢它會再調一次	8-9
或者根據你的前六句話重調一次	8-9
就你不斷的講它不斷的學喔	8-9
這樣子它的正確率不斷的提高	8-9
這是我們今天大部分是這種	8-9
就是incremental 的或者是online 的	8-9
那當然 batch 的話是會其實會效果更好	8-9
你給它一堆	8-9
它一次喔	8-9
因為你一步一步的時候可能都 step by step 的時候很可能每一步都不是optimum	8-9
因為你再加東西之後呢你的前面那個就已經不是optimum	8-9
喔所以呢但是反過來呢我們在user 來講是這個比較比較attractive的喔	8-9
所以大概這是adaptive adaptation 的這些東西	8-9
那我們底下就會講adaptation 幾個重要的基本的方法跟觀念	8-9
然後我就會給你這些reference 等等喔	8-9
那我會告訴你哪個方法的reference 哪一個等等等等	8-9
那這是後面的從下週以後我們的上課的方式會變成這樣	8-9
那不像我前面會把它講得那麼清楚	8-9
好那我們今天上到這裡	8-9
我們今天最主要要講的不是這個而是 e  m  theory 所以我們今天要回到九點零我們來講的是 e  m	9-1
那麼九點零要說的其實是一些個重要的我們在這個語音這個領域一些很幾個比較重要的方法	9-1
那麼我講的第一個是就是 e  m  theory  e  m  algorithm	9-1
那麼除了 e  m 之外第二個我後面要講的是這個 m  c  e	9-1
那麼 m  c  e 帶出來的就是 DISCRIMINATING  training 那這也是今天另外一個非常重要的研究領域喔	9-1
那麼 m  c  e 講完之後第三個是 m  m  i  e 就是 Maximum  Mutual  Information	9-1
這個也許暫時沒時間講我們先講前面這兩個	9-1
那麼我們先從 e  m 開始	9-1
e  m 我們已經聽過很多次了因為我們從頭一直在講怎麼樣做 e m	9-1
一直在講用 e  m 幹嘛用 e  m 幹嘛那 e  m 是什麼	9-1
e  m 它它主要是拿來估計某一個統計模型的參數	9-1
ok 也就是說一個一個任何一個統計模型的的裡面的參數我們都	9-1
我們一天到晚都要估計的但是呢你在估計的時候常常發生困難那我們常常就用 e  m	9-1
那麼一個最簡單的例子是 gaussian	9-1
那麼你可以想像假設我有一個東西是一個 gaussian	9-1
那麼這個 gaussian 的最主要的參數是什麼第一個它的 mean 第二個它的 variance	9-1
假設我有一堆 x 假設我有一堆 x	9-1
那麼譬如說 x  one  x  two 我有一大堆 x 這是我的 observation	9-1
當我有這一堆 observation 之後	9-1
我要用一個 gaussian 來 model 它那就是要一個 mean 跟要一個 variance	9-1
那這個怎麼做呢如果只有這樣這是很容易啦	9-1
那我們講的是說類似這樣的問題只是說沒有這麼容易而已	9-1
那麼另外一個情形譬如說我現在不是一個 gaussian 我有好多個一個 gaussian 做不出來我有好多個 gaussian 可以做的出來那就是 g  m  m 對不對我可以做成一大堆 gaussian	9-1
那我要求每一個 gaussian 的 mean 跟每一個 gaussian 的 variance	9-1
欸那這時候就比較難了到底應該怎麼樣子去求它的每一個 mu  I 跟每一個 sigma  I	9-1
等等那像這類的情形呢那都是我們講的就是	9-1
你你要你要你要做某一個 probabilistic 的 model	9-1
上面有一堆參數你怎麼求這些參數 given 這一堆 observation	9-1
那當然你如果要用這一堆 observation 這一堆 observation 去求這一堆參數通常是有一個 criterion	9-1
所以我們通常就是說你有一個 given 的 criterion 在那個 criterion 之下你要來求這堆東西	9-1
那我們這邊先舉兩個簡單的例子第一個 criterion 就是 maximum  likelihood	9-1
這是我們已經用了很多了所謂的 m  l	9-1
那第二個就是 m  a  p 就是 Maximum  A  Posterior	9-1
那這兩個是最常用的 criterion	9-1
那麼舉例來講所謂的 maximum  likelihood 那你已經應該已經非常熟悉我們一再的使用	9-1
那它的基本精神就是我假設這些個 parameter 集合成一個 set 叫做 theta	9-1
也就是說這一堆這一堆東西叫做 theta	9-1
就是我的這些 mean 啦 variance 啦等等等等等等	9-1
假設我有我有 n 個假設我有 n 個 gaussian 的話我就有 n 個 mean 跟 n 個 variance	9-1
這堆東西我叫做我的 parameter  set 就是 theta	9-1
那麼於是呢我所謂的如果如果我的 criterion 是 MAXIMUM  likelihood 的話	9-1
那就是 m  l 的 criterion 那麼 m  l 的 criterion 就是這個東西等於 maximum	9-1
也就是說也就是說如果你我要找這組 set  theta 就是這組參數我要找這一組參數 theta	9-1
such  that 那麼 given 這些 theta 之下我 observe 到所有的 x	9-1
這個 x 就是我這邊的所有的 observation 這就這就是我的所有的所有的 observation  x 的機率是最大的	9-1
也就是說 given 這一堆 theta 其實 given 這一堆 theta 的意思就是 given 這 given 這一堆 theta 的參數就表示這一個 distribution 已經做出來了	9-1
那這個 distribution 裡面呢我看到這個的機率要最大對不對	9-1
也就是你要找某一種你給他一個這個機率模型的長相	9-1
然後裡面所有的參數你可以去調調到最後使得我在這個機在這個 model 之下我看到這些東西的機率是最大的	9-1
那這個就是所謂的 maximum  likelihood 因為這個東西就是所謂的 likelihood  function	9-1
這是如果用 m  l 就是 maximum  likelihood 來作為我的 criterion 的話	9-1
那如果不是用這個而是用 m  a  p 的話也一樣那這個你應該也很熟我們之前也已經講過很多次了	9-1
m  a  p 是反過來是這個東西要 maximum	9-1
也就是說呢是 given 看到這些東西之後那麼那個這個 parameter  set 的機率最大的	9-1
那是所謂的那個叫做 A  Posterior  probability 也就是事後的就是說假設我 observe 到 x 了	9-1
在 given  given  observe 到這堆 x 的條件之下我去調所有的 theta 這裡面每一個都去調	9-1
然後看到底那一組 theta 的機率是最大的	9-1
因為 given 這一個東西之後的這裡每一個值都有它的機率	9-1
因此我就變成是 given 這個 x 然後去調所有的 theta 看哪一個 theta 讓我最大	9-1
那就是那這個東西就是所謂的 A  Posterior  probability 所以呢這就是所謂的 m  a  p	9-1
那 m  a  p 的這個機率跟 m  l 的機率這兩個剛好反過來就這樣子	9-1
那我們之前也一再說過所以你應該是知道的那這個通常這個 A  Posterior 的這個機率通常是難求的	9-1
所以呢我們常常是把他倒過來用 Bayes  theorem 變成變成這個乘上這個然後除以他等於 maximum	9-1
但是因為這個時候呢這個 x 是 given 的我要求的是 theta	9-1
所以這個 x 是 given 的所以呢這個除不除沒有關係我們不除它就剩下這兩個相乘	9-1
所以呢通常這個 maximum  A  Posterior  probability 是變成是要這兩個相乘是 maximum	9-1
那前面第一項其實就是 m  l 的這個東西	9-1
但是它比 m  l 多了第二項就是還有一個這個的 distribution 在	9-1
那這是他們如果用 m  a  p 來做的話就是這樣子	9-1
那我們現在講的 e  m 不是在講這兩件事	9-1
而是說這是兩個例子	9-1
就是 given 某一個 criterion	9-1
在 given  criterion 之下我要求這些東西的時候	9-1
那所以呢可以是 m  l 的 criterion 我要 maximize 這個東西	9-1
也可以是 m  a  p 的 criterion 我要 maximize 這個東西	9-1
or 是隨便其他什麼都可以	9-1
只要你 given 一個 criterion 我要做要求這個求這些個 parameter  theta	9-1
那麼在某些狀況之下我們都會需要用 e  m	9-1
那麼這裡舉一個簡單的例子是在說這個 case 是簡單到不需要用 e  m 的例子	9-1
它是說假設我只有一個 gaussian	9-1
當你現在有好多個 gaussian 的時候其實這個時候是可以用 e  m 來做的	9-1
當你現在有一把 gaussian 我現在這麼多東西	9-1
這麼多 observation 我假設有一把 gaussian 到底哪一個	9-1
這個怎麼樣調這些 gaussian 的 mean 跟 variance 才能夠最好	9-1
這個其實是用 e  m 比較好是可以用有很多方法來做但是 e  m 是一個比較好的方法	9-1
但是反過來我今天如果只有一個	9-1
喔我假設我只要一個 gaussian	9-1
它的 distribution 很簡單只有只有一個我只有一個 mean 跟一個 variance	9-1
那這個時候我只有一個	9-1
這個 i 都不要了	9-1
假設是這個 distribution 很簡單我其實用一個 GAUSSIAN 就夠了只有一個的話那這個時候其實你如果要的是 m  l 的 criterion 的話	9-1
這個答案很簡單就是這樣喔	9-1
那這是可以證明的我這邊不去證它不過你可以在很多課本上查得到	9-1
就是說你你如果只用一個 gaussian 去 model 一堆 data 的話這一堆 x  n 這一堆 n 就是我的這一堆 observation 喔	9-1
那這一堆 x  n 就是我的這一堆 observation 這每一個 x 可以是一個 n  dimension 的是一個 n  dimension 的 random  variable	9-1
那麼我現在如果有一堆 x 假設我只用一個 gaussian	9-1
它有一個 mean 跟一個 covariance 來做的話	9-1
那這個答案很簡單你如果要的是用 m  l 的你要的是用 m  m  l 的 principle 的話呢	9-1
其實我只要把所有的 x 拿來求它的 mean	9-1
那個 mean 就拿來當 mean 就好了	9-1
然後有了 mean 之後我就可以求它的 covariance	9-1
那這就是那個 covariance 這樣答案就對了喔	9-1
所以換句話說如果只用只用一個 gaussian 當然我這樣畫的是好像 one  d 的	9-1
其實在這裡講的是一個 n  d 的 problem	9-1
假設這裡每一個 x 都是 n  dimension	9-1
然後我要用一個 gaussian 來來 model 它	9-1
這一個 gaussian 其實就是我們這邊講這裡面的一個橢球那它有一個 mean	9-1
這是它有一個 mean 然後呢它有它的肥度	9-1
這個肥度在每一個 dimension 都有它的肥度那就是它的 covariance  matrix	9-1
那你如果是要求這兩個東西的話呢其實如果只有一個 gaussian 是很容易做的	9-1
你就你就是把這一堆 data 分別去求它的 mean 跟 covariance	9-1
這一堆 data 得到的 mean 就是那個 maximum  likelihood 的 mean	9-1
那麼這樣子求出來的根據這個 mean 就可以求它的 covariance  matrix	9-1
那你得到的 covariance  matrix 就是它的 maximum  likelihood 的 covariance	9-1
這是你在很多課本上都可以查的到的一個非常基本的一個 maximum  likelihood 的 estimate 這樣就可以做出來	9-1
所以你如果只有一個 gaussian 的話呢不用 e  m 這樣就出來了	9-1
但是我們什麼時候要 e  m 呢就是這樣做不出來的時候	9-2
什麼時候是這樣做不出來的呢	9-2
那就是我們底下講在很多時候你要算這些要算這個所謂 object  function 的時候根本算不出來	9-2
因為它 depends  on  some 有一些個 intermediate 的 variable	9-2
那些我們不知道的 variable 我們稱之為 latent  data	9-2
它們是不是 observable 的我根本看不到所以我就求不出來	9-2
所謂的要算這個所謂的這這個 object  function	9-2
我要算這個 object  function 所謂的 object  function 就是剛才的那個 criterion	9-2
不論你是這個嗯 maximum  likelihood 的這一個	9-2
或者是 m  a  p 的這一個	9-2
這就是所謂的 object  function 我們要 maximize 這個東西	9-2
所以呢這個是所謂的 object  function	9-2
那麼在這個 case 下像剛才這個簡單的 example 裡面其實這個東西可以直接直接算算的出來	9-2
可是呢在有一些比較難的問題裡面呢就是你在算這個 object  function 的過程之中呢	9-2
你需要一些個中間的 variable	9-2
這堆中間的 variable 我們稱之為 latent  data	9-2
因為它們是不是 observable 你看不到的	9-2
這個你知道 latent 這個字的意思是潛藏的就是你看不到的	9-2
是潛藏的你看不到的中間有一堆你要求這個東西的時候中間有一堆 data 你是看不到的	9-2
那麼因此呢你就沒有辦法真的去做這件事	9-2
那一個很簡單的例子就是我們在第四點零講 h  m  m 的時候的 state  sequence	9-2
你記得我們在講 h  m  m 的時候是怎樣的	9-2
我有一堆 state 在那裡跳過來跳過去	9-2
然後我有一堆 observation	9-2
我這個 state 這樣跳過來	9-2
我有一堆 observation	9-2
那問題是我並不知道到底它歸它它歸它	9-2
到底誰歸誰我不知道	9-2
那因此就是那一堆誰歸誰我不知道因此我很難 train 它嘛	9-2
你記得我們當時怎麼辦我們用了很多辦法去去這個假設先先怎麼切然後怎麼切然後再經過很多個 iteration	9-2
像 segmental  k  mean 那弄來弄去之後都先假設它是這樣再跑一次先假設它是這樣切然後再跑一次	9-2
因為其實我根本不知道它到底誰歸誰	9-2
反過來你如果知道它歸它那好辦啦你如果知道這三個歸它你就用這三個 train 它	9-2
這幾個來 train 它就好了那其實就是我們不知道它歸誰嘛喔	9-2
那像這個情形這個 state  sequence 所謂 state  sequence 就是我不知道誰歸誰	9-2
那這個就是這個我們在 train  h  m  m 的時候的所謂的 latent  data	9-2
那同理在很多時候你都有類似的情形就是我的 observation 跟我後我我在這個 case 我後面的 model 是這個 model	9-2
那這個 model 裡面有一堆參數就是 a  b  pi	9-2
應該你都很熟悉就是這堆東西也就是所謂的 lambda	9-2
對不對我現在要要 train 這一大堆參數 a  b  pi 要 train 這一大堆 lambda	9-2
可是呢我只有這一堆東西	9-2
我這一堆東西是最外面的 observation	9-2
這個東西是最裡面的這一堆參數中間還隔了那個 state  state  sequence  q 我不知道	9-2
那在這種情形之下我們就常常就要想辦法那這個時候用的辦法就是所謂的 e  m	9-2
也就是說我要想直接 estimate 這些個參數是做不到的	9-2
因為你沒有這個你沒有這些個這個 latent  data	9-2
那麼舉一個最簡單的例子就是我們剛才講的你在做 h  m  m 的 training 的時候你如果不知道 state  sequence 這是滿難做的	9-2
那我們在四點零那個時候我們說了一大堆的方法來做這件事	9-2
其實那堆方法就是用 e  m 的只不過我們當時沒說而已	9-2
那麼我們再下去你就會知道我們當時是如何使用這個 e  m	9-2
好那到底 e  m 是什麼呢我們剛才已經講了就是在欠缺中間的 data 情形之下	9-2
我從最外層的 observation 要去 estimate 最內層的這些參數中間可能欠缺一些東西的時候所用的方所用的方法	9-2
那 e  m 怎麼做呢 e  m 的基本精神就是底下的這兩句話	9-2
什麼是 e  m  e  m 就是 expectation  and  maximization	9-2
我一面做 expectation 一面做 maximization 這是兩個 step	9-2
這兩個 step 構成一個 iteration	9-2
然後我一個一個 iteration 不斷的跑	9-2
讓它去慢慢趨近我要的答案喔	9-2
這就是 iteration 的方法它是一個 iterative  procedure	9-2
你記得我們在這個講四點零講 h  m  m 的時候	9-2
我們的 problem 三 basic  problem 三就是一個這樣的 problem	9-2
那我們當時也是用一大堆 iteration 讓它慢慢慢慢趨近這個慢慢趨近得到這個 a  b  pi	9-2
那個 iteration 其實就是 e  m	9-2
好那麼什麼是 e  m 呢	9-2
那麼我們現在來說它 e  m 其實就是這兩步這兩步的基本精神就是	9-2
第一步做 estimation 怎麼做 ESTIMATION	9-2
我 base  on  current  estimate  of  parameters	9-2
我們還是一樣哦就是哦擦掉了我要的是這個 theta	9-2
譬如說 theta 是譬如說我要的是一堆參數 a  b  pi 好了	9-2
假設說我 theta 是我要的一堆 model 的參數	9-2
然後我有一堆 observation  x  x  one  x  two  x  t	9-2
我 given 這一堆given 這一堆最外層的 observation 我要去估計最內層的那一些個 theta	9-2
但是呢我沒有辦法直接估計中間少了很多東西怎麼辦	9-2
我第一步呢就是我用 current  estimate  of 這些 parameter	9-2
換句話說呢假設我有一個 iteration 在進行	9-2
在第 k 個 iteration 的時候我有第 k 個 iteration 所得到的 theta	9-2
假設我有一個現在在第 k 個 iteration 我得到的 theta 的話	9-2
然後呢再根據 given  observation 這個 x	9-2
那 given 這兩個東西之後我想辦法去求一個未知的這個嗯 possible  distribution  of  the  latent  data	9-2
這個 latent  data 我們姑且我們說它叫做 z	9-2
z 呢譬如說是一個 latent  data 就是我不知道的	9-2
z 是一個 latent  data 就是我介於中間的	9-2
那麼我不知道的那其實有了這個 latent  data 我才能夠算我現在沒有那個的	9-2
那怎麼辦呢我就去做一個 possible  distribution  of  the  latent  data	9-2
那麼所謂的 possible  distribution 就是我想辦法做一個 z 的 distribution	9-2
所謂的 z 的 distribution 就是說這個 z 呢可以是這樣可以是這樣	9-2
各有一個機率譬如說這是 z  one 這是 z 的 m	9-2
z 可以有某一種 distribution 那麼每一個各有多少值我給它一個 distribution	9-2
那這個 z 的 distribution 怎麼來的我先根據這個估計一個 z 的 distribution 那我們姑且說這個也是第 k 個第 k 個的	9-2
ok 那這個於是呢這樣子之下呢我就可以我就可以算我要的這個	9-2
我我要的這個 object 這個 object  function	9-2
舉例來講剛才的那個 maximum  likelihood	9-2
假設我要的是 maximum  likelihood 就是這個嗯 given  theta	9-2
那這個當然也是指我的第 k 個 iteration 的時候	9-2
這是 k 嗯寫錯了這是 k	9-2
於是我就可以算這個	9-2
我我們剛才講我 z 不知道嘛	9-2
z 不知道的話我想辦法求一個 z 的 distribution	9-2
然後呢當 z 等於這個的時候呢我可以算出來是多少當 z 等於 z  one 的時候我可以算這個 z 等於 z  two 的時候我可以算這個等於多少當 z 等於每一個可能的 z 我都都做出來之後我得到一個我的	9-2
這個是我的這個我要 maximize 的東西我要它等於 MAXIMIZE	9-2
在我們現在以 maximum  likelihood 為例就是這個	9-2
如果是 m  a  p 的話是倒過來然而都可以	9-2
那我們現在姑且以我在這個例子是講 maximum  likelihood 的情形	9-2
那麼因此呢這個就是我們講的第一步所謂的 expectation 喔	9-2
也就是說你你要求一個 expectation	9-2
我不知道怎麼求呢我只好先假設這個 z 有一個 distribution	9-2
我不知道 z 但是我至少可以說我想辦法求出這個 distribution 來 z 等於這個有一個機率等於這個等於有一個機率等於這個等於每一個都有一個機率	9-2
我如果求的出這個機率的話我可以用這個機率的 distribution 來算出一個 z 在這個情形之下的那個 object  function 那個 maximum  likelihood 那個東西有是一個什麼值我算的出來	9-2
那麼這就是這句話所說的意思喔我相對於 possible  distribution	9-2
包括它的 value 跟 probability 就是這些 value 跟它們的 probability 這就是它的 distribution	9-2
那這個 distribution 是是是是這個 latent  data 的 distribution 是那個 z 的 distribution 其實我們不知道的我們估計一個東西出來怎麼估計法	9-2
是根據 current  estimate  of  the  PARAMETER	9-2
就是根據 current  estimate 就是現在這個 iteration 的這個參數的值以及我現在所 observe 到的東西哦	9-2
所以呢根據這個 current  estimate  of  desired  PARAMETER  conditioned  on  given  observation	9-2
根據這兩個我得到這個 distribution 有了這個 distribution 我可以算這個 object  function	9-2
這是這是第一步講的 expectation	9-2
當這個做完之後我現在想辦法把它 maximize	9-2
這就是第二步這個這個所謂的 maximization 就是就是你把它來 maximize 這個	9-2
然後呢那那這個 depend  on 你是 m  l 還是 m  a  p 還是什麼都可以我們這邊講的是 m  l 的例子	9-2
那我現在想辦法把它 maximize	9-2
怎麼 maximize 法那我在中間調所有可能的 theta	9-2
因為我現在我現在既然已經有了這個東西了	9-2
我既然已經有了這個東西了我可以調所有的調所有的 theta	9-2
看哪一個 theta 讓這個東西 maximum 因為我要這個東西 maximum 嘛	9-2
我看這裡面什麼東西是 maximum	9-2
那麼我去我去調所有的 theta 讓它 maximum 的時候那個 theta 就是我的下一個新的 k 加一	9-2
OK 所以呢這是第二第二步講的就是我想辦法產生一個新的 set  of  ESTIMATE  of  desired  parameter	9-2
就是 theta 我我由 k 變成 k 加一了	9-2
也就是說我現在有了這個東西我就可以去調 theta	9-2
那麼調到哪一個 theta 使得這個式子最大的那就是 k 加一	9-2
於是我就可以把這個 k 加一放到這邊來	9-2
就是下一個 step	9-2
那這樣子這就是一個 iteration 然後這樣子一直走一直走一直走	9-2
這樣讓我的 theta 慢慢趨近我要的喔	9-2
那這樣子的畫法其實這個圖就是 e  m 的基本精神就是這樣	9-2
那麼我們如果分清楚一點來看的話呢那他們就是 e 跟 m 的這兩個 step	9-2
一般的說法裡面所說的一個是 e 一個是 m	9-2
那你可以看到我在我在這裡根據這個來算出這個來	9-2
這就是所謂的 e  step 就是 e  step	9-2
然後呢我之後呢我想辦法把這個東西 maximize	9-2
這就是所謂的 m  step	9-2
那麼這兩步 e 跟 m 不斷的不斷的 iterate 那就是所謂的 e  m  algorithm	9-2
ok 這樣那我想這樣應該清楚喔就是等於是說我現在不知道的是 theta	9-2
我我 observe 到的是 x 我還缺一樣東西就是 z	9-2
如果不缺這個 z 根本不要 e  m 了喔	9-2
所以我們剛才前一頁講的如果只是一個 gaussian 的話一個 gaussian 根本就不缺那個那就不要 e  m 了	9-2
那會要 e  m 是因為缺了一個 z	9-2
缺了一個 z 怎麼辦我就去估計 z 一個 distribution 出來	9-2
它在每一 z 的每一個值都可以有一個機率	9-2
那麼在這個這個怎麼估計法我想辦法用現在的 iteration 裡面所假設的一個 theta 值	9-2
然後跟我的 observation 想辦法去估一個新的想辦法去估一個 z 的 distribution	9-2
那在那一個 distribution 之下我可以求出我要 maximize 的那個東西	9-2
那當然那個只是根據這個狀況之下的所以呢我現在去可以去調我的所有可能的 theta 值	9-2
讓這個東西最大	9-2
那樣所得到的就是新的 theta 值就做下一個	9-2
那這就是所謂的 e  m	9-2
這樣講起來是非常抽象的那麼我們底下會會有比較具體的例子來看這件事喔	9-2
所以呢你看第二步所謂的 m  step 呢就是再 generate  a  new  set  of  estimate	9-2
就是我要求一個新的 theta 得到一個新的這個 theta 的 k 加一就是 new  set  of  estimate	9-2
by  maximizing  object  function 那這就是 m  step	9-2
那這兩個 e 跟 m 一直走就是了	9-2
那麼這樣的話呢我們就可以保證我每一次 iteration 我的 object  function 是在增加喔	9-2
每一次增加所以最後它會 converge 這是 e  m 的這個基本精神就是那那邊的那個圖	9-2
我們先來看一個例子這是一個比 h m m 還要簡單的一個簡化的 h m m	9-3
那你看這個圖大概就了解相當於 h m m 裡面兩個 state a 跟 state b	9-3
然後假設裡面 observation 只有兩種就是紅球跟綠球	9-3
那如果是這樣子的話呢假設我現在我的我不知道的是什麼	9-3
我不知道的是所有的參數	9-3
就是我會用我現在 random 的抽一個球出來	9-3
我可以從 state a 來抽也可從 state b 來抽	9-3
所以我抽到 state a 的機率跟抽到 state b 的機率這兩個機率是未知的	9-3
同同樣的呢在如果抽到 state a 裡面呢	9-3
是紅球的機率跟綠球機率呢這也是未知的	9-3
在 state b 裡面呢是紅的還是綠的也是未知的所以我總共有這六個未知的參數	9-3
那你看就知道這個這個 model 是比標準的 h m m 要簡單	9-3
那其實這兩個 p a p b 呢就是我們講的 pi 對不對	9-3
然後呢在 given a 跟 b 之下紅球綠球的機率就是我們講的這個 b 但是呢沒有 a	9-3
我們在這裡 a 應該是哪個 state 要跳到哪個 state 的機率在這裡它沒有	9-3
所以這是個比較簡化的沒有 state transition 我就是 randomly 挑一個 state 這樣我有六個參數要求	9-3
但是我的 observation 只有三個我抓了三次球機得到的是紅綠綠	9-3
那所以呢我的這個 observation sequence 就是紅綠綠	9-3
但是到底是哪一個這個紅是 a 還 b 出來的這個綠是 a 還是 b 出來的我都不知道	9-3
所以呢這個是 latent data 就是 q	9-3
我的 q 呢就是像這種東西是 a a b 還是什麼不知道	9-3
是 a 裡面得到 r 還是 b 裡面得到 r 這個不知道喔那這就是我的 latent data q	9-3
好那假設我的 problem 是這樣的一個 problem 的話	9-3
我如何用剛才的這一個這一個架構來做這件事喔怎麼用這個架構來做這件事	9-3
那第一步呢那你看我們都是假設在某一個 iteration 已經有了才是嘛才能夠走嘛	9-3
那一開始的話顯然 k 等於零 k 等於零的時候隨便	9-3
想辦法你當然 initialization 是重要的啦你 initial value 要取的好啦喔	9-3
那我們現在先姑且說我的 initial value 我就取某一個值我就定一個 k 等於零的時候	9-3
於是這六個未知的東西我分別都給它一個零的值	9-3
那我們舉例來講我這兩個加起來應該是一嘛一個給它零點四一個零點六嘛	9-3
那這兩個加起來應該是一嘛	9-3
那我就給它這個一個零點五一個零點五這個也是零點五零點五我先給它一個起始值	9-3
有了起始值之後我我才可以開始算這一步 OK	9-3
那我們這邊底下寫的這一大段其實就是在講有了起始值之後怎麼算這個東西	9-3
那我們姑且說我們要做的是 maximum likelihood	9-3
所謂的 maximum likelihood 在這裡就是什麼呢就是這個 o given lambda	9-3
我們剛才說你現在我每一次都在算這個東西嘛	9-4
啊	9-4
就是 你每一次在算的這個其實就是那邊的這個式子嗎	9-4
就是在現在given 第k 個	9-4
第k 個iteration 我所估計的theta值	9-4
我得到的那個我要maximize 那個object function	9-4
那我希望下一個iteration 要變大	9-4
喔	9-4
所以我整個的條件就是這個	9-4
我下一個要把它變得更大	9-4
那為了要得到這一點	9-4
那麼我們底下來看這堆數學就是在講這一點是怎麼做到	9-4
那其實就是所謂的converge	9-4
因為你你每一次都變大每一次都變大它有一個上限	9-4
所以最後一定會收斂	9-4
那我們這邊用的符號呢	9-4
這個x 表示是是我observe 的data	9-4
z 是表示我的latent data	9-4
所以x 要跟z 合起來才是我的complete data	9-4
那問題就在於這個latent data 我看不到	9-4
喔	9-4
所以我的data 不所以我的data observe 到的是incomplete	9-4
所以呢我我我所observe 是一個incomplete data 就是一個沒有z 的	9-4
我我要跟z 合起來	9-4
才是complete	9-4
我就缺了這個z	9-4
因此呢我們可以寫底下這個式子	9-4
就是這個theta 是真實的我要求的那堆parameter	9-4
那麼我其實given 那堆parameter 我應該是有x 跟z 才對	9-4
可是呢我其實我只看到x 而已	9-4
那麼還缺什麼 其實是缺這一堆東西	9-4
ok	9-4
我看到的是只看到這個x	9-4
但是我其實需要的是這兩個	9-4
那我缺的是什麼呢 缺的是這一個	9-4
這個式子 其實很容易看	9-4
你知道我因為我右邊都有一個theta 你可以不要看	9-4
這個theta theta 都是given 的條件	9-4
如果這個不看的話呢 就是x 跟z 的joint probability	9-4
是這個z given x 乘上x 的機率嘛	9-4
對不對	9-4
這個condition 乘上x 就變成joint 其實就是這樣子	9-4
所以呢我就缺這堆東西	9-4
因此呢我現在所做的所有的這些data	9-4
這些個東西其實是這一項	9-4
缺了這一項	9-4
所以不是這一個	9-4
那我把這個拿出來看的話呢就是他除以他嘛	9-4
所以這個呢我能夠做的就是他他除以他	9-4
他除以他 就是以log 來寫 就是它減它	9-4
所以這個log 呢就是它減它	9-4
那我們現在都是用log 來做是因為	9-4
你知道log 反正是monotonic 所以我要maximize 這個東西跟maximize 這個log 是一樣的	9-4
可是用log 比較容易 因為兩個變成相減嘛	9-4
所以呢我要的是這個減這個的maximum	9-4
那這時候怎麼辦呢	9-4
就假設z 是可以得的這個呢就是z 的distribution	9-4
那這個玩意其實就是我們這邊講的這個z	9-4
就是這個z	9-4
假設z 的每一個值都有一個機率	9-4
我不知道z 但是我至少知道z 可以這麼有有這麼多個值 每個值都有一個機率	9-4
就是在given 這些條件之下的z 的distribution	9-4
那在我這邊寫就是寫成這個嘛	9-4
就是given 這個x 跟given 第k 個iteration的theta	9-4
given 這個x 跟第k 個iteration 的這個theta 之下的這個z 的distribution	9-4
喔	9-4
其實就是我這邊我這邊寫得比較簡簡寫啦 這個是偷懶的	9-4
這樣比較好看	9-4
但其實就是這個東西嘛	9-4
好	9-4
所以那怎麼辦呢 我就用這個東西來做這邊的所有的probability	9-4
我這邊的這三個probability 怎麼算	9-4
我都用這個來做平均	9-4
所以我就你看我這邊的這個這裡面這三樣東西就是這上面這三樣東西	9-4
但是我呢通通都用這個來做	9-4
來做z	9-4
也就是用這個來用這個distribution 來求它的expectation	9-4
ok	9-4
所以呢這個就是這個	9-4
但是我用z 來做expectation	9-4
等等等等	9-4
這個就是這個這個就是這個	9-4
我都用它來做	9-4
那變這個跟這兩個相減	9-4
那兩個呢分別就是這兩個個積分	9-4
那你看沒什麼特別 我只是在做積分而已	9-4
所以這個就是這個	9-4
然後呢	9-4
我要求這個用這個z 來做expectation 我就是把這個z 的distribution 乘進來	9-4
然後對z 積分	9-4
對不對	9-4
同理呢我要做這個	9-4
做z 的expectation 也是一樣	9-4
就是把這個東西寫進來	9-4
然後拿這個z 的distribution 相乘進去	9-4
然後對z 積分	9-4
所以這裡沒什麼特別我只是把它這樣做而已	9-4
所以就是等於說是這個等於這兩個相減 這才是我我我現在在做是這個東西 這兩個相減嘛	9-4
那我現在這兩個相減呢 這兩個呢我分別都對z 來做expectation	9-4
那其實就是因為我要用這個z 來做這個expectation 嘛	9-4
那要做這個expectation 我就是做就是把這個distribution 乘進來	9-4
然後對z 積分就是這樣子	9-4
那這兩個已經夠複雜了 所以我就把它寫成兩個簡寫的	9-4
前面這個積分叫做q	9-4
後面這個積分叫做h	9-4
那它們都是這兩個的function	9-4
一個是真實的theta 未知的	9-4
一個是我現在假設的已知的	9-4
那它也是是一樣	9-4
喔它們兩個各是一個真實的跟一個已知的	9-4
這兩個的function	9-4
所以我就是q 減掉h 的兩個式子	9-4
ok	9-5
那麼這一堆數學	9-5
我底下的下一頁上面其實是一樣的	9-5
因為這個powerpoint 的困難就是你沒辦法把兩張疊在一起	9-5
所以上面的這一這三行就是剛才的底下這三行 我直接把它co 過來而已	9-5
因為我要接下去的關係	9-5
所以這三行就是上面的這三行 所以我們剛才就做到這裡	9-5
我要的這個	9-5
你你一大堆數學不要被這個數學這個這個迷住了	9-5
其實這堆東西就是在算我就是在算這個我就是在求這個東西我要maximize 這個東西嘛	9-5
那我現在這個東西呢照剛才所推的我已經寫成這兩個q 減h 了	9-5
我要寫成q 減h 了	9-5
那這個q 減h 其實就是我這邊要的這個這個我要maximize 這個東西	9-5
那我現在目的是什麼	9-5
我的目的就是我每做一個iteration 之後	9-5
這個theta 的k 變成k 加一的時候	9-5
這個要變大	9-5
這就是我的目的嘛	9-5
換句話說	9-5
我我的目標就是	9-5
就是就是這個東西 我現在我沒有辦法做真的這個	9-5
因為這個我不知道	9-5
我只能做這個假的	9-5
就是用z 來求的這個這是假的	9-5
那這個假的說穿了就是就是這個是假的嘛	9-5
就是這樣子	9-5
那這樣 那我至少我要的目標是我這個每一次做下一個iteration 這個要變	9-5
每次加了一之後它要變大	9-5
因為我要maximize 這個東西我每一次要讓它變大	9-5
要make sure 這個變大是什麼呢	9-5
那你可以想像	9-5
那這個我我現在的這個q 減h	9-5
其實就是把這個這個theta 放在這裡得到的	9-5
那麼因此呢我現在	9-5
我	9-5
我現在如果要k 加一的話	9-5
這個k 加一是什麼 就是把這個k 加一的這個值代到這裡面來	9-5
喔	9-5
你注意到就是說這裡的話 這是一個未知的東西 我我不知道這個是unknown 的	9-5
這個求不出來的 我永遠不知道它的答案是多少	9-5
我是用這個z 去去假設求的嘛	9-5
那但是這裡面還是depend on 這個東西嘛	9-5
那我現在是希望k 加一的時候要比k 大	9-5
所以呢所謂的k 加一是什麼	9-5
不就是把這個的k 加一把這個k 加一代到這裡面去	9-5
所以呢這個東西其實就是	9-5
q 的的這個theta 的	9-5
q 的這個theta 的k 加一theta 的k	9-5
減掉h 的theta 的k 加一theta 的k	9-5
對不對	9-5
我你看我現在要的左邊這一項要比較大的這一項	9-5
就是把這個theta 的k 加一代到這邊的theta 裡面來	9-5
那也就是代到這個theta 來	9-5
所以就是q 的theta 的k 加一跟theta 的k 減掉h 的theta k 加一theta 的k	9-5
那要大於等於右邊的這一項呢	9-5
就是這個q 的theta 的k theta 的k	9-5
減掉h 的theta 的k theta 的k	9-5
就這樣子嘛	9-5
對不對	9-5
我現在右邊這一項是把theta 的k	9-5
代到這個來對不對	9-5
就是代到這這兩個位置來	9-5
所以就是q 的theta 的k theta 的k	9-5
減掉h 的theta 的k theta k	9-5
所以呢其實這也沒什麼特別	9-5
我只是把剛才這個式子	9-5
我現在把我這個條件	9-5
對不對	9-5
我把這個條件	9-5
這裡面這個每一個個寫成這個式子而已	9-5
就是把這個theta 的k 加一代到這個theta 的位置來	9-5
就是這兩個	9-5
就是上面這個	9-5
然後把這個theta k 代到這個theta 的位置來	9-5
就是底下這兩個	9-5
那它要大於它	9-5
那我如果要它大於它的條件呢	9-5
那我這個式子重寫一下	9-5
就是這個個式子	9-5
那就是它減它	9-5
它減掉它	9-5
然後呢加上它減掉它	9-5
要對不對	9-5
它減掉它加上它減掉它要大於等於零	9-5
所以就所以底下的這個式子	9-5
其實就是這個式子我再調一調就變成這個式子	9-5
當我調成這個式子之後呢	9-5
我底下說的這個式子永遠這一部份永遠是正的	9-5
不用擔心	9-5
為什麼因為jenson inequality	9-5
什麼是jenson inequality 你記得就是講p log p 一定大於p log q	9-5
這個是我們之前說過的	9-5
對不對	9-5
就是你如果p log p 其實是在求求那個entropy 的式子	9-5
但是你如果這個p 跟q 的機率不一樣的話 變成p log q 的話呢	9-5
一定是比較小的	9-5
我們原來寫的時候是寫成負的話是比較大啦	9-5
真正求entropy 是這邊加一個負號	9-5
加一個負號的時候是它大於它啦	9-5
但是我們現在寫成這個的話	9-5
就是就是寫成這樣子	9-5
那你仔細看這兩個h 的話呢	9-5
h 是什麼	9-5
h 就是p log p 嘛	9-5
對不對	9-5
這個是p	9-5
然後呢這是log p	9-5
那當這兩個這兩個一樣的時候	9-5
就是p log p	9-5
這兩個不一樣的時候就是p log q	9-5
因為這個地方這裡面一個是theta 的k	9-5
一個是真實的k	9-5
那我現在是把k 加一跟k 這兩個代到這裡面來	9-5
所以呢一個是兩個相同一個是兩個相不同	9-5
所以一個是p log p 一個是p log q	9-5
因此結果我們就知道它一定是正的	9-5
因此我的條件要我要這個是正的時候呢	9-5
這兩項相減它減它本來就是正的了	9-5
所以你要它正呢只要是這個是正就可以	9-5
因此呢我的條件就是這個是正的	9-5
ok	9-5
我的條件就是這個是正的	9-5
要這個是正的意思就是說	9-5
我這個q 這個q 就是這個function 就是這個東西呢	9-5
我每一次一定要	9-5
每每一個iteration 之後	9-5
我我這個東西要大於這個	9-5
那就是我的q 的這個這個東西呢我每一次	9-5
這個這個這個我嗯我的我的每一次其實就就是maximize 這個q 就好了	9-5
我們本來是說我maximize 這個	9-5
我本來是要maximize 這個嘛	9-5
這這個是這個原來的這個這個e m 我是在maximize 這個東西	9-5
但是但是其實這個東西呢	9-5
這個東西呢就是上面這個東西	9-5
我其實拆成q 跟h	9-5
而h 是沒有問題的是h 一定是是是正的	9-5
所以h 可以不要管了我其實只要管那個q 就好了	9-5
所以我每一次其實就是把這個q 就好了	9-5
所以呢你你看到我後來最後其實做的是這個q function	9-5
而不是	9-5
我可以把這個再簡化ok	9-5
我原來是在講這個東西要maximize	9-5
但是其實這個東西呢你是拆成就是就是這個式子	9-5
那我把它拆成q 跟h	9-5
而h 會會會是正的是沒有疑問的	9-5
因為jenson's inequality	9-5
所以呢我其實只要要make sure q 就好了	9-5
因此呢我就現在就變成說我其實是要算這個q	9-5
那這個q 就是所謂的在e m 裡面的所謂的auxiliary function	9-5
那在我們後面講了很多東西我們都說要算要用e m 來算	9-5
你去讀那些paper 的時候它都會說	9-5
它在找這個q function	9-5
那個q 就是這個q	9-5
就是這個q	9-5
那其實只要這個q 的的你每一次去那其實還是一樣	9-5
我我真正的q 是這個	9-5
是這個是第k 個iteration 的那個theta	9-5
然後這個是一個未知的	9-5
那我每一次呢我就去調這個未知的這個東西	9-5
去找看哪一個能夠讓它maximum	9-5
那通常的辦法是對它微分	9-5
讓它微分等於零	9-5
通常讓它微分等於零的時候呢那個theta theta	9-5
就是我的下一個theta	9-5
那也就是保證這個我下一個q 下一個k 會比這個k 變大	9-5
所以我實際上在做的時候呢是estimate 這個q function	9-5
這個通常在paper 裡面稱auxiliary function	9-5
然後呢或者叫做q function	9-5
然後呢我就是求這個q 的這個值	9-5
然後呢我每那麼然後我就每每一次呢我其實就是在做這個	9-5
所以m step 呢可以由這邊的這個寫法	9-5
reduce 到這個q 的寫法	9-5
然後呢我就在q 上面來做	9-5
那這個就是我們一般所講的auxiliary function	9-5
就是指這件事	9-5
好那這是一堆數學	9-5
那麼底下我們就可以來看我們在第第四點零節講的basic problem 三	9-6
其實我們是用了這個的	9-6
你記得我們basic problem 三是幹嘛的	9-6
我們說	9-6
我已經有了一個某一個我已經有了某一個lambda	9-6
譬如說零到九	9-6
假設是這個聲音是八a b pi	9-6
這個是八的的model	9-6
我現在知道這個聲音是八的話	9-6
我把它放進來	9-6
我希望得到一個新的lambda prime 是a prime b prime pi prime	9-6
那得到這個新的之後呢	9-6
我再把這個重新放進去	9-6
再放進來再跑	9-6
那這個這個的iteration 就是我們當時講的forward backward	9-6
這個forward backward algorithm	9-6
那讓它一直跑	9-6
那麼這個過程其實就是用e m 在做的	9-6
那我們當時是沒有講e m 這個名詞	9-6
我們當時是說你怎麼做	9-6
你你q 放進去之後你可以算alpha 算beta	9-6
然後gama 什麼一大堆算出來之後	9-6
我們說a i j 有一個新的值	9-6
等於什麼除以什麼	9-6
這裡面每一個mean	9-6
mu i j	9-6
有一個新的值	9-6
等於什麼東西除以什麼	9-6
然後每一個covariance matrix 裡面	9-6
那你也可以算出出來它是什麼什麼什麼什麼等等等等	9-6
我們有有這堆東西	9-6
我們們在講那個basic problem 三的時候我們就說哦你代進去之後可以得到alpha beta 然後呢就這些東西就是這樣子	9-6
那麼我們解釋說這個裡面的物理意義你可以發現他們是有道理的	9-6
但是其實那樣不是那樣寫出來的	9-6
這個怎麼來的	9-6
這個就是用e m 推出來的	9-6
所以當時我們在四點零裡面算的那堆東西	9-6
是e m 推出來的	9-6
那就就是這樣推的我們這邊簡單解釋一下	9-6
我現在observation 是o	9-6
然後呢那我不知道這個東西啊	9-6
那所以我們你記得我們當時是講怎樣	9-6
我已經有一個initial 的model 在那裡	9-6
我才能夠做嘛	9-6
那initial 其實應該是可以從零開始	9-6
可是零很難做嘛	9-6
所以我們那時候說我前面initialization 要另有一套	9-6
我們用segmental k means	9-6
去先去想辦法做一個比較好的的initial 之後	9-6
然後放進來去求alpha beta 之後	9-6
可以得到這個	9-6
就得到下一個	9-6
然後呢再回來再做等等等等	9-6
那其實就是這樣做的	9-6
所以呢假設我現在observe 到一個譬如說這個是八的聲音	9-6
那麼我的latent data 這個state q 是不知道的啊	9-6
那怎麼辦呢	9-6
那麼我真正要做的東西其實是這一個	9-6
是given 這個lambda 之下	9-6
given 這個state 這個model 之下	9-6
我不但是要有這個observe 的	9-6
還要有沒有observe 到的q	9-6
一起的機率	9-6
這這樣子才好算	9-6
那這個東西呢你可以想成就是這樣子	9-6
那那它呢那那你這個式子很容易看啦	9-6
就是q 是看不到的	9-6
o 是看得到的	9-6
那這這兩個的joint probability 呢就是一個condition 再乘上那個condition	9-6
對不對	9-6
那lambda 反正都在右邊	9-6
你可以不看那個lambda	9-6
那這個就是這個等於這兩個相乘	9-6
那我怎麼做	9-6
我先做e step	9-6
這個這個這個e step 是什麼呢	9-6
就是這個q	9-6
就是我們剛才講的我要的這個這個q	9-6
裡面一個是真正不知道的	9-6
我永遠不知道的那個那個model 的真正的參數	9-6
一個是我現在第k 個iteration 的參數	9-6
那這個東西是什麼呢	9-6
那其實就是這個	9-6
那這個其實你如果仔細看的話	9-6
那就是其實就是就是其實就是這個嘛	9-6
在這個case 就是這個	9-6
那麼你看我是在	9-6
given 現在這個iteration 的model	9-6
就是這個	9-6
然後given 這個observation o 就是given 這兩個嘛	9-6
given 這兩個之後我其實是有一個q 的distribution 在這裡	9-6
所以given 這個之後呢	9-6
我用q 來做這個expectation	9-6
那麼因此呢我就對對這個每一個q 來做這個這個平均	9-6
也就是這個式子	9-6
你看這個式子就可能可能更清楚了	9-6
那這個就是	9-6
呃	9-6
given 這個現在第k 個iteration 的parameter 參數	9-6
就是這個	9-6
然後given 這個observation 就是這個	9-6
那given 這兩個情形之下我可以得到一個q 的機率	9-6
那就是這個	9-6
given 這個q 的機率之後	9-6
我現在那麼在這個q 的機率之下	9-6
在這個q 的sequence 之下呢	9-6
我現我真正的model 我會會看到的這個的機率	9-6
是我是這個東西	9-6
然後我對所有的q 加起來	9-6
那這個式子exactly 就是哪一個式子呢	9-6
就是你如果仔細看我們之前的這個例子裡面的	9-6
就是這個個式子	9-6
是exactly 一樣的	9-6
喔	9-6
那只是我們現在這這power point 沒有辦法把兩張疊在一起所以你只好跳過來跳過去	9-6
那這個東西	9-6
你看就是	9-6
我我given 這個given 現在假設的這個假設的這個參數	9-6
然後跟observation	9-6
我有q 的distribution	9-6
然後在這個q 的distribution 之下	9-6
我來算我的這個maxima likelihood	9-6
那跟這邊講的這個是一樣的	9-6
就就是這個就是這個	9-6
那麼在這個情形之下呢	9-6
這就是我的e step	9-6
當我把這個e step 算出來之後	9-6
我的m step 呢就是要對它去找最好的lambda	9-6
那怎麼找呢	9-6
就是對其實就是對lambda 去微分	9-6
那麼我這時候對lambda 去微分的時候有一堆constraint	9-6
因為這裡這個lambda 裡面有很多東西包括pi 啦a i j 啦	9-6
這個lambda 裡面就是一大堆這種東西啊	9-6
這是我們在四點零講的	9-6
lambda 裡面一大堆這種東西	9-6
它們都有一堆constraint	9-6
譬如說它的機率加起來要等於一	9-6
啊	9-6
這些東西加起來都要等於等等把這堆constraint 都算進去之後	9-6
我要maximize 這個東西	9-6
用微分啊等等這個方法去做它	9-6
你得到了的就是下一個	9-6
那那一堆東西就是我們在四點零所得到的那些式子	9-6
ok	9-6
所以我們在四點零的時候我們就說喔這樣這樣所以呢我就用這個來做下一個iteration	9-6
那我們當時是對每一個裡面的式子為什麼長這樣	9-6
我們說他們都有物理意義可以解釋	9-6
但是呢這些東西是怎麼來的我們沒有說其實它不是完全憑空想的	9-6
它是用這個推的	9-6
啊	9-6
那因此呢在在這裡面我在做maximize 的時候	9-6
就可以算出它的這些東西出來	9-6
那麼於是呢你有了這個之後呢	9-6
那你就可以得到我們那個時候所要的結果	9-6
然後那也是為什麼我們當時說你每跑一個iteration	9-6
這個機率會提高	9-6
那所謂每跑一個iteration 之後	9-6
每跑一個iteration 之後	9-6
你的這個機率會提高的原因其實就是	9-6
我們這邊所說的這個式子嘛	9-6
就是我們們這邊所說的嗯這個式子嘛	9-6
就是你跑一個iteration 之後	9-6
你的的新的model 看到那個機率一定會提高嘛	9-6
啊	9-6
那這樣子呢我們這這樣就得到我們用e m 來推的那個basic problem 三	9-6
好	9-6
所以你現在如果在回去看我們四點零所說的這個h m m 的這個basic problem 三的話	9-6
你現在去看那個課本裡面講的一堆那裡的一堆數學式子你現在看就沒有問題了	9-6
那它就是在做這件事	9-6
它裡面就會跑出一個q function 出來	9-6
那個q 就是這個q	9-6
然後它就用那個q 去去maximize	9-6
之後呢它就就得到這堆式子	9-6
那就是這麼麼來的	9-6
那這堆式子的物理意義我們當時就已經講過了	9-6
那麼因此呢這個就是所謂的e m	9-6
那麼我們的e m 說到這裡	9-6
那你我們可以回想一下我們後面講的很多東西的e m	9-6
好	9-6
我們當時都跳掉舉例來講在十一點零那裡面	9-6
十一點零的的的speaker 那裡面我們說過很多東西	9-6
譬如說	9-6
這個m a p 的時候	9-6
你做m a p adaptation 的話	9-6
你這個東西要怎麼求	9-6
我們說最後答案是這個	9-6
怎麼求我們沒有說	9-6
我們說用了一堆e m theory 所謂的e m theory 就是用剛才那些方法	9-6
你要要maximize 這個東西裡面有一堆堆unknown 的variable	9-6
我就去做e m	9-6
這是m a p 這裡所用的	9-6
在m l l r 這裡	9-6
我同樣的我要做maximum likelihood	9-6
然後呢我怎麼辦	9-6
我要這個東西maximum	9-6
我調這些a i b i 使得這個機率最大	9-6
怎麼調	9-6
用e m	9-6
那這個詳細的也是	9-6
你現在再再看那些paper 那一大那一大堆數學其實就是e m	9-6
同樣呢	9-6
我們後來講的eigen voice 裡面	9-6
eigen voice 裡面你一個新的speaker 進來	9-6
我怎麼算	9-6
我就是要讓這些東西a i 加起來的這個東西要最大	9-6
調所有的a i 使得它最大的那個a i 就是我要的a i	9-6
這個怎麼求	9-6
用e m	9-6
你如果現在再去看那些paper 的話	9-6
一堆數學式子就是在做那個e m	9-6
等等等等	9-6
那包括這裡的	9-6
這裡的這個s a t	9-6
也是一樣這東西是用e m 求的	9-6
喔	9-6
那等等	9-6
喔	9-6
所以我想這個這些我們當時跳掉的部分你現在應該都容易想像它為什麼是這樣	9-6
那同理譬如說	9-6
我們講這個g m m	9-6
這個g m m 就是一堆gaussian	9-6
一堆gaussian 的時候	9-6
這個	9-6
我我我們說你可以想像想像成	9-6
這是一堆gaussian 這是一堆gaussian 這是一堆gaussian	9-6
我們在做segmental k means 的時候我們說ok 我就做v q 嘛	9-6
譬如說你先先做一個v q	9-6
然後呢拆成左右兩半兩個v q 兩兩個	9-6
然後再拆變成四個	9-6
然後分別就求它的的mean 跟covariance 得到四個	9-6
這是一個比較粗的做法	9-6
這樣子做出來的你如果直接用v q 來做的g m m	9-6
不是最好的g m m	9-6
因為它不見得保證你的那個你的那個那個這個東西這種機率是maximum 的	9-6
你如果真的要做一個好的g m m 的話也是一樣用e m 來做	9-6
等等	9-6
所以這些東西都是可以用e m 做的	9-6
那這個	9-6
所以呢我想這個	9-6
我們現在e m 講完你現在再回去看所有的我們提過的東西其實都容易得到答案	9-6
好我們現在要講下一個m c e	9-7
這裡面的下一個東西是m c e	9-7
m c e 也是另外一個非常重要的觀念	9-7
那麼	9-7
帶到後來來就是所謂的discriminative	9-7
discriminative training	9-7
的觀念是由它開始的	9-7
那麼	9-7
所謂m c e 的就是minimal classification error 的training	9-7
這個觀念的基本精神是說	9-7
我們到目前為止講的所有的方法的training 都是只是為了要maximize 這個機率	9-7
舉例來講我如果要train 零到九這十個音的model	9-7
就是零的model	9-7
一的model	9-7
二的model	9-7
等等	9-7
到九的model	9-7
那我的目的都是讓每一個model 裡面的這個likelihood function 最大	9-7
譬如說如果是一的話	9-7
我就是希望一的聲音放進一的裡面來是最大的	9-7
這個我們到目前為止所有的觀念都是這樣子做	9-7
那這樣是不是最好呢其實這個不見不盡然是最好	9-7
為什麼呢	9-7
因為呢它不見得讓因為我們的目的是error rate 要最低而不是這個機率最高	9-7
這個機率最高是一定表示這個error rate 是最低嗎不一定	9-7
那麼我們舉例來講的話呢	9-7
譬如說在零到九的這個case 而言	9-7
一個很大的問題是一跟七很像	9-7
所以呢我用一大堆一的聲音train 成一個一的model	9-7
我用一大堆七的聲音來train 出七的的model	9-7
雖然這些個一讓這這個model 最大	9-7
這些個七讓這個model 機率最大	9-7
可是一跟七本來就很像	9-7
那麼	9-7
你現你現在如果去算這些個所有的位置一個未知的聲音進來我去算它lambda k 的這個model 的話呢	9-7
這個值你可能如果這個是一的話	9-7
是	9-7
假設這個假設這個聲音是一的話一最大	9-7
那麼六在這裡用六算出來在這裡用用這個八算出來在這裡	9-7
可是七呢呢很接很接近	9-7
對不對因為一跟七很像	9-7
它們兩個很像會很接近啊	9-7
那今天如果說我的training data 跟testing data 不太一樣我換另外一個人講的話	9-7
換另外	9-7
譬如說本來是這個人講train 出來我換另外一個人講的話	9-7
搞不不好它的七會比會比這個一大	9-7
因為這邊已經很像了這邊非常接近	9-7
也就是說	9-7
當我在用傳統的方法在train model 的時候	9-7
我常常只是要讓那個model 本身的機率最大	9-7
但是呢並不表示我的error rate 會最低	9-7
因為它並沒有考慮它們model 之間的相互關係	9-7
在這個case 而言呢這個有所謂的相互關係有所謂的competing class	9-7
這個一跟七就是competing class	9-7
非常近的	9-7
那即使我這堆training data 讓這個一在這裡讓七在這裡	9-7
它們有一些差別	9-7
那換了另外一個人來來講	9-7
或者在另外一個noise 的環境之下搞不好那個七就比算出來就就比一大	9-7
它很可能贏了它之後	9-7
我一就變成七了	9-7
那麼因此呢	9-7
我如何解決這個competing class 的問題	9-7
否則的話呢你很可能換了一個data 之後	9-7
喔	9-7
你你到了test data 之後呢	9-7
在你的test data 裡面搞不好你的competing class 就會得到比較高的分數	9-7
那麼因此呢	9-7
那麼就有人想說	9-7
既然這這樣的話我們應該要想的是除了要讓這個東西最大之外	9-7
我還要想辦法讓它們凡是competing 的model 我把它拉開	9-7
要想辦法讓它們會compete 的部分把它們拉的比較開一點讓它們不會compete	9-7
那拉開的辦法呢就是我用一個新的criteria	9-7
就是minimum error	9-7
那因為我真的目的是要error minimum 才是我的目的嘛	9-7
這個maximum likelihood 並不表示error 會minimum 嘛	9-7
所以我要用一個新的criteria	9-7
就是這這個minimum 的error 來做它	9-7
那這就是這一堆所謂m c e 的原始觀念的由來	9-7
這個在九零年代出來之後是一個非常重要的方法	9-7
那麼很多重要的問題都是由它來解決的	9-7
那麼在英文裡面有一個很有名的問題叫做e set	9-7
你知道什麼是e set	9-7
就是英文字母裡面的b c d e v t z 等等	9-7
那麼這些個就像像我們的一跟七一樣	9-7
是很像很像的	9-7
它們只有最前面一個子音不一樣後面都一樣	9-7
所以呢這個b c d e v t z 的時候	9-7
你你在講一個一個spelling 的時候是很容易辨錯的	9-7
那當然在中文裡裡面有更多這種set	9-7
譬如說八搭它啦嗯這些	9-7
嗯這種都是屬於一種非常confusing set	9-7
那所有的這些set 都會發生這個問題就是它們會非常像	9-7
那麼你即使train 的時候	9-7
用maximum likelihood 把它們train 得很像	9-7
但是呢你test 的時候環環境不一樣speaker 不一樣noise 的環境不一樣之後	9-7
很可能會互相confuse	9-7
那麼後來就是有了這一招之後	9-7
就可以把它們都拉開來	9-7
所以這一招是一個非常重要的	9-7
那在絕大多數情形之下它們都非常有效可以把正確率提高	9-7
那底下我們來講這一招是怎麼做的	9-8
那它的基本精神就是說我改變我的原來的criteria	9-8
原來是我原來是maximize 這個東西我現在要minimize 這個error	9-8
那麼理由就是說	9-8
我的我的這個test data 可能跟training data 不一樣	9-8
就算你的training data 可以讓這兩個分的開	9-8
不過也差了那一點點	9-8
test data 再不一樣的結果就會把它們弄錯	9-8
那它的這套方法的basic 的formulation 是靠這個所謂的classification principle	9-8
那這個沒什麼特別這個跟我們講的其實是一樣的東西它只是另外一個說法而已	9-8
譬如說我們這邊講的零到九	9-8
十個model	9-8
或者這個b c d e v t z	9-8
有譬如說有十個不同的音	9-8
那那那麼有m 個不同的的音	9-8
就表示有m 個不同的class	9-8
那我為每一個class train 一個model	9-8
我為每一個class train 一個model 所以lambda i 呢就是class i 的model	9-8
那每一個class i 就是一個譬如說零或者一或者二或者三	9-8
喔	9-8
那麼然後我所有這些model 的集合大lambda	9-8
就是所有的model 的集合	9-8
那我現在的observation 一個聲音進來譬如說六	9-8
那這個聲音進來的話呢	9-8
我就就可以去算那個聲音對每一個i 的model 的機率	9-8
那我看它誰最大	9-8
它就是誰嘛	9-8
所以呢我我如果把我我我現在進來一個聲音如果是六的話	9-8
我就去算那個六的聲音對每一個lambda i 就是每一個從零到九的每一個model 的這一個分數	9-8
那我把這個分數寫成g sub i 的這個observation	9-8
跟這整套model 因為我要在整套model 裡面都去算	9-8
那麼然後呢我看誰最大	9-8
看哪一個這個叫做g sub j 裡面看哪一個j 最大	9-8
最大的那個j 就是我的答案	9-8
所以呢那這就是我的classification principle	9-8
所以這邊講的其實是跟我們之前所講的其實是完全一樣	9-8
那麼	9-8
嗯	9-8
唯一不同的是它把它稱做classification principle	9-8
那這個是是在pattern recognition 裡面它們用的語言	9-8
那意思是完全一樣的	9-8
那當然它這樣寫的一個好處是說這個g sub i 的function 不一定是這個	9-8
你還可以算別的譬如說你可以把這個prior 機率算進去變成m a p 的probable m 也可以哦等等	9-8
所以不一定是這一個	9-8
所以它就減就用一個符號來代表就是g sub i 的	9-8
我就given 一個observation	9-8
我就把它放到這一整堆的model 裡面去	9-8
那麼然後呢假設如果它是第i 個model 的話	9-8
就是g sub i	9-8
然後我對每一個j 都去算這個分數	9-8
看誰最大	9-8
如果那個最大的是i 的話	9-8
我的答案就是i	9-8
所以呢什麼時候發生錯誤呢	9-8
就是如果你你放到第i 個model 裡面去最大	9-8
但是其實它不是i	9-8
這個時候就發生error	9-8
因此呢這個所謂minimal error 的這個criteria 我就是要要數這個error	9-8
然後想辦法要把這個error minimize	9-8
那麼當我在minimize 這個error 的時候呢	9-8
就自然的就因為很多的error 會發生在這裡	9-8
凡是譬如說我們說這個一跟七很像	9-8
六跟九也很像	9-8
那麼那你這個時候呢	9-8
你很多個error 發生在這個時候所以你如果有夠多的training data 丟進來的話呢	9-8
那如果我要minimize 的是這個error 的話	9-8
那它就要調這些model 參數	9-8
想辦法把它們拉開把它們拉開	9-8
那這個就是這個m c e 基本的精神所在	9-8
那詳細的做法呢在下一頁這裡	9-8
嗯我們也許在這裡休息十分鐘好了	9-8
那麼m c e 的基本精神我們剛才已經提到了	9-8
就是說嗯我們原來的做法都朝向於讓這個maxima likelihood 最大	9-8
假設我我是六的話就讓六的model 最大就好了	9-8
可是這個並不不表示六跟九很像的話六跟九不會confuse	9-8
那麼他們這這些聲音如果很像的話	9-8
雖然各自機率都最大但是他們可能很像	9-8
因此我換一個別的testing data	9-8
換了不同的人不同的的環境搞不好它就贏了它	9-8
所以呢這些competing class 可能會贏過真真正的答案結果就會發生error	9-8
因此呢怎麼辦呢	9-8
我們就用底下的辦法就是反過來我來數error	9-8
然後我要minimize 這個error	9-8
那麼這個做法呢就是我重新定義這個東西叫做d sub i 的	9-8
這個x 是我一堆training data 的observation	9-8
這個lambda 是我原來已經train 好的model	9-8
我們剛才說我的假設我零到九好了	9-8
我零到九我每一個model	9-8
每一個數字零或者一或者二都是一個class	9-8
都有一個model 就是lambda i	9-8
然後我的所有的lambda i 構成一大把的model 就是我的大lambda	9-8
所以這個大lambda 就是我已經原來已經train 好的那一堆model	9-8
那一堆大lambda 已經train 好的model 之後我現在一個新的一個observation	9-8
譬如說是七進來的時候呢	9-8
我就可以define 一個d sub i 的這個東西	9-8
那它是什麼呢它是長這樣的一個東西	9-8
其中第一項呢就是g sub i	9-8
其實就是把它放到它該有的model 裡面去	9-8
假設我這個聲音是七的話	9-8
假設我這個training data 這個training 的聲音observation training 的observation x 是七的話	9-8
我就把它放到七的model 裡面去	9-8
所得到的分數叫做g sub i	9-8
就是我們剛才前一頁的這個東西	9-8
這個g sub i 就是把它放到第i 個model 裡面去的那個分數嘛	9-8
那麼因此呢但就就是那個分數但是我把它變成負的	9-8
那後面這些呢是g sub j j 不等於i	9-8
其實放到其他不同的model 裡面去的分數	9-8
舉例來講	9-8
如果那個這個音是是一的話	9-8
假設說這個x 是一的話	9-8
那麼我把它放到一的model 裡面去得到這個分數	9-8
那放到七呢很接近	9-8
放到到六啊九啊八啊五啊那就很遠了	9-8
譬如說五在在這裡	9-8
ok 所以呢那這些個分數就是這些個g sub j 的	9-8
換句話說假設我這個聲音是一	9-8
這個聲音是屬於一是class 一的那個model	9-8
所以我放到一裡面去呢	9-8
suppose 這個個分數是最高的	9-8
就是這個	9-8
然後呢那其他的g sub j	9-8
j 不等於i 就是其他的這些音	9-8
就是比較低的	9-8
其中呢七跟它是competing 所以呢七很接近	9-8
其他呢跟它比較遠	9-8
那我把其他所有這些東西呢做一個這樣子的計算這一項	9-8
那這一項呢裡面有一個alpha 的值	9-8
這邊是alpha 次方	9-8
那我把其他所有這些東西呢把其他的全部加起來因為這邊m 是總共的class 的數目嘛	9-8
我們這邊m 是總共的model 的數目是m	9-8
所以呢m 減一就是其他所有的	9-8
所以呢那這個是什麼東西呢我們來看	9-8
如果alpha 等於一的話	9-8
我們先看alpha 等於一	9-8
就是這裡邊等於沒有alpha 這裡也沒有	9-8
那就把它全部的平均	9-8
那意思就是說我把這些個所有的從七開始到這這邊所有的其他的東西	9-8
平均起來得到某一個平均的值對不對	9-8
我如果是alpha 等於一的話這個沒有這個沒有嘛	9-8
這個就是把其他的其他的class 都算進來得到一個平均	9-8
有equal weight	9-8
那就是這個值	9-8
那這個時候會怎樣呢	9-8
那顯然這些如果這個辨識是正確的的話	9-8
一應該是最高分	9-8
其他都比較低	9-8
然後這些東西比較低的平均起來當然比它低啦	9-8
所以嗯這個比它這個正確的答案一定比這些錯誤的平均來的低來來的高	9-8
那然後呢我現在因為這個是負的	9-8
這個負的比較多	9-8
這邊加的比較少加起來是負的	9-8
所以呢這個值是負的	9-8
就表示說這個是一個正確的	9-8
這個辨識是正確的	9-8
因為你你你這些東西這些東西分數比較低	9-8
這個個分數比較高嘛	9-8
所以這個負的的比較多這個負的比較少對不對	9-8
所以這個負的比較多這個這個正的比較少	9-8
所以結果還是負的	9-8
所以負的話呢大概可以表示說這是一個正確的classification	9-8
如果是正的話呢就表示錯了	9-8
如果是正的話呢就表示錯了是因為	9-8
你正的話呢就表示這個比較小這個比較大	9-8
表示說這裡面有些東西超過它了	9-8
所以結果呢我辨識出來它的它的分數低	9-8
就發生錯誤	9-8
所以這是我用我用這個function 來來來算我的正確跟錯誤	9-8
那當然你說這樣子這個比不太合理	9-8
因為你把所有的這些東西平均起來拿這個平均值去跟它比	9-8
那本來就應該比它小嘛	9-8
是沒錯	9-8
所以呢你可以那另外一種做法呢另外一個極端就是當你alpha 趨近於無限大的時候	9-8
如果alpha 趨近於無限大會怎樣呢	9-8
你看如果我這些東西都都做個無限大次方的結果就只有最大的那個才能夠算	9-8
其他都其它都沒有了	9-8
所以只有most competing one consider	9-8
如果alpha 趨近於無限大你其實這邊是無限大次方這邊再無限大分之一	9-8
其實還是原來那一個啦	9-8
但是呢就是除了最大的那一個以外其他都等於不要算了	9-8
所以呢如果alpha 是無限大的話呢這些都這些都沒有了	9-8
那麼這個是這個是alpha 趨近於無限大的時候	9-8
那紅的這個是alpha 等於一的時候	9-8
alpha 趨近於無限大的話只有最大那個值才算	9-8
那麼於是呢那你就變成是正確的跟最competing 的那個在比較	9-8
那麼因此呢這個時候如果d 小於零的話呢表示說這個還是負的比較多	9-8
這個是負的比較少	9-8
所以這個還是比它大	9-8
所以還是正確的	9-8
所以呢這個d sub i 小於零表示是正確的	9-8
而只要它大於零就表示是錯誤的了	9-8
ok	9-8
所以呢這個alpha 其實只是一個可以調的值	9-8
看你要弄進來幾個	9-8
因為搞不好我這邊有好幾個competing class	9-8
我們講一跟七好像只有兩個competing	9-8
但是呢其實不一定啊你如果b c d e v t z 的話	9-8
一堆都在這裡啊	9-8
你就應該把這一堆通通都都拿來平均或者怎樣喔	9-8
所以呢你也可以加不同的weight 喔	9-8
所以呢你其實譬如說你的的alpha 不是無限大但是alpha 是十	9-8
或者二十	9-8
或者你選一個比一大很多的值的話	9-8
你就自動去weight 最靠近的對不對	9-8
你這個alpha 越大就是weight 越靠近的	9-8
alpha 越小就把越不靠近的的也都放進來的意思	9-8
所以呢你可以選擇那個alpha 來調看你要把competing class 裡面放進來幾個	9-8
那麼但是不管怎樣呢這個分數其實是告訴我	9-8
如果這個是一的話	9-8
一跟它的competing class 有多接近	9-8
那它它會被competing class 勝過結果辨識錯誤的的危險度有多少	9-8
就是這個d sub i	9-8
那有了這個d sub i 之後呢	9-8
我們當然希望minimize 這個d sub i	9-8
因為現在如果它是越負的話就表示它的這個正確是一個正確的嘛	9-8
所以這個個d sub i 凡是有一個正的就表示會錯嘛	9-8
我把很多training data 放進來	9-8
我把每一個一啊二啊三啊一七啊都放進來	9-8
那麼每一次如果越是容易被發生越是這個competing 的越厲害	9-8
差距越小	9-8
越有可能錯的時候呢	9-8
那這個分數就會越糟糕	9-8
所以我現在只要算d sub i 這個這個越小就越好越大就越有問題	9-8
所以我就是要minimize 這個東西	9-8
所以這個數值其實是代表我的一種error	9-8
那你注意到我現在不是真的算error rate	9-8
不是minimize 那個error rate	9-8
而是在算分數的差距	9-8
也就是說我是在算這個這個正確的那個那個class 的分數	9-8
跟competing 的分數差多少	9-8
我要這個差這個差的越大越好的意思	9-8
那差的越大的話就是這個東西負的越多	9-8
所以我要minimize 這個東西	9-8
那但是這個東西其實非常不homogeneous	9-8
因為你如果是不同的不同的這個七放進來d 放進一放進來	9-8
你譬如說一放進來t 放進來這個情形都不一樣	9-8
這中間可能差很多	9-8
那麼不太容易讓它們這個有一個比較統一的的minimize 的方法	9-8
所以呢就想一個辦法就是把它做一個normalization	9-8
那這個normalization 就是所謂的這個sigmoid function	9-8
那它的長相是這樣子	9-8
那這個圖畫的不太好我們畫的清楚一點的話呢應該是這樣子	9-8
它是一個底下是零上面是一	9-8
它在這個地方有一個很平滑的像像s 的形狀的一個function	9-8
那麼它這個switch 過來的這一點呢叫做theta	9-8
那這個斜率是影響斜率的是gama	9-8
那麼因此呢這個就是所謂的l of d 的這個function	9-8
就是這個function	9-8
長的應該是像這樣	9-8
也就是說呢這個在趨趨近於無限大的時候它一定是零	9-8
所以當你這個這邊我這個這個橫軸是d	9-8
縱軸是l 的d	9-8
那麼在d 趨近於負無限大的時候它一定是零	9-8
在趨近於正無限大的時候它一定是一	9-8
那中間從零switch 到一的這個地方呢	9-8
所以depends on 看你的theta 選什麼	9-8
那麼那麼theta 就是中間switch 的這一點就是所謂的theta	9-8
然後呢這個gama 決定我的斜率	9-8
換句話說你你你這個中間是是這樣的呢還是這樣的	9-8
你可以更陡嘛	9-8
你可以更陡的這樣子	9-8
或者你可以更斜的這樣子	9-8
都可以	9-8
那這個你要多陡多斜呢就是這個gama 來決定的	9-8
那這樣一個function 這樣function 的好處就是說你把所有的值都normalize 到零跟一之間了	9-8
你本來這個值我們剛才上面那個值的話呢	9-8
我是在算每一個model 譬如說b c d e v t z	9-8
這些每一個model 的分數都擠在一起有有有些d 進來的時候	9-8
它全部擠在一起	9-8
有的t 進來它分的比較開什麼的	9-8
那麼這個分數結果有大有小這個這個很亂啊	9-8
那你怎麼辦我把它全部normalize 在零跟一之間	9-8
所以每一個每一個testing data 進來training data 進來	9-8
每一個data 進來我的分數都是在零到一之間	9-8
那在零到一之間之後呢	9-8
我就比較好算	9-8
然後呢越小的話表示越越越是會正確嘛	9-8
因為越小的話就表示正確的答案比competing 的會是勝過competing 的	9-8
那小的越多就是勝的越多嘛對不對	9-8
所以呢越小就是越好的	9-8
但是我經過這個這個東西之後呢我把它全部normalize 在零跟一之間	9-8
每一個data 都給我一個零跟一之間的數字	9-8
那另外一個好處是經過這個function 之後	9-8
它是smooth 的function 所以微分比較好微	9-8
我要minimize 我現在要minimize 這個東西	9-8
這個東西越負越好	9-8
越負表示都是正確的對不對	9-8
你凡是有一個錯的它就會變就會變正嘛	9-8
所以我是越負所以我就要minimize 這個東西	9-8
但minimize 的過程之中這個東西很難minimize	9-8
這個不好minimize	9-8
但是我變成這種東西之後這個好minimize	9-8
所以呢讓它smooth 之後	9-8
我來來來來minimize 它	9-8
於是呢我真正要minimize 的這個overall 的這個performance measure 就是這個東西	9-8
那那這個式子這樣寫有點怪怪的	9-8
其實一個簡單的寫法應該寫成嗯應該是寫成這個	9-8
就是嗯我看看對我就是把這個d sub i 放到這個l 的function 裡面去	9-8
就是這個嗯這個式子	9-8
應該是說我把這個d sub i 放到d sub i 是這個x 在放在這個lambda 裡面	9-8
的這個d sub i 就是這個東西	9-8
我放到這個l 的function 裡面去	9-8
得到一個smooth 的零跟一之間normalize 的值	9-8
然後呢我應該是summation over 所有的x 屬於c i 的	9-8
如果它是它是這個它是b 就放在b 的那個裡面	9-8
它是它是e 就放在e 的裡面	9-8
它是它是z 就放在z 的裡面等等	9-8
都分都分別放在它自己的那個class 裡面	9-8
然後我再把所有的class 加起來的這個東西	9-8
其實底下這個就就是這個東西	9-8
那現在這樣這個這個是paper 上面的寫法有點有點這個不清楚	9-8
其實是一樣的意思	9-8
你看它的它其實就是講你如果是在哪一個class 裡面才算	9-8
不在class 裡面就不算嘛喔	9-8
你在哪個class 裡面的那一個呢	9-8
你就把它放到這個裡面去算這個l i	9-8
那其實就是把那個d 算那個d i 之後	9-8
再放在這個l 的這個sigmoid function	9-8
這個這個你知道這個所謂的sigmoid 就是一個像s 形狀的	9-8
把它放進來	9-8
然後全部加起來	9-8
那其實是就是這個意思	9-8
也就是說假設我們是要分辨這個b c d e v t z 的話	9-8
那麼你就把所有的training 的b 放在b 的那個model 裡面	9-8
得到的這個	9-8
但是呢你同時要把其他的competing 的都拿進來一起算	9-8
得到這個東西的那個b	9-8
然後去做一個sigmoid function	9-8
然後全部的通通都算了之後	9-8
b 通通放成b 的	9-8
d 通通放成d 的	9-8
z 都放在z 的等等	9-8
你全部通通都通通加起來	9-8
你所有的training data 全部加起來這個東西	9-8
我要它是minimum	9-8
當我要minimize 它的時候呢	9-8
我現在呢就是微分	9-8
讓它等於零呃	9-8
那因此呢就是我最我我最後就做這件事情要讓它等於minimum	9-8
那這個東西到時候是要微分不太容易喔	9-8
所以呢我就是要我我現在就是在剛才這個嘛	9-8
這個其實l 的x lambda 就是嗯就是底下的這個l 的x lambda	9-9
就是就是這個東西	9-9
也就是我我我那邊寫的那個式子是同樣的意思啦喔	9-9
就是這個意思	9-9
就是這個東西我要把它minimize	9-9
minimize 調什麼參數	9-9
調調所有model 的參數	9-9
這裡面的每一個b c d e 每一個都有它的model	9-9
譬如說譬如說這個v 有它的model	9-9
就是a b pi	9-9
這個是v 的那個model	9-9
那t 呢有t 的model	9-9
a b pi	9-9
有有t 的model	9-9
每一個都有每一個都有一大堆參數	9-9
那所有的參數一起調	9-9
所以這是一個不容易調的東西	9-9
因為我有我整個的model set 這整個大lambda 是所有的model set	9-9
這個大model 這個大lambda 是所有的model 的model set 所以很多很多參數	9-9
我要一起同時調所有的參數	9-9
讓我這個東西會minimum	9-9
這個minimum 的時候其實就表示說我讓它的competing 的分數拉開	9-9
competing 的分數越拉的開那個個分數那個那個就越低嘛	9-9
所以我就把它minimize	9-9
那這個的過程呢我們用底下這句話是最重要的一句話	9-9
就是說這樣的情形之下我每一個training 的observation	9-9
可以change 所有model 的參數	9-9
而不是change 它自己的 喔	9-9
我們之前的每一個model	9-9
每一個observation 進來	9-9
只change 它自己的參數	9-9
譬如說一個v 進來	9-9
我們只會去train 這個這個東西	9-9
調這個參數	9-9
它動不到別的	9-9
一個t 進來我只會調這個t 的參數	9-9
動不到別的	9-9
可是呢我現在的話呢	9-9
因為我現在是在minimize 這整個的東西	9-9
是在算所有東西之間的差距	9-9
所以呢當你一個v 進來的時候呢	9-9
它不是只調v	9-9
它同時會把其他跟它近的東西一起調	9-9
想辦法把它拉開	9-9
那當然你想這個問題是很複雜的	9-9
當你把這兩個拉開的時候可能另外那兩個會被弄緊啊	9-9
所以呢你就變成都有影響嘛	9-9
所以我是把整個的全部的加在一起之後一起minimize	9-9
所以呢我進來一個v 的時候不是只調v	9-9
而是全部都會凡是跟它competing 的都會被調到	9-9
那麼但是你把它們拉開的時候	9-9
不能別人弄緊了一定要都拉開嘛	9-9
你進來來一個t 的時候也不是只調t 自己	9-9
是每一個都會被調到喔	9-9
所以這個是跟我們之前講的的training 不一樣的地方就在這裡	9-9
那這種training 就是所謂的discriminative training	9-9
那這個觀念後來用到非常廣泛	9-9
一直到今天這個仍然是一個非常重要的大題目	9-9
你如果現在去找paper 的話	9-9
今年仍然有非常多的paper 在講這件事情	9-9
就是用什麼用這類的觀念但是有許多更新的方法	9-9
怎麼樣做這這件事情	9-9
讓所有的competing model 能夠拉開	9-9
然後讓它的正確率能夠提高	9-9
讓這些confusing 的或是competing 的不會製造error	9-9
那在那我們現在講的這個m c e 是這個discriminative training 裡面的始祖	9-9
就是當初最早的想法	9-9
是這樣做的	9-9
那現在這個很多很多了比這個進步很多了喔	9-9
那但是我想最原始的一個就是這個	9-9
那這個觀念應該是最簡單而清楚的	9-9
最容易瞭解的就是這樣子	9-9
所以呢我現在就是就是要minimize 這個東西	9-9
但這邊參數很多啊	9-9
我這邊可能有一大把model	9-9
每一個model 都有一大把參數	9-9
那麼我這個多東西全部要一起調怎麼辦	9-9
那麼這裡通常用的是這個	9-9
這是最普通的嗯方法	9-9
所謂的gradient descent algorithm 或是steepest descent algorithm	9-9
那你看這個式子你可能是看過的	9-9
在很多領域裡面都用這個個東西	9-9
那這個式子的意思其實是這個式子	9-9
也就是說假設對a sub 這個a 是某一個parameter	9-9
這裡面有一大把的parameter 有幾千幾萬個parameter	9-9
那那麼假設每一個parameter 叫做a 的話	9-9
那它都這樣子調	9-9
t 是指它第t 個iteration	9-9
我用很多個iteration 去調它	9-9
那第t 個iteration	9-9
變到t 加一是什麼呢	9-9
是根據它的微分	9-9
那這個的意思很簡單的講就是微分告訴我該怎麼改	9-9
假設說這是那一個參數a	9-9
而這是某一個某一個這個這個是我要的object function	9-9
這是我要minimize 這個function l 的lambda	9-9
那它這個l 的lambda 呢它是function of 一大把參數	9-9
我現在選裡面的一個a 而言的話	9-9
譬如說它是一一一萬個參數裡面的一個a 而言的話	9-9
那那那我怎麼知道呢這depend on 如果我現在的a 在這裡	9-9
第t 個a 在這裡	9-9
這個微分在這裡的話	9-9
這個微分告訴我說	9-9
其實我向這個方向移動的時候會更低一點	9-9
所以呢我下一步就會向這邊移動一點	9-9
因為這邊是有一個local optimum	9-9
那因此呢它就會告訴我應該向這邊移動	9-9
可是我今天如果我這個a 在這裡的話呢	9-9
第t 個iteration 如果在這裡的話呢	9-9
這個微分是這樣子而且很陡	9-9
它就告訴我說有一個minimum 在這裡	9-9
而且是你這個地方是很這個很陡的	9-9
所以你其實是可以走的很快我下一步就多走一點就走到這邊來了	9-9
那這邊是因為很平	9-9
所以呢不要走太多	9-9
因為你走太多可能走過頭可可可能會走到不同的狀況	9-9
因為也許這邊可能會上去了對不對	9-9
所以呢這邊如果很平的話你不要走太多	9-9
這邊如果很陡的話表示你可以再走多一點喔	9-9
那麼因此呢這個你就就用這個微分就是這個這個object function	9-9
對於這個參數的其中一個parameter 的偏微分來看	9-9
那它的正負告訴我是應該向這邊還是向這邊對不對	9-9
應該是小一點	9-9
下一步下一個iteration	9-9
應該是小一點還是大一點	9-9
它的正負告訴我是向這邊還是這邊	9-9
它的值呢告訴我應該走多一點還是走小一點	9-9
喔那麼因此呢用這個方式的話我每一個參數都用這個方式來調	9-9
所以呢這個個epsilon 呢是指這個adjustment 的step size	9-9
就是你每一步到底要走多少	9-9
那麼我主要的每一個下一個iteration 的a 呢跟這一個iteration 的a 呢	9-9
就是根據我的微分來決定應該要向哪邊移動	9-9
要動得多快還是多慢喔	9-9
用這個方式來做	9-9
那t 呢就是我iteration 的這個次數	9-9
那這個式子呢是指對一個參數而言	9-9
那我現在如果要把整個參數全全部寫在一起	9-9
那用vector 的寫法就是這個式子	9-9
那其實這裡這個一萬個a 加在一起就是lambda	9-9
就是指那一大把的parameter	9-9
它是在時間第t 個iteration 的時候	9-9
這是這一大把的參數呢加在一起就是這個大的lambda t 加一等等	9-9
那這一大堆對的每一個的偏微分呢	9-9
我們就寫成這個符號喔	9-9
這個你如果學那個嗯數學裡面就有這個符號	9-9
這就是對每一個參數都去做偏微分的意思	9-9
所以這個式子其實就是這個式子的意思	9-9
那你可以想像的情形是是這樣	9-9
我們也許沒辦法畫這麼多維我們如果畫兩維的話	9-9
也許稍微可以畫一下	9-9
譬如說我有一個碗的形狀	9-9
假設我有一個碗的形狀	9-9
那麼在一個這這個在一個這個二微的空間裡面	9-9
假設我的這個是這是我的	9-9
這個這個是我的這個l 的lambda	9-9
在這個這譬如說這個是a one 這個是a two	9-9
在這個碗的形狀裡面	9-9
那假設我現在是在某一點上面	9-9
在這裡的話	9-9
那麼這個時候呢	9-9
在在a one 而言我有一個這樣子的的一個一個斜率	9-9
得到一個這個斜率	9-9
對a two 而言我有一個這樣子的一個我也得到一個這樣子斜率	9-9
等等喔	9-9
那所以呢我a one 朝向a one 的那個那個方向走	9-9
a two 朝向a two 的那個方向走	9-9
因此我最後得到一個朝這個方向的	9-9
其實就是走向碗底的喔	9-9
因此你可以想像這是一個這個很多dimension 的一個這個一個這個function 的話呢	9-9
它會朝向這個一個local 的optimum 去走	9-9
那麼當然這裡有一個重要的問題就是說它只會走到local optimum	9-9
它不知道哪裡是global optimal 呃	9-9
這個是這個only converge to local optimum	9-9
local minimum	9-9
跟我們前面講的e m 也是一樣的	9-9
我剛剛才漏掉講e m 有同樣的問題	9-9
e m 其實它是每一次向上走只走到一個local 的maximum	9-9
它不見得找到global 的	9-9
這裡也是一樣你可以看得出來嘛	9-9
喔它就會走到local 那裡	9-9
所以你的起始點是非常重要的	9-9
那這個這個initialization 如何選擇一個好的initialization 很重要	9-9
那基本上來講在這個case 就是你原來那個model 要train 得好	9-9
一開始的這些model 都已經train 得非常好的話	9-9
那基本上應該是一個比較好的initial 的condition	9-9
這個時候你再向這個這個再在這個地方調的話基本上它可以調得比較好	9-9
嗯就這樣的意思	9-9
所以我得到的是local minimum	9-9
你就經過很多的iteration	9-9
不過這個程式是有技巧的	9-9
就是說這個因為你現在有一大把參數成千上萬的參數一起調	9-9
是很難調的	9-9
那有本領的慢慢調調出來可以得到一個非常好的結果	9-9
那包括譬如說這個每一步到底要走多遠	9-9
這裡有一個epsilon	9-9
這是一個step size 的參數	9-9
你也要小心的選	9-9
你可以想像如果這個epsilon 太小的話	9-9
我收斂太慢	9-9
一次只走一點點我走不過來	9-9
可是你如果epsilon 太大的話很容易一走走過頭	9-9
對不對你這個一走走過頭結果就就就收斂不到那個那個minimum 去嘛喔	9-9
所以這個怎麼去選擇也是一個問題	9-9
然後你可能每一個參數可以選它自己的epsilon 等等	9-9
所以有很複雜的方法來做它	9-9
不過基本原理呢大部分用這個方式來做呢	9-9
可以做到	9-9
那它的基本精神就是我們剛才講的	9-9
我每一個training data 呢它可以	9-9
它是用來調所有的model	9-9
而不是調單獨它自己	9-9
一個v 進來的時候不是只調v	9-9
它所有的會跟v confuse 的音全部都調了	9-9
一起都拉開來	9-9
等於是這樣的意思	9-9
那這個觀念就是所謂的m c e	9-9
那m c e 可以拿來做很多事	9-9
我們舉一個例子就是m c e 來做feature optimization	9-9
也就是說如果我我原來的如果我原來的這個m f c c 不夠好	9-9
我可以做一個新的一組	9-9
譬如說我原來的m f c c 我叫做x	9-9
就是這一堆三十九維的	9-9
這x	9-9
我覺得這個不夠好	9-9
我可以做一個y 是譬如說a 乘上x	9-9
對不對	9-9
我用一個a 的matrix 乘上x	9-9
得到一個新的y	9-9
得到一個新的y	9-9
我可以用y 來當當我的feature	9-9
那這個a 裡面就有三十九乘三十九個參數	9-9
你要求這個a 可不可以求呢可以	9-9
那用m c e 來求	9-9
那這個就是用用m c e 去找一個好的feature	9-9
那怎麼做呢你可以想像今天還是一樣	9-9
我的原來的要minimize 的東西是這個嘛	9-9
我現在呢這個x 變成f of x	9-9
所謂的f of x 就是我這邊的a x	9-9
我這個我把這個這個乘上這個a	9-9
叫做f of x	9-9
那於是呢我就把原來的所有的parameter 所有的這些feature 都變成f of x	9-9
當我這個feature 變成f of x 的時候我所有的model 都改了	9-9
是以是以這個f 為基礎的model	9-9
那這時候變成成一堆完全一堆新的東西	9-9
然後我也一樣可以去minimize 它	9-9
去調所有的參數	9-9
調什麼參數	9-9
就是調這裡面的所有的參數	9-9
因為這裡面的每一個這裡面的每一個參數都把我的這個m f c c 參數改變了	9-9
然後也因此把所有的model 都改變了	9-9
那我就可以調所有的東西	9-9
之後可以調到一個比較好的嗯	9-9
所以這裡裡我寫的就是說你可以用這個方法f 是一個transformation function	9-9
把你的原來的x 的feature 就是m f c c 調成y	9-9
那你也可以去那這個時候未知的參數就是這一把	9-9
就就去調所有的這這一把參數	9-9
那這裡的每一把都去做微分	9-9
你也一樣可以得到嗯	9-9
那當然因為這樣你改變了feature 所有的model 都改了	9-9
那你你就可以去minimize 它你也可以得到一組比較好的feature 等等嗯	9-9
那這一類都是m c e 的的基本精神	9-9
就是等於是我就是把這一大把的所有的參數拿來把它轉化成為它們的competing 之間的距離	9-9
喔我我這堆東西東西等於就是它們的competing 的model 之間的距離嘛	9-9
然後我要minimize	9-9
會造成error 的的的問題我就把那個東西minimize	9-9
那這樣子的觀念就是要把所有的會competing 的東西拆開來	9-9
盡量把它們拉開來	9-9
這就是所謂的discriminative training	9-9
那這個m c e 的方法在九零年代是非常成功	9-9
它apply 在這種比較小的vocabulary	9-9
像這個b c d e v t z 這個這種這種東西可以分得非常好	9-9
正確率可以提高很多	9-9
同樣我們拿來辨識中文的一跟七啊六跟九啊	9-9
也可以大幅提高正確率	9-9
這都很成功	9-9
但是當我們的字彙大到幾萬個字的時候	9-9
這個m c e 不太容易做了	9-9
因為你可以想像它太複雜了	9-9
那麼後來像在最近幾年非常熱門的是很多新的方法	9-9
精神也是這樣	9-9
但它是比這個還要複雜很多	9-9
那也都目的都是一樣的就是要把凡是confusing 的地方	9-9
把它拉開來	9-9
但是你不能為了你不能把這兩個拉開之後把另外兩個又弄得很近了對不對	9-9
假設說這兩個比較近這樣子	9-9
你為了把這兩個拉開	9-9
結果這一拉開它跟它太近了不行啊你要把每一個通通都拉開啊	9-9
你要想辦法做到把每個都拉開	9-9
那這個是非常非常重要的一個研究題目	9-9
在今天仍然是一個非常精采的研究領域	9-9
不過基本精神大概是這樣	9-9
那這個是m c e	9-9
那麼m 跟這個m c e 相關我們講一件事	9-9
就是我們在講十五的時候	9-9
呃我有提我們那時候在講feature base 的十五點零的feature base 裡面有一個地方有用到m c e 的	9-9
我們們那時候跳跳掉了	9-9
你現在就知道了	9-9
就是在feature base 的嗯這裡	9-9
這裡我們在講feature base 的時候我們做temporal filtering	9-9
那麼我們那時候就說用l d a 可以做	9-9
其實一樣的用m c e 也可以做	9-9
那這個你可以想m c e 跟l d a 其實精神是很像嘛	9-9
l d a 也是把它一個class 分得很開嘛	9-9
分開嘛對不對	9-9
l d a 的意思就是把每一個class 要拉開來	9-9
那所以當你有了class 的information 你就可以把它拆開	9-9
那同樣的m c e 也可以做的嗯	9-9
所以我們這個地方如果用m c e 來推一樣可以推得出來	9-9
ok 好	9-9
那到這裡我們大概把九點零的兩個重要的東西就是e m 跟m c e 都講到了	9-9
後面還有一個m m c e m m i e 我們等有時間再講	9-9
再那麼麼下週我想我會講的應該是這個十三跟十四吧	9-9
十三我看看十四是這個ya 這個spoken document understanding organization	9-9
然後是那十三應該是retrieval 對	9-9
我想我們下週會講這個十三跟十四喔	9-9
那這樣的話我們大概這個比較重要的一些個topic 大概都會cover 到	9-9
在這個學期之內	9-9
ok 好我們今天上到這	9-9
我的目標標是要在	9-9
這兩週把所有還沒有	9-9
ok 所以目標是要在這兩週之內把把所有還沒有這上面還沒有說到都要說到	9-9
那底下我們應該要進入的是第	10-1
我看我們還可以講個兩三分鐘我們稍微開個頭	10-1
就是十點零我們還沒有講	10-1
就是	10-1
utterance verification	10-1
跟key word spotting	10-1
那這個其實就是	10-1
我們剛才講的	10-1
譬如說我辨識出來的每一個答案	10-1
我都可以給它一個分數	10-1
怎麼樣做這個分數	10-1
才是可靠的	10-1
那這個分數告訴我	10-1
哪些可信哪些不可信	10-1
那就是所謂的utterance verification	10-1
verify 它	10-1
那然後這個	10-1
我們做key word spotting 其實是用這套東西來做的等等	10-1
那這個reference 我後面再說	10-1
那基本上	10-1
這裡面一個最基本的精神是來自所謂的likelihood ratio test	10-1
那	10-1
這個東西是	10-1
最古典的通訊原理裡面	10-1
做雷達的時候	10-1
恩	10-1
這個	10-1
他們所謂的detection theory 裡面的核心	10-1
那detection theory 是數十年前的	10-1
最古典的但是也是到今天為止最重要的	10-1
之一	10-1
所謂的detection theory	10-1
那這在幹嘛呢這個是	10-1
雷達裡面偵測敵機有沒有來	10-1
那你知道呢我基本上是放一個訊號出去	10-1
如果前面有敵機的話它會有一些反射的訊號回來	10-1
我從反射回來的那些訊號裡面偵測	10-1
有沒有敵機	10-1
那麼在另外一個例子呢就是	10-1
這個	10-1
海上的冰山	10-1
你知道這個這個在古代	10-1
在北大西洋航行的船隻	10-1
最怕碰到冰山	10-1
那在海面上這個這個大海裡面如果有一個冰山你不容易看得出來	10-1
用人眼睛看不太到	10-1
所以呢你除了用人眼去看之外就是用個雷達	10-1
你放一些訊號出去	10-1
然後如果是海面它也會反射	10-1
如果是冰山它也會反射都不大一樣	10-1
因此你就可以判斷	10-1
用根據那個來判斷它有還是沒有	10-1
那不管是有沒有冰山還是有沒有敵機	10-1
這種detection 就是都只有兩個hypothesis	10-1
就是有還是沒有	10-1
有敵機跟沒有敵機有冰山跟沒有冰山	10-1
所以你收到的某一種observation	10-1
它會有兩種distribution	10-1
譬如說這個是這個是零是沒有敵機	10-1
或是前面沒有冰山的話	10-1
我收到的很可能是一個非常random 的某一種樣子的observation	10-1
那h 零就表示沒有敵機或者沒有冰山或者前面沒有我要我擔心發生的事情	10-1
那可是如果是有的話呢	10-1
如果前面有了敵機或有了冰山的話呢	10-1
它可能會有另一個長相	10-1
這樣子	10-1
那麼於是呢我就要靠這兩種不同的distribution	10-1
我今天如果抓到一個收到一個signal 是這樣子的	10-1
那我應該判斷它是有還是沒有呢	10-1
如果收到一個在這裡的話	10-1
我應該判斷它是有還是沒有呢等等喔	10-1
那這就是所謂的dictation theory	10-1
也就是所謂的hypothesis testing 阿	10-1
那我基本上就是兩個hypothesis	10-1
就是有跟沒有	10-1
那麼它們各自可能有一個prior probability	10-1
就是有本來會有多少機率沒有本來會有多少機率	10-1
然後我的observation 是x	10-1
那這個x 呢我有兩種distribution	10-1
有的時候跟沒有的時候的distribution是不一樣的	10-1
那於是我現在就根據要根據這個x 我要判斷	10-1
我根據這些假設喔我現在知道這個	10-1
假設這個是知道的這個是知道	10-1
那我要根據這個來看	10-1
那怎麼辦呢	10-1
最基本的原則還是我們所熟悉的m a p	10-1
那也就是說你看這個其實就是m a p	10-1
就是posterior probability	10-1
當我observe 我要的是這個嘛	10-1
就是當我observe 到x 的時候	10-1
零的機率跟一的機率誰大	10-1
那這個其實就是a posterior probability	10-1
就是我observe 到這個之後我的我的想要知道那件事情的機率	10-1
那麼如果說是observe 到這個x 之後	10-1
零的機率就比較大	10-1
表示說呢我想可能是沒有	10-1
譬如說我observe 到這個話呢	10-1
零的那個機率比大	10-1
一的機率比較小	10-1
那我猜沒有可能比較合理	10-1
可是如果我observe 到這來的話呢	10-1
那一的機率比較大零的機率比較小	10-1
那我就應該猜它來了	10-1
那於是呢我這個就就可以根據這個這樣來判斷	10-1
如果這樣來判斷的話就是把這兩個m a p 的機率除一除求它的ratio	10-1
如果它大於它的話這個就大於一小於一嘛	10-1
那我如果這樣的話呢我那拿這兩個a posterior probability 的機率的的ratio 除一除	10-1
看它比一大還比一小來判斷	10-1
那這就是這個m a p 的principle	10-1
那但是這個機率怎麼求呢again 我不會求	10-1
跟我們之前講的都完全一樣我就倒過來	10-1
所以我不會求這個機率我會求反過來的	10-1
因為零跟一的這個是零還是一的這個機的這個機率就在這裡嘛	10-1
我如果倒過來的話這兩個是已知的嘛	10-1
我至少可以可以先量好如果前面有冰山的話它會怎樣	10-1
那如果前面沒有冰山的話我這個可以先量出來得到一個機率嘛	10-1
所以呢我如果倒過來的話呢這個機率就是這個機率就是有的嘛	10-1
然後呢再乘上那個除上那個	10-1
這就是我們講的那個bayes theorem 就倒過來的	10-1
然後呢你如果這樣的話呢	10-1
我這個剛才的這個式子就可以重寫成為這樣子	10-1
於是我就變成為我就會變成拿這兩個機率比一比	10-1
那就兩個機率其實就是這邊講的這個機率跟這個機率或者這個機率跟這個機率	10-1
那這兩個機率比一比	10-1
我跟一個threshold 去比	10-1
就可以判斷了	10-1
那這兩個機率是什麼就是likelihood ratio	10-1
因為它們各自是likelihood function	10-1
這兩個就是就是likelihood function	10-1
那因此這個叫做likelihood ratio test	10-1
那這個是古典的雷達理論裡面最核心的一個東西	10-1
所以這兩個ratio 就是 likelihood ratio	10-1
就是用這個方式來判斷喔	10-1
那我想這是一個簡單的introduction	10-1
那我們下週從這裡開始	10-1
我們講用這個觀念來做我們要做的事情	10-1
ok 好今天提到這	10-1
ok	10-1
我們今天是本學期最後一次上課喔	10-1
我們要把所有沒有講完的都在今天講完	10-1
喔我們先進入第十點零	10-1
就是utterance verification	10-1
那麼我們上週開始講到這件事	10-1
就是說呃utterance verification 基本觀念來自從前古典的通訊原理裡面的detection theory	10-1
那這個原來是用來做這個雷達的application 裡面用的	10-1
它就叫detect	10-1
就是要detect 說前面有沒有某一種我知道可能會發生的事情	10-1
那這種有沒有發生的情形就是所謂hypothesis	10-1
那麼換句話說呢假設你要判斷前面有沒有敵機	10-1
敵機來了就是一	10-1
沒有敵機來就是零	10-1
或者說是這個海上航行的船隻要看前面有沒有冰山	10-1
那麼有冰山就是一	10-1
沒有就是零	10-1
那麼這個時候呢我有兩個prior probability	10-1
就是有敵機跟沒有敵機的	10-1
或者是有冰山沒有冰山的一跟零	10-1
這兩個prior probability	10-1
那麼於是呢我的這個靠什麼呢	10-1
我靠傳一些個訊號出去	10-1
然後呢它會碰到前面的狀況之後	10-1
會有一些反射的訊號回來	10-1
那不管不管是海面有沒有冰山的話呢沒有冰山海面還是會反射一些一些訊號回來	10-1
但是如果有的話呢	10-1
反射的東西會不一樣	10-1
那麼我們舉例來講用最簡單的例子來說呢假設我收到的x	10-1
那麼這個x 呢這個喔它在如果說是如果說是海面上沒有冰山的話呢你就收到一些這樣子的	10-1
這個那這個就是probability of x	10-1
那麼given 沒有的時候的情形	10-1
那如果是有的話呢	10-1
如果有的話它可能會有一種譬如說這樣子的	10-1
那這個呢就是probability of x 呢	10-1
如果有的話的distribution	10-1
當然實際上不是那麼簡單我這只是用一個one d 的簡單的呈現的方式來說假設它們有這樣子不同的話	10-1
那麼因此呢我可以depends on 這個我收到的訊號來判斷	10-1
譬如說我如果今天收收到一個訊號是在這裡的話	10-1
收到一一個訊號在這裡的話	10-1
那麼顯然沒有的如果有有一個前面有某些事情發生了的機率是比較大	10-1
而前面沒有什麼狀況的話呢機率比較小	10-1
反過來呢我如果發生某一件是收到某個訊號在這裡的話呢	10-1
那麼表示說呢顯然是沒有什麼事情的機率比較大	10-1
有的是有什麼事情的機率比較小等等	10-1
那這個想法呢也就是所謂的其實最古老的m a p 來自那個年代	10-1
就是a posterior probability 呢	10-1
我看這個東西	10-1
那這個也是在看這個	10-1
那麼如果說是我收到的這個x	10-1
那given observation 這個x 之後	10-1
如果沒有的機率比一比有的機率大的話	10-1
那我就假設是沒有	10-1
那就是像這個情形	10-1
沒有的機率比有的機率大的話	10-1
我就假設是沒有	10-1
反過來呢如果有的機率比沒有機率大的話我就算它是發生了	10-1
那就像這樣的情形	10-1
那麼因此呢我就是那這兩個機率是什麼呢	10-1
這兩個機率其實就是a posterior probability	10-1
也就是在喔given 這個observation 之後	10-1
那我要的事情只有我我想要知道的事情只有零跟一嘛	10-1
有跟沒有	10-1
那就是在這個given observation 之下	10-1
前面有跟沒有的機率差多少	10-1
那麼它們的誰大誰小	10-1
那既然是這樣只有這兩個誰大誰小我就可以做個ratio 來看	10-1
那麼如果是大於一呢就是沒有小於一就是有等等	10-1
那這個a posterior probability 呢不太好算	10-1
但是呢我們可以把它倒過來	10-1
這就是我們之前所一再使用的是完全一樣的做法	10-1
就把它倒過來	10-1
倒過來之後呢	10-1
那這個機率就是我們這邊有的機率	10-1
也就是這邊這兩個機率	10-1
那這兩個機率其實就是likelihood function	10-1
所以呢我就有了likelihood 在這裡了	10-1
那麼我有了likelihood 之後呢這個我本來就有	10-1
假設這個是這個可以預先量好準備好的	10-1
同理呢這個東西就是prior probability 我可以假設本來就知道的	10-1
那於是呢這些都可以知道了	10-1
那這個呢這個其實不不需要	10-1
就像我們前面說的一樣	10-1
因為你反正是given 一個x	10-1
所以這個可以不用管	10-1
我只要看哪一個i 比就大	10-1
是零大還是一大所以這個沒有關係	10-1
於是呢剛才這個ratio 又變成這樣子	10-1
當你變成這樣子的時候這兩個就是likelihood function 的ratio	10-1
跟這個剛好倒過來	10-1
那這兩個likelihood likelihood ratio 的function 呢就是所謂likely ratio	10-1
那麼那麼既然這個是likely ratio 的話就拿這個去跟這兩個prior probability 的ratio 去比	10-1
那這個就是threshold	10-1
那麼因此呢這個就是古典的通訊原理裡面或者雷達的problem 所謂的likely ratio test	10-1
我就是看這個likely ratio 來決定有還是沒有	10-1
那麼這個東西呢	10-1
這個原來是在這個古典的這個雷達裡面的觀念	10-1
那我們現在怎麼拿來做utterance verification 呢	10-1
這個意思是說	10-1
喔假設某一段聲音	10-1
我們現在判斷它是某一個word i 了	10-1
那倒底是不是呢我要不要相信它呢	10-1
因為我們知道我判斷的每一個word 都會有error 嘛	10-1
所以呢你很可能我有我判一句話判斷出來有word one word two word three	10-1
這樣子到word n	10-1
我有得到這堆word	10-1
那麼倒底他們對不對呢	10-1
我們上週曾經說過	10-1
我們可以給它每一個recognized 的word 一個confidence measure	10-1
就是給它一個分數	10-1
譬如說呢這個我判斷是零點九	10-1
這是零點七這是零點二	10-1
然後這個是零點四	10-1
那這些這些東西呢	10-1
你可以假設我們讓它是介於零跟一之間	10-1
當然你也可以讓它介於不同東西之間	10-1
假設零跟一之間的話呢這個高又表示我相當相信這個答案是正確的	10-1
很低就表示呢八成這個是錯的	10-1
那當然這有很多種方法來做它	10-1
那麼我們這裡講的這個是其中的一種	10-1
最簡單的做法	10-1
那麼今天其實要做這個有很多種方法	10-1
這種東西就是所謂的confidence score 或者confidence measure	10-1
就是我判斷我的recognize 的result 出來之後我給它一個分數	10-1
我倒底要不要相信它	10-1
那麼你可以猜得到呢如果是這樣的情形的話呢	10-1
你要相信的可能是這個	10-1
你不要相信可能是這個	10-1
那麼呢這個要不要相信就就depend on 狀況了	10-1
因此如果在一個dialogue 系統裡面	10-1
它說我要去我要買飛機票或者我我要幹什麼的話你可以判斷你你所recognize 出來的那些word	10-1
它的分數高不高	10-1
然後我要不要相信它	10-1
那如果不太可靠的話呢我寧可如果它說的這這個key word 在這裡的話呢	10-1
我不要太相信我寧可重新再問一次等等	10-1
那這樣子的觀念就是所謂的utterance verification	10-1
那因此呢我要的是說假設某一個聲音這是一個x 是我的observation	10-1
我把它recognize 成為w i 了	10-1
我認為它是w i 了	10-1
但是到底是不是呢	10-1
那我就做一個這樣子	10-1
那你現在看這個式子就知道這個式子其實跟這個幾乎長得一模一樣	10-1
就是如果它真的是w i 的話	10-1
啊其實是這個式子啦	10-1
啊哦是那個就是那個	10-1
就是說我真的是w i 或是不是w i 的機率喔	10-1
那麼因此呢那就變成說是這個如果我真的是w i 的話	10-1
那麼我我會看到這個x 的機率	10-1
除以不是w i 這個底下這個東西w i 的bar	10-1
等於是說不是w i 的時候的機率	10-1
那我拿這兩個來ratio 一下	10-1
那這個意思其實就是跟這邊一樣	10-1
為什麼會有這個是跟不是呢	10-1
其實就是原來這邊的有跟沒有嘛	10-1
就這邊有跟沒有是一樣的意思嘛	10-1
那麼因此呢你實際情形就是說	10-1
我現在的這個w i 是那個你所判斷出來那個word 的的hidden markov model 的話呢	10-1
那分子的這個東西	10-1
其實也就是我們平常所說的那一個	10-1
是一樣的嘛	10-1
given lambda	10-1
就是你如果是那個word 的話那個word 的hidden markov model	10-1
然後在那個model 之下	10-1
given 那個model 這個的的情形之下的話	10-1
啊我會看到這個聲音的這個observation 是多少	10-1
就是這個東西嘛	10-1
這也就是我們basic problem one 所求的這個東西	10-1
其實也就是這個嘛	10-1
這個w i 就是這個lambda 嘛喔一樣的	10-1
然後這個就是這個的observation 嘛	10-1
那我現在分子再多一個呢就是不是w i 的	10-1
那麼不是w i 那是什麼呢	10-1
這個有很多不同的做法	10-1
基本上我們稱之為anti model	10-1
或者說是background model	10-1
或者是類似這樣的名字	10-1
譬如說它叫做這個alternative 這裡寫錯了應該是alternative hypothesis	10-1
那總之就是不是w i 的意思	10-1
那有很多種做法	10-1
那麼並不確定哪一種才是好的	10-1
因為不同的實驗做出來的的情形可能不太一樣	10-1
那麼你可以猜得出來就是depend on 這些是什麼word	10-1
然後depend on 你你到底判斷它是怎麼情形	10-1
那麼可能在不同的狀況之下不同的做法各有它的好處	10-1
所以呢我們其實並沒有真正的標準答案說哪一種才是最好的	10-1
這邊有幾個例子就是說你你這個是不是w i 這個model 是什麼	10-1
你可以trained with undesired phone units	10-1
所謂undesired phone units 就是說不是w i 的unit	10-1
譬如說假設說你現在是我現在我要算這個w one 的話	10-1
它是phone one phone two 到phone k	10-1
所排成的這個word	10-1
那所謂的un undesired phone units 就是	10-1
除了這些以外的其它的phone	10-1
那那suppose 這個如果是w i 的話剛好跟這些phone 都一樣	10-1
不是的就是其它的phone	10-1
因此呢我可以用所有其它的phone	10-1
train 一個model	10-1
這是一種做法	10-1
那你可想而知它的意思等於是說	10-1
它的這個哦它的這個變成是probability of x	10-1
given w i 除以probability of x	10-1
given 不是w i	10-1
那麼因此呢基本上來講	10-1
如果這個這個這個x 真的是w i 的話呢	10-1
這個分數會比較大一點	10-1
可是呢這裡面都不是這些phone	10-1
所以呢這個放到這裡面去呢	10-1
應該是積分數會很低	10-1
應應該是很小	10-1
所以呢這個大的除以一個小的越除之後會越變得更大	10-1
所以呢如果說它真的是它它真的是這個word 的話呢	10-1
基本上本來這就會比較大	10-1
而這個因為都是其它的phone 嘛	10-1
就會更小	10-1
那麼這兩個一ratio 的話呢就會使得它變得更大	10-1
反過來如果它不是這個這個word 的話呢	10-1
基本上這裡會比較小一點	10-1
如果這個聲音不是這個word 的話	10-1
它會小一點	10-1
可是呢它不是這個word 它就會有一些其它的phone	10-1
所以呢這個在其它的phone 裡它的分數會大一點	10-1
那這個一除之後呢小除以大呢會變得更小一點	10-1
因此呢我們平常只看這一個嘛	10-1
對不對	10-1
我們平常講好像是看這個	10-1
那麼這個大看誰大	10-1
那現在除了這個之後呢	10-1
其實是使得大的會變得更大	10-1
因為它要除一個比較小的	10-1
小的會變得更小	10-1
因為它要除一個比較大的	10-1
那麼因此它會把大的跟小的分得更開一點	10-1
那麼你更容易判斷這樣它倒底是不是	10-1
用一個這樣的觀念	10-1
那麼因此這個是講說我可以用這個undesired phone units	10-1
來train 一個這個這個anti model	10-1
或者叫做background model 就是要除的這個東西	10-1
那也有的時候人家用所謂的cohort set	10-1
所謂cohort set 的意思是說	10-1
假設我算出來的時候	10-1
這個w i 的分數是最高的	10-1
所以我判斷這個是w i	10-1
那麼其它的其他的word 呢	10-1
有差很多很低	10-1
可是有一些是比較接近的	10-1
w k 或者是w j	10-1
這些是比較近的	10-1
也就是說這些個word 比較會跟它confuse	10-1
這些個就比較不不會了因為差很多	10-1
那或者說這些裡面其實它有比較接近它的phone	10-1
會造成困擾的	10-1
那這樣話所謂的cohort set 就是	10-1
我把跟它比較比較接近的這一堆	10-1
其實也就是我的confusing 啊我的competing competing words	10-1
我把這些東西拿來	10-1
把這些個word 拿來train 一個model	10-1
那基本上就是我希望能夠跟它分開的地方	10-1
我拿那些東西拿來拿拿這些比較接近的這些東西就是所謂的cohort set	10-1
就是跟它比較像的這些word	10-1
把這些word 的聲音拿來train 一個model	10-1
那這個呢放在這個東西	10-1
那那這個意思呢你可以想像跟剛才這個是有像但是不太一樣	10-1
它變成是說呢	10-1
我現在我那個聲音如果是這些word 的話	10-1
基本上是會跑到這裡面來分數比較高一點	10-1
那在跑到那放到這裡來分數就會低一點	10-1
所以呢我這個就會變成一除之後比較比較小嘛	10-1
那反過來呢我如果是它的話呢	10-1
那這個會比較大嘛	10-1
我等於用這些來做來做對比	10-1
然後make sure 我我不會弄錯我我是這個還是這裡面的	10-1
我要把這些東西是它的還是這些要分清楚	10-1
喔那如果這樣做就是所謂的cohort set	10-1
那competing units 是說我這個其實我也可以把它變成units 變成phone	10-1
譬如說我的我的p 這個這個phone one 我有一堆它的competing 的cohort set	10-1
譬如說這個p 這個phone 的k phone 的j	10-1
啊這些東西是跟它比較比較容易混淆的	10-1
phone two 的話有這些東西跟它比較混淆的	10-1
那我拿這些東西來train	10-1
那我等於說是拿這些competing units	10-1
來弄一個model	10-1
喔的意思差不多是這樣子的	10-1
等等所以這裡的到這個這個background model 到底要怎麼做其實並沒有一定	10-1
那麼也不見得哪種一定最好	10-1
那麼在實驗裡面顯示是不同的case 不太一樣	10-1
所以我們不太知道exact 到底怎樣	10-1
但基本上這都是很常用的做法	10-1
那於是呢我現在不管怎樣我就是有一個這個真正的model 跟一個competing model	10-1
一個或者anti model 或者background model	10-1
那這兩個之之間的關係	10-1
我拿這個ratio 來做其實就是原來這邊的likelihood ratio 的意思	10-1
就是一個有是那個word 或不是那個word	10-1
好那這樣子做之後	10-1
那這樣子所得到基本上就是我們所謂confidence score	10-1
或者confidence measure	10-1
那麼這樣做的話呢我比較可以判斷它啊用這個判斷它好還是不好	10-1
那麼它的正確機率倒底是多高	10-1
當然現在這樣做的時候不一定是介於零跟一之間	10-1
你可能還需要一個另外一個呃另外一個mapping	10-1
或者是一個normalization	10-1
你如果希望它是在零跟一之間的話	10-1
那當然也不是一定要	10-1
那這個是早年最最基本的做這個verification 算這個confidence score 的方法	10-1
那當然到到了近年做的方法非常多了	10-1
不限於這一種喔	10-1
那有很多其它的方法可以做的	10-1
那你如果有興趣的話在很多的文章都會看到	10-1
他們怎麼樣做這些東西	10-1
那在在古古典的這個裡面這個threshold 怎麼算的呢	10-1
是它們的這兩個prior probability 的ratio	10-1
那在這裡我們當然不太知道這裡倒底是多少	10-1
不過事實上呢我們不太需要真的算這個標準的來	10-1
而這個是depend on 我們要它的error 是多少	10-1
那這就是所謂的這個type one error type two error	10-1
這個這個名詞也都是古典的detection theory 裡面講雷達所用的名詞	10-1
就是所謂的missing 跟false alarm	10-1
那你知道所謂的false alarm 的意思就是說敵機沒有來	10-1
但是你認為它敵機來了	10-1
所以呢我就這個這個這個拉警急警報	10-1
大家虛警一場	10-1
後來敵機沒有來	10-1
這所謂的false alarm	10-1
那或者說就是false detection	10-1
那什麼是missing 呢	10-1
是說敵機來了你不知道	10-1
那這個是missing	10-1
那這兩種在之所以稱為type one type two	10-1
是因為這兩種error 並不對稱	10-1
它的後果不同	10-1
那麼也就是說在如果你是其實判斷後來做這個零跟一的數位通訊的時候	10-1
也是用這個做的	10-1
數位通訊的基本原理也是靠這個來判斷零還是一	10-1
可是在數位通訊而言零當成一或者一當成零是對稱的	10-1
都發生一個error	10-1
所以沒有什麼不同	10-1
可是在這裡的話呢這兩個代代價是不同的	10-1
如果前面有冰山而你沒有發現冰山的話	10-1
很可能因此船就撞到冰山船就沉了	10-1
這個false 這個missing 的代價非常大的	10-1
反過來呢如果你沒有冰山而以為是有的話呢拉個警報	10-1
大家虛警一場後來呢就繼續這沒事	10-1
所以這兩個代價不同所以它們當成兩種不同的error 來看	10-1
就是所謂的type one 跟type two 或者missing 跟false alarm	10-1
那麼在這個情形之下的話呢	10-1
你就會有這個錯誤率	10-1
false alarm rate 或者是這個false rejection rate	10-1
那或者說是這個missing rate 等等喔都有	10-1
那麼這些個	10-1
那在我們這裡的話呢我們可以把它看成是	10-1
是一個跟我們之前講這個retrieval 的時候很像的一個問題	10-1
那麼假設說這一堆是正確的字	10-1
correct word	10-1
那麼這堆呢是這個verified word	10-1
那麼因為我現在是在做verification	10-1
我要verify 它對不對喔	10-1
那你會發生這種情形	10-1
這什麼意思呢就是說這個當你判當你得到某一個word 的時候你要再verify 一次它是不是	10-1
如果呢它的分數確實夠高而我認為是了	10-1
那正確的字而且我認為是了在這裡	10-1
但是也很可能它雖然是正確的	10-1
這個字其實辨視是正確的	10-1
但是因為我的這個分數不夠高	10-1
而我就放棄了	10-1
我寧可當它不對	10-1
我放棄掉了是這塊	10-1
那反過來呢這一塊呢是說其實那個word 是錯的	10-1
那個word 是錯的但是呢因為它的分數夠高	10-1
我當它對了	10-1
結果這個是弄錯了	10-1
那如果這樣的話這裡也有三塊就是a b c	10-1
因此你有兩個正確率	10-1
就是a 除以a 加b 跟a 除以a 加c	10-1
那a 除以a 加b 的呢這個呢	10-1
其實這個就是相當於我如果一減掉的話那這個其實就是喔missing	10-1
那如果一除以a 減a 加c 的話呢	10-1
那這個就是相當於false alarm	10-1
也就是說這個所所謂的一減掉這個a 除以a 加b	10-1
其實也就是b 除以a 加b 嘛	10-1
那這個意思就是明明是正確的word 而且你已經辨識出來了	10-1
但是你因為分數不夠高你就放棄了的話	10-1
那這個是你miss 掉的東西	10-1
那這個除以a 除以a 加c 其實也就是也就是c 除以a 加c 了	10-1
那就是false alarm	10-1
就是明明是錯誤的字	10-1
錯誤的word 但是你把它當它是對的了	10-1
那在在你total 裡面你當它是對的有多少	10-1
這是false alarm	10-1
那麼你如果這樣看的話呢這就是這兩個false alarm rate 跟這個missing rate 之間的關係	10-1
那如果是這樣看的話	10-1
這個跟我們在那這兩個圖你看就跟我們講retrieval 的時候的record 跟precision	10-1
其實是非常像的	10-1
那我們說的這兩個其實就是一個是record 一個是preci precision	10-1
這個其實是record	10-1
這個是precision	10-1
因此呢其實我們講這些這些正確率錯誤率	10-1
跟record rate precision rate 其實都是同一回事	10-1
那麼這些東西都有其實都有一個關係的	10-1
那我們那個時候說record 跟precision 你可以畫一個圖	10-1
那在這裡其實也是一樣	10-1
record 跟precision 你用一來減就變成missing 跟false alarm	10-1
差不多的東西	10-1
那麼因此呢我們也可以畫一個圖	10-1
我們舉例來講我如果畫成這個false alarm 在這裡	10-1
這個是一減掉missing 的話呢	10-1
那麼絕大多數的時候你所得到的curve 是是像這樣子的	10-1
那麼換句話說呢	10-1
你如果如果你要false false alarm 低的話	10-1
那麼missing 就會高	10-1
所以一減missing 就會低	10-1
那反過來你如果missing 要低的話呢	10-1
就是就是這個要高	10-1
那於是呢你的fall 你的你你的false alarm 就會高	10-1
那麼那麼事實上你可以想像這個就是選擇這個threshold 的問題	10-1
當你如果threshold 選得低的話	10-1
threshold 選得低的話	10-1
那麼動不動你就會認為是是對的	10-1
動不動你就覺得是有敵機來了	10-1
那麼因此false false alarm rate 就會高	10-1
可是我就比較不會有missing	10-1
那我如果threshold 選得高的話呢	10-1
我就不太會發生false alarm	10-1
因為我選得很高大部分都不會發生嘛	10-1
所以false alarm 就會就會低嘛	10-1
可是我的這個我也會missing	10-1
來了我也沒有會沒有看到	10-1
所以呢你就是depends on 你選擇threshold 的時候呢	10-1
那麼那麼這個false alarm 跟missing 的rate 變化的關係是像這樣	10-1
那麼當然ideal 的case 呢是是是在頭上	10-1
這是ideal case	10-1
那也就是所謂的ideal 就是你不管不管這個如何	10-1
你的false alarm 永遠是在零	10-1
我的missing 也永遠是在零喔	10-1
那這個是ideal	10-1
那你真正做的情形就會變成這樣子	10-1
那你越往這裡面越往那個地方貼近的話就表示越好啊等等	10-1
那這個跟我們在講retrieval 的時候的那一個喔喔我們有一個就做這個record precision plot	10-1
把這二個rate 畫做一張圖是一樣的意思喔	10-1
那現在是在我們用這個false alarm 跟這個missing 來畫的話就是這張圖	10-1
這張圖其實有個名字的這個叫做所謂的r o c curve	10-1
在古典的在古典的這個detection theory 裡面	10-1
這是所謂這r o c curve	10-1
就是這個喔receiver operating characteristics	10-1
也就是你你在你那個你那個雷達或者你那個你那個receiver 你操作的真正的特性就在這裡	10-1
那你如果是這條curve 畫出來越貼近於這個邊的話就表示越好	10-1
你離那個越遠就表示越差啊	10-1
這個等於是	10-1
這樣的關係所以這就是這邊所講就是說你的一個threshold 呢是是這個你真正怎麼調不是根據這個來調	10-1
其實不是跟據這個來調	10-1
那怎麼調呢就是就是看你要怎麼樣的performance rate	10-1
看你要這兩個precision 跟record	10-1
或者說是false alarm 跟missing	10-1
那他們之間你要他們的關係是什麼來來調這條curve	10-1
好那有了這個之後呢	10-1
我們剛才這個是講的是對每一個word 可以做這樣的事	10-1
那其實不不一定要對每一個word 來做我也可以對每一個frame 來做	10-2
對每一個phone 來做喔	10-2
那這樣的話我真正在辨識的時候我不見得像剛才那樣我只是為辨識到那個word 我來算一算它好不好	10-2
不是這樣子	10-2
我可以從頭在整個recognition 的這個架構裡面我就從頭來做這件事	10-2
那這個就變成所謂frame level 的confidence score	10-2
我可以每一個frame 都做	10-2
那我如果每一個frame 都做的話就變成這樣	10-2
那這個式子其實就是剛才的這個式子	10-2
是一樣的我剛才我是在看它是是這個word 跟不是這個word 的ratio	10-2
那現在呢我變成是這個這個frame 是不是那個state	10-2
所以呢lambda i 呢就是state i of 某一個phone p 的它的hidden markov model	10-2
所以呢我也只是看那一個frame 我本來剛才是在看整個signal 整個utterance	10-2
那我現在不是了我現在只是看那一個frame	10-2
我每一個frame 呢就有那個frame 我我當它是某一個phone p 的hidden markov model 某一個state i 這個lambda i 的話呢	10-2
它在那個lambda i 的的分數	10-2
跟它不是那個的分數	10-2
所以這個是那個不是那個lambda i 的anti model 或者background model	10-2
那它可以trained with cohort set for the phone unit p	10-2
就是它的那個phone 的cohort set 等等	10-2
有我們剛才說的那種情形	10-2
那這樣的話我就得到那一個frame 的時候的	10-2
我完全只有那個frame 的那個那個feature vector	10-2
在是這個state i 跟不是這個state i 的一個這樣子的confidence score	10-2
那然後呢這個score 可能就是我們剛才提過這這些score 不見得是這個分數剛好在零跟一之間或者怎樣所以你其實可以做一些normalization	10-2
或者做一些mapping transformation	10-2
讓它它的range 比較符合我們要的	10-2
像這裡這個case 它是說我可以做一個什麼呢	10-2
我可以做一個這個這個其實是一個log sigmoid function	10-2
你看這個裡面這個裡面這個東西其實就是我們之前講過的sigmoid	10-2
那麼外面呢其實是再加了一個log	10-2
sigmoid 是怎樣呢我們之前說過	10-2
它相當於這種東西	10-2
也就是在這這邊趨近於零這邊趨近於一	10-2
那在零到一之間呢我我有一個transition	10-2
把它switch 過來	10-2
那個transition 的位置呢	10-2
就是theta	10-2
然後這個這個這個transition 過去的斜率	10-2
是由這個gama 來決定	10-2
換句話說呢我這個可以可以更更斜	10-2
也可以更陡等等	10-2
那這個時候這個就depends on gama	10-2
所以gama 決定這個斜率theta 決定我這個transition 的範圍	10-2
那這個是一個sigmoid 介於零跟一的	10-2
是裡面這塊	10-2
現在再加一個log 會怎樣呢	10-2
加這個log 之後呢你可以想像會變成怎樣呢	10-2
log 的一變成零	10-2
所以這邊呢是變成零	10-2
log 的零會變成負的無限大	10-2
所以你得到的是一個這樣子的curve	10-2
那那這個點呢就是log 零點五的地方就是了	10-2
那現在呢如果不同的那同樣的我這一點仍然是theta	10-2
這點仍然是theta	10-2
那我的不同的斜率其實就就造成我不同的這個譬如說我可能有的時候是這樣的	10-2
那也有的時候是這樣的	10-2
喔等等	10-2
那那就是不同的斜率	10-2
那我不同的gama 就造成一個這樣的關係	10-2
那總之呢它是在原來是趨近於這邊的時候是正的趨近無限大它是一的會變成零	10-2
然後負的會變成負無限大	10-2
那這就是所謂的log sigmoid function	10-2
那如果是這樣的話呢我現在就是把把這個喔我把這樣所得到的這個這個raw i 啊	10-2
就是本來這個東西啊	10-2
我把它放到這來	10-2
放到這來之後呢放進這個function 裡面去呢所以如果說是我們原來這個東西本來就是在算它大小	10-2
看我們原來這個東西本來就是看它的大小	10-2
那麼越大表示它越可可靠越小表示越不可靠	10-2
那我現在是如果每一個frame 都做這件事的話呢	10-2
而且我給它每一個frame 這件事情我都給它做一個log sigmoid 的話呢	10-2
那就是說如果它大的表示比較可靠的呢	10-2
就會趨近於零	10-2
如果是小的話呢就變成負無限大	10-2
那這幹嘛呢	10-2
這個其實就是拿來做在我們正常的辨識裡面	10-2
那這個相當於說我們在八點零裡面所講的那個search 的process	10-2
我們你記得我們八點零的時候我們說我我現在譬如說有六萬個word	10-2
構成一個tree	10-2
我有六萬個word 的一個lexicon	10-2
你這裡面每一條path 就變成一個word	10-2
你這個走到底之後你會接下一個去去copy 你又可以走出一堆word 出來	10-2
你如果在這裡的話我也可以接一個下面一個tree	10-2
然後這個呢走完之後又有又可以接下一個等等等等	10-2
那我們的辨識是在這個上面做一個viterbi	10-2
那這個viterbi 這樣一路走	10-2
你可以得到譬如說呢這個是這樣子再走過來再走過來	10-2
那這樣就得到某一個word 接某一個word 接某一個word 等等	10-2
這是我們在八點零裡面說的這個辨識的架構	10-2
那這邊講的其實是你在做算這個中間的時候呢	10-2
我隨時可以多加一個這個分數	10-2
我一路加這個分數的時候你記得我們在viterbi 裡面我們是通常譬如說做這個做這個beam search	10-2
也就是說我會保留分數最高的一堆path 往前走	10-2
那這個時候我中間如果我一路走的時候我每一個frame 都加一個這個的話	10-2
這邊是這個橫軸是時間嘛	10-2
每一個frame 它都在每一個frame 它都有都有一個分數有一個位置又這樣子走過來	10-2
那我每一個frame 每一個t 這邊就是一個o t	10-2
那我就可以在那個model 裡面的那個state 去算它這個分數	10-2
然後放進這個sigmoid 來	10-2
就會得到像那邊那個情形	10-2
也就是如果是如果這個分數高的話	10-2
它其實就是零	10-2
所以呢沒什麼影響	10-2
所以不怎麼影響	10-2
可是如果說它的分數低的話	10-2
就這個小的話呢	10-2
這個小的話那邊就會變得變得很負的	10-2
變成非常負之後呢就讓你這條path 就不見了	10-2
因為會變成分數這條path 對不對你如果走到這邊的時候	10-2
這個confidence measure 很低的話	10-2
那它就會變成很負的	10-2
於是你那個那個分數就會變得很低	10-2
就會就會被排除在我的那個beam search 裡面喔等等	10-2
所以呢這樣一來的話呢我就可以這樣一路走到中間我可以用這個	10-2
那隨時在看你哪一個frame 走到哪裡覺得很不可靠	10-2
那條path 可能是不對的	10-2
啊就可以這樣子做	10-2
所以這個是在frame level 做的confidence score	10-2
那光是這個可能還不太夠	10-2
因為frame level 常常不太可靠	10-2
因為那個frame 不太對	10-2
但是也許前後的frame 都對的話表示這個可能還是對的	10-2
因此呢我們再加上phone level 跟word level	10-2
那phone level 是什麼呢	10-2
就是我在算這個frame level 的同時呢	10-2
我當我辨識到一個phone 的時候	10-2
把那個phone 一路走過來所有的frame 平均一次	10-2
所以這個只是做一個平均的意思	10-2
你看這個raw j 其實就是就是這個東西	10-2
這個raw j 就是這個東西就是它的那個那個frame u 的分數	10-2
那麼我假設我那個phone p 它的長度是tau 的話就是有tau 個frame 是那個phone 的話那我就把那tau 的分數做一次平均去把它通通加起來再除以tau 就	10-2
就做一次平均	10-2
所以你看它這個u	10-2
就是這個t	10-2
它是從t 減tau 加一加到t 嘛	10-2
就是你從這個前面tau 個一路平均下來平均到這裡	10-2
所以我就是把它一路做一個平均	10-2
那如果是這樣的話呢	10-2
我就evaluate at the end of phone	10-2
那現在你如果從這裡走從這裡走走到這邊的時候是一個phone 的話	10-2
譬如說走到這裡是一個phone 的話	10-2
那我在這個中間每一個frame 我都算一個分數	10-2
當這個phone 走完的時候我算一次	10-2
這個phone 的平均分數	10-2
那同理呢我也可以做一個word level 的	10-2
就是當我走走完一個word 的時候那個word 是由好幾個phone 拼成的	10-2
那我那個word 是由好幾個phone 拼成的話我就把所有的那些phone 的分數分數加起來再平均一次	10-2
那我假設那個word 有n 個phone unit 喔	10-2
n 是total number of phone unit in the word	10-2
那我就是把它那所有那些phone 的分數再平均一次等等	10-2
那它也是evaluate at the end of word	10-2
當你一路走走到這邊底的時候	10-2
我走到一個end of word	10-2
那這個時候呢我有我可以把這上面這幾個phone 的分數再平均一次	10-2
那就是這個word 分數等等	10-2
那如果這樣做的話呢我就變成每一個unit 每一個frame 每一個frame 再算一次	10-2
那每一個phone 再算一次	10-2
到每一個word 再算一次	10-2
那用這個方式來做的話我就可以得到一個generalized 的confidence score	10-2
我一路做都在做	10-2
那跟剛才這個不一樣	10-2
剛才這個是我辨識出某一個word 來去做一次	10-2
那現在這個不是	10-2
現在這個是我一路每一個frame 每一個frame 都在做	10-2
然後呢每走完一個phone 做一個phone	10-2
每走完一個word 做一個word	10-2
那於是呢我就有這三種分數	10-2
這個是frame 的那這是phone 的這是word 的	10-2
這個是word 的這是phone 的這是frame 的	10-2
我分這三種分數我分別都weight by 一個加一個weighting factor 之後呢	10-2
這個就是我的multi level confidence score	10-2
就是包括所謂multi level 就是有phone frame level phone level word level	10-2
我把它全部weight 起來	10-2
那那這個地方的話呢	10-2
那我們這邊看到是說以這三個weight 呢	10-2
如果不是在end of the phone 的話	10-2
phone 的weight 就是零	10-2
如果不是在end of the word 的話呢word 的weight 就是零	10-2
所以呢那麼它只有在end of phone 的時候算phone 的分數	10-2
end of word 的時候算word 的分數	10-2
那這個point 在這邊講就是你的frame level 的分數	10-2
也許不夠stable	10-2
所以你average over phone over word 會比較好	10-2
也就是說我這個這個frame 的分數也許算出來不太可靠	10-2
如果前面後面都有一堆的話呢	10-2
你算出來比較可靠所以你把它把整個的前後一起平均一下會比較好	10-2
那這個就是底下啊就是底下這張圖所畫的	10-3
也就是說這個是這個做這個viterbi 過程之中的那個圖	10-3
我們從前在講四點零五點零一直到後面我們常常在用的圖	10-3
橫軸是時間	10-3
縱軸是一系列的state	10-3
那假設說譬如說這邊走的是這個三個state 是一個phone b	10-3
這三個state 是一個phone a	10-3
那它們b 跟a 連起來是一個word w	10-3
那如果是這樣的話呢	10-3
那這個的意思你看就知道	10-3
就是說我這些frame 這個frame 是在這個state 裡面	10-3
然後這三個frame 呢是在這個state 裡面	10-3
這兩個frame 是在這個state 裡面	10-3
那這些加起來就是這個phone 的三個state	10-3
同理這個加起來就是這個phone a 的兩個state	10-3
你如果這樣看的話那這些加起來就是這個word w	10-3
那如果這樣它這邊就是說你的phone level frame level 的分數呢	10-3
每一點都加	10-3
每一點都算	10-3
然後呢你如果phone level 的話呢你就是在算譬如說這邊的這五個點這五個frame 是這個phone a 嘛	10-3
所以你就把這些東西平均起來	10-3
你在end of the phone 的時候在這裡的時候你算一次這個phone 的分數	10-3
那你如果要算這個word 的分數的話呢是這個全部平均起來	10-3
這些東西等於是這些通通一起算了	10-3
那這是一個phone 這是一個phone 這個跟這個平均起來	10-3
你會得到一個這樣子的	10-3
那在這裡的時候把這個word 再算進去	10-3
那然後這樣這個分數幹嘛呢	10-3
這個就這個就用在我們這就是我們第八點零所講的viterbi beam search	10-3
就是在這個裡面算的時候	10-3
我們在當時有過的這個分數	10-3
那這是在intra word transition as example	10-3
就是在這個裡面嘛	10-3
你如果記得的話就是在這個裡面的某一個transition 的時候	10-3
那這個viterbi 是怎樣的呢	10-3
這個是在時間t 會在state q t 在of word w 的時候的分數	10-3
在時間t 的時候走到state q t of 這個word w 的分數	10-3
那是怎樣呢是在t 減一的時候在q t 減一的w 裡面	10-3
然後呢再加上我從t 減一到t 呢我有transition probability	10-3
然後我有把新的frame 放在q t 裡面的機率等等加起來	10-3
這個其實就是我們在八點零裡面所說的那個分數	10-3
我現在可以比它再多加一個	10-3
多加了一個這個是什麼呢	10-3
這個就是我們剛才講的這個喔multi level confidence score	10-3
我把這這confidence 加在這裡	10-3
所以呢我在做的時候這個就是我們原來的做viterbi 時候那個公式的那個分數	10-3
我現在多加一個這個	10-3
那這個呢就是這個raw 的m i t 這個東西	10-3
就是我們剛才講的這個multi m 就是multi level	10-3
那麼在state i 在時間t 的時候的得到的這個分數	10-3
我把這個分數呢加在這裡	10-3
那它的功能其實就是我們剛才講的這個現象	10-3
也就是說你你如果那個分數大的話呢	10-3
其實就沒有功能	10-3
它就是變成零嘛	10-3
當你分數大時它趨近於零嘛	10-3
趨近於零所以呢就等於沒有一樣	10-3
可是如果分數差的話	10-3
分數小的話呢就變成非常negative	10-3
變成負得很厲害	10-3
負得很厲害的話呢你就把你那條path 會把你那條path 的分數變得很低	10-3
所以呢只有unlikely path 會被reject 喔	10-3
你越是不太可能的path 中間你你一路走那幾個frame 或幾個phone 越不太可能的話	10-3
分數馬上會變得很低	10-3
就是unlikely 就會被reject 掉	10-3
然後呢你這個對可能的path 沒有什麼影響因為它會變成零	10-3
因此呢你在beam search 很有幫助	10-3
就在中間走的時候呢它會自動把不太可能的東西拿掉等等	10-3
好這個是講這個confidence measure	10-4
底下我們來說一下key word spotting	10-4
那key word spotting 是什麼	10-4
那麼我們從開學第一週就說過了	10-4
這個是語音辨識裡面另外一塊很重要的東西	10-4
就是user 講了這段話之後	10-4
我並不是要去辨識從頭到尾他講了什麼話	10-4
我只要抓裡面他有沒有說某一個key word	10-4
或者說有沒有說某兩個key word 或者等等	10-4
我只要抓裡面的key word 就好了	10-4
至於別的地方他在說什麼其實我不管	10-4
這些東西對我而言是不重要的	10-4
那麼這就是所謂的key word spotting	10-4
也就是說呢你要決定在你有一個這個key word set	10-4
這個key word set 是我所pre defined	10-4
我預先訂好的這一組key word set	10-4
我現在就是要看你這個聲音裡面有沒有那個有沒有任何一個key word 呢在那個裡面	10-4
那別的東西我不管	10-4
因此我並不需要去辨識所有的word in utterance	10-4
然後呢這個也因為這樣子所以呢我也許不很多辨識不出來沒有關係	10-4
所以呢我我常常會對這個聲音的要求比較unconstrained 一點	10-4
譬如說它也許是比較在吵雜的環境之下	10-4
或者是比較這個讓他隨便說或者怎樣	10-4
你只要把這個key word 說出來我能夠知道	10-4
那這個是用在非常多的地方像speech understanding	10-4
或者是dialogue 裡面用的很多	10-4
因為你可能就是要知道他講的這個	10-4
舉例來講如果要買飛機票	10-4
你我要問的就是你要從哪裡出發台北	10-4
我要我要從台北出發我要到紐約去	10-4
其實我要從台北出發裡面	10-4
其實key word 只有台北兩個字	10-4
那麼除了台北以外的其它的字呢都是廢話	10-4
你要到哪裡去我要去紐約	10-4
其實我就要抓到紐約那二個字	10-4
其它字都是不重要的	10-4
所以你在spoken diag dialogue 裡面	10-4
或者是這個阿通常我們很講究的就是去抓到這裡面的key word	10-4
那key word spotting 的方法其實也是很多種	10-4
那千變萬化也很難說哪一種最好	10-4
因為事實上depends on 你你你沒有辦法define 一組key word set 然後說我們以這個為準	10-4
然後把所有的實驗拿來做到底誰比較好	10-4
因為不同的application 我要的key word 就是不一樣的	10-4
那有的時候我要這堆key word 有的時候我要那堆key word	10-4
那麼到底哪一堆那麼在這堆key word 的時候很可能你用這個方法做出來是最好的	10-4
當你換成另外另外一堆key word 的時候搞不好變成另外一種方法是最好的	10-4
其實也沒有統一說哪一種最好	10-4
那麼這個是一般來講的這個key word spotting 最基本的架構大概就是這樣子	10-4
它包括三種部分	10-4
三個部分第一個就是我用filler model	10-4
第二就是我用了剛才verification	10-4
第三個就是我做search	10-4
那我們來說一下什麼是所謂filler model 呢	10-4
filler model 就是指除了key word 以外其它的所有東西	10-4
我用一些model 去吃它	10-4
讓它吃進去	10-4
舉例來講就剛才這句話而言	10-4
這是一個key word	10-4
我要抓到這個key word 是某一個key word	10-4
我要抓到這是某一個key word	10-4
之外的其它這些聲音呢	10-4
那我希望有另外另外一組model 把它吃下去	10-4
那就是所謂的filler	10-4
那其實這些我並不需要講究它到底是什麼word 不重要	10-4
所以我不需要每個word 都有一個model	10-4
而是一大堆這個譬如說等於是就是all others 的意思	10-4
all other words	10-4
我希望有一個model 是all other words	10-4
如果不一個不夠的話你給我兩三個也可以四五個也可以	10-4
就這些就是所謂all other words	10-4
我就把所有的other word 都在這裡吃進去	10-4
那麼因此呢你看到它這邊就像這樣	10-4
譬如說我有一個key word model set 這個是我總共有n 個key word	10-4
我有n 個key word	10-4
那麼這裡面就是我的key word 的model	10-4
每一個key word 有一個hidden markov model	10-4
另外我還有一個filler set	10-4
我有m 個filler	10-4
那麼於是我真正的辭典裡面就只有這麼多了	10-4
我就不需要那麼多word	10-4
這就是跟大字彙辨識不一樣的地方	10-4
那麼我不見得還需要把六萬個word 都放在這裡	10-4
當然後來也有人是那樣做的	10-4
那是另外一種做法就是我我也就把六萬個word 都放在這裡面然後來辨識也可以	10-4
但是基本上很多時候呢是可以不用那麼麻煩	10-4
假設這邊是一百個key word	10-4
這邊我放五個filler model	10-4
那總共一百零五個model 就夠了	10-4
於是你講的那句話裡面	10-4
很可能你講那句話呢只有這個是一個key word k a	10-4
這是一個key word k b	10-4
其它這些東西是什麼	10-4
都是我用filler model 來吃它	10-4
所以很可能會發現某一個filler model 適合在這裡	10-4
你因此你辨識出來變成一堆fi 一堆filler	10-4
然後中間填進來一個key word	10-4
又一堆filler 填進來一個key word 等等	10-4
你變成一個這樣子的話那其實你就抓到了這兩個key word	10-4
至於這堆filler 對不對也無所謂啦	10-4
那麼其實那都不重要	10-4
我就要抓到這兩個filler	10-4
啊抓到這兩個key word 就是了	10-4
那這個filler 是什麼呢	10-4
這個depends 也不一定	10-4
那麼有的時候有的人他只用一個filler model	10-4
我所有的all other words 通通train 成一個model 之後	10-4
那我就一個就夠了	10-4
那也有的人覺得一個不夠	10-4
我要做兩三個或者四五個	10-4
喔不同的	10-4
讓它們吃一些不同的狀況	10-4
那不管怎樣呢在這個裡面的話呢	10-4
在這種狀況之下如果用這種方式來做的話	10-4
我的總共的這個辭典不太大	10-4
就是只有n 個key word 加上m 個filler 嘛	10-4
所以我的辭典是比較小的	10-4
我辨識起來比較容易	10-4
所以呢我我只要管這樣就夠了	10-4
那這個時候呢凡是你那個聲音裡面真的有key word 的話	10-4
這個key word 的分數照說會比filler 來的高	10-4
所以它比較容易出來	10-4
反過來如果它不是那個key word 的話呢	10-4
它八八成在key word model 裡分數會比較低	10-4
但是在filler 裡面分數比較高	10-4
如果它不是那個key word 的話	10-4
所以那個filler 就會出來	10-4
好這樣子所以呢我辨識出來就會得到一串這樣的東西	10-4
那這就是用filler model 來處理	10-4
再來呢當我知道這個是這個可能是key word a 這個可能是key word b 的時候	10-4
再來我就做一次verification 就是我們剛才講的類似	10-4
那verification 有很多種做法	10-4
那最簡單的做法就是我們剛才講的這種	10-4
用這種confidence measure 這種方法來做	10-4
那你也可以做更複雜一點等等	10-4
那如果是這樣子的話呢	10-4
那我們就可以那這就是verification	10-4
你有每一個key word 你有n time model set	10-4
每一個key word 你可以用n time model	10-4
然後你可以做verification	10-4
你可以訂一個threshold 去判斷它是不是等等	10-4
那當然你在做recognition 中間這個還是需要做search 嘛	10-4
那只是說我現在這個辭典的詞詞數很少啦	10-4
不要六萬	10-4
我可以少	10-4
所以我這個search 可以容易一點就是了	10-4
那因此呢我們說基本上來講呢要做key 這個key word spotting 呢	10-4
大概要用的東西包括這個filler model verification 還有search	10-4
那底下這個圖是說呢你可以去規定你的search 可以怎麼走	10-4
譬如說如果你畫成這個圖的話就表示說我一開始可以有可以是從開始	10-4
也可以連續走了好幾個filler 之後才看到一個key word	10-4
那key word 我可以連續好幾個key word 之後	10-4
後面可以再接filler 等等	10-4
看你這個圖怎麼畫的你就可以允許這個句子裡面出現怎麼樣的結構	10-4
這是另外一種畫法	10-4
那你也可以看到它的意思是說一開始的時候呢可以有一個filler	10-4
也可以有好幾個filler	10-4
也可以根本沒有filler 一開始就是進入我的key word	10-4
那key word 之後也可以就沒有別的東西了	10-4
也可以有一個filler	10-4
也可以有好幾個filler	10-4
所以depend on 你這個圖怎麼畫	10-4
你就可以允許這個句子結構是怎樣	10-4
那這個point 是說如果我的key word 夠多的話	10-4
我key word 還是一樣可以做成一個tree	10-4
就像這個一樣	10-4
只是現在這個tree 很小沒有六萬個就是了	10-4
如果你key word 有很多的話	10-4
我仍然可以變成一個tree 的結構	10-4
變成一個lexicon tree 的結構來做	10-4
就我們做中文的部分而言	10-4
那麼通常我們最可能的做法還是一樣	10-4
譬如說每一個syllable 是一個arc	10-4
於是如果我的key word 有幾百個的話	10-4
我還是可以變成一個由syllable 構成一個tree 就像這樣子	10-4
那我的filler 會是什麼呢	10-4
我可以把譬如說所有的聲母train 成一個聲母	10-4
所有的韻母train 成一個韻母	10-4
那這個聲母跟韻母的就是一個filler	10-4
連起來就是一個filler 的syllable 喔等等	10-4
可以用這種方式來做	10-4
那當然我們原來說過的八點零所說過的那些search 方法	10-4
像a star 什麼multi path 等等其實都可以用喔	10-4
這個depend on 你怎麼畫之後你要怎麼做都可以	10-4
那這邊是講再進一步還可以在剛才的基本架構之下你還可以變很多花樣	10-4
有各種各樣情形	10-4
那我想我們這邊就把它跳過去了	10-4
之後呢還可以再進一步呢就是把它變成key phrase 的spotting	10-5
或者key key phrase 的detection	10-5
那換句話說呢	10-5
你有的時候其實我真正要抓的不是一個key word	10-5
而是一個key phrase	10-5
這個key phrase 是指幾個key word 連起來	10-5
或者是key word 的前後連著function words	10-5
舉例來講你如果買飛機票你說from taipei to hong kong	10-5
那這個時後taipei 跟hong kong 是我的key word	10-5
那個from 跟to 呢就是我的function word	10-5
那我可以把function word 跟這些key word 連起來	10-5
這就是個key phrase	10-5
那你可以猜得到為什麼要這樣做的原因是因為我的key word 可能有很多	10-5
譬如說我的其實這個from 後面接一個city name	10-5
後面接一個to 再接一個city name 就是這樣一個key phrase	10-5
那所以你你譬如說我的city name 這個航空公司它也許全世界有三百個city 它都會到	10-5
我就把這三百city name 放在那裡	10-5
那whenever 前面接了個from 後面接了個to 我都可以連成一個一個key phrase	10-5
那這樣的話呢我就可以有很多的key phrase	10-5
就是由這些key word 所構成的等等	10-5
同理我可以譬如說on sunday	10-5
這是一個function word 後面接一個key word 等等	10-5
這就變成一個key phrase	10-5
那這主要的point 是說呢你一個單獨的key word	10-5
不如把它串成一個key phrase 比較可靠	10-5
為什麼單獨的key word 比較會被local noise	10-5
或者其它confusing sounds 來trigger	10-5
舉例來講你說hong kong	10-5
有一個key word 是hong kong 但是今天剛好你那門一打開有人說kong 這麼一聲	10-5
那麼那個kong 的noise 搞不好就被認為是hong kong	10-5
因為你那個key word 那個裡面key word hong kong 是一個重要的key word	10-5
它很容易可以被trigger 到喔	10-5
那麼因此呢你你你與其把每一個key word 當成key word	10-5
不如把它們串成key phrase	10-5
這樣比較長	10-5
然後就比較比較穩定一點	10-5
那你可以就把整個phrase 就當成一個unit 來做	10-5
那其它的都跟剛才一樣	10-5
那於是你可以把這些key word 呢拼成像這邊這樣	10-5
在這個例子是是一個network	10-5
那麼每一個arc 呢代表一個group of 可能的key word	10-5
譬如說six thirty pm	10-5
這是講時間	10-5
那既然是six thirty 當然你可以是seven thirty	10-5
可以eight thirty	10-5
同樣你可以是six forty	10-5
或者six fifty 都可以嘛	10-5
所以這個是代表一堆key word	10-5
這是代表一堆key word	10-5
那他們都可以連起來	10-5
就構成千千萬萬個不同的key phrase	10-5
我也後面也可以是後面接monday evening	10-5
既然是monday 當然也可以是tuesday 可以是e 可以wednesday	10-5
可以是even evening 當然就可以是morning 或者afternoon 等等	10-5
你都可以嘛	10-5
所以呢你把它連來連去的話就可以構成非常多的	10-5
那這個例子是在講時間	10-5
那這邊可以連這邊可以連那可以這個連連來連去就構成各種狀況	10-5
所以呢每一個arc 代表一個group of 可能的key word	10-5
然後呢這個那哪一個key word 可以跟哪一個key word 連	10-5
你可以有它的grammar 可以有文法	10-5
這個文法可以是用人define	10-5
也可以是用統計	10-5
所謂用統計就譬如說像n gram 這種東西啦	10-5
你可以用個語料去算它的n gram 就知道什麼後面會接什麼	10-5
那如果沒有夠多語料就用人去人去set 也可以manually	10-5
就像six thirty 這種東西其實我們人可以set 說	10-5
或者說from taipei to hong kong 這個人可以來set	10-5
taipei 這個city name 前面加什麼東西等等	10-5
那這個好處就是說你得到這個key phrase 的話	10-5
通常它比較清楚代表某一種semantic concept semantic concept	10-5
譬如說代表某一種觀念	10-5
你在做understanding 或者dialogue 的時候就很清楚這代表一個時間	10-5
那這代表某一天等等等等	10-5
那底下這邊它是在說你可以用一些自動的方法去判斷	10-5
哪一個key word 後面會接哪一個key word	10-5
你可以譬如說把這個某一些代表某一種意義的key word 連成兜在一起	10-5
變成一個concept group	10-5
譬如說city name	10-5
那你所有的city name 在一起變成一個group	10-5
那你可以說誰可以接誰	10-5
誰後面可以接誰	10-5
你可以用一些統計的方法去算它們的黏性	10-5
那它跟它會不會黏在一起	10-5
你可以用數數統計的counts 來算	10-5
那它跟它黏在一起的次數多不多等等	10-5
那這個是講forward backward bi gram	10-5
假設你有某一個某一個key word	10-5
譬如說是c 零	10-5
它後面會不會接一個c 呢	10-5
你可以算這個c c 零後面接c 的	10-5
那這個就是bi gram	10-5
反過來呢你有一個c 的時候它前面有沒有c 零呢	10-5
你也可以統計這個	10-5
那就是反過來的background 反過來的bi gram	10-5
就是backward bi gram 喔	10-5
所以這個是反過來就是你後面如果有一個c 的話	10-5
後面有一個c 前面會不會有c 零呢	10-5
這也可以統計	10-5
那我們平常講的bi gram 是反過來	10-5
是前面有一個c 零後面有沒有c	10-5
所以這兩個都可以算你可以把forward 跟backward bi gram 去算一個幾何平均啊等等	10-5
都可以用這個方法來來統計他們黏不黏	10-5
他會不會跟它黏啊等等這類的方法	10-5
好這部分我們說到這裡	10-5
那麼我前面的reference 的話	10-5
啊第一篇就是我們剛才講的generalize confidence score	10-5
也就是喔這邊的這一套	10-5
我有frame level phone level word level 然後multi level 然後拿來做search 喔	10-5
這一塊其實就是我這邊的reference one 在講的	10-5
那reference two 是比較早的相當早	10-5
那其實是當年的key word spotting 的早年	10-5
剛出來的時候頭幾篇裡面的相當重要的原始paper	10-5
那它講的方法當然是今天來看是已經比較out of date	10-5
但是呢大概把重要的觀念都提到了	10-5
所以這仍然是一個非常好的reference	10-5
這是早年最早的幾篇這個key word spotting 的paper	10-5
那麼最後這篇就是我們講的key phrase detection	10-5
就是我剛才講的這堆	10-5
這堆key phrase 喔	10-5
怎麼樣把它key word 黏起來變成key phrase 的	10-5
就是剛才的這個最後這篇	10-5
就是這一篇	10-5
那這兩篇是比較我這邊並沒有提到	10-5
但是大概你可以看得到都是在講怎樣在做verification	10-5
然後怎麼樣算confidence measure 等等喔	10-5
這些都是相關的reference	10-5
好關於十點零我們就說到這裡了	10-5
ok 我們開始哦	11-1
我們這段九點零跟十點零我們先跳過去我們今天先講十一點零哦那我先說一下就是九點零這裡面	11-1
很重要的東西是包括這個em theory em algorithm	11-1
那這個嗯我們在期中考之後會會來講	11-1
但是現在我先把它略過去因為嗯這裡面有點boring 數學很多	11-1
那麼但是呢是很重要的東西我們後面會一再地要用到	11-1
我們em 這邊會說	11-1
然後後面m c e 這個也是很重要的東西我們也是後面會說的	11-1
只是說我們現在嗯先進入後面的部份	11-1
這樣子讓我們這個進度比較合理一點	11-1
那我們先開始十一點零	11-1
我們上週下課前已經稍微說了一下	11-1
那麼十一點零在講的東西是不同的speaker 的聲音都不一樣會怎樣	11-1
那我們說過的情形是	11-1
如果說這堆是這堆是譬如說阿	11-1
這堆譬如說是e	11-1
本來是可以這樣區別開來的	11-1
但是你如果想想做成speaker independent	11-1
用很多很多譬如說五百個男生五百個女生	11-1
把所有的不同的speaker 的都考慮進去的話呢	11-1
你很自然情形就會變成阿會變成很多	11-1
它有不同的人會散開來	11-1
e 也會變成很多	11-1
於是呢阿可能會變成這個樣子e 可能會變成這個樣子	11-1
於是他們就疊在一起了	11-1
那這個時候呢無可避免地	11-1
你雖然因為我的training data 包含了很多人的聲音了	11-1
但是也因此呢也因此呢它這個可以handle 很多人聲音	11-1
但是因此它正確率就會低	11-1
因為他們無可避免地會疊在一起	11-1
那這時候怎麼辦呢我們最常用的辦法就是speaker adaptation	11-1
這是到目前為止比較好的辦法	11-1
那麼adaptation 的意思是說我只要用少量的data 就可以確定	11-1
那個speaker 的聲音是怎樣的	11-1
譬如說這個這個新的speaker 用這樣的系統的時候他講第一句話的時候我們就可以發現	11-1
其實這個人的阿是在這裡的	11-1
所以呢它的其實是阿是這個	11-1
這個人的e 其實是在這裡的它的e 其實是這個	11-1
那其實呢我們就把它拆開來了喔	11-1
等等那這個就是講speaker adaptation	11-1
那麼我們底下講的幾個都是speaker adaptation 裡面的比較嗯重要的這個代表性的經典作品	11-1
那麼稍微早一點	11-1
但是是到目前為止我們都知道它確實有效而且普遍地使用的	11-1
那就是這一個我們第一個要講的就是這個map 的這個adaptation	11-1
然後呢再下一個是所謂的m l l r	11-1
就是maximum likelihood linear regression	11-1
然後再來一個呢就是這個eigen voice	11-1
那這三個可以算是嗯speaker adaptation 裡面比較早出現比較早但是也比較有代表性	11-1
大家都覺得相當相當不錯也有恩一定的經過一定的稍微早一點經過好幾年	11-1
大概經過時間考驗之後大家都覺得不錯的	11-1
那在底下還有兩個就是這個c a t 跟s a t	11-1
那我們大概就講這五種這個比較	11-1
有代表性那事實上它的方法千千萬萬	11-1
近年尤其還有很多那我們講近年的我們都不太講我們講這都稍微早幾年的	11-1
原因是近年還沒有經過時間考驗	11-1
我們不能確定它們真的夠好阿	11-1
那麼因此我們講的是比較早的	11-1
那再講eigen voice 之前我們還要講這個嗯pca	11-1
因為這個eigen voice 是以pca 為基礎來做的等等	11-1
好那我們現在先看第一個	11-2
就是這個map 的principle	11-2
那麼所謂的map 呢這個這個觀念其實我們已經講過很多次都知道	11-2
我們現在只是把這個這個maximum a posterior 這個觀念呢	11-2
放到再來做這個speaker adaptation	11-2
那這個意思你可想而知就是我現在已經有一堆這個speaker independent model 了	11-2
那這個speaker independent model 就是我們前面說的我用了譬如說五百個男生五百個女生	11-2
所以train 了一個general model	11-2
對多數人都可以用	11-2
只是正確率不高	11-2
那我有了有了這個model 呢	11-2
那我現在這個model 假設我有一堆tri phone 或者一堆什麼	11-2
我們假設是tri phone 好了	11-2
譬如說有五千個tri phone	11-2
那麼這個m 就是五千	11-2
然後每一個tri phone 有一個hidden markov model	11-2
它有a b pi 都有了	11-2
那不過這個是general 對一千個人所train 的	11-2
那現在呢這個新的speaker 來了	11-2
這個speaker 講了一句話是他的o	11-2
那這個speaker 講的講的這句話o 呢你可以想像它裡面有很多phone	11-2
他講的這段話裡面呢	11-2
它有一堆phone 你可以抓說ok 這個是這段是某一個tri phone	11-2
這段是某一個tri phone 等等你可以去抓	11-2
因此呢這段tri phone 相當於這堆音	11-2
於是你想辦法用這個去adapt 這個東西	11-2
把這個呢把這個e 呢調到這邊來對不對	11-2
然後這個tri phone 相當於這堆音	11-2
所以呢我想辦法拿拿這個來	11-2
去去調這個	11-2
那麼就知道那個r 呢其實是這個嗯等等	11-2
那麼這個是它的基本的想法	11-2
所以我現在就是given 這個speaker 的adaptation data o	11-2
裡面有一堆不同的縫	11-2
於是想辦法用它去train 它用它去train 它等等	11-2
那你你怎麼根據這堆聲音去調這個	11-2
跟這堆聲音去調這個呢	11-2
那它基本的原理就是所謂的map	11-2
那這個式子沒什麼特別就是我們平常講的map	11-2
也就是given 一個observed 這個data	11-2
那麼base on 這個observed data condition 呢	11-2
我想辦法去調這個裡面的所有的model	11-2
那麼使得我找其中的一組model 能夠讓這個機率最大	11-2
那麼使得我找其中的一組model	11-2
能夠讓這個機率最大	11-2
那這個機率就是所謂的a posterior 的機率	11-2
a posterior 這個a posterior 的機率也就是在given observation 的條件之下	11-2
我調所有可可能的這個這個這個model 參數想辦法讓這個機率變得最大的那一個最大的那一組就是我要的	11-2
所以這個其實沒有什麼特別跟我們之前所講的所有的a posterior 的機率是一樣的	11-2
所以這個其實沒有什麼特別跟我們之前所講的所有的a posterior 的機率是一樣的	11-2
那這個式子也就是跟我們前面所講的所有的map 是一樣的	11-2
那現在後面也是一樣	11-2
因為這個機率我們不會算	11-2
但是我們比較會算的是反過來的	11-2
於是就把它倒過來這就是bayes theorem	11-2
倒過來之後就變成這樣子寫	11-2
然後乘上那個的機率除以observation 的機率	11-2
那到這裡還是跟我們之前講的完全一樣的習慣的作法	11-2
就是這個時候因為我現在是調所有的lambda 參數	11-2
想辦法找一個這個機率最大的	11-2
所以呢	11-2
那麼我就把這個那麼底下這個倒是無所謂因為這個反正對所有的o 都是一樣的	11-2
對所有的lambda 而言我現在是要找lambda 嘛	11-2
那麼這這這個對所有的o 都一樣所以不用看	11-2
所以我只要maximize 上面這兩個	11-2
於是就變成那兩個	11-2
那到這裡的時候呢有個問題就來了	11-2
雖然這個機率我可以算	11-2
這個機率我們會算因為given 這些model	11-2
given 這些model 可以看到這些observation 這個我們是會算的	11-2
h m m 就會算這個東西	11-2
可是這個是什麼呢這個實在不知道	11-2
這個model 的機率是什麼	11-2
我們really 不知道	11-2
你凡是所有的要做map 都會碰到這個問題	11-2
我們要知道怎麼算這個	11-2
那在這裡的話呢這是當初做這做這個方法的人他下了一堆功之後做了一堆assumption on 這個機率	11-2
因為這個不知道嘛	11-2
所以他做了一堆assumption	11-2
有了這堆assumption 之後就可以推這個式子	11-2
他推了一堆數學	11-2
那堆數學是基基本上就是based on em theory	11-2
那個這個em 就是我們剛才說在九點零我們會詳細說的	11-2
那現在我們先姑且先把它直接跳過去	11-2
那那一堆這個theory 的部份	11-2
我們跳過去等到嗯過兩週之後我們會講那個em theory	11-2
到時候你就會就可以看在paper 裡面寫得很清楚	11-2
我們就不詳細去推它	11-2
他推了之後得到這樣子的答案	11-2
他有一整套答案我們這邊這邊只舉一個例子	11-2
我們說過從這裡開始	11-2
因為我們講的都是研究的課題	11-2
所以我們不再詳細地說每一件事	11-2
我們只是拿代表性的東西來說一下	11-2
那詳細的留給你做為這個各位的寫期末報告的題目	11-2
所以我們就不多說我們就舉個例子	11-2
它那樣推之後得到一個像這樣的答案	11-2
這個答案是他的答案裡面整套裡面的一個	11-2
我們拿一個來看	11-2
這個是什麼呢就是它的mean 怎麼調	11-2
假設這個是一堆一堆mean	11-2
我們舉例來講我的某某一個	11-2
我的某一個tri phone 的某一個state	11-2
某一個tri phone 的某一個state 它是一堆gaussian	11-2
那麼於是呢這每一堆gaussian 有一個mean	11-2
那麼這些mean 呢	11-2
應該怎麼調	11-2
我本來的tri phone 的mean 是這樣子	11-2
現在知道了這個聲音	11-2
這個speaker 是這樣子的	11-2
於是呢這堆聲音拿來調這個這裡面	11-2
於是呢我我的那個tri phone 的那個mean 呢	11-2
那個tri phone 在裡面的這個gaussian 這些mean 要調	11-2
這些mean 怎麼調呢	11-2
這個mu j k 就是這些mean	11-2
是它的某一個tri phone 的lambda i 裡面的第j 個state 的第k 個gaussian	11-2
ok	11-2
所以mu j k 是一個gaussian 的mean	11-2
第j 個state 第k 個gaussian 的mean	11-2
那怎麼調呢它把它從mu j k 調成mu j k 的star	11-2
那這個調的過程呢用這個式子	11-2
這是他經過做了一個assumption on 這個東西然後用em theory 去推推出這個式子來	11-2
那這個式子到底是什麼呢這個看起來有點複雜我們稍微看一下它的意思	11-2
它是有意思的	11-2
那麼這個mu j k 呢是某一個參數	11-2
所以這邊是那個mu j k 是那個參數	11-2
然後這邊是什麼呢有這個gama t 的j k	11-2
這是什麼東西呢	11-2
這就是我們在四點零裡面	11-2
推hidden markov model 裡面的嗯basic problem 三的時候我們用過的	11-2
這個gama t 的j k	11-2
那當時我們推過這個gama t 的j k 是相當於前面這些東西alpha t 跟beta t 的j	11-2
這些是什麼這就是我們當時的foreword 跟backward 的variable	11-2
那麼就是在時間t 走到這個state j 等等	11-2
那麼這個alpha t beta t 等等	11-2
那這兩個相除之後的意思	11-2
相當於我們當時說的gama t 的j	11-2
那麼就是given 這個model	11-2
given 這個model	11-2
然後我現在看到這整個的observation o 的情形之下	11-2
在時間t 等於j 的機率	11-2
時間t 等於j 的機率是這一塊	11-2
就是這個	11-2
然後呢要乘上後面這個	11-2
後面這個是什麼呢	11-2
是我把現在這個時間t 的這個o t	11-2
放在第k 個gaussian 上面	11-2
除以放在全部的gaussian 裡面	11-2
那這個的意思我們當時也說過就是你等於是	11-2
我現在如果有有一堆很複雜的distribution	11-2
你把它看成是好多個gaussian	11-2
好多個gaussian	11-2
那裡面呢假設我現在要考慮的是第k 個gaussian	11-2
第k 個gaussian 的話呢它是裡面的某一個	11-2
譬如說是這一個	11-2
這是它的第k 個gaussian	11-2
那麼於是呢我現在就把我的時間	11-2
我先把我的時間t 的那一個o t	11-2
放在第k 個gaussian 上面	11-2
這個得到的機率是多少	11-2
以及放在整個的這裡	11-2
那它變成這個是這個機率是多少	11-2
那這個機率除以這個機率	11-2
所以整個的是這個嘛	11-2
所以呢我把我現在時間o 的那個observation 放在這一個gaussian 上面的機率	11-2
除以放在全部的gaussian 的機率	11-2
那就是這個東西	11-2
那等於是說我現在在算的是	11-2
我不光是在時間t 是在state j 上面	11-2
在在這裡或者在這裡	11-2
而且呢我還把還算它現在是在這一個gaussian 裡面的機率等等	11-2
那這個是所謂的gama t 的j k	11-2
所以呢這個這個是gaussian 的index	11-2
這個是state index	11-2
然後幹嘛呢它在這邊去做一堆summation	11-2
t 等於一到大t 這是什麼就是我整個的observation	11-2
那這裡講的這個observation 應該是指譬如說這一個	11-2
譬如說這一段我們知道它是應該去adapt 這個e 的這個tri phone 的	11-2
那麼因此呢這個呢就是我所謂的時間t 等於一到大t	11-2
那用這堆呢去adapt 這一個	11-2
待會呢這個r 呢是這段	11-2
這是這是這是另外一個從一到大t 呢我去adapt 這一個	11-2
那麼這個時候我怎怎我怎麼辦呢	11-2
是用這樣這個式子	11-2
這個式子什麼意思呢	11-2
看起來有點頭大	11-2
不過我們可以用簡單的符號來想	11-2
它的意思呢就是這樣	11-2
這個式子你可以看成是一個	11-2
a 加上summation b t summation over t	11-2
然後呢是a 的v 加上summation 的b t o t 的t	11-2
我們這我這只是把符號簡化一點	11-2
這樣會比較好看	11-2
所以呢所謂的a 就是這個tau j k	11-2
所謂的bt 就是這個gama t 的j k	11-2
如果寫成這樣的話呢比較容易看	11-2
變成這樣子	11-2
變成這樣之後你怎麼看式子呢	11-2
我現在如果把它看成這樣的話	11-2
我現在如果先不把這個	11-2
它應該是這個括號在這裡啦我現在如果括號先不不括在這裡	11-2
我如果括號括在這裡	11-2
比較容易想像它是什麼	11-2
那這個時候其實就是這兩個相加分之這兩個分別除以這兩個	11-2
那這個意思其實就是	11-2
一一個譬如說一個alpha 乘上v 加上一減alpha 乘上o t	11-2
那這就是一個內差嘛	11-2
就是一個內差嘛	11-2
換句話說	11-2
我今天如果原來這個v 就是我的mu j k	11-2
就是我的某一個mean	11-2
那我如果原來某一個mean 在這裡	11-2
現在呢這個人講的這個聲音	11-2
他的他的聲音不是exactly 在這裡他的聲音在這裡	11-2
那怎麼辦我就在這兩個中間做個內差	11-2
然後當成中間那個值	11-2
對不對	11-2
就是說我我原來的mean 在我原來的mu j k 的mean 在這裡	11-2
某一個譬如說e 的音它在這裡	11-2
現在這個人講了講了e 它它在這裡	11-2
因此我就取中間的那一點	11-2
那中間這點就是在做這個內差	11-2
那所以一個是alpha 一個是一減alpha	11-2
那這個內差讓它這個靠近誰呢	11-2
由這個alpha 來決定	11-2
這個alpha 呢其實就是這個a 加b 這個b 分之a 嘛	11-2
來決定說這個比較靠近哪邊	11-2
它等等於是你如果這樣看是這個意思	11-2
那現在其實不是這樣	11-2
其實我們說這個括號不是這樣括的	11-2
這個括號是這樣子括的	11-2
那意思是什麼呢因為我講的不是只有一個聲音	11-2
而是我有一堆聲音嘛	11-2
它的這個它的這個這個e 有一堆從e 到t 呀	11-2
有一堆啊	11-2
那這一堆不是都一樣啊	11-2
因此呢你可以想像它其實不是只有一個	11-2
而是有好多個	11-2
它有好多個在這裡	11-2
這是t 等於一t 等於二一直到t 等於大t	11-2
有這麼多個在這裡	11-2
所以它的內差呢是要要跟每一個分別去做	11-2
等於是這樣子嘛	11-2
我變成是從這點向這些每一個點去移動	11-2
對不對所以呢就變成這個所有的那這個這個b t 就是這個b t 就是我們這邊的gama t 的j k	11-2
那這些個b t 呢告訴我每對每一個t 而言的那個o t	11-2
它呢在不同的地方	11-2
那我到底應該各weight 多少	11-2
然後呢那我其實把它們全部平均起來得到一個	11-2
所以我基本上是從這點向這些點去移動	11-2
但是呢我把它weight 起來	11-2
最後移動一個值	11-2
那那個值就是這樣	11-2
基本上是是這樣算的	11-2
那比較像這個東西不是不是exactly 這個東西啦	11-2
並不是等於	11-2
只是說你可以想像成像這樣的東西但但但但是它一個一個都去移動之後平均起來得到一個	11-2
那等於是這樣的意思	11-2
所以呢這句話這就是我們底下講的這句話他說weighted sum	11-2
把這個原來的mean 向向o t 的方向移動阿	11-2
那麼向所有的這些o t	11-2
凡是它掉在第j 個state 跟第k 個gaussian 的這個條件之下	11-2
那麼向那就based based on 這些東西這些gama	11-2
去向這些東西去移動	11-2
然後移到一個某一個合理的位置去	11-2
那當然現在這些b t 是沒有問題我們就有有gama 可以算	11-2
那tau j k 是什麼呢	11-2
tau j k 就是等於是這裡的一個weighting	11-2
你可以看到它是一個parameter having to do with prior knowledge about mu j k	11-2
通常呢是跟那個number sample use to train 這個有關	11-2
換句話說	11-2
你如果原來這個mean 是用非常多的data train 出來的話	11-2
我這個可能比較相信這個比較可靠	11-2
現在你這個人只講了這這幾個音我就把它調過去嗎有點危險	11-2
那在這個情形之下我就把這個weight 比較重	11-2
我就把這個值變得比較大	11-2
如果我這個是用夠非常多的data train 出來比較可靠的話	11-2
我就weight 它比較重一點	11-2
我讓這個值比較大	11-2
因此我就移動比較少	11-2
那反過來如果我原來train 這個的時候這個聲音本來就不夠多	11-2
我本來就data 不夠多所以不太可靠的話呢	11-2
我就weight 少一點	11-2
我就讓這個值小一點於是就比較靠比較向這個方向移動	11-2
等等ok	11-2
所以呢這個移動多少這個alpha 是跟它們的相對大小有關嘛	11-2
那麼因此呢是跟這個地方跟這個這個原來這個mean 的可靠度有關	11-2
因此呢我就跟我的prior knowledge 有多少有關好	11-2
那麼跟我原來用多少sample train 出來有關	11-2
那這個其實這個參數就是它原來的假設這個prior knowledge 裡面的東西	11-2
ok 好那這樣我們大概可以解釋這個式子的意思	11-2
那它其實不光是這樣	11-2
它其實這個這個式子並不是這樣用嘴巴講講它的道理出來不是	11-2
它是完全用數學推出來它有一堆很很完整的的的的的theory	11-2
根據em 去推推最後去推出這個式子來	11-2
只是推出這個式子之後我們可以看得出來它式子是有道理的就是了	11-2
那麼因此呢我們這樣做之後我現在這個mean 呢可以用這個方式來調	11-2
可以調到那麼你現在聽到它的那個聲音是e 的話我就可以調那些e 的那些model	11-2
讓它呢比較像那個新的speaker 講的聲音等等喔	11-2
那這個不光是這個mean 可以調所有參數都可以調	11-2
包括這個gaussian 裡面的covariance matrix	11-2
這裡面的covariance matrix 做的東西都可以調	11-2
它的weight 也都可以調等等	11-2
那我們這邊就不多不多講但是如果有興趣可以去看這個原始paper 裡面都有	11-2
那這個辦法有個最大的弱點	11-2
就是只有那些有data 的才會調	11-2
unseen model 就不會動	11-2
那什麼意思呢你可以想像我現在user 講的這句話裡面有什麼phone 我就調什麼	11-2
那沒有的phone 我就沒有調啊	11-2
也就是說呢你你今天真正的這個model 這整個的state 上這個整個空間裡面有所有的音的譬如說五千個tri phone 在這裡	11-2
那現在user 講了這句話之後那他總共只講了裡面的十個phone	11-2
於是呢那十個講到的phone 可以調	11-2
這個phone 說到了它呢把它調過來	11-2
那這個phone 說到了呢那這個phone 它調過來	11-2
那這個phone 說到呢它調過來	11-2
這個phone 說到呢它調過來	11-2
假設我有五千個tri phone 在這裡的話呢它其實總共只調了這裡他講的這句話總共只有十個phone 的話就調了那十個而已	11-2
其他的就會全部都不動	11-2
阿那這個也就這邊講的就是只有有data 的才會動	11-2
unseen model 全部不動	11-2
那麼那這個其實是map 的基本精神因為map 就是這樣子	11-2
就是given observation	11-2
那given 這個東西之後我調這個	11-2
那當然我沒有看到當然就不動啦	11-2
那因為這樣的關係所以呢它的一個最大的弱點就是你要有夠多的data	11-2
你通常一句話只有十個phone 的話你只會調十個	11-2
那講了一百句話呢其實可能只有裡面並不是一百乘以十	11-2
很多常用的phone 已經出現很多次	11-2
沒有常用的phone 還沒有講到	11-2
那因此你講了夠多data 它可能還沒有調很多點它還是沒有調到	11-2
那麼因此呢它的performance 是你如果這個data 這個adaptation data 你這個speaker 講的話有限的話	11-2
它呢其實performance 進步呢會是比較有限的喔	11-2
這個是map 的方法的基本的缺點	11-2
這是原始的map 方法的缺點	11-2
那麼我們如果畫一個圖來看的話呢	11-2
就可以畫成這樣	11-2
這個是adaptation data 的量	11-2
那這個呢是我的正確率	11-2
那假設這個上限是speaker dependent model	11-2
那這邊呢是speaker independent model	11-2
也就是說如果你針對某一個speaker 跟它收集大量data 之後	11-2
你可以train 到這麼好	11-2
可是我們現在如果拿一千個speaker 的話不會太好	11-2
就會有個差距是在這裡	11-2
那現在你讓那個那個speaker 來講話	11-2
他講的講的話我這個正確率會慢慢從這邊慢慢上上來	11-2
基本上它是隨著你的data 越來越多我會進步	11-2
那就是我們剛才講的因為你講的一句話裡面有十個phone 我就調了裡面的十個phone	11-2
你講了十句話裡面有五十個phone 了喔我會調裡面五十個phone 等等	11-2
所以基本上你你你講的data 越來越多的時候呢你這個會慢慢上去	11-2
那最後它應該會趨近於這個s d 的model	11-2
它的上限是慢慢接近於這個地方	11-2
這是我們講的map	11-2
那當然它的好處是說當你的data 夠多的時候它會趨近於這個地方	11-2
但是它的壞處就是說你一開始的時候它其實進步得很慢	11-2
這邊還差很多它進步得很慢喔	11-2
這是map 的原始map 的這個的缺點	11-2
但是它的好處它的它的這個map 的這個這個principle 這個maximum 這個a posterior 這個原理是非常精確的一個原理	11-2
所以這個式子是相當有道理的	11-2
那麼只是說呢它這樣做不了太好就是了	11-2
那這個map 的方法我就說到這裡	11-2
那麼你如果要詳細看的話就是它的原始paper 就是這一篇	11-2
雖然一九九四年已經十年多了哦	11-2
不過這個應該可以算是一個重要的經典	11-2
所以嗯所有的講到這個的paper 都要site 這一篇因為這個是嗯我們今天來看仍然相當不錯的一篇喔是值得參考的	11-2
你如果有興趣的話	11-2
那這個方法有它的弱點我們剛才講了	11-3
因此呢底下我們來講下一個方法就是如何克服這個弱點	11-3
那麼後來就有人想了這個方法	11-3
所謂的maximum likelihood linear regression 喔m l l r	11-3
那它的意思是什麼呢	11-3
我把這個gaussian	11-3
先把它分成一堆class	11-3
然後呢為每一個class 建立一個transformation	11-3
喔現在不是這樣啦	11-3
假設我現在的這一堆所有的tri phone 的那些不同的音的mean	11-3
我們先說mean vector 好了	11-3
假設它們在這裡	11-3
那剛才我們說如果是map 的話呢	11-3
我現在是聽到什麼聲音我會調這個	11-3
聽到這個聲音調這個沒聽到的我全部都不調嘛	11-3
那這樣的結果呢我我我只有所有的unseen model 都看不到嘛	11-3
對不對我們剛才的問題就是這個	11-3
所有的這個這個unseen model 我都沒有辦法調嘛	11-3
那它現在的辦法呢這個這個maximum likelihood linear regression 最大最大的目的就是我要unseen model 全部都要調	11-3
你只要講一句話我就開始全部都動	11-3
那怎麼可能呢	11-3
我我我只看到我只聽到那幾個音我憑什麼可以全部去動呢	11-3
他說我現在把它分分群	11-3
舉例來講譬如說這一群其實都滿接近的	11-3
我我把它叫做c one	11-3
那這是一群	11-3
這群滿接近的我都叫做c two	11-3
這群比較像的聲音我把它叫做c 三	11-3
然後我我為每一群定義一個transformation	11-3
就是這裡面的每一個這個mu mu j k 還是一樣	11-3
就是第k 個第j 個state 的第k 個gaussian	11-3
那我現在怎麼調呢	11-3
都有一個公式就是a 乘上這個加上b	11-3
所以呢譬如說c one 的話我就會有它的a one	11-3
跟b one	11-3
使得告訴我說這一群全部怎麼調	11-3
都有一個共同共同的方向	11-3
都向這個方向調	11-3
那c two 我也有一個a two 跟b two	11-3
它給我一個共同的方向說是這樣調	11-3
c 三我也可以求出一組參數就是a 三b 三	11-3
它給我一個共同的方向是這樣調等等	11-3
那我如果可以找得出這些來的話呢	11-3
我就直接調了	11-3
舉例來講假設我今天這個還是一樣這個user 說了這句話	11-3
這裡面呢這一段是某一個phone	11-3
這段是某一個phone	11-3
那麼根據這些個phone 的話呢	11-3
啊turns out 它是這裡這裡的某一個	11-3
這裡的某一個	11-3
這裡的某一個	11-3
那於是c one 裡面呢我我聽到的是這些	11-3
別的都沒有聽到	11-3
但是我根據這個聽到的呢	11-3
我就根據這聽到的這這些聲音	11-3
我就求出整個的a one b one	11-3
於是我整個一起動	11-3
那同理呢我如果這邊我這邊有聽到譬如說這個那裡有什麼聲音	11-3
這邊有個什麼聲音	11-3
它那剛好是在這裡	11-3
跟這裡跟這裡ok	11-3
我就根據這些東西呢	11-3
我就調出一個a 三b 三來	11-3
但是a 三b 三不是只調這三個而是我整群一起調了	11-3
那麼以此類推	11-3
我雖然user 只說少數幾句話	11-3
我只要每個class 裡面都有說到	11-3
於是我就整個一起動了	11-3
這是它的基本觀念	11-3
所以呢它就define 一個這樣的transformation	11-3
那這個a mu 加b 這個transformation 是一個非常簡單的linear transformation	11-3
當然是比較粗的	11-3
跟剛才不一樣	11-3
你知道剛才這裡面的它是用這個去算的	11-3
用這個去算所以它是完全在算機率然後去調那些東西	11-3
那我現在這裡沒有	11-3
它這裡只是給它一個很粗的	11-3
所以所以這個是一個比較粗的transformation	11-3
那這個a mu 加加b 的這個東西其實就是multi dimension 的linear regression	11-3
那你記得我們從前講的我們從前講的linear regression 我們在七點零的時候說過這件事	11-3
就是什麼是linear regression 如果two dimension 的話呢就是你給我一堆點	11-3
我想辦法找一條直直線	11-3
這條直線是y 等於a x 加b	11-3
然後我希望有了這條直線之後所有的所有的點呢	11-3
跟它的距離是最近的	11-3
那這個是所謂的linear regression	11-3
在two d two two dimension 的平面上的時候這是所謂的linear regression	11-3
那現在這個一樣	11-3
不過變成n dimension	11-3
變成multi dimension 的時候呢我不是a y 等於a x 加b 而是什麼呢	11-3
是整個的n dimension 裡面的的那個vector 是乘上一個matrix 加上b 是一樣的意思	11-3
所以這叫做linear regression	11-3
ok 那麼如果我現在用這個linear regression 的方式	11-3
來為這一群一群的class 都找出他們的transformation 的參數來	11-3
這樣子的話呢那這個怎麼找	11-3
每一個class i 我都要找它的a i b i	11-3
那憑什麼呢	11-3
憑這個	11-3
那它用這個這是什麼這是likelihood function	11-3
也就是說如果你給我lambda 是原來這一堆原來這一大堆的model 叫做lambda	11-3
現在你你現在給我a i b i 之後	11-3
ok 原來這一堆	11-3
給我這個a one b one 之後呢	11-3
我的新的新的model 就變成這個lambda 裡面的所這邊c one 裡面的所有的mean	11-3
都用a one b one 去調它	11-3
調完之後的那個model	11-3
我要看到我的這個observation 裡面的這些個聲音	11-3
的機率是最大的	11-3
ok 所以呢就是說譬如說我這個c one 用這堆a one b one 去調之後呢	11-3
調完的model	11-3
我要看到這些掉在這裡面的這些聲音的機率是最大的	11-3
那這個機率呢其實就是likelihood function	11-3
given 某一組model 之後看到聲音的機率	11-3
這個是這個likelihood function	11-3
所以呢我要它是最大的	11-3
然後去找最大的那組a 跟b 就是我的a one b one	11-3
ok 所以呢我就在調所有的a 跟b 裡面去找	11-3
讓這個機率最大的	11-3
那那這個呢就是maximum likelihood	11-3
因為我現在是這個是likelihood function	11-3
我要maximize 這個東西所以這個是maximum likelihood	11-3
那那麼這樣做的話呢所以我現在這個名字就叫做maximum likelihood linear regression	11-3
這四個字是這麼由來的喔	11-3
這個linear regression 是指這個公式	11-3
是一個很粗的transformation	11-3
那它本身是一個linear 的	11-3
transformation linear regression	11-3
那maximum likelihood 是指說這兩個參數怎麼求	11-3
是用maximum likelihood 方法來求的	11-3
那當然你要求要maximum 這個方法當然不容易	11-3
那個詳細的數學推導也有一大堆	11-3
那根據什麼還是一樣根據em	11-3
喔那這個em 我們留留到後面會說	11-3
那基本上呢你可以想像這個em 是很重要	11-3
因為像這類都有同樣的問題就是你給我一堆observation 我就要去找這個參數	11-3
跟前面是一樣的	11-3
前面的這裡也是一樣	11-3
你給我一堆observation 之後我要去找這裡面一大堆參數	11-3
那怎麼找我們都是用em 的方法喔	11-3
那這邊也是一樣用em 的方法來找的	11-3
那如果是這樣的話呢我們就就是這邊講就是說我我所有的gaussian	11-3
我在同一個class 裡面的話呢	11-3
我都我都用同樣的一組a i b i 去調它	11-3
所以這就是parameter parameter sharing	11-3
或者adapt data adapt adaptation 的data 的sharing	11-3
也就是說我現在只要聽到這些個聲音	11-3
那麼它們這幾個聲音聽到之後	11-3
我所有的這些model 這些個mean	11-3
都share 了共同的這些個data	11-3
都share 了共同的data	11-3
所以是這個是data 的sharing	11-3
同樣呢我用這些data 求出這些參數之後呢	11-3
它們share 了共同這些參數	11-3
所以是這些個model 的parameter 的sharing	11-3
我這個都是sharing 的觀念	11-3
於是這樣的話呢我沒有看到的model 也都可以跟著調了	11-3
沒有看到的model 我都可以跟著調	11-3
那這個時候很大的一個問題是你怎麼分群對不對	11-3
這才是問題	11-3
到底哪些個該變成一群然後它們用同一組參數	11-3
哪些個該變成一群變成同一組參數呢	11-3
怎麼分群呢	11-3
當然你可以想像兩個原則	11-3
一個是data driven	11-3
一個是knowledge based	11-3
也就是說呢所謂的knowledge driven 意思是說我們可以有一些knowledge	11-3
譬如說這裡這一堆都是ㄓㄔㄕㄖㄗㄘㄙ大概是比較像的我們給它們一群	11-3
這一堆是這個ㄅㄆ　	11-3
這個這個ㄉㄍ比較像的ㄉㄍ給它一群ㄅㄆ給它一群等等	11-3
母音給它一群子音給它一群等等　	11-3
這個是可以完全根據knowledge 就可以分群	11-3
但是更重要的是什麼呢data driven	11-3
也就是根據data 去算	11-3
通常我們去算gaussian 的distance	11-3
這個常用的辦法是算gaussian 的distance	11-3
也就是說你你每一個gaussian 你可以算	11-3
嗯對不對你如果這裡有一個gaussian	11-3
這裡有一個gaussian	11-3
你可以算它們之間的distance	11-3
那麼根據這個distance 來算說凡是distance 比較近的那一群	11-3
那麼它們在一起的	11-3
我我假設它們是共用的喔	11-3
你可以算gaussian gaussian 的distance 這樣來做	11-3
這是data driven	11-3
那通常是可以這兩者並用就是你一面用data driven 的方式	11-3
一面用一些knowledge	11-3
這樣子來分群	11-3
但是問題是到底應該分多少群才好呢	11-3
你可以想得到的是你不能分太多群	11-3
也不能分太少群	11-3
為什麼呢	11-3
如果分太多群的話你就沒有用了	11-3
你如果這個一群這個一群這個一群這個一群那你每一群都要一組a i 都要有data	11-3
那你結果等於等於每一個自己都要調一樣的	11-3
所以顯然你要有夠多的在一起一群	11-3
夠多的在一起一群那麼群的數目不能太多	11-3
這是第一個原則就是你不能太多群嘛	11-3
你如果太多群的話就每一群都需要夠多的data 才能做你這樣就不行了	11-3
所以呢群數不能太多	11-3
反過來呢也不能太少	11-3
因為因為你群數很少顯然太粗嘛	11-3
如果我這邊總共只分三群的話	11-3
很顯然是說這一大堆不太像的通通都變成一群了	11-3
都用同一條顯然不好嘛	11-3
所以呢你也不能太太少群	11-3
太少群就會太粗	11-3
所以呢我一定是要這個不多不少	11-3
那這個東西原則是什麼呢	11-3
基本的原則就是說我要有夠多的data 就可以給它一群	11-3
那什麼意思呢	11-3
就是如果說你這裡面明明有相當多的data 了	11-3
假設說這些也有data 這個也有data 這個這個也有data 這個也有data	11-3
如果data 夠多的話明明這個data 夠train 兩群的話	11-3
那我寧可把它分成兩群對不對	11-3
我這群找出一個a i b i 來	11-3
那這個可以變成另外一群	11-3
對不對我只要我的data 夠多	11-3
如果我data 夠多到可以得到a one b one 跟a two b two	11-3
兩組參數的話我寧可分成兩群嘛對不對	11-3
那麼我我只要data 越多到讓我可以把它分得細我寧可分得細比較好	11-3
我如果那麼多data 結果只弄一個比較粗的一群的的調是比較不不理想嘛	11-3
所以我的這個基本的principle 是應該是我基本上是它們一定要像	11-3
這個所謂的similar property 就是要像	11-3
就是要像底下那樣我根據他們的data driven 跟knowledge 來判斷它們是應該是一群的	11-3
要夠像	11-3
一方面呢如果它們有夠多的data 就自成一群	11-3
那這個是我們講的分群的原則	11-3
可是你想這怎麼做呢	11-3
我怎麼知道哪哪些又像又有夠多的data 呢	11-3
因為user 顯然它隨便在說不同的話	11-3
他不斷的話不斷的說進來	11-3
你怎麼知道誰哪些是剛好是一群而且有夠多的data 呢	11-3
那比較好的辦法就是一個tree structure	11-3
那這個所謂的tree structure 是怎樣呢就是我想辦法先把它們之間的關係先建好一個tree	11-3
我們舉個例子來講	11-3
假設這裡面所有的所有的gaussian 的mean	11-3
我們都是在最底層	11-3
是一群	11-3
是一系列的	11-3
然後呢如果它跟它比較像我們可以用它跟它比較像	11-3
它跟它比較像	11-3
那它跟它呢比較像	11-3
那它跟它比較像等等	11-3
那麼於是呢我們可以得到一個像這樣子的tree structure	11-3
譬如說我們得到一個這樣的tree structure	11-3
那這個時候呢完全我就告訴它們這個tree structure 告訴告訴我們它們之間的相似性	11-3
這個tree 怎麼建的這個tree 就是根據這個data driven 跟knowledge driven 想辦法建這個tree	11-3
然後這個時候depend on 這個user 說了什麼話	11-3
它什麼話進來我去看它的data 在哪裡	11-3
我們舉個例子來講	11-3
假設它的假設它的聲音進來的時候呢	11-3
這個data 很多	11-3
多到它自己以為它自己知道怎麼怎麼調的話	11-3
我根本它自己就是一群	11-3
可是呢這些都沒有data	11-3
當這些都沒有data 的時候呢	11-3
那那我很可能就是把這這些東西合在一起	11-3
看成是這一個	11-3
於是呢這個也是一群	11-3
那麼假設這個量這個data 量夠多到可以得到一組a two b two 的話	11-3
那你可以想像的是對對於對於這個而言	11-3
我其實完全根據這個我就知道它怎麼調	11-3
可是因為其它都沒有data 嘛	11-3
我就這整個的呢我就也用我我整個就都用這個來來調了	11-3
但是呢我也很可能是是另外一種狀況	11-3
是說其實這裡面的這裡有一些data	11-3
這裡有一些data	11-3
它們兩個加起來的data 夠多到可以train 一個	11-3
如果是這樣的話呢	11-3
那我也許就讓這些個這些個變成另外一個	11-3
然後呢得到一組等等ok	11-3
所以呢就是說我我完全depend on 這個data 進來的狀況	11-3
我如果有一個tree 已經建好的話	11-3
depend on 我的data 進來的情形	11-3
那麼你發現說這裡有一點data	11-3
但是不夠train train 一個這個a i b i	11-3
這裡有一點不夠	11-3
但是那那這邊沒有	11-3
如果這邊沒有的話呢它們share 一個還是在這裡還是不夠	11-3
它們沒有它們share 一個在這裡還是不夠	11-3
但是呢我這個跟這個加起來夠了	11-3
於是呢到這邊為止我在這裡夠了	11-3
於是呢我就變成這一群我可以得到一個	11-3
等等喔	11-3
那也就是說完全那就是就是這邊講的就是說這個我可以dynamically adjust class	11-3
當你摸了data 當你的data 不斷進來的時候	11-3
那麼那麼你你可以建一套這個把這個tree 建好	11-3
然後我有一個演算法	11-3
然後當我的聲音進來的時候depends on 我現在你說的是什麼話哪些音掉在哪裡	11-3
然後我去看每每一個地方	11-3
到底哪些地方構成夠多的data 可以train 嘛	11-3
可以train 出這個a 跟b 出來	11-3
我就在那邊呢看到高到什麼層次嘛對不對	11-3
如果這邊的data 不夠這邊的data 不夠但是它們加起來的話呢到這邊才夠	11-3
於是呢其實這邊就共用一個了	11-3
那麼於是呢我我就可以說是這個這個每一個狀況是完完全全是depends on 這個data 進來我隨時在調	11-3
那麼我我也很可能說是這個每每一群完全看狀況來決定誰誰變成一群	11-3
所以我的聲音當user 的聲音不斷說進來的時候我隨時在調這邊的東西	11-3
然後看這個嗯哪些夠多了可以調成一群我就調成一群	11-3
然後呢你繼續下一段話再說進來幾句話的時候我這個就會又變了	11-3
我又可以不斷地調因此我可以不斷地調的比較好	11-3
那那就這邊所講的	11-3
我我dynamic 來來調所有的class	11-3
當我越越說越多話的時候	11-3
然後呢那這個原則呢就是這個node including minimum number of gaussian	11-3
但是呢有足夠的data 的就變成一個class	11-3
那一方面呢就是我們講為什麼要minimum number of gaussian	11-3
就是要細嘛對不對	11-3
如果它們已經夠了話我就它們自己變成一個	11-3
這樣這個比較細對不對	11-3
那這邊因為沒有啊	11-3
沒有我就只好跟別人一起合用一個對不對	11-3
所以呢當我沒有data 就跟別人合用一個難免比較粗	11-3
凡是有細的地方我就把它變細喔	11-3
所以呢就是minimum number of gaussian	11-3
但是有夠多的data 就可以做	11-3
那如果這樣子來的話呢那我就可以達到我的目的	11-3
那麼有一個tree structure 之下	11-3
看data 進來的狀況	11-3
然後我隨時調中間的東西	11-3
我去隨時調它的參數	11-3
那這個想法呢嗯獲得了相當不錯的結果	11-3
那你可以想像它的它的情形	11-3
跟剛才的map 比起來最大的不同就是它現在的curve 會變成這樣	11-3
就是我一開始還是從這裡開始	11-3
但是它的斜率比較高	11-3
它會比較快	11-3
它的它斜率會比較快	11-3
因為我我現在data 不斷進來它馬上就正確率會提高	11-3
但是有有個問題就是說它它會它會比較快saturate	11-3
那為什麼會saturate 因為畢竟它這是一個是一個比較粗的model喔	11-3
那麼嗯就是說這句話就是ma 就是faster adaptation 你調得比較快	11-3
你你會進步得比較多喔	11-3
你只要有much less data 你就可以調所以它進步比較快	11-3
可是呢它有一個很大的問題就是這個saturate at low accuracy	11-3
你你這個再多data 也沒有用了	11-3
為什麼因為它是一個比較粗的model	11-3
它的model 本身不夠精細	11-3
因為它只是一個a mu 加b	11-3
這個東西只是一個linear model 不是一個很好的model	11-3
所以呢你現在不管怎樣都是是這樣用這個a 跟b 在調是一個比較粗的	11-3
所以你不太可能可以調到那麼好喔	11-3
所以呢你如果是這個m l l r 的話呢是像這樣的	11-3
那麼我我開始比較快	11-3
可是呢我沒有辦法像map 可以一直上去	11-3
map 可以這樣一直上去	11-3
可以趨近這個真正的你的你的這個s d s	11-3
它沒有辦法	11-3
它到了一個地方它就saturate	11-3
它跑不上去了	11-3
這個是m l l r 的情形	11-3
那那這個東西呢他還有一個地方可以進一步做就是什麼呢我這個這個a 呢	11-3
可以是full matrix	11-3
也可以reduce 到diagonal 或者block diagonal	11-3
什麼意思呢就是我這個a	11-3
基本上這個a 是你可以看到是三十九維假設我這這裡是三十九維的參數的話	11-3
那這個a 是三十九乘三十九的一個matrix	11-3
如果它是三十九乘三十九那是很大啦	11-3
那變成是一個這麼大的matrix	11-3
那這裡面三十九乘三十九要參數很多	11-3
那我如果data 只有那麼少可能沒有辦法調那麼多	11-3
那怎麼辦呢	11-3
第一個辦法就是我假設它只有diagonal	11-3
只有對角線才有值	11-3
其它都是零	11-3
我如果這樣的話呢我只要三十九個	11-3
我只要三十九個參數就可以描述這個a	11-3
當然你假設它這邊都是零的話是有一點這又是又是一個簡化的假設	11-3
因此呢你如果這樣做的話呢你假設它是diagonal 的話	11-3
你可以reduce 到diagonal 你的你的這個需要的data 量就會少	11-3
因為我只要調這些就夠了	11-3
所以呢我需要的data 量比較少我就可以調出這個a 跟就可以求出這個a 跟b 出來	11-3
所以我需要的data 量比較少	11-3
因此呢你可以得到的情形是	11-3
你得到的情形是這個會更快	11-3
這個我如果是diagonal 的的a 的話	11-3
它會調得更快	11-3
會更快上來	11-3
因為我只要我只要那三十九個參數	11-3
本來是三十九乘三十九我變成只要三十九個就夠了	11-3
所以我比較少的data 我就可以把那個調好	11-3
會進來會進來更快	11-3
可是呢我會我會更快saturate	11-3
我會我會這個進步得更快可是我會更快就就上不去了	11-3
因為那個更粗嘛	11-3
對不對你可以想像因為它更粗	11-3
更粗所以它有更大的問題就是它上不去	11-3
所以呢這是reduce 到diagonal	11-3
那這個折衷的辦法呢就是block diagonal	11-3
所謂block diagonal 呢是說呢我現在把它變成中間是一塊一塊的	11-3
那麼不是零的不是只有對角線而是這一塊這一塊	11-3
也就是說讓它們相鄰的這些東西有關係	11-3
但是別的地方讓它是零	11-3
當我變成這樣一塊一塊的時候	11-3
當然我的參數是介於這個跟full 的中間	11-3
我需要的參數比三十九要多	11-3
但是比三十九乘三十九還是少很多	11-3
所以那個是介於中間的	11-3
那你如果把它變成block diagonal 的話呢你得到的情形大概也就是在這個中間	11-3
這兩個折衷的辦法在這個中間這樣子	11-3
那這些東西呢就是構成所謂的m l l r	11-3
那這個東西在嗯這個方法在嗯相當長的時間	11-3
很多人在不同的系統裡面使用	11-3
效果都很好	11-3
所以這個是恩另外一個非常重要的被普遍使用的方法	11-3
在我們這裡就是底下的第三個reference	11-3
是它的我這邊都只給這個第一篇哦就是它的這個原始paper 第一篇是在這裡	11-3
那在這個之後有一大一大堆人在作跟這個以這個為基礎在發展	11-3
就像那個也有	11-3
那那個我這邊就不列了你自己可以去找喔	11-3
那所以呢這個m l l r 的這個原始paper 是這一篇	11-3
是九五年所以大概也已經十年的歷史	11-3
那當然當它的這個方法出來的時候	11-3
得到這個現象	11-3
就是mr這個m l l r明顯比map快的時候	11-3
那map的人就覺得說不服氣了	11-3
他說啊你你會這麼進步是因為你分群	11-3
是因為你分群你做這個tree structure	11-3
其實我map 我也可以分群啊	11-3
那你可以想想看map 是不是可以分群可以啊	11-3
我其實map 這裡我也可以以群為基礎來做	11-3
我不要以每一個mean 來做	11-3
我也可以以群來做我也可以做一個tree structure	11-3
所以後來就有tree structure 的分群的map	11-3
那如果如果是那樣做的話呢那map 這條這條曲線的差異它也可以它這個也可以向上向上shift	11-3
那它的好處是它最後還是可以收斂到最上面去	11-3
所以它有它的有各種不同的方法喔	11-3
那我們這邊都不多講就是說你如果有興趣自己去找都可以找得到	11-3
那麼map 我也可以用tree structure 做也可以把它弄上來喔等等	11-3
那嗯那當然就是說這個不同的方法他們自己各有不同的狀況	11-3
那麼有的時候是a 比較好有的時候是b 比較好	11-3
這個看情形	11-3
那在這裡的話呢	11-3
這個恩我們這邊只給我這邊只所列只是最原始的paper 而已你如果去找的話後面還會有很多篇	11-3
怎麼樣改進	11-3
這後面也還會有很多篇怎麼樣改進	11-3
那我們就不多說了就是了	11-4
那再下來我們要講的就是這個pca	11-4
那pca 是一個數學的方法	11-4
用在很多地方	11-4
那麼包括pattern recognition machine learning 什麼東西都都都在用它	11-4
那麼我們這裡用pca 的目的是要作底下的這個eigen voice 喔	11-4
所以eigen voice 是另外一個在九八年到兩千年之間所出現的一個新的方法	11-4
那它有它的有趣的地方	11-4
所以這是我們底下要講的東西	11-4
那要講那個之前就要先我們先要簡單地講一下什麼是pca	11-4
然後我們才可以開始往下講喔	11-4
ok 好我們先在這裡休息十分鐘	11-4
ok 好我們開始接下去講底下這一段	11-4
我們要先說一下這個pca	11-4
然後呢以pca 為基礎我們就講底下的eigen voice	11-4
那麼pca 是一種數學工具	11-4
那麼用在很多地方所以你也許在別的課學過也不一定	11-4
那麼如果學過就當成是一個複習喔	11-4
那麼pca 是幹嘛的呢	11-4
它的基本想法是這樣	11-4
假設說最簡單的想法假設我有兩兩個dimension x one 跟x two	11-4
我有一堆data	11-4
在這個空間上面分佈	11-4
那我現在如果是用我傳我原來的x one x two 為來為軸的話	11-4
它們的distribution 其實是比較緊的	11-4
譬如說我在x one 上面我看到它是這樣的一個distribution	11-4
它是一個這樣的distribution	11-4
那我在x two 上看到的是這樣的distribution	11-4
那事實上呢是不是只有這兩個軸可以用呢其實不然	11-4
我們知道在兩度空間裡面你其實可以選擇的軸有無限多個	11-4
舉例來講我如果選擇這個軸的話呢	11-4
假設這是y one 的話	11-4
那這個軸裡面呢我看到其實就會變成一個這樣子的distribution	11-4
那這個散開就比這兩個x one x two 都散開得多	11-4
那散開得多是什麼意思呢你可以想像因為我們是用分出來	11-4
這個是ㄚ這個是ㄧ這個是ㄨ	11-4
你如果它擠在一堆的話呢ㄚ跟ㄧ跟ㄨ就比較擠在一堆	11-4
你如果把它用這個軸把它拉開來的話呢	11-4
這個ㄚㄨㄧ可能就拆得比較遠ok	11-4
那這個是pca 的基本的想法	11-4
就是說你的data 原原來可能是用某一種物理量的軸	11-4
來來分佈的一個空間	11-4
在這個軸上面它的distribution 可能不太好	11-4
它們可能是這個比比比較緊的	11-4
可是你如果可以找到一個軸	11-4
讓它的分佈散得最開的話	11-4
散得最開之後你很可能因此你就能得到比較好的	11-4
這個比較能夠把它拆得開來	11-4
那這個是pca 的基本的想法	11-4
就是假設我們這邊是講兩維的空間	11-4
但是事實上是可能是要n 維啦喔	11-4
所以我假設是一個n 維空間的random vector x	11-4
那x 就是這些個點就這些點	11-4
它都是n 維的	11-4
所以呢每一n 這樣寫的意思是說我的每一維都是一個real value 的random variable	11-4
那麼我有n 個real value 的random variable	11-4
構成一個n 維的random vector	11-4
它的dimension 是n	11-4
那相當於這邊的這個空間	11-4
這邊畫的是二	11-4
那然後呢我們在pca 裡面都是先假設它是zero mean 的	11-4
不是zero mean 的話呢我就先把它減掉mean 把它變成zero mean 就是了	11-4
它是zero mean 的	11-4
然後呢我希望找到一組新的orthonormal basis vector	11-4
e one e two 到e k	11-4
那譬如來講這個y one 就是我們剛才講的e one 啊	11-4
就是我希望找到這組basis e one e two	11-4
我有一組新的e one e two	11-4
那我希望在這個basis 上可以怎樣呢	11-4
可以做到第一個呢我這個e one 的transpose 這個t 是transpose x 是maximum	11-4
什麼意思其實e one 你知道我現在的e one 是一個是一個都是一個這個column vector	11-4
這是所謂的e one	11-4
我都是用column vector 來代表	11-4
所以e one 的transpose 呢就是這樣子的	11-4
是一個row vector	11-4
這是e one 的transpose	11-4
然後呢乘上x 的話呢	11-4
x 是是一個column 的	11-4
這個是x	11-4
所以e one 的transpose 乘上x 其實是什麼就是他們的內積嘛	11-4
對不對就是它跟它的內積	11-4
所以呢其實就是e one 跟x 這兩個vector 的內積	11-4
那如果它是內積的話那那其實因為這兩個vector 都是內積你就可以知道就是這個e one 跟的長度跟x 的長度跟它的cosine theta 相乘	11-4
但是因為e one 的長度e one 我讓它是單位長的unit vector	11-4
這是e one	11-4
它是單位長的unit vector	11-4
所以呢讓它單位長是一	11-4
於是這是什麼這就是它的投影嘛	11-4
所以呢你如果說是我們來看如果這個是e one 的單位長的unit vector	11-4
而我的這是我的x 的話其實就是什麼	11-4
就是它的投影嘛對不對	11-4
我得到的這個這個值其實就是x 乘上cosine theta 就是它的投影	11-4
所以呢這邊說了半天的意思	11-4
這個e one 的transpose t	11-4
其實就是指所有的x 投影到這個e one 的軸上來的投影的值	11-4
就是這個東西就是這個東西那就是就是它的投影的意思	11-4
ok 所以呢我們現在說	11-4
我現在就是要這個投影的這個variance 要最大	11-4
這是我的最最大的principle 就是在這裡	11-4
也就是說我現在要找出一個一個新的basis 來	11-4
它的unit vector 是e one	11-4
然後我把所有的點通通都投影到這個上面來的時候	11-4
在這裡的variance 要最大	11-4
也就是能夠散得最開	11-4
如果散得最開的話呢那我就最容易把它區別出來	11-4
所以呢我就是把所以這個e one 的transpose 乘上x 呢我們說其實就是在作內積	11-4
然後其實就是指x 投影到e one 上面去的投影的值	11-4
那麼我要這個投影的是maximum	11-4
這是我的第一個目標	11-4
也就是說x 有maximum variance 當投影到e one 上來	11-4
那當你這個e one 決定之後我可以決定e two	11-4
e two 是怎樣我這邊是只能畫一維但事實上當然不只一維	11-4
當我這個e one 決定是這樣之後呢	11-4
垂直於e one 的呢可以有無限多個vector	11-4
對不對我第二個dimension 是我我重新定義一組這個這個這個basis 嘛	11-4
所以第二個dimension 是要跟它垂直的	11-4
可是given e one 之後垂直於它的是無限多個	11-4
我要選擇哪一個呢我要選擇那一個是在投影上去是最大的那一個	11-4
ok 所以呢從i 等於二開始	11-4
譬如說i 等於二的話我就是要e two 要跟e one 垂直	11-4
然後呢我現在e two 的i 跟x 要maximum 對不對	11-4
所以呢也就是說當我e one 選定之後	11-4
我在所有垂直e one 的裡面選擇第二個e two	11-4
使得它們的投影在上面是最大的	11-4
那有了e two 之後呢我就可以選選擇e 三以此類推	11-4
那麼我e 三要跟e one 跟e two 垂直	11-4
那麼當然跟e one e two 垂直的e 三又有無限多個	11-4
我要選的那一個是投影最大的	11-4
以此類推所以這個是第二第二條式子的意思	11-4
就是你你選擇的每一個e i	11-4
都要它跟前面的i 界e one 到e 的i 減一都要垂直的那個e i	11-4
而那個e i 呢要它的它投影上去要最大	11-4
那麼因此呢就是說也就是說我的x 要有maximum variance	11-4
投影到每一個e i 上面去	11-4
這樣我總共選擇k 個出來	11-4
那這個就是我們要做的事情就是pca 的目的就是這樣子	11-4
於是我就在這邊可以找到k 個basis	11-4
那麼等於是一個新的一個dimension 的空間	11-4
所有的點都投影的那個空間上面去	11-4
那它們每一個空間我都散得最開	11-4
這是pca 的目的	11-4
那這個詳細的我們就不說我們就說它的solution	11-4
solution 是什麼呢	11-4
你就是把所有的x 去求它的covariance matrix	11-4
那麼什麼是covariance matrix 呢這個應該很熟悉了	11-4
因為我們的每一個gaussian	11-4
我們每一次從頭講的gaussian 就是那個就是covariance matrix	11-4
那麼by definition covariance matrix 就是底下這個東西	11-4
expectation value of x x transpose	11-4
就是x 跟x 的transpose	11-4
那是什麼呢	11-4
我們說x 是一個column vector	11-4
x transpose 呢是一個row vector	11-4
那這兩個去做結果是怎樣呢	11-4
這個column 跟個row 相乘呢	11-4
就乘成一個matrix	11-4
所以呢你就得到一個所以你得到的就是一個一個matrix	11-4
這個matrix 裡面的每一個東西是什麼呢	11-4
譬如說這是第i 個第j 個的話	11-4
那這個是什麼	11-4
這個就是x i 跟x j	11-4
那真正講起來它應該是這個是要扣掉它的mean	11-4
所以應該是x i 減掉x i 的mean	11-4
乘上x j 減掉x j 的mean 的這叫做covariance matrix	11-4
不過我們這邊是因為都已經先說它是zero mean 了	11-4
我先說它是zero mean 所以可以不寫這個就是了	11-4
因為它是zero mean	11-4
那基本上你應該是這樣子一個東西	11-4
對不對	11-4
那也就是說這裡的所謂的x i 就是指這邊第i 個	11-4
x j 就是指這邊的第j 個嘛	11-4
那其實就是就是這裡這個x 裡面的第i 個跟第j 個之間的covariance	11-4
那這樣就構成一個matrix	11-4
那這個matrix 就是我們這邊所謂的這個covariance matrix	11-4
所以呢你怎麼做這件事呢這個solution 就是說我現在去先把這一堆data	11-4
你先拿這堆data 每一個都是有有n 個dimension	11-4
n 個random variable 嘛	11-4
那我就把這些東西把第i 個跟第j 個去算它的covariance ma 的covariance 值然後就構成一個matrix	11-4
那你zero mean 就就這個就不用管了	11-4
那麼沒有zero mean 就把mean 減掉	11-4
得到這個matrix 之後	11-4
你只要去求這個matrix 的eigen vector	11-4
那這些個eigen vector 裡面呢每一個eigen vector 都對應到一個eigen value	11-4
那你就選擇那eigen value 最大的那k 個	11-4
就是你要的這些個basis	11-4
那這話怎麼講呢	11-4
你你回想一下你從從前學的線性代數裡面的eigen value 跟eigen vector	11-4
任何一個matrix a	11-4
我如果乘上一個vector u 的話	11-4
任何一個matrix a 乘上一個vector u 的話基本上是把它變成另外一個vector v	11-4
ok	11-4
基本上乘上一個vector v	11-4
會變成另外一個vector u	11-4
但是呢如果說它沒有變成另外一個vector	11-4
還是自己的那個vector	11-4
只是scale by 一個parameter lambda	11-4
這個時候這個就叫做eigen vector	11-4
對不對	11-4
那這個時候的eigen vector 裡面的這個scale 的這個vector 呢	11-4
這個scale 的這個vector 呢就是eigen value	11-4
所以這是eigen vector 的定義嘛	11-4
就是我我每我matrix 可以找到它的eigen vector	11-4
使得相乘的時候呢其實只是一個scaling	11-4
而那個scale 呢就是我的eigen value	11-4
那你如果回想這件事情的話那這邊的是一樣的	11-4
你就是去把那個covariance matrix 求出來之後	11-4
就求這個covariance matrix 的eigen vector	11-4
那你如果回想你的數這個線性代數的話呢	11-4
這邊如果是n by n 的話呢	11-4
我可以找到n 個eigen vector	11-4
這n 個eigen vector 我可以把它排起來	11-4
可以把這個matrix 變成diagonal	11-4
也就是說有一個這樣子的關係	11-4
這是我的第一個eigen vector	11-4
這是我的第二個eigen vector	11-4
等等等等	11-4
我總共有n 個eigen vector	11-4
我n by n 的matrix 有n 個eigen vector	11-4
可以排成一個matrix	11-4
如果是這樣的話呢	11-4
我中間這個matrix 就可以變成所有的eigen value lambda one lambda two 一直到lambda n	11-4
其它都是對角線以外都是零	11-4
然後第三個matrix 呢是完全一樣	11-4
只是把它transpose 過來	11-4
所以第一個row 是我的第一個eigen vector	11-4
第二個row 是我的第二個eigen vector	11-4
等等等等	11-4
最後一個呢是我的第n 個eigen vector	11-4
那這三個相乘就是我原來的那個covariance matrix	11-4
這個應該你在線性代數有學過這個東西	11-4
這個式子其實只是這個式子的衍伸	11-4
這個是我每一個eigen vector 都長這樣	11-4
然後我有n 個eigen vector	11-4
這n 個把它排起來	11-4
然後把它這個做一些matrix 的重整	11-4
就可以變成那個式子	11-4
那這個意思是說	11-4
我我一個matrix 可以拆成三個matrix 相乘	11-4
其中中間那個變成對角線的	11-4
而對角線上的每一個element 就是我的eigen value	11-4
那每一個eigen value 是對應到一個eigen vector	11-4
也就是說每一個eigen vector	11-4
有一個它所對應的那個eigen value	11-4
是那個值	11-4
所以呢value eigen value 跟eigen vector 是對應的	11-4
因此呢這邊的第一個eigen value	11-4
是對應到這邊的第一個eigen vector	11-4
第二個eigen value 是對應到第二個eigen vector 等等	11-4
那麼於是呢	11-4
我們可以把一個matrix 這個co 這個covariance matrix	11-4
拆成這三個相乘	11-4
其中中間是這個對角線的	11-4
那這個是我們如果對它做eigen value 跟eigen vector 的分析的話可以得到的	11-4
那這時候呢它這邊說呢我們是	11-4
怎麼辦呢我們通常的作法是把這個eigen value 照大小順序排列	11-4
我這邊可以照照大小順序排列	11-4
就是最大的那個eigen value 放在最上面	11-4
第二大的排在第二個	11-4
到後面越來越小等等	11-4
那如果這個是最大的話呢	11-4
那那它所對應的就是最大的那一個的eigen vector	11-4
這是第二大的eigen value 它所對應的就是第二大的那個eigen vector 等等	11-4
當我把它排成這樣子之後呢	11-4
我現在可以做一個很重要的簡化	11-4
就是我不要全部了	11-4
我只選擇前面的k 個就好了	11-4
譬如說我只要選擇前面的k 個	11-4
這邊我也只要選擇前面的k 個	11-4
這邊我也只要選擇前面的k 個	11-4
那麼這三個相乘仍然是一個n by n 的matrix	11-4
因為這個是n by k	11-4
嗯我這邊是用小k n by k 那這個呢是k by k	11-4
這個是k by n	11-4
所以乘出來呢仍然是一個n by n 的matrix	11-4
而那個n by n 的matrix 會跟這個非常像	11-4
只差一點點	11-4
那麼為什麼會非常像	11-4
是因為我們現在已經把按照大小順序排列	11-4
那這個是最最大的那個第二最大的	11-4
那你剩下的可能是很小的	11-4
那剩下可能是很小的所以呢你你你這邊很小的東西去乘進去加進去那個值很小很小	11-4
所以大部分的它的covariance 裡面的大部分東西	11-4
都用前面的k 個dimension	11-4
或者這邊的k 個eigen vector	11-4
跟這k 個eigen vector 幾乎就已經能夠呈現原來的了	11-4
因此呢我只要選擇前面的k 個就可以了喔	11-4
它是這樣意思	11-4
那當然那個k 是多少我們沒有說	11-4
但是我基本上呢就是我可以把原來的n by n n 度空間reduce 到只有k 度空間	11-4
那這k 度空間其實就是這邊的第一個	11-4
這個e one 就是這邊的第一個dimension	11-4
e two 就是第二個dimension 這樣我總共k 個	11-4
得到一個k 度空間的話呢	11-4
almost 就是原來的了	11-4
因為剩下的這個效果影響都很小	11-4
因為這些只是這些	11-4
這些都是很小的值了	11-4
所以呢那麼這些影響都不大了所以呢會得到一個非常接近原來的東西	11-4
那這樣的結果就是我們這邊所說的	11-4
我現在呢就是選擇k 個	11-4
這個k 個就是這邊的夠大的eigen value 的k 個	11-4
k 個夠大的eigen value	11-4
剩下都很小了的	11-4
那這k 個也就是covariance matrix 裡面的k 個eigen vector	11-4
相當於那k 個最大的eigen value	11-4
於是呢我現在得到一個新的vector	11-4
我現在變成k 個dimension	11-4
就用這這個東西構成一個新的k 個dimension	11-4
那麼然後呢我的每一個x	11-4
就乘上這個a 的transpose	11-4
就對應到那個那個新的空間裡面去	11-4
那這裡你看那這個東西其實就是我們上面剛才所講的這個	11-4
e one 的transpose 乘上x	11-4
e two 的transpose 乘上	11-4
我每一個都一樣	11-4
就變成所謂a 的transpose	11-4
a 是這個嘛	11-4
a 是這堆k 個	11-4
就是這一個這k 個這個就是所謂的a	11-4
那a 的transpose 就是這個嘛	11-4
a 的a 的transpose 就是這個嘛	11-4
所以呢a 的transpose 乘上x 的話呢	11-4
其實就是每一個eigen vector 都乘上x	11-4
跟上面這個其實是完全一樣的	11-4
阿啊我剛才擦掉了	11-4
跟那個是完全一樣的	11-4
所以呢那這個意思呢其實就是我現在把這個點	11-4
全部這些點全部對應到一個新的一個sub space	11-4
只有k 個dimension	11-4
這個k 個dimension	11-4
是原來n 個dimension 的一個sub space	11-4
可是這些點投上去它在上面所呈現的	11-4
的distribution 跟原來是差不多一樣的	11-4
那麼這個意思呢我們不太容易畫出來	11-4
不過如果我們用三度空間來畫的話	11-4
我本來是三度空間的一堆點	11-4
那我的sub space 就是兩度空間	11-4
也就是說我在這上面找到某一個譬如說這一個這個平面	11-4
這是一個兩度不太好畫我有有一個兩度空間的平面	11-4
我看看怎麼畫喔	11-4
這樣子好了喔	11-4
就是說我在原來的這個三度空間的上面	11-4
我找出一個兩度的平面	11-4
那這個平面such that 我這些點通通投影到這上面來	11-4
之後我得到這些個點	11-4
那turns out 在這些點在這個兩度空間的sub space 這個sub space 上所呈現的	11-4
跟我原來在三度空間呈現幾乎是一樣的	11-4
而反而在這上面呈現反而是拆得最開的	11-4
拉得最開的	11-4
就好像剛才的這個軸	11-4
我這個是兩度把它呈現在一一度上面是一樣的啦	11-4
我這是兩度上面把它把它投影到一度上面這我把它拉得最開	11-4
那同理呢我這邊畫的是三度把它投影到兩度來也是一樣的	11-4
那我就是把它變成我我在等於是本來是這麼多個點但是呢我把它找到一個兩度空間的這個找到這個兩度空間的這個平	11-4
之後呢我它投影投影在這上面之後呢	11-4
其實反而是拆得最開	11-4
雖然dimension 減少了	11-4
但是反而是拆得最開	11-4
我就在上面做這n 個做這k 個dimension 就好了	11-4
喔就這個意思	11-4
那這個就是什麼這個就是y 等於a 的transpose x	11-4
那我就等於是我把原來的x 上面的每一個點嘛	11-4
對不對我x 上面的每一個點通通都分別乘上這個	11-4
就投影到這個新的空間上面	11-4
那就那邊那個case	11-4
我就把x 上面的每一個點通通投影到那個那個sub space 來	11-4
於是這個k 個dimension 這個sub space 呢	11-4
它的dimension 是比原來的n 小	11-4
但是呢當你這個所有的點投影上去的時候呢	11-4
其實這些投影的這些y	11-4
已經是跟原來的整個的distribution 是最接近的	11-4
是非常接近的	11-4
而我現在dimension 小很多	11-4
而最大特點就是說我現在的每一個dimension 上面的variance 都是最大的	11-4
所以呢就好像這個東西我投到這邊這個是最大的	11-4
那同理呢我投到這邊的時候呢這上面每一個dimension 都是最大的	11-4
它的它的每一個每一個dimension 上面呢這個投影都是拆得最開的	11-4
那這個就是這個的這個pca 的意思	11-4
那底下有講一下這個pca 怎麼prove	11-4
那我想這個比較不那麼有興趣你看一下	11-4
那我們不不詳細地講	11-4
那基本上呢它的意思就是其實不難prove	11-4
那我只要做這個嗯做個lagrange multiplier 然後去maximize 就是了	11-4
舉例來講這個e one 怎麼求	11-4
e one 就是要這個東西maximum 嘛	11-4
就是要e one 的transpose 乘上x	11-4
的variance 要maximum	11-4
那這個variance 是什麼呢	11-4
這個variance 就是它的平方求平均	11-4
它的平方就變成e one 的transpose	11-4
乘上x 乘上x 的transpose 再乘上e one	11-4
然後求平均	11-4
但是這邊的random variable 只有x	11-4
e one 不是	11-4
所以呢我平均就到中間來了對不對	11-4
所以呢我這個的variance 就是它的平方	11-4
就是e one 的transpose x x transpose e one	11-4
然後我這個時候求平均就是平均中間這一塊	11-4
而平均中間這一塊其實就是我的covariance matrix	11-4
這個就是我的covariance matrix	11-4
ok 就是這裡這個東西	11-4
所以呢我就是要這個東西要等於maximum	11-4
那我substitute 什麼constrain	11-4
e one 是要單位長	11-4
那我就maximize 這個東西substitute 這個constrain	11-4
怎麼做	11-4
用lagrange multiplier	11-4
所以我的object function 呢就是	11-4
嗯這裡有點寫錯了	11-4
這個應該是這個不是variance 這個是expectation 喔	11-4
也就是說	11-4
這個東西應該是這個東西吧	11-4
所以這個不是variance 這個是expectation	11-4
那我現在就是把這個我要maximize 的這個東西	11-4
減掉這個lambda 就是我的lagrange multiplier	11-4
乘上這個constrain	11-4
我要maximize 這個東西我就對它微分	11-4
對每一個e one 的component 去微分等於零	11-4
那這樣子呢我就可以微分之後我就可以得到這個式子	11-4
這個式子是什麼其實這個東西就是covariance matrix	11-4
那那個covariance matrix 乘上e one 等於lambda one e one	11-4
這個就是eigen vector 的式子嘛	11-4
這個就是那個covariance matrix	11-4
乘上它的eigen vector	11-4
就等於原來的eigen vector scale by 一個eigen value	11-4
就這樣子就出來了喔	11-4
所以這是第一個就可以這樣子做	11-4
而且你可以發現那個eigen value 也就是我要的那個最大的那個那個variance	11-4
所以呢我這個第一個eigen vector 得到的就是第一個dimension	11-4
而這上面的那個variance 那個最大的我maximize 那個variance	11-4
就是這個第一個eigen value	11-4
那以此類推我現在第二個照做	11-4
第二個照做我的constrain 只多了一個第二個還要跟第一個垂直	11-4
照做我就可以得到第二個等等	11-4
那麼因此呢我的第二大我的第二大的那個variance 就是這個lambda two	11-4
就是這個第二個eigen value 就是它第二大的	11-4
然後它的相對的的vector 就是e two 等等	11-4
那麼這樣一來呢我的這個這個pca 的原理大概就這樣	11-4
這是一個非常簡單的解釋當然pca 本身含有也是有很多學問的	11-4
那在一般的譬如說pattern recognition	11-4
或者說是嗯machine learning 啊什麼這些書上都可以找得到	11-4
那我這邊有給你一個reference	11-4
如果有興趣的話光是pca 可以寫一本書的	11-4
那這是大概是寫得最完整的一個關於pca 的一本書	11-4
這我們圖書館是有的	11-4
好那有了這個pca 之後我們現在要來看的	11-5
是怎麼樣用pca 來來做這個eigen voice	11-5
那麼eigen voice 的想法是延續剛才講的	11-5
就是說我們這個聲音	11-5
我現在一個新的speaker 來	11-5
我怎麼樣子在很多unseen 的data 裡面	11-5
我要能夠一起調嘛	11-5
我不能只聽到那幾個聲音之後調那幾個聲音	11-5
我要一起調	11-5
我怎麼樣可以一起調呢	11-5
我們m l l r 等等有它的辦法它做一個tree structure 來來做這些事情等等	11-5
那eigen voice 是另外一個想法	11-5
也是一樣的目的	11-5
我希望能夠在最少的聲音聽到最少的那些音	11-5
我要整個model 全部一起調	11-5
那怎麼做這件事呢	11-5
那我們現在來看	11-5
我先是假設我有這個一群這個train training speaker	11-5
每一個training speaker 我可以為他train 出他的speaker independent phone model	11-5
那麼我們舉例來講假設每一個train 每一個training 的speaker	11-5
那麼我就請那個speaker 發夠多的聲音	11-5
把他的所有的音都唸到之後	11-5
train 出他的speaker dependent phone model 來	11-5
那如果是這樣的話呢我現在就可以把它所有的phone model 兜起來兜成一個很大的vector	11-5
就是這邊所謂的super vector super vector	11-5
什麼意思呢	11-5
譬如說我現在有一個speaker one	11-5
那個speaker one 他train 他唸了夠多聲音之後為他的每一個phone	11-5
都train 出它的model	11-5
假設說這個這個是某一個tri phone 這個是ㄧ這個是ㄚ這個是ㄨ這個是ㄊ等等等等	11-5
那然後呢每一個model 裡面的每一個state	11-5
每一個model 裡面的每一個state 都是一堆gaussian	11-5
每一個gaussian 的mean 就是這些東西	11-5
那我就可以把所有的這些個gaussian 的mean	11-5
把它全部串接起來	11-5
然後變成一個很大的vector 叫做super vector	11-5
舉例來講它的	11-5
它的每一個mean 是一個一個的mean	11-5
我就把它一路這樣串接起來	11-5
那這是一個非常大的vector 可以多大呢	11-5
我們舉個例子像這樣子	11-5
假設它是五千個tri phone	11-5
五千個tri phone	11-5
每一個tri phone 有三個state	11-5
每一個state 裡面有八個gaussian	11-5
每一個gaussian 有一個mean	11-5
那個mean 是三十九維我們算是四十維	11-5
那這樣一乘是多少是四百八十萬個參數	11-5
構成一個四百八十萬維的一個非常大的matrix	11-5
ok 所以呢	11-5
那一個speaker 我就得到一個很大的一個vector	11-5
大到什麼程度呢有四百八十萬個component 在這裡	11-5
是一個四百八十萬維這個n 是很大很大的的一個vector ok	11-5
那這是一個一個training speaker 可以這樣	11-5
那我現在呢有一群	11-5
譬如說我有一千個training speaker	11-5
那每一個每一個training speaker 都做這件事	11-5
那第二個training speaker 呢他的聲音不一樣啦	11-5
所以他有另外一堆點	11-5
他有另外一堆點	11-5
那邊就可以得到另外一個	11-5
也是四百八十萬維的另外一個ok 等等	11-5
那這樣子的話呢我現在如果有有一千個training speaker 的話	11-5
我就得到一千個這樣子這個四百四百八十萬維的這個大的vector	11-5
既然有一千個了	11-5
我可以想像成是一個random vector	11-5
它有一千個sample	11-5
就好像這邊的一千個一樣	11-5
這邊有一千個點嘛	11-5
或者說你可以想像成是這邊的一千個點	11-5
那所不同的是我現在這個空間非常大	11-5
不是這邊的三度	11-5
我這邊是四百八十萬維的	11-5
ok 那麼因此呢如果是這樣的話你可以想我我這個空間是什麼	11-5
這個空間好比就是這個空間好比就是一個四百八十萬維的一個空間	11-5
那每一個speaker 其實是裡面的一個點對不對	11-5
我第一個speaker 得到一個四百八十萬第一個speaker 得到一個四百八十萬維的vector	11-5
相當於這裡面的一個點嘛	11-5
這裡面的一個點	11-5
它是一個第一個speaker 得到一個四百八十萬維的vector	11-5
相當於一個這個四百八十萬維空間的裡面的一個點	11-5
第二個speaker 呢也得到一個四百八十萬維的vector	11-5
是這裡面的另外一個點等等	11-5
那我現在有一千個speaker 就是這裡一千個點嘛	11-5
對不對我就一千個點在這裡	11-5
所以呢我等於是有一個四百八十萬維的空間	11-5
那這個空間上的任何一點其實都相當於一套model	11-5
因為你空間上的任何一點你都可以想像是這空間上的任何一點	11-5
譬如說這裡的任何一點	11-5
你都可以想像是一個就是一個四百八十萬維的一一個一個這個vector	11-5
如果這樣想的話	11-5
那麼喔不是這邊上的任何一點是是那邊那個空間上的任何一個點	11-5
都是一個四百八十萬維東西	11-5
那這裡面的譬如說前面若干維相當於某一個mean	11-5
這邊若若若干維相當於某一個mean 對不對是不是這樣子	11-5
就好像原來的這邊的若干維是相當於某一個mean	11-5
這邊的若干維相當於某一個mean	11-5
它是這串起來的嘛對不對	11-5
我本來這個就是這樣做的嘛把一個一個mean 把一個一個mean 串起來變成一個大vector	11-5
等等一個一個mean 串起來變成一個大vector	11-5
那這樣構成那那個空間裡面的構成那個空間裡面的那一點	11-5
因此呢現在那個空間裡面的任何一點你也可以想像成相當於某一個mean 某一個mean 某一個mean	11-5
所以呢你任何一點呢相當於某一組這個model	11-5
然後呢也就是相當於某一個speaker 可能是這樣子的	11-5
因此呢你可以想像這裡的這個上面的每一點	11-5
都可以相當於那一大群的	11-5
這裡的每一點相當於四百八十萬維	11-5
相當於那一大群的phone model 的的這些個mean	11-5
那麼如果是這樣的話	11-5
那上面的每一點其實相當於一個speaker	11-5
也就是說每一個training speaker 是它那裡的一個點	11-5
好如果是這樣的話我現在可以對那個點對那堆我現在有一千個點在這裡啦	11-5
我就可以對這一千個點來做pca	11-5
怎麼做	11-5
第一個要把它變成zero mean	11-5
所以減掉mean 嘛	11-5
因為我們剛才講了我pca 都是都是當它是在zero mean 之下才有這堆solution	11-5
所以呢我要先讓它是zero mean	11-5
所以我第一個呢減掉mean	11-5
減掉mean 之後我再求它的covariance matrix	11-5
求出來之後呢	11-5
這個covariance matrix 我就可以求它的eigen value 跟eigen vector	11-5
那底下這個式子就是我這邊的這個式子	11-5
就是你這邊的是k 個	11-5
這k 個就是我這邊紅色的這k 個	11-5
就是這第一個	11-5
然後這個lambda i 就是我中間這個這k 乘k 個lambda i 的matrix	11-5
然後右邊的這個呢	11-5
這個的transpose 就是這k 個	11-5
那這樣乘起來呢幾乎就是原來的covariance matrix	11-5
所以這個式子就是我這邊所紅色的這個式子	11-5
而這個lambda i 就是我的那i 那k 個最大值的eigen value	11-5
那這k 個呢就是我的eigen vector	11-5
有最大的它的lambda one 大於lambda two 大於lambda k	11-5
就是第一大第二大第三大這樣我總共k 個	11-5
這樣的k 個呢就是我的k 個最大eigen value 的那k 個eigen vector	11-5
然後呢那當然你要怎麼選擇k	11-5
你要使得大於k 的已經小到夠小了	11-5
也就是說你怎麼選擇這個k 呢	11-5
一定要讓這後面已經很小很小零點零零零多少	11-5
很小所以呢它們的效果在這裡不明顯了	11-5
那在我們做過的經驗這個k 大概從五十到兩百五十之間的差不多啦喔	11-5
你雖然原來這邊有四百八十萬個或者多少個	11-5
你這邊的非常大	11-5
這邊的dimension 可能是這個n 可能是	11-5
當然不一定要四百八十萬	11-5
可能是夠大的至少上萬哪喔	11-5
成千上萬的	11-5
但是你最後可能只要五十到兩百五十個	11-5
就變成一個相當小的就夠了	11-5
因為其他東西都已經效果非常小	11-5
因為這些值都是非常趨近於零的eigen value 都可以不用了	11-5
那這個意思等於是說	11-5
我這邊本來是一個四百八十萬維的空間	11-5
每一點是一個speaker	11-5
他有他的全套的model 的參數	11-5
是一個點	11-5
那我現在呢等於是說	11-5
我把它reduce 到一個五十維或者是兩百五十維的一個小的sub space	11-5
那這個小的sub space 裡面的每一點都是對應到那一點的	11-5
舉例來講呢譬如說這一點就是投影下來對應到這一點	11-5
這一點就投影下來對應到這一點	11-5
這一點就投影到這一點	11-5
這一點呢是投影到這一點	11-5
那每一點在這邊都有一個它對應的	11-5
那這些它所對應的就是我們剛剛講的y	11-5
那它跟它的關係就是y 等於a 的transpose x	11-5
就是就是我們剛剛講的這個嘛	11-5
喔就是這個	11-5
我投影過來就是y 原來是x	11-5
就是這個關係	11-5
好那有了這個之後呢	11-5
那我現在可以怎樣呢我下一頁的上半段	11-5
其實跟剛才是一樣的	11-6
就是因為powerpoint 我沒有辦法同時呈現兩張	11-6
接不起來	11-6
所以這一塊其實就是剛才的底下這塊是完全一樣拷過來而已	11-6
那這個時候呢這個我的新的那個k k 的space 就是我所謂的eigen voice space	11-6
也就是說我現在不再需要考慮這個四百八十萬維的大空間了	11-6
我只要考慮這個五十維的小空間	11-6
這五十維的小空間的每一個點也就是原來的那個點	11-6
只不過原來那個呢是y	11-6
我現在到這邊的呢是x	11-6
那我們說呢y 呢等於a 的transpose x	11-6
所以呢它們有一個直接的one to one 的mapping 的關係就是這個關係	11-6
所以我現在只要在這個上面考慮就行了	11-6
那這裡的每一點	11-6
其實呢我也只要我把它inverse 回去就可以算出x 來	11-6
所以每一點的y 呢我都可以對應到x	11-6
而那個x 呢就有四百八十萬個值	11-6
它就相當於所有的phone model 的mean	11-6
對不對我那個x 找出來之後	11-6
就相當於那那這些個值就對應到這個mean	11-6
這個值就對應到這個mean 等等	11-6
因此呢我這四百四百八四百八十個一出來的話	11-6
我其實就已經這個model 就已經有了	11-6
好因此呢我現在只要考慮這個我現在只要考慮這個五十維的空間	11-6
在這五十維的空間裡面就是我們所謂的eigen voice space	11-6
就是由這些eigen vector 所展開的	11-6
然後呢我現在每一點	11-6
其實代表整套的phone model 對不對	11-6
就是我們這邊講的因為這裡的每一點是y	11-6
y 都可以對應到x 的關係是這個	11-6
所以你當然也可以我用這個的inverse 去乘的話	11-6
就可以得到x 嘛	11-6
那x 就是這上面的點	11-6
那這上面的點是四百八十萬維的	11-6
所以就對應到所有的phone model 的所有的mean 都在那裡了	11-6
所以呢每這上面的每一點	11-6
其實都代表整套phone model 的參數	11-6
那麼因此呢這個呢等於說是那這個k 個eigen vector 其實代表最重要的speaker 的特性都在那裡了	11-6
那我是怎麼求出來的我是用很多的training speaker	11-6
他們的大量的training data 所train 出來的那些一大堆的phone model	11-6
然後得到了這一堆	11-6
那這個的每一個呢就代表了我的最重要的speaker 的特性	11-6
什麼叫做最重要的speaker 的特性呢	11-6
那麼他們研究結果譬如說第一個e one	11-6
你可以猜e one 是什麼e one 就是男生跟女生	11-6
你的第一個e one 的vector	11-6
它的一面就是一面就是男的一面就是女的	11-6
然後越是越是粗粗厚的男生就是e one 的值越從這邊跑	11-6
越是嬌細的女生聲音越往這邊跑	11-6
那基本上幾乎就是一半男生一半女生	11-6
當然你有的時候有一點點不同	11-6
有一些男生的聲音很嬌細的就會跑到這邊來	11-6
有些女生聲音很粗厚會跑到這邊來是會	11-6
不過基本上這個幾乎就是e one 就是男生跟女生	11-6
這是通常我們本來如果你把一群speaker 去分兩群的話通常就會分成男的女的喔	11-6
那這個是speaker 最明顯的區別就是這個性別	11-6
那其實e one 就是性別	11-6
那同理e two e 三大概都可以找到一些物理意義	11-6
那麼因此呢這些就是最具有最重要的speaker 的特性就在裡面	11-6
那麼因此你大概有五十個到兩百五十個之間你這個space 幾乎就是所有的speaker 在這裡了	11-6
那你如果這樣想的話呢	11-6
每一個新的speaker 也可以在這邊找到一點	11-6
現在一個這這一千個speaker 都是training 的speaker	11-6
train 好這個model 之後	11-6
一個新的speaker 來了	11-6
他講了一句話	11-6
那怎麼辦	11-6
我就根據那個speaker 的那一句話想辦法locate 他在這裡他在這個eigen voice space 裡面的哪裡	11-6
如果他是這一點的話	11-6
那我就可以同樣地用這個inverse 回去	11-6
就知道喔它原來是這個上面的這一點	11-6
如果是那一點的話呢那一點是四百八十萬維的	11-6
於是就已經告訴我所有的這個它的所有的mean 是什麼都有了ok	11-6
那這就是它的基本精神	11-6
所以呢一個新的speaker 進來	11-6
我只要在這個eigen space 裡面找到一點就是了	11-6
那這個eigen space 裡面所以一個新的新的speaker 進來	11-6
我就想辦法在這個兩百五十維裡面找到它的那一點	11-6
那那一點怎麼找	11-6
那一點就是a i e i 嘛	11-6
就是我現在有每一個eigen vector	11-6
分別找一個相對於那一個的coefficient	11-6
然後做一個linear combination	11-6
a i e i 就得到我的y 嘛	11-6
因此我要找的就是這五十個a i	11-6
那這五十個a i 怎麼找	11-6
maximum likelihood	11-6
一樣我用這個式子	11-6
那也就是說呢我現在你可以想像是我我只要找到這五十個a i 的話	11-6
a i e i 就可以得到我的在這上面的這一點	11-6
那這一點呢又根據這個transformation 我就知道它是在四百八十萬維上那一點	11-6
就得到一全套的得一到全套的的所有的phone model	11-6
那因此呢given 這堆的model 的話呢	11-6
那麼我會看到現在它講的這句話的機率最大的會是哪一個值	11-6
所以我還是一樣根據user 說的這句話新的speaker 進來講的這句話	11-6
我根據這句話	11-6
我要找這一點	11-6
怎麼找就是找這些個a i 的值	11-6
such that 這些a i e i 加起來之後	11-6
所對應到這一點對應到那一點之後的那一那一點所對應的那四百八十萬個model	11-6
四百八十萬的值的那些個model 呢裡面會看到這個的機率是最大的	11-6
那這個呢這一樣又是maximum likelihood 這就是likelihood function 嘛	11-6
對我就得到maximum likelihood	11-6
然後我怎麼求這個東西呢用em	11-6
還是用em	11-6
所以這個em 是很重要的我們後面會說這個em	11-6
那麼這樣一來的話呢我就是要找所有的a i	11-6
裡面使得這個機率最大的	11-6
使得這個likelihood 最大的那一個那組a i 找到的話	11-6
那就是我要的a i	11-6
這組a i 一找到	11-6
我就把這個a i 對應回去	11-6
a i e i 就可以得到這個y	11-6
有了這個y 我就可以對應回去得到x	11-6
有了這個x 我就有了他全套的聲音	11-6
again 這裡我有一大堆unseen 的聲音我都一起找到了	11-6
這邊雖然只有少數這幾個音	11-6
這裡只有這一堆這一堆phone model 聽到而已	11-6
我只有聽到這堆phone	11-6
可是根據這堆phone 我找到這堆a i 的時候	11-6
a i 所對應的這一點跟這一點	11-6
可不是只有這些phone	11-6
而這個是對應到所有的phone	11-6
所以呢所有的unseen 的model 一起看到	11-6
那這個就是eigen voice 基本精神	11-6
那麼也就是說呢我現在是	11-6
我只需要small number of parameter	11-6
這些a i	11-6
這個a one 到a k 就是這k 個a i 的值	11-6
我只要有這些個值的話呢已經就足夠讓我可以specify 整個的整個的speaker	11-6
因為我把所有的model 全部算出來了	11-6
那這樣子的話呢我可以只需要很少量的data 我就可以很快速地調過去	11-6
你只要講第一句話	11-6
你只要講第一句話第二句話	11-6
這邊就已經非常清楚告訴我這些東西的a i 是什麼值	11-6
我就對應到就出來了	11-6
所以結果你所得到的情形呢是	11-6
比剛才這個如果我現在這個圖上來看的話呢	11-6
你可以想像的這個eigen 這個eigen voice 的的斜率是更高的	11-6
這個是我們的這個eigen voice 的話	11-6
它的斜率是更高的	11-6
就是因為我只要最少的你譬如說只有五十個或者說兩百五十個	11-6
這個參數非常少	11-6
所以呢我只要很少量的data	11-6
就可以讓我把這五十個a i	11-6
或者兩百五十個a i 找到之後	11-6
我就可以很快地調過來	11-6
那麼因此呢它是一個比起來是它的速度比剛才那些都快	11-6
我只要很少量的data	11-6
所謂的rapid 的意思	11-6
這個快速是指我需要的data 少	11-6
你只要講少數幾句話	11-6
我就整套全部學到了好	11-6
那麼只需要very limited quanity of training data 我就可以調得很好	11-6
那但是呢這也有一個缺點	11-6
它是saturate at low accuracy 它一樣同樣同樣的問題	11-6
這邊雖然很好	11-6
斜率是是最快的	11-6
但是呢它又有同樣的問題就是我會又會在更低的地方saturate	11-6
為什麼會在更低的地方saturate 呢	11-6
這個最大的問題應該是說因為我的too few free parameter	11-6
我現在參數只有兩百五十個嘛	11-6
那我等於說用這兩百五十個或者五十個參數	11-6
要對應到四百八十萬個參數去	11-6
所以這個matrix 本身的精確度是是一個問題嘛	11-6
對不對我現在這邊只有兩百五十個	11-6
可是我這邊要對應到四百八十萬個去	11-6
所以這個matrix 的精確度是不容易做得很好嘛	11-6
那麼因此呢我這邊太少只有只有五十個或者兩百五十個	11-6
當你data 再多的時候它有沒有會更好呢不見得了	11-6
因為你的data 再多的話你這中間不夠好的話你就好不了了	11-6
所以呢它有同樣的問題就是performance 會saturate at lower accuracy	11-6
好因為我too few free parameters	11-6
這是它的一個限制	11-6
那既然是這樣於是就有人想說我其實也可以用我們上面所說的tree structure	11-6
或者是這個分群的方法	11-6
喔沒錯他們後來他們就有人做了tree structure 的分群的方法	11-6
我也一樣地可以把這個做成一個這樣子的結構	11-6
就是tree structure 的結構嗯	11-6
那當我的data 越來越多的時候	11-6
我變成一個一個的	11-6
我我的這個這個sub space 變成一個一個的	11-6
變成更精細的	11-6
我data 少的時候我就只有一個	11-6
data 多的時候我就拆成很多個	11-6
他們也可以這樣做	11-6
你如果這樣做的話呢那這個就可以saturation 這個就會上去嘛喔等等	11-6
那這些我們就不講了你如果興趣你自己去找找reference 可以找得到	11-6
所以呢這是它的基本上的limitation	11-6
不過也有可以克服它的辦法	11-6
讓它的saturation 向上移動	11-6
那當然它的eigen voice 還有一個很大的限制就是說它的所需要的計算量跟memory 跟training data 都是比較大的	11-6
那你可以想像我要我要做一個夠好的covariance matrix	11-6
需要譬如說一千個speaker 或者多少個	11-6
所以我的需要的training data 也是比較多的	11-6
然後我要做四百八十萬維的這個pca	11-6
這個計算量是很大的	11-6
memory 也是夠大的喔	11-6
所以基本上的cost 是比較高的	11-6
但它有它的很精采的地方就它可以做一個這樣子的的的結果	11-6
使得我可以用很少量的很少量的這個speaker 的聲音	11-6
我就可以很快地調回去	11-6
就可以由這一個這個點對應到那邊那個點去可以就對應到那四百八十萬個參數去	11-6
喔這個觀念是相當相當值得學習的	11-6
那這裡面我們再如果再回過頭去看一下剛才這裡的話呢	11-6
其實你也有改改進它的空間	11-6
就譬如說呢你你這個vector	11-6
不一定要是這四百八十萬維	11-6
你也可以用別的來做	11-6
譬如說我可以用m l l r 裡面的a 跟b 來做	11-6
那也就是說	11-6
那這個的point 是說	11-6
我們剛才講這裡很大的一個問題就是你這你這兩百五十維	11-6
你要對應到這四百八十萬維	11-6
所以中間這個transformation 會變成要要要這個transformation 不容易做得做得精確	11-6
所以最好這邊不要那麼多嘛	11-6
不要這麼多的辦法呢就是我改用a 跟b	11-6
你記得我們我們上一上一堂課講的m l l r 裡面	11-6
我把這個空間分成一群一群	11-6
這是c one 裡面有a one 跟b one	11-6
這是c two 裡面有a two b two 的等等	11-6
那我現在不要拿這些東西來做這個vector	11-6
我用這個東西來做	11-6
可不可以也可以	11-6
那其實這些a one b one a two b two 其實代表的也是那個speaker 嘛	11-6
對不對你如果給我一個x i 的speaker independent model 的話	11-6
你給我a one b one a two b two a 三b 三	11-6
其實也就一樣define 了那一組vector	11-6
那所以我就不要用這麼多了我就用這個a one b one	11-6
那這樣就少了很多喔	11-6
所以呢我也可以用這個方式	11-6
就是這個嗯我用這個在這裡嗯就是m l l r 裡面的a 跟b 的column	11-6
譬如說這個這個a 的matrix 我就把這一個一個排起來	11-6
也可以	11-6
那這樣的話它就不會有四百八十萬維	11-6
也許只有譬如說一萬維或者多少	11-6
我的dimension 可以大為縮小或者只有五千維呀什麼的	11-6
那這樣子的話我比較做起來會比較好做	11-6
而且也比較克服一些困難喔等等	11-6
那這些都是可以做的空間	11-6
那嗯這個是我們這邊講的	11-6
那它的基本精神你現在大概可以了解	11-6
那我們等於用pca 的方法	11-6
把每一個speaker 本來一個speaker 有他的model	11-6
有一大堆參數	11-6
那那些參數不管怎樣	11-6
不管你是用它的mean 還是用它的a 跟b	11-6
總之排成一個很大的vector	11-6
那我現在呢把這個這個多維的高維的vector reduce 到一個很低維的空間來	11-6
靠什麼用pca 的方法	11-6
是我想辦法去找那些個dimension 它的variance 最大	11-6
是我想辦法去找那些個dimension	11-6
它的所有的統這個變化都在這上面呈現了	11-6
所以呢我那麼多的變化我就在一個很小的五十維的空間裡面呈現了	11-6
就是這個東西	11-6
那麼於是呢我就變成一個小的space	11-6
就是我的eigen voice space	11-6
於是呢我現在的每一個speaker 是這上面的一個點	11-6
就是training speaker 一二三每一個就是這些點	11-6
同樣地每一點你都可以想像是一個speaker	11-6
那新的speaker 進來也就是這裡面的一個點	11-6
所以每一個新的speaker 我只要找到它的coefficient a i	11-6
就可以了	11-6
那些a i 就對應上這些點我就對應到這全部的東西	11-6
那a i 怎麼做	11-6
用em 做maximum likelihood 可以得到	11-6
那這就是這個eigen voice 基本精神	11-6
那你如果要詳細看的話	11-6
eigen voice 的原始的paper 是再下來這篇喔	11-6
就是這一篇	11-6
那兩千年	11-6
那這個這個裡面有詳細說	11-6
by the way 我這邊講的這這幾個東西都是用em train 的	11-6
所以呢在這些paper 裡面你看到一堆數學	11-6
看不懂它在說什麼的時候其實那堆就是在講em	11-6
那麼所以呢等到我們講到em 你那堆就會看懂喔	11-6
那就像我們這邊講的這裡	11-6
我要求這個怎麼求	11-6
它會有一大堆數學那堆數學其實就是em	11-6
那然後呢就用那個就可以求出來等等	11-6
那這個是講這個eigen voice	11-6
好那我們這邊講的所有的這些都是很好的期末報告的題目	11-6
我下週會再講一下期末報告的的規定是怎樣怎麼做	11-6
不過基本上就是說嗯你可以完全用讀paper 然後就寫這個reading report	11-6
因為因為paper 很多你可以去找你只要根據一個	11-6
光是這一個題目你就可以找到一堆	11-6
喔譬如說eigen eigen voice	11-6
你就找你從這個去找的話你就會有有有一堆	11-6
你光是看這些就可以就做也可以	11-6
那當然你要寫程式也可以你可以做程式	11-6
然後可以做這個computer 的這個報告也可以	11-6
那當你在做computer 的報告的話	11-6
你可能你的data 不夠	11-6
你可以用我們所提供的data 就是嗯習題	11-6
習題都會給你很多data 嘛你可以用那個習題data 來做	11-6
但是習題給你的data 不見得一定適合你要做的題目	11-6
譬如說如果要做這種speaker 的題目的話	11-6
你需要有很多不同的speaker	11-6
每一個speaker 的量要夠多	11-6
然後我才可以做這些事情	11-6
那我們給你的data 不見得符合的時候	11-6
你可以跟助教討論	11-6
那麼在可能範圍之內我們會請助教提供你這些個data	11-6
喔所以你要做程式的也可以	11-6
那這些東西都是可以做報告的題目	11-6
那麼再下來的應該還有兩個我不準備再花很多時間講了	11-7
一個就是這個speaker adapt training 就是s a t	11-7
一個就是class adapt training 就是c a t	11-7
那這兩個嗯應該就嗯是再下去的兩個也是蠻有代表性的喔	11-7
那我們也許先我們先在停在這裡休息十分鐘	11-7
那麼底下的一堂課我們請助教來講第二個習題	11-7
那麼我們的第二個習題是train language model 的n gram 的習題	11-7
那第二個習題我們會在我們待會在討論個交習題的時間	11-7
我想會是在期中考以後	11-7
考完期中考以後你再做就行了	11-7
不過我先給你這樣子	11-7
ok 我們先在這裡休息十分鍾	11-7
現在來	11-7
我們現在來看那個嗯第二題習題喔我們請助教來講	11-7
各位同學我現在來講一下這一次的作業二	11-7
嗯我們的作業二是要你去做有關於language model 的training	11-7
那我們用的工具是s r i l m	11-7
對那等一下會提到就是怎麼去找這套工具	11-7
那再來就是我們之前有學到過perplexity所以這次我們是用它來看	11-7
在language model上面的一些evaluation 的結果	11-7
s r i l m 可以在這個地方找就是這是他們的官方網頁	11-7
那它上面只有提供source code所以你下載之後	11-7
要再自己去compile build	11-7
那它的平台應該是unix 跟linux 我沒有看我沒有看過windows就是	11-7
好那下面就是它download 的網頁	11-7
那我們現在是用版本一點四點六	11-7
它有出一個一點五點零是新版的	11-7
不過那個是beta 版所以我們是還是先用舊就是目前最stable 的版本一點四點六	11-7
那在這個部份就是	11-7
後面會有一個簡單的一些指令告訴你說怎麼樣去怎麼樣去安裝這個軟體	11-7
那它自己有一個官方的install 的 document	11-7
不過基本上就是你們光看這個document 大概還是很容易裝不起來	11-7
那我們會在我們的網頁上面提供一個有關於詳細安裝的一些就是方一些過程	11-7
還有一些常見的問題跟解決的方法那	11-7
所以這個的話可能要請你們在安裝的時候到網頁上去看	11-7
那這部份可能會是你們遇到最大的麻煩那	11-7
這邊解決完之後面應該就還好	11-7
那這邊是大概就是你們download這個檔案回去之後去看一下大概是怎麼樣去install這部份	11-7
那再來是我們檔案是分兩個喔一	11-7
個是我這個這個這個投影片那另外一個是我們這次用來做作業二的一些data一些training的corpus	11-7
那檔案叫做h w two 點tar 點g z	11-7
那這個也是會放在網頁上讓大家下載	11-7
那一樣我們也是有限制下載的時間那時間到了請大家就是在這之前要下載完	11-7
那後面是大概會	11-7
這個完這個powerpoint 的後面會大概教你一下你把這個檔案下載下來之後你要怎麼做	11-7
那基本上就是把它展開然後到那個目錄下去	11-7
然後你如果已經compile 完s r i l m 的話	11-7
在這邊你可以直接做一些很簡單的工作就可以得到得到結果	11-7
那這邊就是就是大概在講怎麼做那	11-7
這邊也是請同學下課下載回去做再自己看一下這裡怎麼做	11-7
那基本上來說這裡應該都不會遇到問題啦	11-7
是重要就是你在compile 的時候可能會比較麻煩而已	11-7
那我講一下就是我們這一次提供的corpus就是有分為商業跟體育兩個corpus	11-7
那當然就是我們會交叉做比對	11-7
就是你拿商業的新聞來做training corpus然後去看看它對體育的新聞的perplexity怎麼樣	11-7
然後再看看它對於商業新聞的perplexity 怎麼樣	11-7
那就是讓你看說同質性的語料它的perplexity 的高低的變化	11-7
那就是有兩個有兩組test 跟兩組training 所以就是會有四種比對的結果	11-7
那最後你會有一個類似就可以得到這樣子的結果就是你的training 是商業跟體育	11-7
然後test 是商業跟體育那	11-7
結果你就是要把你	11-7
這個作業的第一題就是你要把這四個結果求出來然後交給我們	11-7
對	11-7
那第二題就是你要說一下就是你在這個結結果裡面觀察到的現象	11-7
那再來是你可以做一些bonus 的部份就是	11-7
你可以把一些data 做做變化譬如說像這邊講到	11-7
你可以把兩個training data 放在一起做做更大的一個training data	11-7
或者是你可以把train 跟test 放在一起那看看就是	11-7
因為我們剛才就是如果你只用商業的train然後去拿它的test data 來做test 的話	11-7
這個叫做open test	11-7
就是你的training 跟跟test 是不一樣的東西	11-7
那如果你是把test 加進來一起算的話就叫close test	11-7
就是你可以把可以把test 的語料拿進來做training	11-7
那結果應該會理論上要比較好才對因為它會比較更接近你的test data 的	11-7
那我們就是有提供一個e mail 信箱讓你交作業	11-7
那也是記得要交到交到這個e mail 去	11-7
那主旨跟格式也都放在這邊	11-7
那希望同學就是不要遲交因為這個作業應該也不會太難	11-7
那大概是這樣子	11-7
ok	11-7
嗯稍微補充一下喔	11-7
就是我們現在要做的第二題是train n gram language model喔	11-7
那給各位用的就是這個s r i	11-7
就是這是stanford research institute 他們所發展的一套	11-7
s r i 的tool kit 就是專門train n gram 的	11-7
那裡面其實還有很多東西你如果去看的話喔	11-7
那麼包括我們講的各種smoothing 的方法什麼它都有的喔	11-7
你都可以用	11-7
然後	11-7
那麼詳細的東西	11-7
那這個是講怎麼樣子去download 它等等	11-7
然後怎麼樣安裝喔	11-7
我想剛才已經說過了	11-7
那麼	11-7
嗯我們的我們的data 我們的training 跟testing data 還是一樣要	11-7
我們還是七十二小時吧	11-7
你現在裝好沒有	11-7
已經放上去了所以我們就從今天中午十二點開始七十二小時就是三天之內	11-7
好不好	11-7
到星期五中午以前你download 完畢	11-7
之後我們就收掉了就不再裝了	11-7
ok	11-7
嗯	11-7
那同樣的情形這所有的training data 都是屬於有智財權的	11-7
所以麻煩各位就是你用就好	11-7
那麼不要留給別人ok	11-7
喔	11-7
然後你這個課結束我沒有叫你交回來但是你也就不要流出去了就是了	11-7
啊	11-7
那這只是給給你作習題用的而已啊	11-7
那這裡面有一點要注意就是說	11-7
並不是你拿到一堆文章就可以train language model 為什麼因為我們中文是有詞的	11-7
你如果得到一堆一堆一堆字的話	11-7
你只能train 字的n gram	11-7
喔	11-7
那你記得我們說過我們中文的話你可以做字的n gram 你你也可以做詞的n gram	11-7
詞的n gram 你要先知道誰跟誰是一個詞	11-7
這是一個三字詞這是兩個字詞這是一字詞	11-7
那這個詞的n gram 會比字的n gram 好很多	11-7
那字的n gram 有字的n gram 的好處是不用斷詞詞的n gram 的話你得斷詞	11-7
ok	11-7
喔	11-7
那我們現在的給你的應該是已經都斷好詞的對不對	11-7
對	11-7
都是斷好詞所以你可以直接可以做詞的n gram	11-7
但是當然你也可以把那個斷詞的詞的邊界拿掉你可以做字的n gram 你也可以做的	11-7
喔	11-7
所以這都是有你可以做的空間在內	11-7
那然後這個嗯我們就是給你兩套	11-7
一套是體育新聞	11-7
一套是這個工商產業新聞	11-7
所以你基本上可以發現這兩套的的詞彙跟句型都是不太一樣的所以它們的perplexity 什麼都是不一樣的	11-7
那這點我們在講language model 那時候都提過這些事情所以我想你大概可以回回想一下大概就會了解所以中間跑來跑去	11-7
會有各種狀況	11-7
然後你也可以做各種不同的組合等等喔	11-7
好那我們嗯討論一下交報告的時間	11-7
那我想是今天是五月二號	11-7
今天是五月二號	11-7
下週是九號再下週是十六號	11-7
我們這天考期中考	11-7
再下週是二十三號	11-7
所以呢我覺得合理的時間是期中考後一週	11-7
怎樣有沒有問題	11-7
好不好我們就是期中考考完你還有一週的時間嘛	11-7
這個鐵定一週是做的出來的啦	11-7
所以這個我們就以期中考後的一週五月二十三號為deadline	11-7
好不好	11-7
喔	11-7
ok 好那就這樣子	11-7
那我們期中這個這個題目的部份就到這裡	11-7
你幫我回到上課的地方	11-7
待會下課拿下來發	11-7
你印好了沒有	11-7
印好了ok 好那就	11-7
我們待會下課的時候我們會發上一次考試的考古題	11-7
好	11-7
這樣你就會知道我們	11-7
來這樣就可以你你拿這個好了啊	11-7
這樣子你就這個嗯你就知道我們考試會怎麼考的喔	11-7
好那我們回到剛才說的	11-7
嗯ok 我們已經說到eigen voice 講完了我們底下要講的是s a t 跟c a t啊	11-7
這個我們就很快說一下s a t 的觀念是什麼呢	11-7
s a t 是所謂的speaker adapt training	11-7
它的觀念是說想辦法decompose phonetic variation 跟speaker variation	11-7
這是什麼意思	11-7
就是說我們我們講的這個這些個distribution	11-7
是包含這兩種variation 在裡面	11-7
譬如說我們說這堆是ㄚ	11-7
這堆是ㄧ	11-7
那ㄚ跟ㄧ是有區別的	11-7
這是所謂的phonetic variation	11-7
可是問題是有一堆speaker variation	11-7
也就是說你今天如果讓讓五個人來唸ㄚ的話他每一個人唸的ㄚ不太一樣	11-7
所以呢你如果五個人來來唸ㄚ的結果呢	11-7
這堆就會變大	11-7
同樣呢你如果讓五個人來唸ㄧ的話每一個人唸的ㄧ也不太一樣於是它也會變大	11-7
於是就會搞在一起	11-7
那這是這個搞在一起使得我們分不清楚的原因	11-7
其實是因為這兩種phonetic variation 跟speaker variation 混在一起了	11-7
所以它的想法是什麼呢	11-7
它說我想辦法把speaker variation 拿掉	11-7
讓它儘可能把speaker variation 除掉	11-7
讓它儘可能只剩下phonetic variation	11-7
就是說如果不同的speaker 的ㄧ不太一樣	11-7
我有沒有辦法把它除掉	11-7
然後呢使得它最後只剩下原來的這一個	11-7
如果最後只剩下原來這一個	11-7
這些ㄧ的話呢	11-7
這是真正的phonetic 的ㄧ	11-7
就是這樣子	11-7
因為speaker 的不同而造成那個變化呢我讓它拿掉之後	11-7
變成這樣子	11-7
那同理呢ㄚ我也把這些儘可能把這個speaker 的variation 拿掉	11-7
我看能不能讓我的ㄚ呢變成只有這一堆	11-7
如果這樣的話呢他們的每一個都比較compact	11-7
那就可以得到一個這個這個這個我儘可能把這個speaker variation 除掉之後呢就得到一個比較compact 的model	11-7
那這個compact 的model 的話呢我就可以拿這個來train speaker	11-7
independent model 的話就會比較好啊	11-7
所以呢就可以for first adaptation 也就是說呢	11-7
我的這個我們不管哪一種方法都是從這個s d 從都是從這個s i 開始train 的嘛	11-7
那這個s i 如果可以好就會好嘛	11-7
對不對我想辦法把這個s i 提高	11-7
怎麼提高法呢	11-7
就是我這個s i 儘可能是用	11-7
儘可能是想辦法先把speaker variation 除掉之後變成這種我再來train 就會比較好	11-7
但怎麼除法呢	11-7
它說至少我可以用m l l r 裡面的y 等於a x 加b	11-7
什麼意思	11-7
我們本來是y	11-7
等於a x 加b 是什麼	11-7
是說這個x 是speaker independent model	11-7
然後經過這個transformation 之後呢得到這個是speaker dependent model	11-7
對不對	11-7
這個是針對某一個speaker 的	11-7
這個是那麼因此呢從這個觀念來來想你就可以想這個a 跟b 其實是a 跟b 其實是這個嗯描述了這個人的東西	11-7
對不對	11-7
那既然如此當然我也可以反過來	11-7
你可以想像是說我每一個人的聲音我就來做這件事	11-7
你可以想像我的inverse 是什麼呢	11-7
就是x 是等於y 減b	11-7
然後乘上這個a 的負一吧	11-7
大概是這樣	11-7
這就是x	11-7
所以你今天如果我我把每一個每一個speaker 的s d 算出來之後	11-7
我來做類似這樣的一件事情的話	11-7
其實我就等於是把它的speaker 的特性somehow 把它的speaker variation 除掉之後	11-7
想辦法走向s i 嘛	11-7
對不對	11-7
那其實這個東西其實也是一個這個東西也也是一個等於是嗯你你你寫成另外一個就是其實也也可以寫成a bar 的y	11-7
加上b bar 等於x	11-7
你也可以這樣寫嘛	11-7
所以你就是說你你現在是可以把一個speaker dependent 的東西拿來	11-7
也是一樣做相同的一個linear regression	11-7
可以變成s i 的	11-7
等於是這個意思嘛	11-7
那它的想法就是用這個方式來做	11-7
所以呢我可以用m l l r 裡面的a 跟b 你求出來之後	11-7
你也可以一群一群來做	11-7
然後呢想辦法用這個方式來除掉它speaker 自己的東西之後	11-7
剩下一個比較s i 的	11-7
然後這樣的話呢那每一個人那這樣這時候不同的人的ㄚ就會比較像	11-7
不同人的ㄧ就會比較像	11-7
於是我就得到一群那這個是真正的speaker independent	11-7
可是它們是很compact 的	11-7
那不會再散得那麼開	11-7
因為我已經把這些東西都除掉了	11-7
所以即使是你找了五五百個男生與五百個女生	11-7
這一千個人他們的ㄚ搞不好都比較接近	11-7
所以都在一起了	11-7
那這樣子	11-7
那用這個方式來做	11-7
那這個觀念就是所謂的s a t	11-7
我畫的這張這個底下這半就是這件事情	11-7
所以呢譬如說speaker one speaker two 到l 個speaker	11-7
每一個人分別找出他自己那一堆的a i b i 來	11-7
然後呢就這個你就可以做這樣的事情之後把它的儘可能把它的speaker vari variation 都拿掉	11-7
剩下一個比較乾淨的	11-7
然後呢拿來train 一個比較compact 的speaker indepen independent model	11-7
那這個model 就拿來做那邊的s i model 之用	11-7
然後你現在不管後面做什麼都可以	11-7
那基本上那個應該是比較好	11-7
所以你起點比較高你這個狀況就比較好	11-7
嗯這就是s a t	11-7
那詳細的數學式子講寫起來很簡單就是底下這個	11-7
你原來的s i model 怎麼做的	11-7
其實就是這個式子	11-7
這個也是一個maximum likelihood	11-7
就是我如果我要找一堆m a 我要找我要找這堆model	11-7
使得given 這堆model 之後	11-7
我看到的這些聲音	11-7
那這些就是所有的譬如說一千個一千個training speaker 五百個男生五百個女生的一千個人的聲音	11-7
的機率是最高的	11-7
這個就是likelihood function	11-7
那我要我要找找那一組model 的參數	11-7
就是譬如說這所有的phone model 裡面的mean 啊covariance 這些東西我要找所有這些東西	11-7
使得given 這堆model 之後	11-7
我會看到這些training data 的機率是最高的	11-7
那我調這些東西調到那個最高的那一組就是我的s i model	11-7
所以s i model 說穿了就就是就是在做這個式子	11-7
那我現在不同的是怎樣	11-7
我現在稍微改變一點點	11-7
是說我要做一個比較compact 的s i model	11-7
把這個model 變成一個compact 的s i model	11-7
然後呢我這裡面有一堆a i b i	11-7
對每一個speaker 都有一組a i b i 在那裡	11-7
因此呢我現在要找的是不光是這一組compact 的model	11-7
還包括所有的a i b i	11-7
那這些東西我都要一起找	11-7
這些個model	11-7
compact model 以及所有的a i b i 都要找	11-7
然後都通通都要調	11-7
看哪一組最後讓我看到這個機率最大的	11-7
那於是我就把這個東西拿來作為我的	11-7
那這個就是我用這個方式來做出來就是我的s a t	11-7
喔這就是speaker adapt training	11-7
那簡單講就是這樣的意思	11-7
那這個詳細你如果要看的話reference 應該是在前面的再下一篇	11-7
喔嗯這個這個是speaker adapt training 喔	11-7
就是就是這一篇呢喔	11-7
就是七號的這篇speaker adapt training 是這個	11-7
那底下呢我們可以再講一下是是這個	11-7
還有一個就是是c a t 是class adaptive training	11-7
這c a t 的觀念是什麼呢其實也很簡單講穿了就是這樣	11-7
就是我的training speaker 先把它分群	11-7
那你知道我們講假設我有五百個男生五百個女生	11-7
不是每一個人的聲音完全不一樣	11-7
有的人的聲音比較像	11-7
那一群人比較像這一群人比較像等等	11-7
那我其實可以做一件事情就是	11-7
把這些speaker 根據他們的聲音的特性來分群	11-7
假設這是一千個speaker	11-7
我先根據他們的某一些特性去分群	11-7
譬如說分成兩群	11-7
結果這邊有五百六十個	11-7
這邊有四百四十個等等	11-7
那如果分成兩群多半分出來一半是男生一半是女生啦	11-7
然後呢再進一步再分	11-7
再進一步再分	11-7
這樣你可以分成根據某一些差異去把它分成一棵tree	11-7
那到時候每一個到時候每一個leaf node 裡面的那一群人的聲音就是很像的	11-7
那麼因此呢你這樣就可以得到一群一群的人就是一個個cluster	11-7
每一個cluster 的人呢	11-7
他們是聲音比較像的一群人	11-7
所以呢我就就把這個training speaker 分成r 個cluster	11-7
那我用一些speaker clustering 的方法	11-7
那這個我們我想這邊就不講你如果有興趣去查reference 都有喔	11-7
我就把它這個人分分群嘛	11-7
我分好群之後呢	11-7
每一群去train 它們的model	11-7
那也就是說我現在可以這個每這個每一群的人	11-7
這一群的人我們可以train 它們的model	11-7
這一群的人我可以train 它的model 等等	11-7
那我新的speaker 呢就是interpolate from the mean	11-7
那麼因此你可以想像這個這個很簡單的觀念就是說	11-7
我現在有這個l 個training speaker	11-7
把它分成r 群	11-7
這是第一群第二群第三群	11-7
每一群它們都train 出一個	11-7
它們的model	11-7
那麼不是每一個人一個model 而是這一群人一個model 所以我總共有r 群有r 個model	11-7
那一個新的speaker 來呢是它們的interpolation	11-7
那也就是linear combination a i m i	11-7
那換句話說	11-7
這個新的speaker 它要它要用怎麼樣子的model 呢	11-7
那就是用它的用它的跟它的每一個人都用一點	11-7
有一個weighting parameter	11-7
那這個觀念其實跟剛才的eigen voice 是很像的	11-7
你如果看eigen voice 是指	11-7
是a i e i 嘛	11-7
a i e i 得到那個eigen voice	11-7
那它現在呢其實很像	11-7
是a i m i	11-7
那不同的地方在哪裡	11-7
這邊的每一個mean	11-7
沒有理由它們是orthogonal	11-7
也沒有理由它們是	11-7
怎樣因為它們只是每一群人這一群不太一樣	11-7
每一群不太一樣我就做一個linear combination	11-7
可是剛才的的eigen voice 的話呢	11-7
它們是都e i 都是orthogonal	11-7
對不對它們全部都是orthogonal	11-7
然後呢它們的這個都是eigen vector 求出來所以它們代表很清楚的speaker 的特性	11-7
那這裡比較沒有	11-7
那只是每一群有一個有一個vector	11-7
那麼有它的mean	11-7
或者是說是你每一個phone model 的裡面的每一個phone 的每一個mean	11-7
它都有一個值	11-7
對每一群人而我就做一個linear combination	11-7
那這個a i 怎麼求	11-7
a i 一樣我用maximum likelihood	11-7
那用什麼方法還是一樣用e m	11-7
這都是用e m 喔都是用e m	11-7
所以呢	11-7
這邊也是一樣	11-7
我的a i 用maximum likelihood 來求　	11-7
那這個時候常常我可以加一個所謂的mean bias	11-7
什麼是mean bias 　	11-7
mean bias 就是剛才這個　	11-7
我這個c a t 可以用這個來做	11-7
你可以想像的是我如果得到一個這個的話呢	11-7
這個是一個基本上是所有的人的聲音都像的是那一個	11-7
然後呢你現所以呢這個m b 就是那個mean bias的	11-7
就是這個mean bias	11-7
那這個就是剛才的這邊的這個compact 的speaker independent model 所train 出來的	11-7
那train 好之後	11-7
我我以那個為準	11-7
然後新的speaker 來看它跟它差多少	11-7
它跟它差的跟這個有一點像差一點的把它加進去	11-7
跟這個有一點像把它加進去	11-7
這樣再把它加進去	11-7
所以呢以mean bias 為準	11-7
所以mean bias 的位置是一	11-7
然後其它呢再weight 一個a i然後我加進去這樣就可以啦	11-7
喔	11-7
那這個觀念就是所謂的c a t	11-7
class adapt training	11-7
那我想這個簡單解釋就是這樣子	11-7
那詳細的話呢在剛才的再下一篇	11-7
嗯再上一篇這一這一篇第六篇	11-7
class adapt training 就是就是在講這個東西喔	11-7
這個是兩千年的	11-7
那所以我想這些大概是嗯speaker adaptation 的比較代表性的一些	11-7
那我講說其實在最近幾年還有很多	11-7
那我們不講那麼太新的東西因為太新的東西還沒有經過這個時間的考驗	11-7
跟沒有被多數人的認定所以我們暫時不用	11-7
但是都是寫報告的好題材	11-7
所以你都可以去看然後都可以當拿來拿來當當報告	11-7
但是我們在課程課堂裡面當教材來講的話呢我們講比較被肯定的	11-7
那麼有有比較長的歷史然後大家都覺得不錯的喔	11-7
那我們講這講到這裡	11-7
那最後一章是講還有另外一種東西就是speaker 的這個recognition	11-8
換句話說我的我還有另外一個問題是要判斷它是誰呀	11-8
那我們之前講的都是反過來就是你可以想像	11-8
我有兩種啊就是phonetic variation 跟speaker variation	11-8
那我們一直之前講的所有的東西都是希望把speaker variation 消掉	11-8
想辦法強調phonetic variation讓我能夠分辨它是ㄚ還還是ㄧ	11-8
但是反過來有另外一種問題是反過來	11-8
我並不care 你講的是什麼話	11-8
我要知道你是誰	11-8
那這就是我要強調speaker variation 的了	11-8
那我目的是要recognize speaker 而不是recognize content	11-8
那這個的用途你可你可以想像很明顯的有兩大類	11-8
一類就是speaker 的identification	11-8
就是確認他是誰	11-8
那麼	11-8
舉例來講呢假設有一個有一有一間實驗室那麼只有一群人可以進去	11-8
那那那你說我是誰	11-8
然後它可以確認你是不是那個人	11-8
如果是那個人的話呢就讓他進去	11-8
喔	11-8
那個門就可以打開這個芝麻開門喔等等	11-8
那你也可能是就是基本上就是就是在一群人裡面確認他是誰	11-8
那另外一個很常用的例子就是這個勒索恐那個勒索案	11-8
這個電話勒索的時候你把那個電話拿來那個聲音到底是誰的	11-8
你從所有的有前科犯的聲音裡面去找	11-8
喔等等這就是所謂的identification	11-8
那verification 是說要verify 是不是他講的那個人	11-8
他說我是某某人	11-8
那你要確認那個人就是他喔	11-8
那這個就是speaker 的verification	11-8
那這些東西的最基本精神就是所謂的g m m	11-8
那g m m 其實跟h m m 是一樣的東西它只是少掉時間上的state	11-8
也就是說我們原來的	11-8
我們原來的h m m 是這樣	11-8
這一個state 一個state 一個state	11-8
在這個state 裡面有一堆gaussian	11-8
說明在這個state 裡面它是怎麼distribute	11-8
在這個state 有另外一個gaussian	11-8
那麼就不太一樣了	11-8
到這個state 又有另外一個gaussian	11-8
又不太一樣了等等	11-8
那麼因為我們講一個聲音的時候你本來就是從頭到尾有變化嘛	11-8
那我這個是這個譬如說零	11-8
這個是ㄌ這個是ㄧ這個是ㄣ	11-8
所以呢我這個聲音有變化零一定是這樣過來的所以我這個這樣子變	11-8
但是我現在如果是要分這個是speaker 是誰的話	11-8
我不再需要這樣子	11-8
我不care 你是零還是是ㄌㄧㄣ還是ㄣㄧㄌ不是沒什麼關係	11-8
我只要知道這個是張三發的還是李四發的	11-8
所以呢我不再需要分這個時間上的差異	11-8
我乾脆就把它合成一個model	11-8
一個model 就夠了	11-8
因此我就把這些全部train 在一起得到一個	11-8
那這個可能是比較複雜的	11-8
我有很多個gaussian	11-8
那麼通常譬如說	11-8
兩百五十六個或者五百一十二個	11-8
這個gaussian 數目很多	11-8
那麼其實這gaussian 裡面可能有的gaussian 是ㄚ的gaussian	11-8
有的是ㄨ的gaussian	11-8
喔	11-8
那反正都在這裡面	11-8
我有一大把	11-8
那每一個人有這麼一個model	11-8
那這個model 就是所謂的g m m gaussian mixture model	11-8
那除了說它只剩下一個state 之外其它跟我們之前講的幾乎是完全一樣的	11-8
所以我只要有一個喔我只要有一個state	11-8
那麼不管你發的是什麼音反正是這個張三有一套他的	11-8
對不對	11-8
那麼張三有他的兩百五十六個gaussian 的一個model	11-8
李四有另外一個這兩百五十六它的每一個就不太一樣一點就是了	11-8
那於是呢	11-8
每一個人都有一個	11-8
所以呢這就是譬如說這個m 就兩百五十六個model 的gaussian	11-8
每一個gaussian 就是有一個mean 一個covariance	11-8
跟一個weight	11-8
所以呢你要算某一個聲音的話	11-8
就是算這個嘛一樣的	11-8
就把那個把這個聲音裡面的每一個feature vector 代進去	11-8
去算它的gaussian 的分數	11-8
然後呢這個	11-8
你看誰的分數最大嘛	11-8
因此呢你我我如果有三百個speaker 就有三百個這種東西	11-8
我現在一一段聲音進來我就把它的每一個feature vector	11-8
都放進去	11-8
放進這三百個model 裡面	11-8
看誰的model 最大等等	11-8
那這就是這個	11-8
所以這就還是一樣這是maximum likelihood	11-8
因為這是個likelihood function 就是球求這個maximum likelihood	11-8
那當然我現在用的這個嗯parameter 可能有點不同	11-8
因為我們原來	11-8
原來做做這個的時候我們希望儘可能的是	11-8
把speaker variation 拿掉要強調phonetic variation	11-8
所以在那個時候我們用m f c c	11-8
那現在呢我現在是要儘可能地把phonetic variation 拿掉	11-8
我要強調speaker variation 所以不見得m f c c 還對	11-8
那你可以用其它的	11-8
喔	11-8
基本上你就是希望找那些參數是帶有speaker 的特性的	11-8
那麼譬如說你在m l l r 裡面的那些a i b i 是可以拿來用的	11-8
eigen voice 裡面的那些a i	11-8
c a t 裡面的a i 喔	11-8
這些	11-8
c a t 的a i 就是剛才這個嘛	11-8
就是這些a i 嘛	11-8
喔	11-8
那eigen voice a i 就是這些嘛	11-8
那這些應該都代表speaker 的特性	11-8
所以都可以拿來用	11-8
不過其實多數人用的最簡的還是用m f c c	11-8
換句話說m f c c 其實是包含著這兩者的	11-8
我們很難從m f m f c c 裡面真的把speaker 的特性除掉	11-8
同樣也很難真的把這個除掉它兩兩個都有	11-8
所以呢m f c c 其實是可以拿來分辨是什麼音	11-8
也可以拿來分辨是什麼人	11-8
喔所以其實m f c c 是是是可以用的	11-8
喔	11-8
那這就是所謂的g m m 的基本精神	11-8
那底下的這個verification 呢我們要用到這個likelihood ratio test	11-8
那這個是我們在這個十點零裡面會講到那個時候我們再來說	11-8
喔ok好我們這個部份今天說到這裡	11-8
那剩下的時間現在應該是助教有把那個期中期中考的考古題拿來哦	11-8
所以我們可以各位可以來拿一下這個這個期中考的題目喔	11-8
ok 好我們今天上到這	11-8
喂	11-8
那個忘記忘記講一件事喔就是我們想要徵求上課的同學有人抄筆記抄得比較完整的	11-8
我們想徵求一份好的筆記來幫助我們就是因為我們後面要做那個這個課程錄影的我們要做那個喔後製作	11-8
那麼在黑板上我我畫的圖啊這些東西不見得拍得那麼清楚所以需要一個比較好的筆記喔	11-8
所以各位如果如果有筆記抄得比較完整的可以跟我們的助教聯絡如果你的可以提供筆記給我們的話	11-8
ㄜ 我們之前所說的這些都是speaker 的adaptation	11-8
也就是如何去調這個acoustic model	11-8
讓他調到調到適合每一個speaker	11-8
恩 也就是調這些個h m m	11-8
讓他適合每一個speaker	11-8
然後我們針對每一個speaker 去做recognition	11-8
那最後的speaker recognition是反過來	11-8
是說我現在不是要去辨識這個speaker在說什麼	11-8
而是去辨識他是什麼人	11-8
當我們要知道他是什麼人的時候呢我們現在變成是要這個把這個speaker 的variation 強調出來	11-8
而把phonetic variation丟掉	11-8
那麼換句話說呢我們要根據這個人的的話在說什ㄜ欸根據這個人說的話來判斷	11-8
這個他是什麼人	11-8
那這個情形其實最標準的作法	11-8
就是把h m m 本來的這個state 全部merge	11-8
換句話說這個本來我們h m m 裡面為什麼會有這麼多state	11-8
是因為我們我們要分辨他講什麼話顯然是一個phone 接到另外一個phone	11-8
前面這些個phone 是在講某一個音後面是講某一個音	11-8
那麼因此呢我們講這個話必須是這樣一個音接一個音下去的	11-8
所以呢他有一個順序	11-8
這個時間的順序就是由這不同的state 來描述的	11-8
當我們在考慮一個speaker 的時候呢則不同	11-8
當我們考慮一個speaker 的時候我們只要問這個聲音是誰講的	11-8
並不care 它是什麼音	11-8
因此呢我們不再需要根據這個音的時間順序來考慮了	11-8
因此呢最簡單的辦法就是把它們合成一個大state	11-8
當我把它們合成一個大state 的時候我只要一個state	11-8
那裡面有一大堆的gaussian	11-8
那麼這時候一大堆的gaussian 呢你就可以想像它就是一個只有一個state 的h m m	11-8
那這個呢就是我們這邊所謂的g m m	11-8
那不同的speaker 它的這些東西不一樣	11-8
那麼用這個來判斷啊	11-8
這就是所謂的g m m	11-8
那麼因此呢他它跟我們所說的h m m 其實沒有太大不同	11-8
變成只只有一個state 就是了	11-8
那麼在只有一個state 的情形之下	11-8
在只有一個state 的情形之下那我現在就變成有一堆gaussian	11-8
然後呢每一個gaussian 有一堆weight 加起來	11-8
那麼我可以為每一個speaker train 這堆東西	11-8
所不同的是我現在這堆gaussian 的數目應該是蠻多的	11-8
因為你可以想像不管它發什麼音	11-8
它的每一個phone 每一個音它都在這裡面呈現	11-8
那麼所以呢你基本上通常我們們都需要比較多的mixture 的數目	11-8
也就是這個大m 要比較大	11-8
這m 要比較大	11-8
那麼舉例來講呢如果我們是有某一種種語言有六十個phone 的話你至少每一個phone 都要呈現在這裡	11-8
那麼每一個phone 如果有兩個三個gaussian 的話	11-8
那就是一百二十八或者兩百五十六個mixture 喔	11-8
才比較足夠一點	11-8
所以通常是一個比較mixture 數目比較大的一個g m m	11-8
那這樣的話每一個人都做一個這樣的model 之後	11-8
那看是誰的分數比較高就可以了	11-8
這個就是g m m 的做法	11-8
那麼這時候用哪些feature	11-8
depends on 你喜歡哪一種	11-8
那麼雖然m f c c 我們通常拿來是辨識它的fanatic variation 的	11-8
其實它也帶著speaker variation 我們知道因為不同的speaker 它的distribution 其實不一樣的	11-8
所以以m f c c 是最常用的因為你就直接可以用	11-8
當然你也可以用那些些m m r 裡面的參數	11-8
eigen voice 裡面的參數	11-8
c a t 裡面的參數等等	11-8
都是可以的	11-8
這個是講speaker recognition	11-8
你要辨識它是什麼人	11-8
那另外一種speaker variation 呢	11-8
是它說它是誰我要確認它是不是	11-8
啊那這個時候呢speaker variation 通常我們分成兩大類	11-8
一個是text dependent	11-8
就是你要規定它說哪一句話	11-8
那根據那句話來判斷它說的對不對啊	11-8
所以這是所謂text dependent	11-8
一種是text independent	11-8
就是隨便它說什麼話我都可以判斷它是不是它講的那個人	11-8
這是text independent	11-8
那基本上來講呢是text dependent 一般認為比較容易被破解	11-8
那原因很簡單因為你就變成只要講那句話	11-8
那只要那句話這個是誰的聲音就認為是誰的嘛	11-8
所以最最簡單的破解它的辦破解它的辦法就是如果某人他講那句話的時候	11-8
你偷偷把他錄下來	11-8
於是我就可以拿那段他錄好的聲音	11-8
我就可以去破解所有它的東西啊	11-8
所以這個是speaker dependent 的弱點	11-8
但是這樣當然它的正確率比較高因為去針對那句話來做的	11-8
speaker independent 因為不針對某一句話所以呢正確率稍微低一點	11-8
那這裡面一個基本的做法是要做這個likelihood ratio	11-8
這個東西我們現在還沒有講是在十點零裡面	11-8
不過我已經現在先把十點零先跳掉了	11-8
所以我們現在暫時先不說這塊	11-8
這個呢我們等到十點零的這個likelihood ratio 講過之後	11-8
我們再來講它	11-8
ok 以上是我們上週結束前下課前講的十一點零	12-1
底下今天我們主要的工作是要進入十二點零	12-1
十二點零的主軸是這個language model 的處理	12-1
那麼這裡面一個重要的方法是所謂l s a	12-1
就是latent semantic analysis	12-1
那我們來說這個東西	12-1
那其實這個在幹嘛其實還是一樣在做我們這邊所講的adaptation	12-1
只是說我現在從acoustic model 的adaptation 調變成language model 的adaptation	12-1
那麼什麼是adaptation	12-1
就是要調它	12-1
調到你所它所適合的某一個條件	12-1
那我們我們之前在講speaker adaptation 的時候是說	12-1
我這個個hidden markov model 裡面的每一個gao 每一個state 裡面的這些mean	12-1
每一個state 裡面的的這些mean 我都要調	12-1
因為如果是從針對某一個speaker 的話	12-1
針對某一個speaker 的話它的這些東西都會不一樣	12-1
不同的speaker variance會不一樣的	12-1
所以我最好要調這些東西	12-1
調到針對每一個人	12-1
這是我們上週所說的十一點零所說的speaker adaptation	12-1
那我們今天在這十二點零所說的其實是language model 的	12-1
那麼language model 有什麼好調的呢	12-1
因為你說今天後面接天氣	12-1
這個好像不管誰說都一樣嘛	12-1
這個language model 是在講這件事情	12-1
譬如說這是bi gram	12-1
那麼今天天氣後面接好這是tri gram	12-1
你如果這樣看這個是誰說都一樣所以language model 的的adaptation 不是為了調不同的人	12-1
而是調什麼呢調不同的domain	12-1
不同的topic	12-1
也就是說我們之前說過	12-1
在講language model 的時候我們說過	12-1
你如果是在談氣象的跟談政治的	12-1
用的詞彙是不一樣的	12-1
談體育的談財經的	12-1
用的詞彙是不一樣的	12-1
因此它的句型也是不一樣的	12-1
因此你如果今天當我在談不同的topic	12-1
或者談不同的concept	12-1
你如果講的是不同的concept 或者不同的topic 的話	12-1
我用的詞彙會不一樣	12-1
那麼它們的n gram 是不一樣的	12-1
那麼我們們在講六點零的時候曾經說過	12-1
那麼你如果是財經新聞或者體育新聞或者是這個嗯政治新聞	12-1
它們的n gram 是不一樣的	12-1
那我們現在講的是這件事	12-1
所以我language model 也是需要調的	12-1
但是不是在調不同的speaker	12-1
是在調不同的topic	12-1
如果我知道你是在講哪個topic 的話我應該要調到那個topic 去	12-1
然後使用那個topic 的language model	12-1
這是所謂的language model adaptation	12-1
那麼language model adaptation 有很多種的方法	12-1
正如我們十一點零說adapt這個speaker adaptation 也有非常多的方法一樣	12-1
這個也是一個很重要的主題有非常多的方法	12-1
那我們這邊所講的只是是其中之一	12-1
那麼倒不見得說這是最有效喔不見得	12-1
但是它是最可以算少數最general	12-1
可以apply 到很多地方去	12-1
它等於是一種基本的的觀念	12-1
然後從它可以衍伸出很多東西出來	12-1
然後它也可以apply 到很多種不同的的應用上去	12-1
所以它有它的重要性	12-1
所以我們來說這這一個	12-1
並不表示language model 它就一定要用它	12-1
它有很多別的方法	12-1
那麼嗯我們所謂的linguist processing 也不限於是指language model adaptation	12-1
你知道所謂的linguist processing 就是我們講的的在linguist 那一層所做的任何處理	12-1
包括詞字句的各種分析處理	12-1
都是我們所謂的linguist processing	12-1
那麼舉一個例子	12-1
我們在六點零說過詞分群	12-1
因為詞可以分群之後做class space 的language model	12-1
那怎麼分群呢	12-1
它的的分群本身就是一種linguist processing	12-1
那麼所以linguist processing 只是只要在linguist 層次	12-1
那麼做的任何事情都是我們所謂的linguist processing	12-1
那麼這個l s a 呢也是其中的一種方法就是了	12-1
那麼什麼是這個latent semantic analysis 呢	12-1
這個latent 這個字的意思我們已經從前看過了	12-1
就是潛藏的	12-1
那semantic 是什麼意思semantic 是它的語意	12-1
也就是指你裡面真正exactly 的意思是什麼	12-1
那麼因此你其實是在講一句話的時候我們希望知道它真正潛藏在裡面的意思	12-1
那些意思其實就是我們所說的是什麼topic	12-1
跟什麼concept	12-1
所以其實是要再分析是什麼concept 跟什麼topic 啊	12-1
這是我們這段要說的事情	12-1
那這段我這段大部分講的東西是based on 第一篇	12-1
那麼我底下主要以這個為基礎	12-1
那它算算是寫得最完整清楚的是這篇	12-1
那麼後面是些什麼我後面會再解釋	12-1
那麼這個的想法是怎樣呢我們也許用一個簡單的說法來講	12-1
那麼你今天如果說在一句話裡面看到布希	12-1
你大概會猜說它大概是講跟美國政府有關的東西	12-1
你如果看到一個另外一個詞是白宮	12-1
大概也是跟這些事情有關的	12-1
那因此你不管是布希或者是白宮可能是在講類似的東西	12-1
那你看到另外一個詞譬如說李安	12-1
那又是另外一件事它可能是跟電影或者是跟奧斯卡有關的等等	12-1
那麼你如果看到另外一件事情譬如說這個九二一	12-1
哦這是一件地震	12-1
這是又是另外一件事情等等	12-1
所以某一些詞彙可能告訴我那裡面講的concept	12-1
或者說是嗯那個topic	12-1
但是光是這些詞彙其實不容易讓我們了解憑什麼來分析這個	12-1
那在這個l s a 這裡	12-1
它想的辦法是說我另外找一堆文章	12-1
就是所謂的document	12-1
那這邊呢我們可以說是word	12-1
光是一堆words	12-1
其實我們可以猜一堆word 是在告訴我某一些個concept	12-1
只是我不太容易光用這些word 來分析	12-1
但是有一個很重要的東西存在就是document	12-1
什麼document	12-1
我上網去抓就可以抓千千萬萬篇文章下來	12-1
每一篇文章有它自己的concept 跟topic	12-1
假設說我這個叫d one 這個叫d two	12-1
我有我總共有大n 篇文章的話	12-1
那我有這麼多篇文章	12-1
那麼我就可以分析這些文章跟這些詞之間的關係	12-1
雖然每一篇文章我用人去看是可以說ok 這篇是在講什麼topic	12-1
這篇在講什麼topic	12-1
可是我我我如果不是人去看的話很難講它是什麼	12-1
正如這個詞一樣	12-1
我用人去看可以知道它是講什麼它是講什麼	12-1
可是如果沒有人去看的話我憑什麼分析呢	12-1
那它在l s a 這裡它想的辦法就是	12-1
我靠文章跟詞這兩件事情的相互關係來分析它們之間的關中間的topic	12-1
怎麼講呢	12-1
譬如說如果是這這裡有幾篇文章都是在講美國政府什麼的話	12-1
它們可能都有白宮都有布希	12-1
那反過來呢這裡裡有幾篇文章是在講電影啊文化	12-1
搞不好他們都有李安	12-1
等等	12-1
所以呢你可以從這邊來看說誰有哪些詞誰沒有哪些詞	12-1
你也可以從這邊來看說這些詞在哪些文章裡面	12-1
你就靠這中間的關係來想辦法把它區分出它有哪些個concept	12-1
因此呢我想辦法在中間找出一堆東西來	12-1
這些東西就是我們所謂的concept	12-1
或者說是topic	12-1
那它可能譬如說電影是其中一個	12-1
如果是電影的話	12-1
那就可能這邊就是李安	12-1
這邊就是有李安的文章	12-1
那如果是美國政府	12-1
那可能是另外一個觀念一個concept	12-1
那它很可能這邊就有布希有白宮	12-1
那這邊就有這些文章章等等	12-1
那因此呢它等於說是我我在computer 我可以直接抓到data 是一堆詞跟一堆文章	12-1
那我可以算哪些詞在哪些文章裡面	12-1
哪些文章裡面有哪些詞	12-1
用這個關係去分析	12-1
抓出中間到底是哪些東西	12-1
那些東西就是topic	12-1
那有了這個topic 我就知道今天如果這個人講話講了一堆話	12-1
應該是這個topic 的話	12-1
我就會猜它後面講的的詞還是電影有關的詞	12-1
那這個時候電影有關的詞的分數就可以跳高出來	12-1
如果那個人講的那堆事情我發現它是在講這些所以是在講這個的話	12-1
它再來應該會跳出都是跟美國政府有關的這些詞彙等等	12-1
那這個就是我們這邊所講的用l s a 來做language model adaptation 基本的觀念	12-1
也就是我們這裡要說的事情	12-1
那因為這樣的關係所以它現在做法就是	12-1
我要用用一堆word 跟一堆document 來建構中間的關係希望把這中間找出來	12-1
好有了這個背景的了解那我們現在來看這件事	12-1
它就是建一個所謂的word document matrix	12-1
那是什麼呢	12-1
就是我這個matrix 每一個row 就是一個word	12-1
就是這邊的每一個詞	12-1
然後每一個然後每一個docu 每一個column 呢就是一個document	12-1
就是這邊所有的文章	12-1
那就變成一個matrix	12-1
所謂的word document matrix	12-1
那講清楚一點的話就是我有一個辭典	12-1
這個辭典裡面有所有的詞	12-1
w one w two w i 到w 的大m	12-1
其中大m 就是我的詞的總數	12-1
w i 就是第i 這個辭典裡面第i 個word	12-1
那舉例來講譬如說大m 等於兩萬	12-1
假設我考慮一個兩萬詞的	12-1
這邊有有兩萬個詞所以這個matrix 是這個row 的數目是兩萬	12-1
然後呢我另外去上網抓了一大堆的文件	12-1
就所謂的document d one d two d j d n	12-1
d j 是第j 個document	12-1
那總共多少呢有大n 個	12-1
這大n 個呢就是譬如說說n 譬如說是十萬我抓了十萬篇出來	12-1
那就構成一個matrix	12-1
那這個matrix 的裡面的每一個element w i j 是什麼呢	12-1
w i j 有一堆複雜的東西是這樣寫的	12-1
不過最核心的部分就是這個c i j	12-1
c i j 是什麼	12-1
就是number of times w occurs in d j	12-1
也就是說就這個word 而言就這個word而言它在這篇文章裡面出現幾次	12-1
我就可以數一下	12-1
那它在每一篇文章裡面出現都可以數一下	12-1
所以呢它在這裡出現幾次在這裡沒有出現這裡是零	12-1
這個word 是零這個是五十這是二這是三這等等	12-1
這樣我就可以把它全部排出來	12-1
所以我每一個word 出現在每一篇文章裡的次數給他排出來變成一個row	12-1
換句話說你也可以看一個column 是什麼一個column 就是這篇document 裡面哪一個word 出現幾次	12-1
它不出他沒有出現就是零次	12-1
有就是一次兩次五次十次	12-1
這樣呢每一個這就是每一個document	12-1
那你如果這樣子看的話基本上我這個c i j 就是最基本的一個這邊的word 對不對	12-1
這就是w one w two 到w m	12-1
那麼這些個word 跟這些個d one d two 到d n 這些個最基本的關係	12-1
就是這個c i j	12-1
就這個c i j	12-1
只是說你如果光看這個c i j	12-1
嗯其實它已經有相當有意義	12-1
因為你可以猜得出來如果這個word 跟這個word 譬如說一個是布希一個是白宮的話	12-1
它們這兩個row 可能很像	12-1
如果布希布希會出現在哪些文章裡面的時候	12-1
白宮可能同時會出現	12-1
那麼因此呢這個row 跟這個row 會很像就表示這兩個東西是蠻像的	12-1
反過來如果這個row 是布希那個row 是李安的話	12-1
那搞不好它們兩個row 完全不同	12-1
那麼它有的地方它沒有對不對	12-1
它是零的地方它有一堆數字	12-1
它是零的地方它有一堆數字	12-1
如果一個是布希一個是李安的話可能沒有什麼交集	12-1
等等	12-1
所以它們哪一個row 像不像	12-1
你其實在這裡已經看得出來了	12-1
那同理呢兩個column 的話也是這樣子	12-1
每一個column 代表一篇文章	12-1
如果這篇文章是在講奧斯卡	12-1
這篇文章是在講九一一恐怖攻擊	12-1
那顯然它們幾乎很少交集	12-1
除了有一篇有一部電影在演九一一恐怖攻擊之外它們幾乎沒有交集	12-1
那反過來呢如果這篇是在講恐怖攻擊那篇是在講在這個攻打伊拉克那搞不好這邊很有關係了喔等等	12-1
所以呢你這也是一樣你可以用它們出現的這個詞的頻率的的位置	12-1
就知道說誰跟誰比較像喔	12-1
所以呢這邊就是說每一個row 它是一個n dimension 的feature vector 代表每一個word	12-1
每一個row 等於是一個那個word 的n dimension 的feature vector	12-1
那這個只是說它每一個dimension 是什麼每一個dimension 就是相對於說它所有的文章	12-1
每一個dimension 是那相對於那篇文章的出現的次數	12-1
每一個column 呢反過來是每一個document 的feature vector	12-1
等於在描述那篇document 它的特性	12-1
所以是它的column 的feature vector	12-1
那麼然後呢它也一樣它是用每一個word 來做每一個dimension 的關係	12-1
可是如果你光這樣做的話其實是不夠的	12-1
那麼我們說除了c i j 之外呢我們還要做一堆這些東西	12-1
這些東西其實就是在做normalization	12-1
我們希望把這這裡面光是這樣數的話其實有一些問題	12-1
所以我們要再做一些normalization	12-1
第一個normalization 就是除以n j	12-1
n j 是什麼	12-1
n j 是total number of words present in d j	12-1
看這篇文章裡面有多少詞多少個word	12-1
舉例來講假設說這個詞是陳水扁	12-1
他在這篇文章裡面出現兩次	12-1
在這篇文章裡面出現二十次	12-1
欸在這裡只有兩次在這邊只有有二十次那是不是表示一定這篇文章跟陳水扁的關係比較少	12-1
這篇文章跟陳水扁的關係比較大呢	12-1
不一定	12-1
要看這兩篇長短如何	12-1
對不對如果這篇文章總共才三十個詞	12-1
裡面有兩個是陳水扁	12-1
這篇文章很長有三萬個詞	12-1
裡面陳水扁才出現二十次的話	12-1
那誰跟陳水扁關係比較大	12-1
恐怕是這篇而不是那一篇	12-1
因為這篇可能很很短這篇可能很長啊	12-1
所以呢我們應該要對它的長度做一次normalization	12-1
那就是這邊所做的事情ok	12-1
所以斯 n j 是total number of words	12-1
在那個文章裡面	12-1
所以我要除一除	12-1
那我這回才比較像了	12-1
所以我這回等於是說這個這個陳水扁在三萬個詞裡面出現佔百分之多少零點零三次的比例	12-1
這個在這三十三十個word 裡面出現百分之多少	12-1
那這個時候就比較有意義了	12-1
所以這個是這個除以n j 的意義	12-1
那麼前面還有這個一減epsilon i 是什麼東西呢這比較複雜一點	12-1
epsilon i 是這個式子	12-1
它是有t i	12-1
t i 是什麼是c i j summation over j	12-1
換句話說我是把剛才的這個c i j 對所有的j 加起來	12-1
橫的加起來	12-1
也就是說是等於是說total number of word present	12-1
不是應該是total number of times 這個word i occurs in t	12-1
t 是整個的document set	12-1
也就是說呢我這個word 譬如說陳水扁	12-1
它在這邊出現幾次在這邊出現它在整個的十萬篇文章裡面出現了五千次	12-1
ok 那個五千就是t i	12-1
就是我總共出現五千次	12-1
這五千次裡面呢	12-1
那麼我現在來看它的這個我用c i j 除以t i 呢	12-1
就代表說在這五千次裡面它在這篇文章占多少	12-1
這是什麼意思呢	12-1
那你如果仔細想一想	12-1
這個其實就是在算entropy	12-1
那麼如果說這是那個matrix	12-1
這是某一個word i	12-1
這是某一個document d j	12-1
這邊是它的次數是c i j	12-1
所以呢我現在的這個ti 呢是把所有的c i j 全部加起來over j	12-1
就是我這邊所有的數目全部加起來	12-1
如果這個是陳水扁的話	12-1
那麼它在所有的文章裡面總共出現五千次	12-1
在這裡出現二十次	12-1
那就是二十除以五千	12-1
在這邊出現五次就是五除以五千	12-1
等等	12-1
那它分別代表說	12-1
我這個word 在全部的word 裡面佔百分之多少	12-1
它有多少然後這邊是零零零等等	12-1
那等於它等於是某一個機率p i p j 的意思	12-1
等於是某某一種機率	12-1
如果這是這個c i j 除以t i 是某一種機率的話	12-1
那你看這個式子c i 這個機率乘以log 再乘以它這個就是我們講的entropy	12-1
也就是summation 的p i log p i	12-1
這不就是entropy 嘛	12-1
那這個entropy 是什麼意思呢	12-1
你可以想像某一個詞譬如說陳水扁	12-1
它會在我們如果這是d one d j 到d n 的話	12-1
它的這個機率會是怎麼分佈的	12-1
它可能會在很多文章裡面會出現一些	12-1
很多地方沒有出現	12-1
這個可能是跟選舉有關的	12-1
這個可能是跟外交有關的	12-1
這個可能是跟民進黨有關的	12-1
那這個可能是跟民進黨有關的這個可能是跟修憲有關的	12-1
但是還有一堆譬如說是什麼呢但是它就沒有了	12-1
那我如果是李安的話會是怎樣呢	12-1
它可能都沒有	12-1
只有某一些有	12-1
其它都沒有了	12-1
因為就是這些跟電影有關的才有它	12-1
否則就沒有了	12-1
如果是李安的話	12-1
那反過來我換另外一種詞譬如說非常	12-1
或者是今今天	12-1
如果換成這種詞的話會怎樣呢	12-1
很可能全部都有	12-1
那這裡面的這這個最極端的可能就是這個詞	12-1
的	12-1
如果是這個詞的話呢那全部都一樣	12-1
幾乎是完全相同的	12-1
全部都有	12-1
那因此呢這個p log p 的這個entro 這個代表什麼呢	12-1
其實就是它的分佈的情形	12-1
你可以看得出來這個其實是什麼這就是entropy 的差別	12-1
哪個entropy 最大	12-1
這個entropy 最大	12-1
然後呢這個entropy 比較小	12-1
這個entropy 最小	12-1
所以呢像entropy 最大的	12-1
就是這種非常的	12-1
這種東西其實不告訴我它是在講哪一個topic	12-1
從從我們要分析它是講哪一個topic 的觀點來講	12-1
你如果碰到一個今天	12-1
其實沒有告訴我任何topic	12-1
那麼因此呢這種東西我應該儘量不要算才對	12-1
那麼反過來我碰到一個李安	12-1
它非常清楚的告訴我它的topic 跟這個有關	12-1
那麼因此碰到這個的時候呢這個就很重要了	12-1
所以呢我可以用entropy 來判斷說他告訴我是哪一個topic 的重要的程度	12-1
那就是這個pi log pi	12-1
也就是我們這邊的這個	12-1
c i j 除以除以ti 其實就是這個pi 嘛	12-1
那這個entropy 就告訴我這件事情	12-1
只不過呢我現在這個entropy 本身它的range 可以很大可以很小啊	12-1
怎麼辦	12-1
我就做一個normalization	12-1
那你知道我現在總共的word 數目是大n	12-1
所以呢如果這個機率完全相同的時候	12-1
是它的entropy 的上限	12-1
就就是log n	12-1
這就這就是我entropy 的的極大值	12-1
所以呢我就除以log n	12-1
當我除以log n 之後呢就會變成這個entropy 是介於一跟零之間了	12-1
ok	12-1
所以前面除以log n 只是normalize 一次讓它變成介於零跟一之間	12-1
所以呢它叫做normalize entropy of 某一個word 在整個的corpus 裡面	12-1
在整個的document set 裡面	12-1
他顯示我的topic 的鑑別力	12-1
我們說如果是李安	12-1
這個鑑別力是非常明顯的表示它的topic 是電影	12-1
那麼如果這個word 是今天或者是的	12-1
它很明顯的沒有什麼鑑別力	12-1
它沒有告訴我任何topic 的訊息	12-1
喔這就是所謂index in power	12-1
那麼因此如果你這樣做你就知道什麼時候這個epsilon i 會變成零	12-1
就是如果它只有一篇文章出現	12-1
假設某一件事情只有一篇文章有	12-1
其它的完全都沒有的話	12-1
那這個是entropy 最小最小的時候就是零	12-1
那麼這個這個如果存在的話可能是表示某件事情	12-1
譬如說這個某有一個科學上有一個新的發現	12-1
發現一個什麼什麼外太空有一個什麼星	12-1
只有一篇文章其他都還都還沒有任合人都還有在講那件事	12-1
那這個時候只有一篇文章有它	12-1
那如果你講的那個什麼什麼星座的話	12-1
那個顯然就exactly 就是指那件事了	12-1
所以它的鑑別力應是最大的	12-1
這個時候說epsilon 等於零的時候反而是鑑別力最大的	12-1
所以你你要用一來減	12-1
反過來呢什麼時候是epsilon 等於一呢就是真的就是的這個字	12-1
像的這個字的話呢就是每一篇都一樣有	12-1
那這個時候呢	12-1
我的就是它是等於這個t i 除以n 嘛	12-1
對每個都一樣	12-1
這個時候呢我的我的entropy 就是log n	12-1
所以你一normalize 就是一	12-1
而這個東西一減一就變成零	12-1
那這些就是沒有鑑別力的像的這種東西	12-1
ok 所以呢我現在乘上這個一減epsilon i 是這樣的意思	12-1
那麼因此呢我們可以說是	12-1
我們雖然這裡的每一個element 是以c i j 為基礎	12-1
不過我們做了兩個normalization	12-1
一個除以n j 呢等於是對於這個軸上面我們先做一次normalization	12-1
然後呢這個一減epsilon i 呢	12-1
可以算是在這個軸上做一個normalization	12-1
當我這兩個都做過之後	12-1
那這回它是比較清楚地描述	12-1
這個之間的關係我們底下要用這個來做了	12-1
那這個怎麼做呢在l s a 裡面	12-1
它的做法是拿來做一堆matrix 的運算	12-1
那麼這些matrix 運算是什麼呢我們來解釋一下	12-1
那麼這邊的想法其實非常接近我們上週說的eigen voice 裡面的p c a	12-1
是很像的	12-1
所不同的地方我現在這個matrix 不是正方形的	12-1
你注意到我這個matrix 這邊是w one 這邊是w m	12-1
這個m 是詞的總數	12-1
那縱軸是d one 到d n	12-1
這個n 呢是文章的總數	12-1
沒有理由它們會一樣啊	12-1
因此它是一個長方形的matrix	12-1
那麼我們上週講的那些個p c a 是一個正方形的matrix 你可以求eigen vector eigen value	12-1
長方形的不能做了	12-1
那怎麼辦	12-1
我們可以這樣子做	12-1
就是把w 乘以w 的transpose	12-1
這兩個一乘的話	12-1
就會變成是一個正方形	12-1
如果這個是w	12-1
那這個是w 的transpose	12-1
那這個是大m 乘以n	12-1
這個是大n 乘以m	12-1
那我乘出來就會變成一個什麼呢	12-1
變成一個正方形的m 乘以m	12-1
當我變成一個正方形以後這就是這邊講的w w transpose	12-1
變成一個正方形以後我又可以做這就是eigen vector 的分析	12-1
於是我就可以做eigen vector 的分析	12-1
就可以變成三個matrix 就跟上週一樣了	12-1
我變成三個matrix 相乘	12-1
這三個分別是什麼呢	12-1
我拆成三個之後	12-1
第一個是我的eigen vector 排起來的	12-1
e one e two 到e m 就是m 個eigen vector	12-1
因為我現在是m 乘m 的matrix 嘛	12-1
所以我有m 大m 個eigen vector	12-1
所以第一個row 就是我第一個eigen vector	12-1
第二個row 就是我第二個eigen vector 等等等等	12-1
我可以排到第m 個eigen vector	12-1
那這這m 個eigen vector 構成一個m 乘n 的matrix	12-1
就是這個大u 的bar	12-1
就是我這邊寫的這個東西	12-1
就是這個matrix	12-1
那右邊這個也一樣你把它橫的排起來	12-1
這個就是e one 第一個eigen vector	12-1
e two 就是它變成row 了	12-1
然後呢我有大m 個	12-1
把它這樣排	12-1
那這個就是大u 的transpose	12-1
就是這一個	12-1
所以呢我這邊就是我的大u	12-1
這就是我的大u 的transpose	12-1
那中間是什麼呢中間就是eigen value 所構成的	12-1
那我們說呢它只剩下對角線有值	12-1
其它都是零	12-1
那對角線上的每一個值就是所謂的eigen value	12-1
這個eigen value 我們現在故意把它寫成s i 的平方	12-1
所以譬如說第一個呢就是s one 的平方	12-1
第二個是s two 的平方等等等等	12-1
那為什麼寫成平方其實我們後面會有原因	12-1
不過我們現在先這樣寫	12-1
換句話說	12-1
你要把它的什麼是s one	12-1
是它相對於第一個eigen vector 那個eigen value 的square root	12-1
它的square root 叫做s one	12-1
所以它的平方是它的eigen value 等等	12-1
我把每一個eigen value 都寫成他的寫成一個平方	12-1
然後我也按照大小順序排列排下來	12-1
這跟我們之前講的意思是完全一樣的	12-1
好那麼我們現在先要問這個matrix 到底是什麼	12-2
你如果看這個w w transpose matrix 是什麼東西的話	12-2
它其實告訴我word 跟word 之間的相似度	12-2
什麼意思	12-2
譬如說在這個在這個matrix 裡面的某一個	12-2
這個是第i 個	12-2
跟這個第j 個	12-2
嗯這個是第i 個這個是第j 個的這個element	12-2
這兩個相乘的第i j 個element 這個東西到底是什麼意思	12-2
你看這個值其實是什麼	12-2
這個值其實是這邊的第i 個row 跟這邊的第j 個column	12-2
兩兩它乘它它乘它它乘它	12-2
兩兩相乘加起來的	12-2
那其實這個第j 個column 是什麼	12-2
不就是這邊的第j 個row 嗎	12-2
對不對	12-2
這邊的第j 個column 就是這邊第j 個row 啊	12-2
所以其實是什麼就是這兩個在做內積嘛	12-2
其實就是第i 個row 跟第j 個row 在做內積	12-2
其實也就是第i 個word	12-2
這是相當於第i 個word	12-2
這邊是相當於第j 個word	12-2
這兩個word 像不像嘛	12-2
內積就是它像不像嘛對不對	12-2
我都已經裡面都已經normalize 過了所以就是說它像它的相似度嘛	12-2
因此呢我這個這個matrix w 跟w transpose 裡面的第i j 個element	12-2
其實就是第i 個跟第j 個row of w 它們在做內積而已	12-2
也就是說它們之間的相似度	12-2
好那麼有這個意思之後我們現在來看我把它拆開來是什麼意思	12-2
這個拆開來的意思呢	12-2
我們其實我們的目的就是底下要講的這件事	12-2
這個跟上週我們講的eigen voice 的意思是完全一樣的	12-2
當我用eigen value 把它拆開的時候呢	12-2
很清楚地我照大小順序排列之後	12-2
我可以把重要的值大的放到上面去	12-2
當我把重要的值放到上面去之後	12-2
我可以抽前面最重要的	12-2
譬如說前面的這r 個	12-2
這是我m 乘上r 這r 個	12-2
然後這邊我也只抽r 個	12-2
這是r 乘上r 個	12-2
我這邊也只抽r 個	12-2
那這就是r 乘上m 個	12-2
那麼我如果只抽這個的話	12-2
乘起來會almost 跟這個是完全一樣的	12-2
那你回想我們這件事情在上週的eigen voice 裡面是在說怎麼樣的一件事	12-2
我們上週在說的eigen voice 是說	12-2
我把一大堆的把每一個speaker 的所有的參數做成一個大的vector	12-2
這個vector 可能多達它的dimension 可能四百八十萬個dimension	12-2
那麼於是呢我在這四百八十萬個dimension 上的每一點	12-2
其實都代都代表一個speaker 的那一堆model	12-2
不過這個dimension 太大了怎麼辦	12-2
我想辦法找一個它的subspace	12-2
譬如說這個是一個它的subspace	12-2
當然我現在沒有辦法畫那麼high dimension 的空間	12-2
我只能畫三度空間	12-2
於是它的sub subspace 變成是一個兩度空間的	12-2
那麼於是呢我真正做的事情是把這裡的每一點通通投影到這上面來	12-2
那到時候我會發現其實這個這個subspace 呢譬如說只有五十個dimension	12-2
那所有的點投到這五十變成一個很小的空間只有五十個dimension	12-2
而上面每一點呢都代表原來的每一個speaker	12-2
所以我這一點都可以對應回去	12-2
這是我們在上週說的eigen voice 在在在做這件事情	12-2
那現在要做這件事情是很像的	12-2
你現在雖然我現在每一個word 是有譬如說大n	12-2
大n 是十萬篇文章所以它原來是十萬個dimension 的這麼多東西	12-2
但是其實真的要這麼多嗎不見得	12-2
我現在這個matrix	12-2
仍然代表這裡面所有十萬個word 裡面所有的的關係對不對	12-2
我們已經說了這裡每一個都是代表i 跟j 之間的關係	12-2
所以這個matrix 就是代表這十萬這兩萬個word 裡面所有的word 的關係	12-2
而它的關係是用這個十萬篇文章來描述的	12-2
是一個這麼大的一個matrix	12-2
但是其實我可以把它縮減成為我這邊只取r	12-2
這個r 是多少呢	12-2
我這邊有沒有寫嗯我們有有在這	12-2
我們通常做啦我們做過我們如果是以新聞來做的話	12-2
我用各種新聞來做的話	12-2
這個r 大概八百就可以了	12-2
通常r 大概八百做到一千五百都差不多	12-2
這不需要再大了	12-2
所以雖然我這邊有十萬篇文章甚至於一百萬篇文章	12-2
我其實大概r 只要八百個到一千五百個就可以了	12-2
因此這個所有的的word 之間的relation 其實可以reduce 成為只有這r 個這r 個	12-2
這八百個這八百個這八百個	12-2
那麼你其實把這三個這個綠的matrix 乘起來呢	12-2
跟這個是非常像的	12-2
原因是剩下這些都很少	12-2
這些都是非常常小的值了	12-2
我這邊的時候已經是這個最大的值都在這裡了這裡面非常小的值你可以丟掉	12-2
所以後面這堆eigen vector 跟這堆eigen vector 都是可以丟掉的	12-2
那麼這畫這就是我們底下這邊所講的	12-2
那麼這個本來是m 乘n	12-2
這個是m 乘n 乘m	12-2
那也就是這兩個matrix w 跟w transpose m 乘n 跟n 乘m	12-2
那麼我現在呢	12-2
我可以簡化成為只有這個u	12-2
那我這樣的寫法的意思是說	12-2
我如果只剩下r 個r 個column 的話這就是u	12-2
上面沒有bar	12-2
ok 那這個就變成這也變成u 上面沒有bar	12-2
所以呢我凡事沒有bar 的u	12-2
就是其實只有m 乘上r	12-2
然後呢這個是r 乘上r	12-2
然後這個呢是r 乘上m	12-2
這是u 的transpose	12-2
我這邊有寫bar 的	12-2
就表示是dimension 是大m	12-2
沒有寫bar 的就是reduce 到只有r 個dimension	12-2
其中這個呢就只有r 個eigen vector 所構成的	12-2
那麼如果是這樣的話這個意思是什麼呢	12-2
那我們也許應該去了解一下	12-2
你如果回去看matrix 的數學的話它會說	12-2
其實這些eigen vector 告訴我是這樣的東西	12-2
也就是說你每一個eigen vector 跟它的transpose 相乘	12-2
再中在scaled by 它的eigen value	12-2
其實就是一個matrix	12-2
我們拿它的第一個eigen vector 而言	12-2
我這個是一個e one	12-2
這是一個column	12-2
那這個e one 的transpose 呢	12-2
是一個row	12-2
這兩個相乘是什麼	12-2
就是一個整個的matrix 對不對	12-2
它跟它相乘是個整個的matrix	12-2
而這整個的matrix 它的weight 給它一個s i 的平方	12-2
就是給它第一個eigen value	12-2
那就是這個的這個東西	12-2
所以你可以想像呢	12-2
我任何的一個eigen vector 跟它自己的row 跟column 去相乘	12-2
就得到一個component matrix	12-2
e i 跟e i 的transpose	12-2
就是一個component matrix	12-2
我們給它一個weight	12-2
那個weight 就是它的eigen valuesi 平方	12-2
那如果是這樣的話呢	12-2
那麼這一個就是相當於這一個eigen vector 所構成的那一個component matrix	12-2
那因此呢我的這個這個matrix w w transpose 這個東西呢	12-2
你可以看成是這一大堆加起來的	12-2
那這一大堆加起來裡面那它的weight 就是這些個eigen vector eigen value 的值	12-2
那我們說這個eigen value 的值它會把大部分的大的值都集中到上面來	12-2
我們把它照大小排列	12-2
大部分東西擠到這裡擠到這裡	12-2
到後面變成很小很小	12-2
所以後面這些個就不重要了嗎	12-2
所以我這些雖然是全部的i 加起來才會等於原來的	12-2
但是你只要加前面的大r 個	12-2
譬如說r 等於八百的話	12-2
你只要加前面的八百個幾乎就是原來的了	12-2
那八百以後的那一大堆	12-2
一直到十萬個其實都不重要了因為它非常小	12-2
所以你就可以拿掉了	12-2
於是呢我現在這個所有的word 之間的relation	12-2
我就可以reduce 到用這三個	12-2
小的matrix 只有八百dimension 的來描述它綠色的這塊	12-2
那就是我們這邊所用的這個東西	12-2
這也就是我們講的一個dimensionality 的reduction	12-2
我只要選擇r 個eigen value	12-2
夠大的eigen value 值就好了	12-2
那麼我們底下會說其實這八百個就代表八百個concept	12-2
或者說就是它的語意潛藏的concept	12-2
或者說就是它的topic	12-2
那這點我們底下再解釋	12-2
那麼如果說是這樣的觀念	12-2
你可以想像的話	12-2
那麼我們反過來也可以做另外一件相同的事	12-2
就是我現在做w 的transpose 再乘以w	12-2
那這個呢是完全相同的情形	12-2
但是我反過來	12-2
我先把它transpose	12-2
所以我得到一個這樣子的東西	12-2
這是w 的transpose	12-2
它是n 乘m	12-2
然後乘上w	12-2
是這樣的一個這個是w	12-2
這是m 乘上n	12-2
所以這兩個乘完之後變成一個什麼呢	12-2
變成一個n 乘n 的正方形	12-2
n 乘n 的	12-2
那這就是w transpose w	12-2
跟上面剛好反過來	12-2
那它也是一個正方形啊	12-2
所以我也可以做eigen value 跟eigen vector	12-2
那麼因此呢	12-2
那同樣的情形你也可以看這裡面的第第i 個跟第j 個	12-2
的這個element 的意思是什麼呢	12-2
這邊的第i 個跟第j 個element	12-2
相當於是這邊的第i 個row 跟這邊的第j 個column	12-2
去它跟它相乘它跟它相乘去相加	12-2
那這個第j 個	12-2
那這個的第i 個row 是什麼	12-2
就是這邊的第i 個column 嘛	12-2
這其實就是這個嘛	12-2
所以其實是在這兩個column 在做內積	12-2
所以呢我們說它的i j element of 這個是什麼	12-2
其實就是第i 個跟第j 個column 這個w 的第i 個跟第j 個column 在做內積	12-2
其實就是這兩個document 之間相似的程度	12-2
對不對	12-2
所以呢就是說我現在這兩個document 之間有多像	12-2
就是這個	12-2
所以同樣的	12-2
這項剛才的這裡每這個matrix 裡的每個element 是在描述兩兩word 之間有多像	12-2
它們兩兩word 之間的關係是在這裡	12-2
那這邊是在描述兩兩document 之間的關係是什麼	12-2
那你如果兩兩關係document 之間的關係有了的話	12-2
那你現在就是得到這個matrix ok	12-2
所以呢這個這個matrix 裡面它的i j element 意義跟上面這個是完全對稱的	12-2
它的是對word 我這個是對document	12-2
好如果有了這個的話我下一步也一樣我這個也一樣可以拆開來做	12-2
三個eigen value	12-2
那麼於是呢我的第一個呢就是	12-2
那它的所有的把它的所有的eigen vector 排起來	12-2
我這邊寫做e one prime e two prime e 三等等	12-2
一直到e n prime	12-2
這就是它的n 個eigen vector	12-2
所以呢我這個e這個e i 呢就是嗯應該是有寫在哪裡	12-2
這個e i prime 就是它的orthonormal 的eigen vector 喔	12-2
我這邊講的剛才這邊的e i 是orthonormal 的eigen vector	12-2
也就是說我這個eigen vector 求好之後都把它normalize	12-2
變成單位長	12-2
變成normal 過的normalize 過的都是這個normalized 的eigen vector	12-2
而且呢你可以證明所有的eigen vector 是互相orthogonal 的	12-2
所以它們是orthonormal 的eigen vector	12-2
那我這裡也是一樣e i prime 也都是orthonormal 的eigen vector	12-2
那所謂orthonormal 的意思呢	12-2
就是它們這個它跟它的transpose 它transpose 跟它相乘會變成identity 嘛	12-2
對不對	12-2
這個就是我們剛才說的它們都是orthonormal	12-2
所以它跟它如果直接去做它的transpose 跟它做的話呢	12-2
就變成這個跟這個去做內積	12-2
都是只有它跟它自己做內積別的都是零	12-2
所以它們的這兩個相乘變成identity喔	12-2
這個式子的意思是這樣子嘛喔	12-2
就是這個東西跟這個東西相乘的話	12-2
u 的transpose 就是這個再乘上這個的話	12-2
你如果這個東西乘上這個的話	12-2
其實就是這裡的每一個跟這裡的每一個去做內積	12-2
那它就變成是identity	12-2
這就是它的orthonormal 的意思啊	12-2
那我這邊也是一樣	12-2
好那這樣之後呢這是我的第一個matrix	12-2
然後第二個matrix 呢是所有的eigen value	12-2
也是一樣s one 的平方s two 的平方等等等等別的都是零	12-2
還有第三個就是e one e two e one prime e two prime 等等	12-2
喔一樣的	12-2
這邊已經黑板不夠大了所以我們就不多畫	12-2
不過你可以曉得就是跟上面一樣的意思	12-2
這是e one prime e two prime 等等的一個一個row	12-2
等等一直到e n prime	12-2
當我得到這樣之後	12-2
這就是我的這些個vveigen vec eigen vector 跟它的eigen value	12-2
不過這這裡有一點很有趣的地方是這些個eigen value 是一樣的	12-2
所以我都寫成s one 的平方s two 的的平方跟這邊是一樣的	12-2
不像這裡的話我寫e one 這裡寫e one prime	12-2
表示是不同的vector	12-2
e one prime 跟e one 是不同的vector	12-2
這邊是一樣的	12-2
嗯一樣的但是呢它們的dimension 不同	12-2
因為這邊是r	12-2
這邊是這個m 乘m 嘛	12-2
這是m 個	12-2
這個呢是n 乘n 是n 個	12-2
那怎麼回事呢	12-2
應該是說在m 跟n 裡面的那個minimum 的值之內的	12-2
它們是一樣的	12-2
那麼超過的話呢就都是零喔	12-2
換句話說像我這邊所畫的這個大n 大於大m	12-2
所以你可以想像呢	12-2
在前面的這m 個而言	12-2
的這個就是這一個	12-2
後面這些就都是零了	12-2
這些eigen value 都是零了喔	12-2
那為什麼會這樣這邊都是matrix 數學我這邊不在這裡講這些數學	12-2
但是你可以想像是因為這個是w w transpose	12-2
這是w transpose 這是同樣的東西嘛	12-2
這兩個其實是同樣的東西只是都是同樣的那個w 所產生出來的東西	12-2
所以結果它們的只是說我都拿一個transpose 去乘乘在前面跟乘在後面的不同而已	12-2
所以呢它們大小因此變得兩個不一樣	12-2
但是它裡面真正的eigen value 的數目是相同的	12-2
那什麼數目什麼相同法呢	12-2
就是看誰比較小的那個是相同的	12-2
超過了就是零ok	12-2
所以對i 大於m 跟n 的minimum 的那個地方的話它都是零了	12-2
就是這個意思	12-2
那除了這個之外我上面一樣按照大小數目來排列	12-2
按照它的數值大小按照eigen value 的大小來排列	12-2
那麼因此呢其實這個第一個就是它的第一個	12-2
第二個就是它的第二個	12-2
一直到第m 個就是第m 個	12-2
後面就都是零了	12-2
那麼於是你就可以想到其實這個e one prime 的這個eigen vector	12-2
跟這個e one 其實是有關係的	12-2
因為它們都相對於同一個e one s one prime 的eigen value 啊	12-2
等等這是我們底下要說到的	12-2
那麼於是呢我就得到一個這樣子的關係	12-2
那麼那麼我現在中間這塊呢就是我這邊所謂的s one 平方	12-2
中間這個就是我的s one 平方	12-2
就是指這個matrix	12-2
那中間這個呢我這邊叫做s two 的平方	12-2
就是指這個matrix	12-2
好當我做到這步之後底下這些事情也是一樣	12-2
我一樣的可以發現我只要取r 個	12-2
就夠了	12-2
我再取這裡面的r 個個最大的eigen value	12-2
譬如說r 是八百個	12-2
我只要取這r 個就夠了	12-2
那麼因此我這邊也就取r 個eigen vector	12-2
這邊我也取r 個	12-2
那這三個相乘這三個小的相乘幾乎就跟這個一樣了	12-2
那麼這就是我這邊所講的我這個叫做v 跟v transpose	12-2
這是大寫的v 的有一個bar 的	12-2
那這上面這個叫做v 的bar 的transpose	12-2
那麼我當我寫了這個v 的bar 的時候是指全部的	12-2
就是v 的bar 是指這整個的方的matrix	12-2
整個的方的v 的bar 的transpose	12-2
那當我把這個bar 拿掉的時候呢	12-2
我就只抽了裡面的八百個ok	12-2
當我把這個bar 拿掉之後只抽裡面八百個所以這個剩下的呢	12-2
就是我的v 的剩下八百個	12-2
就是這邊的n 乘上r	12-2
因為我這邊現在只有只有這個八百個了	12-2
那同樣的呢這個是我的	12-2
這個這個就會變成我的v 的transpose	12-2
也是只有r 乘上n	12-2
我也變成只有八百個	12-2
就是這邊的v 的n 乘上r 跟v 的transpose r 乘上n	12-2
那中間這個變成r 乘上r	12-2
所以這個s two 就變成變成這個是s two 的的r 乘上r	12-2
就是這個s two 的r 乘上r	12-2
就變成只有這個變成變成只有這個只有r 個了	12-2
而這個r 乘r 跟這個是完全一樣所以two 可以就根本可以不要寫了	12-2
那這個跟這個是一樣的所以呢就就變成只有這r 個eigen value 了	12-2
那這就是我們在這邊所講的這兩件事情	12-2
我們的這個dimensionality reduction	12-2
那這個意思也是一樣的	12-2
就是說我的這個w t w 的transpose 乘上w 的這件事情	12-2
我等於是把它拆成很多個component matrix	12-2
就是e i prime 乘上e i 的transpose	12-2
這裡的每一個eigen vector 跟它的transpose 去相乘	12-2
就是一個matrix	12-2
那它的weight 就是它的eigen value	12-2
所以你可以寫成這麼多個component matrix 去相乘加起來就是它	12-2
你如果這樣子寫的話	12-2
那麼後面這些eigen value 都很小	12-2
我都可以丟掉	12-2
於是我就變成這樣子	12-2
那當我變成這樣之後呢這回我可以做什麼事情	12-2
這回可以做的事情是我們下一頁所說的	12-2
那這就是在matrix 的代數裡面很重要的一件事情叫做singular value decomposition	12-3
我們真正得到的是這個式子	12-3
這個式子是什麼呢是	12-3
你如果回頭看我們這邊的式子的話	12-3
意思是我取左邊的一半	12-3
我上面這個關係我取取左邊的一半	12-3
就是這個u 跟這個s one 這個s 的兩個	12-3
我本來是我取這邊的u 跟這邊的s	12-3
取左邊的這個	12-3
我左邊取這個u 跟s	12-3
我右邊取這個s 跟這個v transpose	12-3
ok 我右邊取這個這邊的v transpose	12-3
左邊取它的u	12-3
那中間這個我也只取一個	12-3
這邊都是平方哦	12-3
這邊本來是平方我現在只不要平方我只取一個	12-3
我現在不要平方我只取一個	12-3
那就是我下一頁這邊的這個情形	12-3
你如果仔細看的話	12-3
左邊取的就是u m 乘r	12-3
u 的m 乘r 就是這一個	12-3
我左邊取的這個u 的m 乘r	12-3
也就是這邊的這個u 的m 乘r	12-3
右邊取的這個是v 的transpose	12-3
就是這個v 的transpose r 乘n	12-3
就是這個這個那也就是這個r 乘n	12-3
那中間這個呢我只取一個	12-3
剛才這邊我都要平方	12-3
這個是相當於s one 的平方嘛	12-3
這個是s two 的平方嘛都是有個平方的嘛	12-3
那我現在不要平方了我都只取一個	12-3
所以本來這邊寫s one 平方s two 平方我現在都不要平方了	12-3
所以呢就是s one s two 沒有平方了	12-3
那這就是s 的r 乘上r	12-3
我就這三個相乘其實就approximately 就是原來那一個	12-3
ok 我們再看一次	12-3
這個意思是說	12-3
我剛才的話呢是要我必須要把它跟它的transpose 相乘	12-3
我才有辦法做這個eigen value 的eigen vector 的的的分解	12-3
但是這個其實都是兩倍的嘛	12-3
因為這兩次方的意思因為它它它跟它兩個它跟它自己相乘嘛	12-3
對於是一個平方的東西嘛	12-3
那這個也是一樣我也是transpose 相乘所以其實也是平方的意思我可以這樣做嘛	12-3
那因此我現在如果只做一個我不要平方的話呢	12-3
就是這邊只取左邊	12-3
這邊只取右邊	12-3
中間只取一個	12-3
不是平方	12-3
那就得到我們底下這張圖所說的	12-3
這邊只取左邊的	12-3
所以就是取這個	12-3
對不對	12-3
所以你可以想像我現在是左邊嗯不對左邊是取這個	12-3
中間取這個	12-3
右邊取這個	12-3
那中間的這個你也可以算成是這個也沒關係因為這個跟這個是一樣的	12-3
對不對	12-3
所以呢你如果看左邊這兩個的話	12-3
看左邊這兩個相當於是它乘上它	12-3
你如果看右邊這兩個的話相當於是它乘上它	12-3
不過中間這個就是中間這個	12-3
它都是一次方只算一次了	12-3
我剛才是兩次現在都沒有了那個平方都沒有了	12-3
喔這裡平方都沒有了剩下一個s one 跟s r	12-3
那就變成這樣子	12-3
那這三個相乘你可以證明它其實就跟剛才這個等這個approximation 是一樣的	12-3
你這三個相乘的話	12-3
是跟原來那個w 很像的	12-3
不是exactly 一樣	12-3
是把後面這些丟掉了	12-3
是我把這堆東西	12-3
後面的這堆東西都丟掉了	12-3
或者把這些東西都丟掉了	12-3
我把這些個不重要的eigen vector 所代表的那些dimension 都丟掉了	12-3
我剩下一個比較簡單的了	12-3
那麼這樣子做的把一個長方形的matrix 拆成這三塊	12-3
是相當於很像原來的正方形的matrix 拆成這三塊	12-3
這非常像的	12-3
只是因為它是長方形所以必須拆成這樣的拆法	12-3
那這樣的拆法是所謂的s v d	12-3
就是singular value decomposition	12-3
那這個時候你所得到的這些東西叫做singular value	12-3
那麼跟eigen value 有一點不像啊對不對	12-3
它是原來這個東西eigen value 的square root	12-3
那叫做singular value	12-3
不過我還是一樣按照大小順序來排列	12-3
那這些東西呢左邊這個u 呢叫做我的左邊的singular matrix	12-3
右邊這個v 的transpose 叫做我的右邊的singular matrix ok	12-3
那這樣的話我現在就把它展開就變成這樣子了	12-3
當我變成這樣之後這回我們底下就會看到有很多豐富的意義就出來了	12-3
那麼其實為什麼這樣子可以得到它裡面的concept	12-3
這底下我們就可以看得到嗯	12-3
那嗯我們先停在這裡休息十分鐘	12-3
ok 我們下週期中考啊	12-3
下週我出國我不在	12-3
所以我們除了考試之外不做別的事	12-3
因此呢我們考一百二十分鐘就是後面的一百二十分鐘從十點十分到十二點十分	12-3
那麼只考那一百二十分鐘	12-3
那麼前面不上課啊	12-3
ok 我們現在回過頭來說我們這邊講的這些東西到底在幹嘛	12-3
那麼我們剛才說這個上週我們講的這個這個eigen voice 的觀念是	12-3
是說我有一個四百八十萬dimension 的一個很高維的空間	12-3
然後呢我想辦法把它reduce 到一個五十dimension 的的空間	12-3
那麼本來這個四百八十萬維的空間裡面每一點代表一個speaker	12-3
其實不要那麼多維啦	12-3
我只要找到五十維的一個subspace	12-3
其實每一點投影到上面來	12-3
它就在這個五十維裡面已經代表每一個speaker 之間的關係了	12-3
這樣的意思	12-3
那我現在做這件事情其實是很像的	12-3
所不同的是現在我本來每一個word 要多少十萬維	12-3
我們說這十萬個dimension	12-3
也就是說每一個word 在這十萬篇文章裡面	12-3
分別出現的次數為基礎所得到的這個vector	12-3
代表那個word 的特性	12-3
所以呢就word 而言呢我似乎應該我也可以想成是這樣子	12-3
我有一個十萬維的空間	12-3
這是十萬的dimension	12-3
那這裡面的每一個點是十萬維的vector	12-3
就是一個詞	12-3
譬如說這個是某一個w i	12-3
這是某一個詞w j	12-3
那麼基本上是這樣子	12-3
可是真的需要十萬維嗎不見得	12-3
我現在想辦法做一件事情	12-3
就是把這個十萬維reduce 到變成八百維	12-3
那怎麼reduce 跟這個情形是一樣的我在這裡面找一個八百維的subspace	12-3
那也有點像是這樣子ok	12-3
我這邊也有也找到一個一個八百維的subspace	12-3
我這裡的每一點都投影到這八百維的上面來	12-3
然後呢這些八百維的點其實就跟原來十萬維的是一樣的	12-3
那我變成一個新的space	12-3
這邊只有八百維	12-3
我在這邊的每一個原來的這裡的每一個點	12-3
都投到這邊來變成這個	12-3
我其實這八百維其實就是原來的這十萬維	12-3
於是我的每一個詞都只要八百維	12-3
每一個word 都只要八百維就可以描述了	12-3
這是一個什麼樣的關係呢你如果如果回過頭來看的話	12-3
就是這個關係	12-3
不過我現在需要把它擦掉了嗯	12-3
恩我把這個擦掉了	12-3
我要畫就是右右黑板那個那個powerpoint 上面的這個圖啦	12-3
就是這個圖	12-3
這個圖你現在是變成這樣子的一個然後乘上一個這個	12-3
再乘上這樣子一個對不對	12-3
這三個乘起來相當於原來的一個w	12-3
所以這個是我們所謂的u	12-3
這是我們這邊所謂的s	12-3
這邊是所謂的v transpose	12-3
這三個東西相乘會變成原來這個w	12-3
這個是我們在這邊所說的這個singular value decomposition 的意思	12-3
這個u 乘上這個s	12-3
乘上這個v transpose	12-3
會得到我原來這個這個w喔	12-3
不是真的exactly 不過就是一個approximation	12-3
會得到這個	12-3
這到底是什麼意思呢	12-3
我們現在可以來看	12-3
譬如說我們如果前兩個相乘你想會是什麼	12-3
這個跟這個相乘其實仍然	12-3
這兩個相乘仍然得到一個	12-3
這個乘這個對不對	12-3
這個是m 乘上r	12-3
這個是r 乘上r	12-3
所以乘完之後還是m 乘上r	12-3
這個是u 乘上s 的一個matrix	12-3
那這個u 乘上s 的matrix 呢	12-3
它的每一個dimension 是什麼東西	12-3
譬如說這裡的每一個row	12-3
這個是第i 個row 的話	12-3
這裡面只有第八百個dimension	12-3
這裡的每一個row你想想看其實是什麼呢	12-3
跟這裡的每一個column 去做內積	12-3
就得到這邊的那一個row	12-3
我們我們再講一次	12-3
就是這個這裡的譬如說這裡我第i 個row	12-3
我第i 個row 這邊有八百個element	12-3
它是不是這八百個乘上這八百個相加得到第一個值	12-3
這八百個乘上這八百個第二個相加得到第二個值	12-3
對不對它乘上第三個八百個相加得到第三個值等等	12-3
那我這邊有多少個有十萬個	12-3
結果我就得到這十萬個	12-3
對不對所以呢我原來的這十萬個這十萬個呢	12-3
其實就是第i 個word	12-3
在原來這個dimen 在原來的這個十萬維空間裡面的那一個點就是那一個點	12-3
那一個點的那十萬個就是這邊的這十萬個嘛	12-3
那這十萬的每一個你可以看成是這些東西乘上這些東西加起來對不對	12-3
這些東西乘上這些東西加起來等等等等	12-3
你如果這樣看的話我們是不是可以回過頭來想	12-3
這個東西是什麼	12-3
這個東西其實是我們這邊所畫的	12-3
e one prime e two prime 等等	12-3
所以呢這裡面其實它的每一個row	12-3
這個row 是e one prime	12-3
這個row 是e two prime 等等等等	12-3
也就是這個的這個的eigen vector	12-3
那如果我把這個值叫做第一個值叫做a one	12-3
第二個值叫做a two 等等的話	12-3
那這個vect 那這個vector 其實是不是summation 的a i e i prime	12-3
summation over i	12-3
這個就是這個	12-3
再講一次喔	12-3
你仔細你要想一想才能夠想清楚這件事喔	12-3
就是說你可以想成是這個eigen vector e one prime 乘上這個a one	12-3
再加上這個eigen vector e two prime 乘上這個a two	12-3
這個e 三prime 乘上這個a 三	12-3
到這個e 八百乘上這個a 八百	12-3
這些東西是乘加起來其實就是這個嘛	12-3
這中間關係其實就是這樣子	12-3
因此才會這裡的這裡的八百個跟這八百個相乘得到第一個	12-3
就是它們的所有的第一個dimension	12-3
這些個eigen vector 第一個dimension 分別乘上這個	12-3
得到這個的的第一個dimension	12-3
這些eigen vector 第二個dimension 分別乘上這個	12-3
加起來得到這個第二個dimension 等等	12-3
那這樣我有十萬個嘛我十萬個就這樣得到了	12-3
這樣的關係其實就是e one 的eigen vector	12-3
e one prime 乘上這個a one	12-3
e two 的vector prime r 一個vector two prime 乘上a two 等等	12-3
加起來就是a i 的e i prime	12-3
就是這個vector	12-3
那這個的意思其實是不是相當於這八百個e i prime	12-3
就是這八百維	12-3
所以現在這個個其實就是第一維就是e one prime	12-3
第二維就是e two prime	12-3
第三維是e 三prime 等等	12-3
我沒有辦法畫更多但其實這八百維是在這裡	12-3
而這裡的每一點呢	12-3
就是那些個a one 到a 八百	12-3
也就是說	12-3
我現在的a one 到a 八百是什麼	12-3
是這裡的一個row	12-3
就是這裡的這兩個相乘的那個row	12-3
換句話說	12-3
我原來的這個大matrix 裡面	12-3
這十萬維的vector	12-3
代表一個word	12-3
我現在在這個matrix 裡面我只要八百維就代表了	12-3
這八百維跟這十萬維是同樣的事情	12-3
只是	12-3
這八百維你如果分別	12-3
每一維其實代表這八百個每一個分別代表這裡的一個eigen vector 而已	12-3
ok	12-3
所以呢我等於是	12-3
這八百個我等於是在這本來這十萬維的空間裡面	12-3
我找到八百個十萬維的vector	12-3
它們是orthogonal 的	12-3
構成一個八百維的子空間	12-3
那這裡面八百維的子空間裡面每一點其實就是這裡面每一點對應過來的	12-3
而對應過來之後呢	12-3
我只要	12-3
這裡的每一個component 其實就是這些東西的weight 加起來	12-3
就得到原來這一點了	12-3
那這個意思跟我們原來這個意思是完全一樣的	12-3
我等於把十萬維的空間所描述的所有的	12-3
word 所有的詞之間的關係reduce 到一個八百維的空間裡面的關係	12-3
其實它們關係是完全一樣的	12-3
有一點點error 但是error 很小就是了	12-3
ok	12-3
那麼這個情形就是我們在在這邊講的	12-3
那這這八百個其實就是那十萬維裡面的八百個vector	12-3
我在那十萬維裡面找到這八百個	12-3
每一個都是十萬維哦	12-3
這都是十萬維的	12-3
但是我現在有八百個這十萬維的的東西構成一個八百維的空間	12-3
那你裡面的每一個原來這裡的每一點就變成這八百維的那個點	12-3
因此它們這些裡面的每一個值	12-3
就代表他們的weight	12-3
就是這種weight 加起來	12-3
就是這個東西	12-3
那這個話就是寫在這邊的這個	12-3
你現在所謂的u 這個這是這個u 乘上這個s	12-3
得到u 跟s 相乘得到的這個matrix	12-3
u 跟s 相乘這個matrix 裡面的每一個row	12-3
我叫做u i 的	12-3
加一個下下面加一個bar	12-3
一個underline 的u i	12-3
就是	12-3
所以這個東西就是我那邊講的那個u i	12-3
有一個underline 的u i	12-3
那你的這個u i 呢	12-3
就是原來這個u i 乘上s 嘛	12-3
對不對	12-3
就是這邊的這個u i	12-3
這個這個八百維的也就是這邊的這個的	12-3
的第i 個row	12-3
這個u i 乘上那個s	12-3
就得到這個u i 的bar	12-3
的underline 的u i	12-3
這個東西呢就是一個vector	12-3
它的dimension 由原來的十萬reduce 到現在只有八百	12-3
而這個東西呢	12-3
你可以看成什麼呢	12-3
那這八百個row vector	12-3
那其實呢	12-3
就是這八百個row vector of v t v transpose	12-3
就是這邊這八百個row vector	12-3
其實也就是原來v 的八百個column	12-3
這八百個row vector 就是這邊v 的八百個column 是一樣的	12-3
那這些東西呢也就是原來這個w transpose w 的	12-3
eigen vector 裡面最重要的八百個	12-3
構成一個orthonormal basis	12-3
那那個basis 就是我們所謂的latent semantic space	12-3
就是那邊的那個八百維的那個空間	12-3
就是一個潛藏的語意的空間	12-3
它的dimension 呢就是八百	12-3
而在這裡面呢	12-3
我每一個u i 的underline 的u i 呢	12-3
就是一個詞	12-3
也就是說我在原來的要十萬維的那一個詞	12-3
現在變成只有八百維就夠了	12-3
那也就是說我把這個十萬維的裡面	12-3
我找到一個八百維的把它通通都投到這八百維上面去了	12-3
那或者說你可以想像我現在有一個八百維的	12-3
那這裡的每一個維都代表了一堆東西了	12-3
那麼這些究竟是什麼東西呢	12-3
其實你是可以這樣子想的	12-3
這個我寫在下兩頁哦	12-3
哦我想想看在這裡	12-3
在這一頁裡面的這句話	12-3
每一個component 在這個reduce 的word vector 裡面	12-3
就是association of the word with the corresponding concept	12-3
什麼意思	12-3
我沒我我現在說這裡的每一個eigen vector 代表某一個concept	12-3
你可能很難想像為什麼這是一個concept 的one	12-3
這個是concept two	12-3
這個每一個代表一個concept	12-3
代到底到底是什麼concept 呢	12-3
你可以看	12-3
這個eigen vector 就是這個eigen vector	12-3
那如果在這個eigen vector 裡面你可以發現譬如說它這個詞	12-3
佔了零點零三	12-3
這個詞佔了零點四五	12-3
這個詞佔了零點三一	12-3
等等這些加起來之後	12-3
就是這個vector	12-3
那你去看欸	12-3
這邊講的這個零點四五這邊是恐怖攻擊	12-3
這個零點三一這個是賓拉登	12-3
這個零點零三這個是布希	12-3
這個零點零一的那個是白宮	12-3
你發現它們加起來其實它代表就是	12-3
九二一嗯就是九一一恐怖攻擊的那件事情	12-3
它們	12-3
ok 你了解我的意思喔就是說你現在如果去看這個eigen vector 它裡面的值因為它是一個normalized unit vector 嘛	12-3
它某一些一堆相對於一堆詞都是零	12-3
因為它跟那些詞沒有關係	12-3
它跟某些詞有關係	12-3
如果這個是恐怖攻擊它有零點四五	12-3
這個是賓拉登它有零點三一	12-3
這個是白宮它有零點零三	12-3
那個是紐約那個是零點二什麼東西你加起來發現你會發現	12-3
它其實就是在講那個event	12-3
那那個就是一個concept	12-3
那同理呢你如果看一一三的話它有另外一堆是零	12-3
另外一堆有數字	12-3
你可能發現這個是講台北市政府	12-3
那個是講國民黨	12-3
譬如說這個有零點二一	12-3
這個講台北市政府	12-3
這個零點零五	12-3
嗯這個零點一三	12-3
這是講國民黨	12-3
那這個是零點一二	12-3
這個是講這個什麼選舉	12-3
你把它加起來發現這個就是是馬英九	12-3
那如果這樣的話那這個其實就是e two 這個就是馬英九	12-3
啊等等	12-3
所以你可以這樣想的話呢	12-3
那麼其實你只要看它裡面相對於每一個word 的weight	12-3
你可以看得出來它其實代表	12-3
某一種concept	12-3
那如果是這樣的話那現在這個word 是什麼	12-3
這個word 是這個concept 有零點幾這個concept 有零點幾這個concept 有零點幾加起來的結果	12-3
就是這個	12-3
於是你會發現說	12-3
現在如果這個這個word	12-3
你會發現ok 它	12-3
馬英九有零點三五	12-3
陳水扁有零點二一	12-3
這什麼什麼都有結果發現它什麼	12-3
它其實是對美外交	12-3
啊等等	12-3
ok	12-3
那所以這個word 其實就是對美外交	12-3
那它就是跟這些個concept 都有關係	12-3
那麼它們的關係就是這個weight	12-3
就是這些a i	12-3
ok	12-3
那就是這邊講的這個意思	12-3
就是每一個component 在這個reduce 的vector	12-3
u u j 的這個嗯underline 的u j 就是這個東西裡面的這裡的每一個a i	12-3
這些東西其實就是它的	12-3
這個word	12-3
這個word w i 我們講譬如說這個是這個對美外交	12-3
那這個word 的話呢它裡面的	12-3
譬如說這個跟這個陳水扁有多少的關係	12-3
跟馬英九有多少關係	12-3
跟外交部有多少關係跟等等等等等就剛好那它就是它們的	12-3
相對於每一個corresponding concept 的association	12-3
那就是這些東西	12-3
ok	12-3
那這樣的的話呢嗯	12-3
恩我們等於是把把這個word 的所代表的這些concept	12-3
我們把它抽象的具現出來	12-3
發現這些東西是e one e two e 三到八百個	12-3
當然剛才舉的例子是比較具體一點或者應該講誇張一點	12-3
你真的是不見得看得出來啦喔	12-3
你如果真的去做這樣的分析之後	12-3
你要看每一個e i 代表什麼concept	12-3
不是那麼容易看	12-3
但是有一點這樣的味道	12-3
那基本上呢你可以發現	12-3
大概每一個分別因為它們都是orthogonal 的	12-3
基本上是都是不同的東西	12-3
代表不同的concept	12-3
那如果是這樣的話呢我上面這句話的意思	12-3
就是說我現在的word 在這個空間裡面所代表的是	12-3
只要越接近表示它們的是比較相關的	12-3
因此呢你譬如說這個如果是	12-3
一個陳水扁一個總統府	12-3
那這就會比較就會在附近	12-3
你如果是賓拉拉登跟這個阿富汗也會比較接近	12-3
那所以呢在這個地方	12-3
你在這邊也會比較接近不過這地方難看難看因為有十萬維你搞不清楚	12-3
但是這邊呢會清楚很多	12-3
相關的詞彙它們相關的詞它們的concept 接近的話	12-3
它在這些dimension 上是會接近的	12-3
因此呢如果有有類似的similar 的這個語意上的關係的話	12-3
它們在這個空間裡面的relation 應該是比較接近的	12-3
而且這裡有一個很大的好處是現在	12-3
它們只要有appear in similar type of document 就可以了	12-3
不需要exactly in 不需要in exactly same document	12-3
什麼意思呢	12-3
就是說你如果在原來的這個w i 裡面	12-3
原來在這個w i 裡面	12-3
你如果要賓拉登跟這個阿富汗有關的話	12-3
它們必須出現在相同的文章裡面	12-3
這篇文章裡面賓拉登也出現好幾次阿富汗也出現好幾次	12-3
在這篇文章裡面也是	12-3
必須它們在同樣一篇文章裡面一再的同樣的出現	12-3
我才知道這兩個賓拉登跟阿富汗是有關的	12-3
可是我現在不是不再有十萬個了	12-3
我現在只有八百個	12-3
那每一個八百個是concept 不是那篇文章啊	12-3
我等於把很多篇文章reduce 成為一個concept	12-3
所以呢這個時候我不再需要它們要出現在同樣的文章裡面	12-3
我只要讓它們出現在類似的文章就可以了	12-3
similar type document	12-3
那很可能這些文章merge 成為一個	12-3
這些文章merge 成為一個	12-3
嗯你可以這樣想	12-3
所以我才會變成由十萬變成八百嘛	12-3
所以它們只會出現在類似的文章裡面	12-3
它們就會發現在那八百維的空間裡面是很接近的	12-3
雖然在這邊沒有出現在同一篇文章裡面也可以	12-3
那這是它一個非常大的優點	12-3
也就是本來我們講的某一個concept	12-3
不是一定要哪個詞才是那個concept	12-3
當你每次講對美外交的時候不是一定要有陳水扁也不是一定要有外交部	12-3
你凡是講到	12-3
布希你凡是講到什麼	12-3
凡是講到這個什麼其實它們都是講同一件事	12-3
那麼因此呢你不見得要有同一個詞	12-3
也不見得要在在同一篇文章裡面	12-3
那麼因此呢它現在就是這樣子	12-3
那麼我現在就是就是這個這個它們不見得一定要出現在同一篇文章裡面	12-3
只要在相類似的的文章的type 裡面我就可以把它們抓到發現它們是很接近的	12-3
那這個大致就是我們剛才講的這個這一頁的意思	12-3
因此呢我現在要代表一個word	12-3
原來是要這邊十萬維	12-3
現在變成就是這個u u i 的bar	12-3
就是這兩個相乘的	12-3
這個東西的u i 的bar	12-3
我只要八百維就夠了	12-3
那麼這個意思有一點好像是說我把原來的這個十萬維	12-3
這十萬維呢是等於是discrete	12-3
由n 個document 所define 出來的	12-3
我有十萬篇文章對不對	12-3
我有十萬篇文章所define 出來的十萬維	12-3
我現在reduce 到變成只有八百維	12-3
而這好像是變成continuous 的了	12-3
因為這裡的這八百維是這八百維	12-3
這八百維是這它有零點幾這個零點幾這些加起來	12-3
所以好像是一個好像continuous 的東西	12-3
把一些東西reduce 成為一維	12-3
把一些東西reduce 成為一維這樣子來看	12-3
那麼因此呢那這八百維的的每一個每一個dimension 是什麼	12-3
就是我這邊的eigen vector	12-3
那也就是也就是我現在我的vector 就在這個上面表現喔	12-3
那這個觀念如果你可以想像的話	12-3
那下一頁跟這個是完全平行的	12-4
我下一頁我圖都完全一樣就是上面圖是完全co 過來的	12-4
就上面這個圖也就是下面這個圖	12-4
是完全co 過來的	12-4
那我現在是反過來來看後面這一半	12-4
我剛才是講這兩個相乘是這個	12-4
那我現在可以看另外一件事情是這兩個相乘	12-4
這兩個相乘了還是這個	12-4
還是一個這樣的東西	12-4
這個是r 乘以r	12-4
這個是r 乘以m	12-4
所以這兩個相乘之後還是r 乘以m	12-4
就是這個東西	12-4
那同樣的情形我現在在這上面看的	12-4
這裡的每一個譬如說第j 個document 這個	12-4
這也是八百維	12-4
其實相對的是這邊的八百維	12-4
這是e one e two 這邊的八百維	12-4
e m	12-4
那如果你剛才那個觀念可以了解的話現在是完全一樣對稱過來	12-4
我剛才是這兩個變成一個r 乘m 乘r 的	12-4
這寫錯了	12-4
這個是r 乘n 啦	12-4
我現在是這兩個相乘的仍然是一個r 乘n 這寫錯了	12-4
變成一個仍然是一個r 乘n 的	12-4
但是現在這裡的每一個column 其實只有八百維	12-4
就代表剛才這裡的column	12-4
是有二萬維	12-4
我剛才的每一個column	12-4
這邊的兩萬維	12-4
也就是這邊的兩萬個嘛	12-4
也就是說在這裡的每一維它這邊有多少個詞對不對	12-4
它每一個維代表它跟這個詞之間的關係	12-4
每一個維代表它跟一個詞之間的關係	12-4
這樣我也總共有兩萬個詞	12-4
所以有兩萬維	12-4
來代表這個document	12-4
那我現在這個document 也不再需要兩萬維了	12-4
我只需要八百維了	12-4
為什麼只需要八百維這意思是一樣的	12-4
你可以想一想這八百這八百個	12-4
變成我的b i	12-4
如果這個叫做b one	12-4
這個叫做b two 的話	12-4
這個b one 其實乘上這個e i	12-4
summation 的b i e i	12-4
這個b one 乘上這個e one	12-4
b two 乘上這個e two	12-4
全部乘起來加起來就是這一個	12-4
這個詳細的數學我想你自己去去figure out	12-4
這個這個觀念跟剛剛是相同只是反過來	12-4
所以呢這邊的這個兩萬維的這個vector	12-4
其實你可以看成是這八百維的e i	12-4
分別weighted by 這個八百個b i 加起來的結果	12-4
所以b i e i 就是這個	12-4
就像剛才的a i e i prime 就像這個是一樣的	12-4
那麼因此呢我這邊的也有相同的情形	12-4
就是我本來的一個document	12-4
是要兩萬維的	12-4
這是另外一個space	12-4
我的documents	12-4
我這邊有一個兩萬維的空間	12-4
這裡面的每一個點代表一篇document	12-4
代表一篇文章	12-4
那我也是一樣這兩萬維裡面我重新找一個八百維的子空間	12-4
之後我把這些所有的點都投影到這個這個八百維上面來	12-4
因此我其實reduce 成為一個八百維的一個新的只有八百維的subspace	12-4
這裡面的每一個dimension	12-4
其實就是e one e two e 三	12-4
當然我只能畫三個但是你只能想像有八百個	12-4
這個e one e two e 三就是剛才那些個那些個eigen vector	12-4
那於是我把這些點呢重新點到這上面來只有八百維了	12-4
那之後呢那就是我們這邊所講的這件事情	12-4
那這裡面的所有的話都跟剛才是平行的	12-4
所以這個意思你只要剛才的了解的話其實就是反過來就一樣了	12-4
所以這個第一句話的意思	12-4
跟剛才的那第一句話是一樣的	12-4
我的每一個剛才說我每一個row 代表一個word	12-4
本來說要八百個我現在只要本來要十萬個	12-4
現在只要八百個了	12-4
就是這個u i bar	12-4
u i 的underline 就是u i 乘上s	12-4
s 就是這個u i 乘上這個s	12-4
這個乘上s 就是這個i	12-4
那我現在也一樣就是這個v j 的bar 是什麼	12-4
就是這一條	12-4
真正講應該是右邊這個啦	12-4
就是我們真正講它是一個column	12-4
它是一個column	12-4
所以呢我現在的這一個	12-4
這一個就是我現在講的這一條	12-4
就是我這邊說的v j 的bar transpose	12-4
v j bar 的transpose	12-4
嗯這樣子寫是因為我完全follow 剛才reference 裡面第一篇的reference 它的寫法	12-4
它的寫法裡面凡是寫一個v j 這種東西都是一個row	12-4
所以現在這個明明是一個column	12-4
是一個column 所以它就必必須要寫一個transpose	12-4
所以我這邊講它其實是一個column	12-4
但是你要把它寫成就把它寫成transpose 所以就是就是這個東西	12-4
就是這個v j 的bar 這個東西v j 的bar 的transpose 的這個東西	12-4
那它是什麼是s 乘上v j 的transpose	12-4
它是什麼	12-4
它就是這個s 乘上這裡的這一個	12-4
這裡的這一個第j 個	12-4
這個s 乘上這個就是這個嘛就這樣看對不對	12-4
這個乘上這個就是這個	12-4
所以也就是s 乘上v j 的t	12-4
就是這裡的就變成加一個bar	12-4
所以加一個bar 是表示這兩個相乘的結果	12-4
剛才再加一個這兩個相乘的結果是一樣的	12-4
那這個是一個這是寫成一個column	12-4
你如果要寫成row 的話就變成這樣子了	12-4
這個是因為那篇我第一個reference 它的寫法	12-4
把這個寫成這個也是可以的那只是把它transpose 一下	12-4
所以這個的transpose 變成這個嘛	12-4
那這個transpose 變成這個嘛	12-4
把它transpose 過來的話就變成這個是寫成row 的寫法	12-4
所以我說這個是row 的寫法	12-4
這是column 的寫法就是了	12-4
但這個意思是完全一樣的	12-4
就是我現在本來是有這裡本來是有兩萬個dimension	12-4
代表兩萬個詞的這個vector row vector	12-4
現在變成只有八百百維了就是這個意思	12-4
那麼因此呢我這本來是有兩萬個word 所代表的這兩萬個dimension	12-4
也就reduce 到只有八百維了	12-4
那這每一個維是什麼呢	12-4
每一個維是這邊的eigen vector	12-4
這個eigen vector 是什麼其實你也可以看	12-4
譬如說你如果看這個eigen vector 的話呢	12-4
這個呢其實相當於這個	12-4
那你看這上面是什麼譬如說e one 是什麼是這個	12-4
那你看它是它是這個這個嗯這個就是零點三五這零點多少這零點多少	12-4
那你看它哪些是零	12-4
它代表哪些詞你把那些詞的觀念加起來其實就是代表它的那個concept 等等	12-4
那我們剛才的的詞的時候的那個dimension 呢	12-4
你應該我可能講錯一點	12-4
你這個零點零三這個其實是這個對應的是這邊的document	12-4
所以你應該是說譬如說這個零點四五這個零點三一	12-4
這篇document 是在講九二一恐怖攻擊	12-4
這個零點三一是在講阿富汗跟達凱組織什麼什麼	12-4
你把它加起來的話它們就是所以這裡的每一個是代表這邊的一篇文章	12-4
ok 所以呢我們剛才在詞的那裡的那八百維	12-4
在詞的那裡的那八百維每一維	12-4
是這個e i prime	12-4
那這個e i prime 這裡面每一個component	12-4
其實是代表哪一篇文章	12-4
你可以去看這些文章加起來是什麼意思	12-4
就是那個concept	12-4
那我現在的這裡的這個八百維就底下這個八百維呢	12-4
變成是我是這個	12-4
那每一維是這裡的每一個component	12-4
它代表的是它這邊的的詞	12-4
你把這些詞的意思加起來就是那個觀念	12-4
就是那個concept ok	12-4
好那麼如果是這樣的話	12-4
那我們也同樣的嗯就是這邊所講的	12-4
就是說我現在就是把這個嗯這些eigen vector	12-4
就做成變成normal 這是orthonormal basis	12-4
來展開一個space	12-4
那它dimension 就是底下這個space	12-4
那在這裡面也同樣情形我們在下一頁的這一句話	12-4
是在講這一件事情	12-4
就是說呢每一個component 在這裡面呢	12-4
就是代表association with 這個concept	12-4
也就是我這就是就是在講這件事	12-4
這裡的每一個component b one b two 分別代表這邊的e one e two 的weight	12-4
就是這個意思喔	12-4
所以呢每一個component 在這個裡面的	12-4
就是這個component 的就是這裡每一個e i 的association 或者它的weight	12-4
所以就代表這個document 其實是哪些concept	12-4
那因此呢這樣你大概可以想像就是說	12-4
我每一篇我每一篇document	12-4
其實是裡面有好些個concept	12-4
而每一個word 也有好些好些個concept	12-4
那麼因此呢	12-4
這裡的每一個document 是是一堆這種東西	12-4
有好些個concept 加起來	12-4
這裡的每一個word 也是這些東西	12-4
也是好好些個concept 加起來喔這樣子	12-4
好那如果是這樣子的話	12-4
那麼我們上面這句話也有類跟剛才是這句話是對應到剛才這句話了	12-4
那麼上面這句話是對應到上面這句話	12-4
就是說我現在在這個空間裡面	12-4
如果兩篇文章講的東西很像	12-4
它們都在講紐約恐怖攻擊的話	12-4
那兩篇文章在這邊就會很接近	12-4
就會在這裡	12-4
那因為它們相對的那些dimension 會在一起	12-4
所以它們就會在這裡接近	12-4
而它們接近的時候呢	12-4
它們不需要有exactly same words	12-4
就是說我原來在這個我原來在這個裡面	12-4
你如果要說這篇文章跟這篇文章像的話	12-4
除非它們的word 都一樣	12-4
同樣的word 它們都多	12-4
同樣的word 它們都少	12-4
這樣才我才知道它們兩個是相像的	12-4
我現在不用了	12-4
我現在是在那個八百維的空間裡面	12-4
不見得需要有完全相同的word 才知道它們像	12-4
我只要知道那八百維裡面那空間上距離近就像喔	12-4
所以呢我現在就是說它們不需要再有完全相同的word	12-4
只要它們有有這個類似的type of word	12-4
同一類的word 在一起的話	12-4
就表示它們是同一個了	12-4
所以呢它們只要是concept 接近的話就會在那裡就會接近	12-4
好那如果這個個concept 你可以了解這個這個想法你可以了解的話	12-4
那我們這兩頁的底下這句話你大概就可以想像了	12-4
就是說我把原來的所謂的association structure between words and words and document	12-4
就是這個matrix	12-4
這個matrix 所描述的就是word 跟document 之間所有的relation	12-4
我現在呢可以可以完全保留	12-4
幾乎是完全保留	12-4
而且我可以把 noise information 拿掉	12-4
但是我的 dimension reduce 到變成一個只有 r 個 dimension	12-4
也就是說原來的這一堆詞	12-4
這些 word 之間的關係	12-4
reduce 到這邊來	12-4
那麼所有關係都在	12-4
原來這些 document 之間的關係	12-4
reduce 到這邊來原來都還在	12-4
而且我還可以把 noise information 拿掉	12-4
什麼叫 noise information 拿掉	12-4
你可以想像其實就是在這個過程之中我把這些東西拿掉	12-4
我把這些東西拿掉我把這些東西拿掉	12-4
就這些東西其實是很可能造成 noise 的部分	12-4
我保留了最乾淨的部分這是 eigen value 的意思	12-4
eigen vector 的意思	12-4
你記得 eigen vector 就是在做它不是隨便找一個八百維	12-4
它是找最有意義的八百維	12-4
你記得我們在講 p c a 的時候講過一個 case	12-4
就是你如果這些點在這裡的話	12-4
你最後找的是這個這個軸	12-4
因為在這個軸裡面它的分得最開	12-4
而不會你不會找到這一軸	12-4
因為這軸它們比較緊	12-4
你不會找這軸這軸它們比較緊	12-4
一樣的意思	12-4
那我其實是找一個真正能夠描述它的 distribution	12-4
最清楚那些 dimension	12-4
我現在八百維都是這樣來的	12-4
都是找到它最能夠描述它 distribution 的那些 dimension	12-4
所以呢我是把一堆我丟掉的是那些	12-4
所以我丟掉的這些東西基本上是比較 noisy 的	12-4
這些東西或者這些東西是比較 noisy	12-4
我可以得到比較乾淨的	12-4
那麼於是我可以得到一個但是我的這個 association structure 幾乎是維持不變	12-4
然後呢還有還有一個有趣的地方是重要的地方是這樣	12-4
我由這個十萬維成八百維的時候	12-4
這是一個詞的空間	12-4
我這兩萬維變成這八百這是一個文件的空間	12-4
可是你發現這兩個空間其實它的每一個 dimension 是對應的	12-4
這個 e one 就是 e one prime	12-4
每一個 dimension 就是對應的	12-4
為什麼	12-4
因為它們都是對應到同一個 eigen value ok	12-4
也就是說這裡的 e one 的那個 concept	12-4
其實對應到這個 eigen value	12-4
這個 e one prime 的那個 concept	12-4
也是這個 e one prime 其實是同一個	12-4
所以雖然說它們是兩個你真正講起來是兩個 space	12-4
這個八百維這個也八百維	12-4
可是其實如果 e one 是描述恐怖攻擊的話	12-4
這個 e one prime 也是的	12-4
是同一個 concept	12-4
如果 e 三是描述對美外交的話 e 三 prime 也是的	12-4
它們其實是同一件事	12-4
因此呢你也可以想像成我真的需要畫兩個嗎不用	12-4
我可以畫成一個行不行可以	12-4
所以呢在有的人的說法裡面它就說	12-4
其實我只有一個就夠了	12-4
這個 dimension 是 e one	12-4
同是也是 e one prime	12-4
因為是同一個 concept	12-4
這個 dimension 是 e two	12-4
同時也是 e two prime	12-4
是同一個 concept	12-4
你如果這樣看的話呢	12-4
我的詞也在這裡	12-4
我的文件也在這裡	12-4
它們通通都你可以看的是都在同一起	12-4
這樣也可以	12-4
但是你可以想的其實是這兩個啦	12-4
其實是這兩個啦只不過它們的每一個 dimension 其實是指同一件事	12-4
那畢竟這裡的每一個是一個兩萬維的代表 document 的一個 concept	12-4
這裡每一個是十萬維的代表 word 的一個 concept	12-4
但是其實是講同一件事	12-4
所以你也可以是想的是同一個	12-4
你如果想成是 concept 的話呢	12-4
那就是同一個了	12-4
這是另外一個說法	12-4
這是 concept one	12-4
這是 concept two	12-4
如果這樣的話這就是同一個了	12-4
ok 那這個就是我們這邊在做這個 s v d 的意思	12-4
那有了這個之後	12-4
我們再下來的如果這點都能夠想像的話再下來就比較容易了	12-4
譬如說我們可以拿來做什麼事	12-4
這個剛才講的這套就是所謂的剛才講的這套就是所謂的這個嗯 latent semantic analysis	12-4
所謂的 lsa	12-4
那這個東西可以拿來做很多用途	12-5
不僅僅是做 language model	12-5
那麼我們可以看一下有些什麼用途	12-5
譬如說把詞分群	12-5
詞分群幹嘛	12-5
就是把相類似的代表相同意思詞 group 在一起	12-5
這個可以做很多用途	12-5
譬如說一個例子是做 language model 做 class space language model	12-5
你記得我們之前講過的	12-5
我可以把相類似的詞變成一個 class	12-5
然後拿那個來做 class 的 n gram	12-5
而不要做每一個 word 的 n gram	12-5
那這個詞分群這一個方法就是用這個	12-5
那同樣呢因為你知道你哪些詞在一起嘛	12-5
你現在每一個詞都是這八百維裡面的一個點了	12-5
那你可以在這邊做 v q 或者做什麼東西都可以了嗎對不對	12-5
你就可以把詞分群了嘛	12-5
那同樣你可以做 information retrieval	12-5
就是說你在做搜尋的時候	12-5
譬如說你現在打進去 google	12-5
我要找九一一恐怖攻擊	12-5
那它只會找文章裡面有九一一恐怖攻擊的事情	12-5
如果文章裡面沒有講九一一但是它講了賓拉登呢	12-5
會不會呢不見得會	12-5
可是在這裡我就會啊	12-5
因為我知道九一一跟賓拉登是在一起的它們很近嘛	12-5
所以呢因此我在做 information retrieval	12-5
就是你在搜尋的時候我可以不完全根據字	12-5
而是你可以看它們的在這裡面的距離近不近	12-5
近的就是就是有關的嘛	12-5
那這邊就是剛才已經講過了	12-5
因為我的 word 在這裡	12-5
你只要看兩點接近就代表它們的意思是相關的	12-5
那一個可能的做法	12-5
這是一個例子不是這是 example 的 similarity 你怎麼量這個詞像不像	12-5
這個例子就是內積嘛	12-5
對不對你現在有兩個 vector 在這裡	12-5
你算它們像不像你就算內積嘛	12-5
那這個內積就是就是這個嘛	12-5
就是內積除以它們的長度	12-5
這就是 cosine theta	12-5
就代表它們的相似度嘛對不對	12-5
這是一個例子你可以用這個	12-5
那就用這些 vector 代進來做	12-5
那這裡的每一個 vector 其實就是 u j 的 bar	12-5
就是 u s	12-5
這裡的每一個我每一個 vector 怎麼算就是畏用這個來算	12-5
就是由 underline 的這個 vector	12-5
也就是 u 乘上 s	12-5
u i 乘上 s	12-5
所以你就是 u i 乘上 s 去算	12-5
就變成這樣子	12-5
那這就是詞的相似度	12-5
你可以詞可以做 clustering	12-5
同樣呢我文章也可以做 clustering	12-5
文章 clustering 是什麼呢	12-5
你可以想像有一個什麼用途譬如說 class language model	12-5
我現在從我現在上網抓十萬篇文章	12-5
我可以用這十萬篇文章 train 一個 language model	12-5
不過這個 train 出來 n gram 是很亂的	12-5
因為裡面什麼東西都有	12-5
它從從這個李安的電影到賓拉登到什麼全部都在一起	12-5
混在一起變成一個大的 language model 之後	12-5
中間的各種 n gram relation 是被攪混的	12-5
那因此呢我可以把它分群嘛	12-5
我如果上網找了一百萬篇文章之後把它分成一百群	12-5
每一群是比較接近的文章	12-5
再把那一群去 train language model	12-5
那就會比較好嘛對不對	12-5
那這就是所謂的 class 的 language model 的意思	12-5
我可以把找到的一百萬篇文章先分成一百群	12-5
分別去做 language model	12-5
同樣我也因此可以做 language model 的 adaptation 喔	12-5
這個我們底下還會再說到不過你可以想到就是	12-6
那我現在如果知道這一百個	12-6
我分成一百群做成一百個 language model 的時候	12-6
它們每一百每一個有各自的 topic	12-6
有各自的 concept	12-6
因此 depends on 你現在講什麼我就用哪一個	12-6
那這個就是 language model 的 adaptation	12-6
同樣我也可以做 information retrieval	12-6
我今天我要找什麼東西的時候	12-6
我如果把這一百萬篇文章先分成一百群	12-6
你輸入一個一個 instruction	12-6
我先看你要找找的是哪一群	12-6
再從那一群裡去找比較好找嘛等等	12-6
所以你文章分群的話也可以很有用的	12-6
那這些都是我們的 linguist processing	12-6
那文章分群在這邊的這邊的這個剛才已經講過了	12-6
那我的做法我就是找文章之間的關係	12-6
這也就是剛才一跟這個完全一樣的	12-6
求 cosine theta 就是做內積	12-6
然後除以它的長度	12-6
只不過我現在每一個 v 呢是這個 v	12-6
這個 v 是什麼v 是這個 s 乘上這個 v j	12-6
所以呢就變成這樣子	12-6
嗯那這就是文章分群	12-6
那同樣呢我現在要做搜尋	12-6
我要做 information retrieval 的話	12-6
你說我要找什麼東西的時候	12-6
我們原來像 google 現在的基本上現在是一個 lexical matching	12-6
就是在 match 它的 word	12-6
你說我要找九一一恐怖攻擊它就去找所有的九一一恐怖攻擊的	12-6
但是如果那篇文章裡面沒有講九一一恐怖攻擊	12-6
可是裡面講了賓拉登講了阿富汗	12-6
它不知道	12-6
那你如果有這個的話你就知道了因為你現在是 concept matching	12-6
所以你可以去 match 它的 concept	12-6
所以呢你的相關的文件你是可以找得到的	12-6
只要有相類似的 concept	12-6
它們不需要有 exactly same words	12-6
也可以找得到	12-6
那這個做法怎麼做	12-6
簡單的解釋就是說你把你的 query 當成是一個新的 document	12-6
放進去	12-6
然後去量它的 similarity 這是一個簡單的例子	12-6
什麼意思就是說你的我現在要找我要找賓拉登恐怖攻擊什麼你就把你那個文章	12-6
把你這句這個 query 你把它輸入這個 query	12-6
當成是一個 document	12-6
你就去看那個 document 跟其它所有的 document 那個相似度	12-6
就放在那個空間那個那個八百維的空間裡面去算	12-6
它跟誰像那就是誰嘛	12-6
喔等等這就是做這個 information retrieval	12-6
好那這時候有一個重要的問題我們要解決的要說明的就是所謂的 fold in	12-6
什麼叫做 fold in	12-6
fold in 是說我們這這整套是假設	12-6
我上網找到十萬篇文章	12-6
我有一個辭典是兩萬詞我都已經做好了之後	12-6
我整個這樣做做完了	12-6
做好之後我得到這一套	12-6
但是呢我網路上不斷有新的文章出來啊	12-6
今天每天多了一萬篇新的文章我這新的文章怎麼辦	12-6
我每次新的文章來我要重新 train 一次	12-6
那這樣不是累死了嗎	12-6
所以最好是我 train 好之後新文章可以放進來	12-6
我把新的文章塞進這個 model 把它放進來就好了	12-6
我 as long as 新的文章所描述的這些 concept 沒有新的	12-6
如果有新的 concept 的話那當然你得要重新把它弄進來了	12-6
但是如果說你描述的新的文章沒有新的 concept	12-6
只是原來的話	12-6
你就把它塞進來就好了	12-6
怎麼塞就是這邊所謂的 fill fold in	12-6
就是把新的文章只要沒有新的 concept 的話	12-6
就是 assuming 它們沒有新的 concept 所以這些 u 跟 s 都不變	12-6
u 就是左邊的這個 matrix	12-6
左邊這個 singular matrix	12-6
v 就是右邊singular 這個左右都不變	12-6
那麼中間這個都不變	12-6
如果這樣子的話呢我新的怎麼做	12-6
我只要讓這個再多增加一維	12-6
這個叫做 d d p	12-6
我剛才如果 d one 到 d n 是十萬篇文章的話	12-6
我新的文章進來就是 d 十萬零一篇	12-6
那個 p 就是在後面我多加一維就好了	12-6
如果這個多加一維的話呢	12-6
那這邊其實發現就是這邊多加一維	12-6
所以這就是 v p	12-6
這邊多加一維意思就是這邊多加一維那這個不動	12-6
這個不動那就是我們這邊所講的這件事情	12-6
ok 就是說你現在如果一篇新的文章進來	12-6
outside outside of 原來那個 training corpus t 的話	12-6
我們只要假設如果它的整個 language pattern	12-6
跟它的 concept 都是不變的話	12-6
那麼我其實只要把新的放到這來	12-6
因為新的那些文章我馬上可以數一數它裡它裡面有哪些 word	12-6
我已經把它裡面哪些 word 數一數我就得到一個新的 d p 在這裡	12-6
所以 d p p 大於 n 就是指在原來的十萬之外的	12-6
譬如說十萬零一	12-6
我就排在這裡	12-6
排進來之後呢那我整個可以不要動	12-6
其實原來的這個 relation 不動只是這邊再多一行	12-6
這一行是 v p 就是了	12-6
就是這邊的這一行跟這邊這行是一樣	12-6
就好像這邊這一行跟這邊這一行是一樣的嘛	12-6
所以你這邊多一行就是了	12-6
或者說這邊也多一行	12-6
這邊也多一行	12-6
那麼所以這個乘以這個多的這一行就是這一行	12-6
那這一行就是原來的這一行就這樣子	12-6
你把這行塞這邊多一行這邊多一行就好了	12-6
那麼它們的關係是什麼呢就是這個式子	12-6
這個式子其實就是原來這個式子是一樣的	12-6
原來的這個式子是說你這個整個的 w	12-6
是 u 乘上 s 乘以 v transpose	12-6
這 u 乘上 s 乘上 v transpose	12-6
那這個意思你可以看成是	12-6
u 乘上 s 乘上這裡面的每一個 column	12-6
就是這裡的那個 column	12-6
就是這個意思 ok	12-6
所以你原來的 w 是它乘以它乘上它	12-6
跟它裡面的這個 column	12-6
相當於它乘以它乘上這個 column	12-6
是一樣的意思	12-6
那我現在只要把這個 column 換作這個 column	12-6
這個 column 換成這個 column 就一樣了	12-6
所以就變成這個式子	12-6
所以這個 d p 呢就是 u 乘上 s 乘上 v p 的 transpose	12-6
就是這個東西	12-6
它這邊有一個這個 t 還是一樣的	12-6
因為在在那篇 paper 的 notation 裡面	12-6
這個 v 是一個 row	12-6
你要現在是一個 column 所以就是加一個 t 就是了	12-6
所以這個 column 其實就是這個 u 乘上 s 乘上這個	12-6
就是這個式子	12-6
那如果是這樣的話呢這個很容易求啊	12-6
那 u 跟 s 你已經知道啦	12-6
所以這個你也就知道啦	12-6
那就底下這個式子的意思	12-6
ok 這個 u 跟 s 是我原來 train 好的	12-6
新的文章進來這個就數一數有幾個 word 就出來了	12-6
所以這個也很容易求	12-6
如果這也很容易求的話呢那這兩個都已知那這個很容易求嘛	12-6
就是你拿這個式子求一求變成這個式子	12-6
底下這個式子跟上面這個式子是完全一樣的	12-6
只是說你現在如果 u 是已知 s 是已知	12-6
 d p 是很容易求的也是已知	12-6
那你 v p 怎麼求就是這樣求	12-6
只是在解這 equation 而已	12-6
那這個也沒什麼特別其實就是你要的這個v 這個是 v p 的	12-6
這個是這個式子是 v p 的 underline	12-6
這個 v p 的 underline 呢就是這個 v p 乘上 s	12-6
就是這個 v p 就是這個乘上跟 s 相乘	12-6
那其實是什麼呢	12-6
你可以看其實就是這個式子重新解一解而已	12-6
我只要把它的這個 transpose 這個 transpose	12-6
然後呢這個這個 transpose 就變成 v p s	12-6
那因此這個 transpose 搬過來就變成 d t b 五	12-6
就變成這樣的意思	12-6
因此我就得到一個這個或者說這個	12-6
就是一個 r dimension representation of the new document	12-6
所以一個新的文章把它塞進來這樣塞就可以了	12-6
直接帶到原來的關係塞進來就可以 work	12-6
那這個意思其實也可以說就是把你的 d p 投影到這個跟 u 去相乘	12-6
就是投影到新的空間	12-6
你可以想像我這個 d p 這個 vector d p 是什麼	12-6
是在這裡的一個新的點	12-6
它在一個新的點進來	12-6
不過這點我其實可以對應到這邊來	12-6
看這點對應到哪裡來就是這點	12-6
就把這點對應過來就是了	12-6
那怎麼對應	12-6
其實就是在每一個上面做投影對不對	12-6
我拿它對每一個 e i 去做投影	12-6
那這件事情其實就是這件事情	12-6
你看這個 u 其實就是 u 其實就是一堆 e i 嘛	12-6
所以你去跟它去做所以你這個 d p 去跟這個去做去相乘	12-6
其實就是它分別去做內積	12-6
它跟它一個個分別去做內積它就分別在做投影	12-6
所以投出來就是那裡面那一點	12-6
所以就得到	12-6
所以它跟這八百個去做內積就是這八百個投影	12-6
就得到八百個維就是這裡的八百個或這裡的八百個	12-6
就這樣的意思	12-6
所以這樣我就 fold in 進去了	12-6
好有了以上這些我們現在底下就可以來說我怎麼來做 language model	12-7
那你可以想像我們這邊所講的 lsa 其實不限於做 language model	12-7
其實是一個很 general 的 concept	12-7
可以拿來分析很多東西可以做很多用途的	12-7
那我們可以拿來做 language model adaptation	12-7
當你說到哪裡的時候你知道你在講哪一個 concept	12-7
所以後面的 word 你可以猜它應該是哪一個 word	12-7
我可以用那個 concept 跟那個 topic	12-7
其中考的	12-7
你如果看考古題就知道我上面都是期中考的形式都跟考古題一樣啊都是那個樣子的	12-7
那我上面是是有寫說這個open everything	12-7
也就是說是open everything 你不需要去記任何東西	12-7
啊	12-7
那你可以帶任何你要帶的reference 來看	12-7
但是呢我有另外一個條件就是說所有文字要用中文寫	12-7
啊	12-7
那意思是說你如果open everything 的話	12-7
如果你可以直接從課本上抄一段	12-7
你根本不用懂它的意思你也可以抄一段來因為它是跟課本一樣	12-7
所以一定要給你滿分	12-7
所以這個不合理嘛喔	12-7
所以你必須要看懂了	12-7
用中文寫才有分數	12-7
ok	12-7
所以我想這個這個規定就是比照去年的考古題是一樣的就是那樣子	12-7
那我們的範圍考到八點零我們講過了喔	12-7
好我們現在來看最後這一段	12-7
就是這個怎麼樣拿來做n gram	12-7
做n gram 的基本精神是說ok 如果我這邊都已經知道的情形的話	12-7
我現在怎麼做n gram	12-7
今天如果一個人在那邊講話	12-7
他已經講了一堆話了	12-7
他已經講我已經辨識出出來他講了這一堆話講完之後講到這裡的時候辨識下一個字	12-7
這是我這邊講的w q	12-7
喔	12-7
這個地方有一點confuse 就是在這裡的時候這個w q 變成q 是sequence number sequence index	12-7
因為之前的我們的w i i 都是辭典裡面的第i 個word	12-7
i 是在辭典裡面的第i 個word	12-7
但是現在的w q 不是辭典裡面第q 個word	12-7
而是你現在在講話的時候講到第q 個word	12-7
ok	12-7
你講到第q 個word 的時候呢	12-7
你前面所講的	12-7
從這個d 的q 減一就是你前面的recognize history	12-7
那麼這邊呢叫做d 的q 減一	12-7
那你馬上猜得到是怎麼回事了	12-7
我就已經辨識到q 減一為止的這個呢當成一篇新的document	12-7
當成一篇新的document 塞進來嘛	12-7
我把那個當成一篇新的document 塞進來之後我就可以得到它的八百維的vector	12-7
那我就知道那個八百維的vector 在這裡放到這裡來	12-7
那你就把這個d 的q 減一放到這邊來	12-7
那你就知道我現在在講的是這個topic	12-7
所以從這那當然在這附近的word 會發生的機率就高了嘛	12-7
那這個其實就是對應到這邊來我們說這兩個其實是同一個空間啊	12-7
所以我就把它對應到這邊來那你就看這邊的空這哪些word 跟它相關嘛	12-7
那你應該會講跟它相關的word	12-7
喔	12-7
基本觀念就是這樣子	12-7
所以呢我的language model 可以這樣子做	12-7
那麼我就是把w q 是第q 個word	12-7
然然後呢q 減一就是到目前為止你所recognize 的history	12-7
那麼因此呢	12-7
這件事情其實就是把那個d q 減一把它塞進來	12-7
把那個d q 減一這個這個equation 就是剛才這個equation	12-7
就是剛才這個equation 就是fold in 這個equation	12-7
所以這個意思只是把只是把這個d 的q 減一呢我重新放到這個vector 裡面來或者這個vector 裡面來	12-7
變成那八百維裡面的那一點	12-7
有了那個之後呢其實我現在就可以算	12-7
given given 它你前面講的這些話	12-7
所以下面的那個出現那個word 它機率是多少	12-7
那你可以想像就會就會在它的附近	12-7
所以你就可以根據這個來算	12-7
其中這個word 你可以算它的u q	12-7
這個d 你可以算它的v	12-7
那麼嗯我這裡稍微有一點點錯	12-7
這裡是v 的q 減一的underline	12-7
應該是representation of v 的d 的q 減一by v 的q 減一	12-7
這邊應該都是q 減一啦是指這個都是指前面這q 減一的到q 減一為止所辨識出來的結果嘛	12-7
所以那個是d 的q 減一	12-7
然後representation by v 的q 減一這兩個都應該是減一的啦	12-7
啊	12-7
應該都有減一	12-7
然後呢我現在就是算	12-7
那麼因此呢我就可以我的這個history 就可以用v 的q 減一來代表	12-7
而我現在下一個word 的w q 也可以用u 的q 來代來代表	12-7
那這個u 的q 也就是把我要哪一個word 我現在放在這個裡面放在這個dimension 裡面	12-7
那就是我u 的q 嘛	12-7
那這個是我d 的q 減一嘛	12-7
那這兩個其實是同一件事	12-7
因為是同一個我們講你其實可以放同一個八百維裡面來看	12-7
那你就可以在裡面看它們之間的關係	12-7
喔就這麼回事兒	12-7
那這個詳細的說法呢是可以說它是可以跟n gram 整合在一起的	12-7
為什麼說跟n gram 整合呢	12-7
因為這邊講的這個跟n gram 是互補的	12-7
為什麼	12-7
n gram 給我們local relationship	12-7
而這裡的l s a 給我的是semantic concept	12-7
這兩個是互補的	12-7
怎麼講	12-7
我的n gram 是告訴我說這個word 後面要接這個word	12-7
我如果遻這是畫隻字詞的話	12-7
或者這兩個word 後面要接下一個word	12-7
這三字詞的話	12-7
對不對	12-7
所以呢它n gram 是告訴我說locally 這些relation	12-7
但是沒有告訴我它沒有去分析這邊到底講什麼話	12-7
那我現在這種l s a 講的是你在說什麼concept	12-7
是一個global 的relation	12-7
但是它沒有講local relation	12-7
那exactly 這個後面會不會接它呢這其實是n gram 告訴我們的	12-7
這個不見得告訴我啊	12-7
ok	12-7
所以l s a 告訴我的是它到底是在講什麼topic 所以它應該出現什麼word	12-7
那n gram 是告訴我前面有什麼東西是後面接什麼東西所以它們兩個是互補的	12-7
一個是local 的一個是concept	12-7
那你也可以說	12-7
這個l s a 呢比較強調是主要的content word 的關係	12-7
而n gram 是把所有的的word 包括function word 一起算	12-7
什麼叫function word 就是我們之前講的譬如說的	12-7
譬如說他的爸爸	12-7
那這個的後面都算進去	12-7
那我在算它們的n gram 的時候	12-7
我把所謂的function word 就是這東西	12-7
或者非常	12-7
就是說沒有真正的意思	12-7
不真正的代表content 的內容的東西	12-7
像非常啦這個什麼這種什麼的啦這都是屬於function word	12-7
那麼如果講爸爸	12-7
這就是content word	12-7
或者說李安或者說馬英九這種都是所謂的content word	12-7
所以呢我們這邊講的這個semantics 比較強調的是主key content word	12-7
是content word 裡面的key word 那麼應該會出現哪些東西	12-7
但是它漏掉了這些function word	12-7
因為function word 我們一開始就把它拿掉了嘛	12-7
那樣才能夠得到一個比較清楚的東西	12-7
所以function word 我一開始就拿掉了所以呢function word 其實也是你在算n gram 當然是跟function word 一起算的	12-7
喔	12-7
所以它它們是是互補的	12-7
喔	12-7
因此呢	12-7
你可以怎麼做	12-7
我現在的這個這個這個出現下一個word 的機率given 前面的history	12-7
這個大h 呢是所有前面的history	12-7
我可以包含兩件事情	12-7
一個是n gram	12-7
一個是前面的這個	12-7
這個d 的q 減一就是我已經辨識到這裡為止	12-7
到底辨識多少東西我這個可以放進這邊來看	12-7
它是什麼	12-7
對這個是d 的q 減一	12-7
但是另外呢我還可以這個是是什麼	12-7
h 的q 減一的n 呢就是到q 減一為止的n gram	12-7
我可以到word q 減一為止的n gram 前面的n 個	12-7
我就可以算n gram	12-7
那這兩個可以一起用	12-7
所以呢我可以given n gram 再given 這個之後來算這個機率	12-7
那這個詳細怎麼算呢我這邊就不再說下去了	12-7
那在paper 裡面有你如果有興趣的話去看那個paper	12-7
同樣呢它們個推導我不認為一定是最理想的所以那個一定還有改進的空間	12-7
啊	12-7
那麼所以那邊我就不再講下去但是你基本上可以想像是這麼回事	12-7
所以我就可以這樣子做	12-7
那底下要講的一件事情就是說	12-7
你如果這樣子講的話變成我每每辨識一個word	12-7
就有一個新的d q 減一出來對不對我每辨識一個word 這邊就q 加一	12-7
我辨識一個word 出來我就得到一個新的document	12-7
對不對我這就放進來了每辨識一個word 我就變成一個新的	12-7
那這樣的話我每次都要把它重新再算再放進去不是很麻煩嗎	12-7
其實不麻煩	12-7
這只是要iterative 加進來就好	12-7
我每一次只加一個word	12-7
每一次只加一個word 是什麼	12-7
只加那一個word 就好	12-7
因為你的那個d 的q 減一就變成一個一個row 一個column 在這裡	12-7
對不對	12-7
你如果在d 的q 減一的那個你前面辨識到d 的q 減一的時候變成一個column 在這裡	12-7
再多辨識一個word 的話	12-7
那個word 看它哪裡就後面加個一	12-7
如果下一個d q	12-7
如果下一個word q	12-7
它其實是這裡的某一個word	12-7
你辨識出來	12-7
你就這邊這邊加一嘛	12-7
你就這邊加一就好了嘛	12-7
這個加一之後就整個全部都照做就行了	12-7
那這個加一的動作其實很簡單就是這邊加一	12-7
假設你下一個word	12-7
下一個word 進來是第i 個word 的話你就在第i 個component 那邊加一個一	12-7
別的地方方加零就可以了	12-7
那這邊為什麼會有這些呢	12-7
那只是因為我們一開始算這個的時候	12-7
有這個normalization 你記得	12-7
我們一開始的時候	12-7
我有除以n j	12-7
還要再乘以e 減epsilon i	12-7
我有這個normalization	12-7
所以你不是光是如果光是這個就加一就好啦	12-7
可是因為有這兩個所以你要稍微做一點調整	12-7
就行了	12-7
那就是這邊講的	12-7
你譬如說原來的那個東西不是直接加一	12-7
要除以q	12-7
要乘以q 減一再除以q	12-7
q 就是全部的長	12-7
就是全部的字數嘛	12-7
所以q 這個q 其實就是我們剛才那個n j 的意思就是這邊的n j	12-7
就是n j 就是全部的字數詞數	12-7
就是n j	12-7
那我現在呢都除了n 減一的都除了q 減一的	12-7
都除了n 減一的都除了q 減一的所以我現在要乘上q 減一除以q 因為現在變成q 了	12-7
所以就變成要乘上這個factor	12-7
同樣呢我那個e 也不是直接加一	12-7
因為要除以q	12-7
還要乘以e 減epsilon	12-7
所以這就是一個normalization 的過程	12-7
所以你如果前面d 的q 減一做完之後	12-7
下一個d q 很簡單就這麼做	12-7
我iterate iteratively 加上去就可以了	12-7
那如果d 是這樣做的話	12-7
那我這邊要算那個vector 也很容易算照算	12-7
所以這就是那個式子	12-7
我照算那個就是要算那個vector 或者這個vector 也很容易算就照樣代進去	12-7
就是這樣斷就可以了	12-7
那這樣我們就得到這個嗯用l s a 來做language model 的方法	12-7
那這個就等於說是我用l s a 的分析我來判斷說我現在應該在說什麼話	12-7
然後你就	12-7
那麼因此呢我的language model 應該跟著它走	12-7
那這邊最後這句話就是講這件事情就是說呢	12-7
你一開始的時候	12-7
你的v q 會在那個移動	12-7
會在那個空間裡面移動	12-7
最後會settle down somewhere	12-7
也就是說假設	12-7
譬如說這個布希發表這個一個演講	12-7
就是說這個什麼什麼國情諮聞	12-7
他在講某一段在講譬如說在講經濟政策的時候	12-7
你一開始它的它的頭幾句話會跳來跳去	12-7
因為頭幾句話它不見得exactly 針對這個主題	12-7
所以一開始的時候你可能在這裡會發現它會動一動	12-7
可是講到若干句話以後	12-7
你就會清楚它是在它的主題是那個	12-7
所以它就會settle down 在那個地方	12-7
ok	12-7
所以你開始講的時候它會動一下	12-7
然後呢最後會settle down 哪裡	12-7
之後一直在講那個topic	12-7
等到它這段講完	12-7
下一段它要講外交政策的時候呢	12-7
那又會開始動	12-7
動到另外一個地方去	12-7
然後過一陣之後會settle down 說它講的是中東	12-7
就是中東	12-7
等等	12-7
所以呢基本上是一個這樣的過程	12-7
那這個是l s a	12-7
有了這個l s a 之後呢	12-7
我們現在來說一下就是我剛才前面給的reference	12-7
我剛才講的絕大部分都是based on 這一篇	12-7
這算是寫得最清楚最完整	12-7
所以我想這是一個很好的reference	12-7
那跟這個相關的這是一個嗯special issue	12-7
裡面有很多篇文章	12-7
它講的不見得是這個	12-7
但是它講的都是language model 有關的	12-7
以及linguistic prosody 有關的	12-7
還有dialogue	12-7
因為其實dialogue 裡面dialogue 你要跟這個系統對話你要跟user 對話你很多都是要用這個linguist processing 的觀念	12-7
所以呢它是把dialogue 跟這個放在一起	12-7
有很多篇	12-7
這其實是相當好的一個reference	12-7
我想你現在其實是有些東西是可以看的	12-7
所以這也是一個很好的reference	12-7
不過我們剛才沒什麼說到	12-7
你可以去裡面看很多關於language model 相關的文章	12-7
那第三篇呢是把這個再衍伸	12-7
其實同一個作者	12-7
他從二千年到二千零五年	12-7
嗯這個是二千零五年	12-7
他重寫了一篇	12-7
這個時候他把它已經把它extend	12-7
它把它改名叫做latent semantic mapping	12-7
那現在不再它變成一個mapping 了	12-7
所以不再限於詞跟document	12-7
它可以把它extend 到很多不同的對應關係跟不同的應用	12-7
所以呢這篇就會比這篇多了很多東西	12-7
變成是不同的應用啊等等	12-7
那還有它有很多新的reference 出來	12-7
都在那個裡面喔	12-7
我們舉個例子來講	12-7
它一個應用是e mail 的那個垃圾信件的排除	12-7
就每一個user 我都收到到一大堆垃圾信件	12-7
但是哪些是我的垃圾信件哪些是我要看的	12-7
你其實可以把它歸類	12-7
然後你會要看的一定是哪些topic	12-7
那那些你不要看那些topic 就是你的垃圾	12-7
啊所以根據那個就可以來做垃圾信件分類等等	12-7
也可以用這個再來做其它很多application喔	12-7
這都在這篇裡面有講	12-7
這是三	12-7
然後因為我們這邊講一大堆matrix	12-7
那麼你如果對matrix 不是那麼熟悉的話	12-7
很多課本關於matrix 的	12-7
那我這邊已列其中的一本	12-7
不一定是這一本這本很多啦	12-7
你可以找一本來	12-8
那底下我要說一下第五個	12-8
就是probabilistic	12-8
也就是說這個剛才講的這些都沒有太多機率	12-8
它有統計啦	12-8
其實裡面的每一個這個element 是統計嘛	12-8
是還有entropy 還有什麼	12-8
是有統計	12-8
但基本上它整個不是靠機率來算的	12-8
它是用matrix 來算的	12-8
那後來就有人發現其實這樣不夠好	12-8
更好的應該是整套都用機率	12-8
那就好像h m m 一樣	12-8
整套都用機率有什麼好處	12-8
就是它可以容易學習嘛	12-8
你可以把h m m 那一類的所有的那些個training 的方法都拿來	12-8
然後可以讓它用各種的machine learning 方法來做	12-8
然後這個你新的data 進來可以不斷地學習喔	12-8
很多很多好的方法好的情形在有了機率都可以	12-8
所以後來就有人說我應該把機率來重新formulate 這個問題	12-8
那這個就是所謂的probabilistic latent semantic analysis	12-8
或者說index p l s i	12-8
那這一篇是p l s i 的原始paper	12-8
最早出現的一篇是在a c m 的sig i r 裡面	12-8
一九九九年	12-8
倒不是最好看的一篇	12-8
那不過你可以去找就是說	12-8
嗯在這個之後會有好幾篇	12-8
裡面有寫得非常完整的會比較好看的喔	12-8
那我們來說一下這個是什麼	12-8
這個就是在我剛才的再下一頁	12-8
基本上還是一樣的事情	12-8
我有一堆文件有一堆詞	12-8
不過這裡我用的詞term 不太一樣	12-8
我這個詞現在叫做term	12-8
所以呢這個term t j	12-8
term 就相當於我們原來這邊的word	12-8
我這邊的word 其實它叫做t j 叫做term	12-8
那這邊的呢就是document	12-8
變成d i	12-8
不過我現在變成大寫的就是	12-8
所以符號有點不一樣不過你大概知道我還是這兩個東西	12-8
就是這個詞或者是term	12-8
跟這個文件document 之間的關係	12-8
然後我希望在中間找一堆就是我所謂的topic	12-8
這還是一樣是topic	12-8
所不同的是我現在全部都變成用機率的關係	12-8
什麼機率的關係呢	12-8
你可以看到	12-8
這個就是說我看到一篇document	12-8
它會是在講哪一個topic	12-8
有一個機率	12-8
我假設這邊有八百個topic 的話	12-8
我看到一篇東西	12-8
我可以算說它有多少機率講這篇	12-8
零點三的機率講這個topic	12-8
零點一的機率在講這個topic	12-8
這些topic 它機率是零或者怎樣	12-8
所以我看到一篇文章	12-8
我可以去分析它談每一個topic 的機率	12-8
同樣的呢	12-8
我如果知道它是講某一個topic 的話	12-8
ok	12-8
會在這個topic 裡面會講到這個word	12-8
或者這個term 的機率是多少	12-8
我可以算這個term 的機率	12-8
所以呢在某一個topic 裡面	12-8
某一個term 會用到的機率是什麼	12-8
是這個	12-8
當我有這兩個機率之後	12-8
我現在變成這個式子	12-8
也就是說	12-8
我現在如果在一篇文章裡面	12-8
要看到那個詞的機率	12-8
是一個機率	12-8
不是直接數的了	12-8
我們剛才在這裡的話	12-8
在這一篇文章裡面它出現幾次	12-8
我數一數就知道是幾嘛	12-8
這是一個deterministic value	12-8
對不對	12-8
它文章裡面它出現幾次我數一數就好了	12-8
現在不是了	12-8
現在是一個機率	12-8
在它裡面會出現這個的機率是多少呢	12-8
應該是這樣算的	12-8
就是呢你你如果看到它	12-8
你可以分析它可能是哪一個topic 的機率	12-8
然後在那個topic 裡面它會講到這個term 的機率	12-8
然後你現在這個乘起來之後把所有的topic 加起來	12-8
這才是這個機率	12-8
那你現在要要train 這個東西	12-8
就是要讓這個東西跟真的在這裡所看到的這個數字要像	12-8
那也就是我這個機率	12-8
那這個是真正的frequency count of term in document	12-8
我要這個東西	12-8
那我要把這個東西maximize	12-8
這個就是一個likelihood function 這是一個likelihood function	12-8
在這個document 裡面看到這個term 的like 的likelihood function	12-8
然後我現在要maximize 這個likelihood function	12-8
我用這個來train	12-8
所以我有一大堆文件	12-8
一大堆詞我可以train 這個東西	12-8
那這整個的數學的formulation 跟這個完全不一樣了	12-8
因為它不再用什麼vect matrix 這些都沒有了	12-8
它完全用機率所以數學的formulation 完全不一樣了	12-8
它的觀念是很像的	12-8
這個觀念幾乎是相同的	12-8
啊它完全用數學來做	12-8
那這怎麼train	12-8
又又是用e m	12-8
那e m 我們考完期中考之後我們會講e m	12-8
那你就會清楚它裡面的一大堆數學	12-8
那你如果有興趣的話這也是一個很好的這個我們這邊講的這些都是蠻好的這個寫報告的題材喔	12-8
所以我想這一段應該是到這裡	12-8
ya 我們十二點零講的就是這些東西	12-8
那我想這個	12-8
ok 我們今天就上到這裡好不好	12-8
ok 嗯本來今天我說是要講的是第九點零的e m 喔	12-8
不過我後來決定說我應該再多講一個十五點零的這個robustness	12-8
那原因也是一樣就是啊這是另外一個非常重要的大領域	12-8
那啊有非常豐富的研究主題在裡面	12-8
所以呢我覺得好像應該我們先講這一個	12-8
這樣子讓各位可以早一點可以接觸這些東西	12-8
那啊你可以早一點想可能的研究的題材	12-8
那我們如果把這個十五點零講完	12-8
我們再回去講九點零我想應該是ok	12-8
那如果這樣的話呢我們你可以這個早一點開始多想一點其它的可以做報告的題目	12-8
那當然另外一個原因是因為其實十五點零	12-8
跟我們之前講的九十一點零跟十二點零是啊啊都是屬於這個adaptation 系列的	12-8
那其實這個觀念都是相通的	12-8
所以呢也許我們是可以一路接下來講	12-8
是比較順一點	12-8
那麼因此我想想看我今今天的目標應該是	13-1
不是十十四十四跟十五吧	13-1
我們今天的目標先是	13-1
耶不是十五那是十三	13-1
十三跟十四阿	13-1
十三是retrieval	13-1
十四是今天的目標是把十三跟十四十三是retrieval	13-1
十四是這個understanding organization	13-1
我希望今天是把這兩個講完	13-1
然後下週我們應該還需要完成的包括十就是utterance verification 跟key word spotting	13-1
key phrase spotting 然後呢十六dialogue	13-1
dialogue 十七是dsr 然後十八是conclusion	13-1
所以這個這個現在希望做到這樣	13-1
那我們儘可能在最後的時間把所有的還沒有說說的都說到	13-1
那我們今天天先從十三點零的retrieval 開始	13-1
那麼retrieval 要講的事情是喔你如果記得我們開學第一週給你看的demo 裡面就有這樣的retrieval	13-1
也就是說我們只要輸入入說我要找以色列與阿拉法特	13-1
他自動就會把所有跟以色列與阿拉法特相關的新聞通通抓出來	13-1
那那個case 就是一個以語音為基礎的information retrieval	13-1
那麼所謂的information retrieval	13-1
那麼你知道這這個詞是一個重要的研究領域	13-1
那他的的最明顯的代表就是你今天的google	13-1
所以你上google 的時候你只要輸入你的你要的keyword 你	13-1
他就會給你所有跟這個keyword 相關關的一大堆的網頁	13-1
然後裡面說的所有東西那你都有了	13-1
那那個就是information retrieval 的最具體的代表	13-1
那這個領域事實上是一個重重要的領域	13-1
裡面的學問非常多	13-1
不過以google 他們的工的動作主要的是這個文文字跟文字的比對	13-1
也就是他把網頁上所有的全球網網路上所有的網頁上的文字	13-1
跟你現在輸入的keyword 去做比對	13-1
那當然你要同時跟全球網頁去做比對這有很多技術	13-1
那不過那就是所謂的information retrieval	13-1
那我們現在講的是把語音跟它整合	13-1
因此我現在要做的事情是我我輸入的keyword 可能是語音的	13-1
那我那些網頁可能是指指語音的information	13-1
那麼因此呢這裡面至少有一個是語音的	13-1
那就是我們這邊講的事情那這裡面的第一篇可能是最早而有最完整的描述述的	13-1
這篇文章相當長	13-1
但是他的內容把一些重要的觀念都提到了	13-1
那麼說的是相當完整而清楚的	13-1
那也是最早的一篇最完整的paper	13-1
所以我把它它列在這裡	13-1
那我想雖然早	13-1
了一點不過你今天來看仍然是非常好的的reference	13-1
好所以對這個領域而言我想這個仍然算是非常好的	13-1
所以呢這個是recommend 你可以看的	13-1
那另外呢這個文字的retrieval 本身是是一個重要的學術領域	13-1
那麼有很多的教科書	13-1
這邊舉其中的一本為例	13-1
那不見得只有這一本	13-1
那不過這應該把裡面很多重要的基礎基本的知識都在都有提到	13-1
那此外呢information retrieval 在恩學會而言最重要的activity 應應該是在acm 的special interest group	13-1
所以呢他他們有他們的網站	13-1
他們每年有每年他們的會議	13-1
每年都有非常多的paper	13-1
那那都在這裡面等等	13-1
那那麼其他的reference 待會說到的時候再來說	13-1
ok 那這邊講的大概我們都說過所以大概都知道我們講的事情就是還是一樣去上上網去找要找的東西	13-1
唯一不同的是今天我們講的是今天你上google 這個網頁上基本上上是文字的比對	13-1
網頁雖然也有一些聲音也有一些畫面	13-1
不過基本上不是在比那些而是在比比對文字	13-1
但是呢未來很可能網頁上越來越多的東西可能能是multimedia	13-1
那他們可能並沒有multimedia 他都帶了聲音帶了語音	13-1
但但那些語音可能並沒有人去幫你把它轉寫成為文字	13-1
就直接就是語音在那裡	13-2
於是你就變成要用語音直接用聲音去找到你要找的的東西	13-2
那反過來呢user 這一端也很可能是用手機pda 或者在車上什麼的	13-2
的去找要找的東西	13-2
那這時候user 的query 進進去也可能不是打的keyword	13-2
而是用語音講的	13-2
那因此呢我就變成兩面都可能是語音的	13-2
那就是是我們這邊所講的這個這個用語音來做的這個information retrieval	13-2
那麼這個時候呢那麼基本上我們要用他們原來的這些這些文字比對的所有有的技術	13-2
但是我們得把它轉到語音來	13-2
那麼語音裡面面有一堆語音的問題需要解決	13-2
除此之外呢那當然網頁上還有很很多文字	13-2
如果我我只是用這種東西在在上網的話怎麼辦呢	13-2
那就是這些文字我要要語音合成	13-2
變成聲音	13-2
那再來呢你今天上網是很多互動的	13-2
就說你你輸入keyword 然後他給一堆東西去選	13-2
你在選選你在動是很多互動	13-2
那這個互動呢就用dialogue 用對話等等	13-2
那就我們這邊所說的事情	13-2
那這個圖我們很早就講過了	13-2
那也就是說你今天的的google 是輸入一堆文字	13-2
就網路上去找網路所有的文字	13-2
文字跟文字再做比比對	13-2
那我們未來輸入的可能是聲音	13-2
音而網路上的東西也很可能是multimedia 的你	13-2
你就是靠它的聲音	13-2
他可能沒有人把幫你你寫成文字	13-2
那你靠它的聲音	13-2
於是你可能用聲音去找聲音	13-2
你也可能還是用聲音音去找一堆還有很多網頁是文字	13-2
聲音還要去找文字	13-2
然後你回到家裡的時候你的你的輸輸入可能還是文字	13-2
那這個時候你要找找的也是聲音	13-2
所以這些都是我們所說的範圍圍之內	13-2
ok 那我想我們用這張張圖簡單的解釋所謂的information retrieval	13-2
那也就是我們說這個information retrieval	13-2
它本身如果以文字去找文文字就像google 的動作	13-2
那樣的一件事	13-3
那麼最基本的就是這張圖所說的事情那	13-3
那網頁上有千千萬萬的幾十億的網頁	13-3
那所有的網頁頁我們就稱之為document	13-3
那每一個document 我們都想辦法用某一些個feature	13-3
把它呈現變成通常是一個feature vector	13-3
那就是這個這個動作我們稱之為indexing	13-3
那我們想辦法那像像今天的google 他我們基本上是用文字	13-3
以他文字上面面的一些東西西來做feature	13-3
但是我也是一樣把每一篇文件每一個document	13-3
想辦法用一堆feature	13-3
最後是用一個feature vector	13-3
用一個vector 來呈	13-3
呈現那就是所謂的那那這個動作就是所謂的document representation	13-3
也就是indexing	13-3
於是呢網路上上的千千萬萬的網頁或者文件就都變成千千萬萬個vector d	13-3
那user 輸入一個query	13-3
輸入一個keyword keyword 或者一組keyword 或者什麼	13-3
那user 輸入東西呢	13-3
也經過類似的動作把它變成另外一個vector q	13-3
q 也是用一堆feature 把它呈現變成一個vector q	13-3
那這就是所謂的query formation	13-3
或者是user request representation	13-3
因此呢所所謂的retrieval 的動作呢就是把你這個q 拿來	13-3
去跟把user 這個q 拿來跟網	13-3
網路上千千萬萬的d 去做一個matching	13-3
看看誰跟誰比較像	13-3
算他們之間所謂的relevant 這個relevant score	13-3
就他們相關程度有多多近	13-3
那然後把根據這個relevant score 排出來	13-3
來把分數最高的一堆寄送回來給你	13-3
那這個就是retrieval 的動作	13-3
那通常有所謂的relevant feedback	13-3
也就是說呢user 或者是是自動的這兩種	13-3
那意思思都一樣就是假設你照我的google 回來可能找回來一萬筆	13-3
不過裡面呢排名名最前面的一百筆可能是真正跟你比較有關的	13-3
那排名最前面一百筆裡面可能是最最前面的十筆出現在妳的螢幕上	13-3
那最前面的十筆裡面	13-3
如果你你自己去看一看可能有五六六筆真的是你要的	13-3
那你如果把那五六筆圈出來	13-3
我把那五六筆再輸入一次做為query	13-3
那這個時候就會找到跟你要的很接近的	13-3
那這這種動作是所謂relevance feedback	13-3
那剛才講的這個情形是user manual 去做但	13-3
但是你當然也可以自動的	13-3
你可以說凡是找回來的前五筆筆或者前十筆或者前二十筆	13-3
我就當他整這個整筆筆東西當成是我要的	13-3
那麼再回去一次把那一堆東西通通加回去	13-3
到那個query 裡面去再找一次	13-3
通常都會更好那	13-3
那我可以自動來做這個事情那	13-3
那或者這個是自動或叫做blind 就是user 沒有去選自動來做也可以	13-3
的詳細我們們後面還會提到	13-3
那這個的基本的意思是說	13-3
絕大多數的user 在輸入query 的時候都是是很懶惰的	13-3
所以通通常user 輸入的query 都非常的簡單	13-3
而這個並不真的描述他真的要的所有的東西	13-3
那麼我我們舉例來講如果他要找這個個九一一恐怖攻擊	13-3
那他他也許就輸入一個九一一或者只輸入一個賓賓拉登	13-3
那其實你它你要找的事情可能包括這個keyword 可能包括紐紐約	13-3
包括這個痾阿富汗包括這個恐怖攻擊包括很多詞	13-3
但是呢你只輸入入個九一一	13-3
那所所有的文件裡面有九一一的才會被找到	13-3
沒有九一一的就沒有	13-3
但是是你如果用這個方法的話呢	13-3
那個時候你找到前二十名或者前一百筆	13-3
那它裡面可能就把跟九一一相關的所有的詞都在裡面了	13-3
這個時候如果果你再回去一次的話就可以把很多東西找回來等等阿	13-3
所以這個是這個feedback 基本的精神	13-3
神那然後呢我們的看找回來的結果就可以來做performance evaluation	13-3
那這個performance evaluation 我們從前也提提到過	13-3
你可以想像成網路上所有的東西相關的是這個圈	13-4
所有相關的文件	13-4
可是系統找給你的是這堆是這個圈	13-4
那麼因此呢真正找對的是是a 的部份	13-4
那麼b 的部份是找來其實不相關的	13-4
你找到的前一百前一百裡面面可能有五十筆都不是你要的	13-4
那這個是找錯的	13-4
另外你會呢你會丟掉一些c 是你丟掉的	13-4
你沒有找到的	13-4
因此有兩個正確確率	13-4
就是一個所謂的recall 一個叫做precision	13-4
recall 是a 除以a 加c	13-4
也就是到底真正有關關的裡面你找到了多少	13-4
那precision 呢是你找到的裡面真正有關的是多少	13-4
這是這兩個正確率	13-4
那麼痾那那通常是這個寫在這裡就是說呢這個recall 是我們們通常user 通通常感覺的是precision precision rate	13-4
也就是這一個a 除以a 加b	13-4
我看我找到到了一百篇裡面到底跟我要的是三十篇還是五十篇	13-4
裡面有一大堆我我不要的	13-4
這是user 所以所可以感覺到的你找的東西好不好	13-4
所以那這這個是precision rate 這是通常user 感覺到的	13-4
但是recall 到底網路上還有多少你沒找到	13-4
我們通常很難判斷因為網路上你也不能去真的用眼眼睛去看	13-4
所以通常不太容易判斷	13-4
那麼怎麼去這個這個痾recall 到底有多少是不太容易做的	13-4
那麼那即使是這個個這個precision 是多少其實也很難	13-4
因為那你看你怎麼看法譬如說你是從一百百篇裡面	13-4
前一百筆裡面我我對的是幾筆呢還是前五十筆	13-4
還是前二十筆還是第一頁頁	13-4
第一頁裡面還是第二頁堶±這個都很難算阿	13-4
所以呢那麼那另外一個重要原因因應該講這兩個正確率應該是互相trade off	13-4
的那你可猜的出來是我retrieval 的時候我選擇的那個threshold 的高低的問問題	13-4
我如果選擇的threshold 很高的話	13-4
我會使得很多不相關的不會進來	13-4
來我threshold 選的高的話我進來的都是相關的	13-4
我的precision 可以很高	13-4
可是那個結果我也會丟掉很多所以recall 會低	13-4
那反過來我如果threshold 低的話	13-4
話那麼很可能就是就是一大堆都進來了	13-4
所以我的recall 會高	13-4
可是我的precision 也就低了因為很多不相關的也都進來了	13-4
所以這兩個通常是是你你調那個threshold 的時候這兩個一起動	13-4
一個高一個會低一個個低一個會高	13-4
這兩個是是trade off	13-4
因此有所謂的這個recall precision pron	13-4
就說你不不能算一個數值而是應該是是這兩個一算	13-4
畫成一條curve 的	13-4
那這個curve 通常會變成這樣這	13-4
這depends on 你怎麼畫	13-4
如果是一一面是pre precision 的話	13-4
一面是recall 的話	13-4
那麼這個痾痾你可能會變成像這類的圖	13-4
也就是說你你這邊要高高的話這邊就低了	13-4
這邊要的話這邊就低了那	13-4
那當然這時候候的最idea 的case 是	13-4
在這裡這是idea case 那麼	13-4
他他們是痾也也許我畫錯了我們	13-4
也許是這樣可	13-4
可能是要這樣子阿	13-4
我我如果recall 要高的話precision 會低	13-4
recall 要這個precision 要高的話呢recall 會低	13-4
那麼因因此呢我的idea 是是這一條	13-4
那麼你你調你的threshold 的時候	13-4
你調調你的threshold 他是在這上面動	13-4
那麼threshold 如果高的話	13-4
你的一邊到這邊來一邊到這邊去	13-4
那你的系統有多好就看這條曲線如何的接近上面那個case	13-4
那在這邊是idea	13-4
就是我不管怎樣我的我的這個這個個precision 永遠是最高的	13-4
recall 永遠是最高的	13-4
你如果這樣畫的話大概是這這樣	13-4
那我剛才那個畫法是用一減跟一減來畫就會到這邊來	13-4
那因為這樣的關係你如如果動不動就要用一條curve 來描述的話實在很麻煩	13-4
那麼因此呢他他們真正在算的時候候常常用一個數字來算	13-4
那個數字就是所謂的non interpolate average precision	13-4
就是以precision 為準	13-4
然後呢我只用一個數數字	13-4
那個數字怎麼算呢	13-4
就是averaged 在所有的你凡是找到對的那裡去算那個recall 那去算那個precision	13-4
我們舉個個例子來講	13-4
假設你你找到的前十筆裡面	13-4
前十筆裡面	13-4
真正你要的是第一筆第五筆跟第十筆	13-4
第五筆第一筆第五筆跟第十筆是你要的其	13-4
其他不是	13-4
中間這些都不都不對	13-4
如果這樣的話我怎麼算	13-4
我算到第一第一筆的時候我的的precision 是一分之一	13-4
然後這些都不對	13-4
算到到第五筆的時時候我的precision 是五分之二	13-4
因為五筆裡面有兩筆是我要的	13-4
這些都不算	13-4
算到第十筆的時候呢	13-4
我是十十筆裡面有三筆這是十分之三	13-4
因此譬如說我我算到我的cut off 如果在這裡的話	13-4
我就拿這三個數字來平均	13-4
那這三個數字來平均這樣得到一一個數字	13-4
那然後我現在在如果用一千個一千個實驗做一千個query 進去	13-4
或者怎樣的話呢	13-4
呢我把全部平均起來	13-4
那這個數字是所謂的non non interpolate average precision	13-4
這樣我就用一個數字來說	13-4
那還是要說我我的cut off 是多少我是算到第十的還是是算到哪裡的	13-4
的那在這個情形下	13-4
我的分數是多少那就用這個數字來算	13-4
好那以上上是簡單的介紹	13-5
那底下我們就要說的我們要做語音怎麼辦	13-5
用語音的話呢我現在變成是說這邊的document 裡面可能是語音那	13-5
那這個query 也可可能是語音	13-5
那麼於是呢原來原來的這套套學問是用文字做的	13-5
裡面如果是語音怎麼辦呢你馬上想到最直接的就是我都把它它做recognition 之後都變成文字	13-5
就可以帶進進來這一套	13-5
那如果用這個方法的話話呢	13-5
那就是所謂用word 來做indexing indexing element	13-5
我就是都做一次recognition	13-5
那基本上就是我把所有的document 如果是語音的	13-5
所有的query 如果是語音的我我都去做speech recognition	13-5
一但做完speech recognition 之後呢	13-5
我就都變成文字了嘛	13-5
那就用文字的方法來做它	13-5
那當然這個時候你顯然要用large vocabulary 的recognition	13-5
去做它	13-5
所以這是一個個large vocabulary based 的方法	13-5
那這個顯然是可以的但是不會很好	13-5
最明顯的原因你馬馬上知道就是error propagation	13-5
我等等於是把兩個系統串連起來	13-5
那第一個系統的error 就到了第二個系統去	13-5
那換句話說	13-5
你這個語音去做recognition 的的時候	13-5
你已經得到一堆堆錯字	13-5
那些錯字也就做了indexing	13-5
所以你得到到的這些d 裡面就有錯字嘛	13-5
那同理你的query 裡面做recognition 也會有錯然	13-5
然後你再進來也有一堆錯	13-5
所以錯的去找錯的所以就找到一堆錯的	13-5
所以以呢這個error 就等於是在兩個系統統你語音辨識的系統跟這個系統兩個串聯在一起的結果	13-5
那就是error 一路propagate propagate 下去	13-5
那第二個明顯的問題就是oov	13-5
因為很多的的keyword 真正要要找的keyword	13-5
並不在在辭典裡面都是oov	13-5
你譬如說我我今天要找的是這個個奧斯卡的得獎影片斷臂山	13-5
這個keyword keyword 這個斷臂山顯然是oov	13-5
你說我要找趙建銘的事件	13-5
趙建銘顯然是oov	13-5
你那個鐵定都是找不到的	13-5
因此你你這些真正的keyword 你都找不到到的話你找到都是一些不重要的字所以其實是不太容易	13-5
那再者呢special term 很多	13-5
如果是在在特別的領域	13-5
譬如說你要找的是這個醫學有關的新的的文件	13-5
或這你要找computer 有關的文件	13-5
或者是要找這個痾這這個股票有關的文件	13-5
凡是一個special area 的話還還有一堆special term	13-5
可能不都不在你的recognition 的那個痾vocabulary 裡面	13-5
可能都是oov 你都辨識不出來	13-5
那麼但但是這裡這個問題常常是針對某某些領域的	13-5
那你又又不可能為每一個領域都去做你的辭典	13-5
所以這是用word 來做indexing element	13-5
這種東西我們是所所謂的indexing element 也就是你將來的你後面的這些個這些個indexing 的動作	13-5
是要要以他們為element 為單位來做的	13-5
那麼你用word 來做是有這些缺點	13-5
但是呢word 顯然是一個重要的方法	13-5
法因為有這些問題所以就就有人想說那我應該用subword unit	13-5
那這就是所謂的subword based	13-5
所謂subword unit 的意思就是說我現再用的是比一個word 還要要小的單位	13-5
什麼是比一個word 還要小的單位	13-5
位像phone syllable 或者類似的	13-5
那麼這有什麼好處呢	13-5
這個時候我就不再被vocabulary 所限制	13-5
而是我只要音對就可以了	13-5
那麼舉例來講如果這個user 說我要找bill gates	13-5
他說我要找bill gates 的話這可能是一個oov	13-5
你辭典裡面沒有這個字你就抓不到這個字了當然就沒有了	13-5
但是如果果你是以音為單位的話以phone 為單位的話呢	13-5
是這幾個字	13-5
所所以呢我只要抓到這個phone 跟這個譬如說這個phone 跟這個個phone 連在一起跟這個phone 連在一起	13-5
那在那一邊文件裡面的那一句話它也有這幾個phone 連在一起	13-5
那那我至少呢這個會抓的到	13-5
我只要這些音這些phone 對的話	13-5
話我這些phone 連在一起我抓到這三個phone 的一個phone sequence 那	13-5
那一邊我如果user 說的話裡面有這三個phone 構成的這個phone sequence	13-5
sequence 在這個文文件裡面你做recognition 我也抓到這三個phone 的phone sequence 你就可以對應出來	13-5
那雖然我的辭典裡面沒有這個字	13-5
所以我真的辨識不出這個字來	13-5
但是我這個還還是可以比對的到的	13-5
那在我們中文而言同樣的情形譬如說我要找呂呂秀蓮我	13-5
我只要抓到譬如說syllable 為單位的話我有這個秀這個個蓮	13-5
雖然說我的辭典裡面沒有這個字所以我不會辨識出這個來可	13-5
可是我如果query 裡面有秀秀有蓮我已經抓到秀蓮了	13-5
那麼在這個文件裡面他也有這個個講到秀蓮怎樣怎樣的話	13-5
我其實就可以這個就對對應到他了	13-5
我即使沒有辭典沒有這個字我沒有抓到他沒有關係	13-5
那這個就是用subword unit 來做的基本的想法	13-5
那麼因此呢我可能是用phone	13-5
或者phone 的sequence	13-5
sequence 或者一系列的兩三個四五個phone 為單位	13-5
同樣我也可以用syllable	13-5
那syllable 其實就是是一個phone sequence 就是一個兩個phone 或三個phone 的phone sequence 啊	13-5
這些些東西來做	13-5
那最明顯的一個好處就是他不不再limited by vocabulary 嘛	13-5
就是我們剛	13-5
剛才講的	13-5
我辭典典沒有抓到的時候我音抓到就可以啦	13-5
那當然你如果這邊如果是文字的話	13-5
文字我可以查字典	13-5
我知道它的音哪對不	13-5
不對所以我只要知道這個音我知道這這個音	13-5
我知道音是可以抓的到啦	13-5
但是當然因為你音抓到你可能因此會跑出更多的ambiguity 出來	13-5
這是會發生生的	13-5
你你這邊說是呂秀蓮	13-5
蓮但是這篇文章可能有張秀蓮還是王秀蓮怎樣的話	13-5
或者什麼什麼什麼修什麼臉臉什麼的耶	13-5
耶結果你可能也就抓出來來了	13-5
所以呢會跑出一些ambiguity	13-5
是自然會發生的	13-5
但是呢它也有好處	13-5
就是說我通常這個個phone 的數目比較少	13-5
你用用word 的話可能你的你要幾萬萬個word 才夠	13-5
這裡可能phone 只有幾幾十個	13-5
你syllable 只只有一一兩千個對不對	13-5
那這時候呢其實我數目還比較少	13-5
我可以handle 一些個oov 痾這這是他最大的好處	13-5
那基本上就是是我用一個segment of 一個或幾個subword units	13-5
那麼幾個phone 或幾個syllable 來做	13-5
這是所謂的subword based	13-5
那當然你也也可以想像到這兩個其實是互補的	13-5
所以何不可以把它兜兜起來	13-5
我也用這個也用這個嘛	13-5
那用這個的時候候可以當我的word 能夠辨識出來的時候	13-5
候那他們他有它的好處	13-5
如果他不是oov 的話	13-5
它的意義明確比較不會ambiguity	13-5
但是這裡的話呢就是你如果oov 的話呢呢我就可以用這個所以這兩個是是可以整合的嘛	13-5
那再來呢keyword based	13-5
keyword based 其實是最古老的方法	13-5
你知道在還沒有google 的年代在還沒有網網路搜尋的年代	13-5
所有的文件就是給我一堆keyword 那	13-5
那麼每一個人寫一篇文章寫完的時候上面都會設幾個keyword	13-5
然後就用這個keyword 去找那	13-5
那當然問題就來了就是誰來設這個keyword	13-5
在古古代沒有網路搜尋的的年代	13-5
那就是user 去設嘛	13-5
所有寫文章的人寫完的時候就規定你要說好	13-5
你的keyword 是什麼	13-5
那就用那一個那	13-5
那當那當然今天時代不同了已經沒有人要求你做做一個網頁還要說你的keyword 是什麼	13-5
所以呢你的key 而網路上的東西隨時在千變萬化	13-5
所以你的你的keyword 最好是自動產生嘛	13-5
那你這時候就要要有一堆好的演算法	13-5
那麼為每一張網頁自動產生它的keyword	13-5
才能夠做這件事	13-5
otherwise 你的你的你的這個keyword base 就不容易做	13-5
那那也因為這樣因為為網路東西是dynamic 的	13-5
所以你的keyword 就不太是fixed	13-5
而是是變成是要會動的	13-5
是dynamic 的	13-5
那麼因此此呢它有它的難度	13-5
那這這是keyword based	13-5
那真正的好的辦法呢常常其實這三個一起用嘛	13-5
我也用用words 我也用subword subword units 我也用keyword	13-5
凡是有的都可以把它合起來	13-5
就是fusion 就就是把它的整合	13-5
就是hybrid 方法	13-5
那這些東西是我們講的我們所用的indexing 的element	13-5
那我我真的做的時候呢	13-5
我可以用一個element 來做也	13-5
也可以用combination of more than one	13-5
那combination of more than one 就是是我們剛才講的	13-5
譬如說我用三個phone	13-5
每三個個phone 構成一個phone sequence 可以呀	13-5
譬如說這是bill 這是一個三個個phone	13-5
然後elk 這是一個三這是一個三個phone 的sequence	13-5
那麼然後呢這是一個三個phone 的sequence 我可以用這樣三個phone 的sequence 來做等等	13-5
那這就是這個combination of more than than one element	13-5
這也可以做	13-5
然後呢那這一些就是我們所就是一般稱之為indexing feature 你真正用的feature	13-5
也就是我們到時候把它表示成成這邊的feature vector 的時候	13-5
是用這樣的東西就是所謂的indexing feature	13-5
來做這些事	13-5
那麼呢這個時候呢	13-5
這個痾當然你也可很多時候是pre define	13-5
就譬如說每一個word	13-5
在這這裡我每一個word 就是一個feature	13-5
那你也可以是三個phone phone sequence	13-5
連續的三個phone sequence 是一個feature	13-5
這個都可以預先define 好	13-5
那當然還有一種呢是自動產生生data driven	13-5
也就是說當你有夠夠多data 的時候	13-5
這麼多的文件	13-5
這麼多的	13-5
那我可以用data driven 的方式去抽	13-5
什麼是最有效的feature	13-5
可以用抽的等等	13-5
那所所有的這裡的每一個feature	13-5
他們通常在他們的領域裡面有一一個專有名詞詞叫做term	13-5
所謂indexing term	13-5
那就是一個這樣的東西西每一個這樣的feature 呢叫做一個indexing term	13-5
然後後有了這一堆indexing term term 之後我基本上就是譬如說比對這一堆東西比	13-5
比對你如果你這邊有提到gates	13-5
這三個phone 連起來的gate	13-5
那如果你有這三個phone 的話你這邊如果也有三個phone	13-5
我就找找找到你	13-5
那那這些你就再看他們之間間到底相似程度有多少	13-5
那麼給他一個多少的分數等等	13-5
那主要就就在做這樣的事情	13-5
那因此呢所有事情都是以這些indexing 的term 或者是是feature 為基礎來做的	13-5
那真正做的時候呢有很多多種不同的model	13-5
那我們這邊列的是大概是最常用的一些model	13-5
痾還有沒有列的	13-5
不過基本上呢最基礎最常見的最簡單而有效的就就是vector space model	13-5
這待會底下我們就會講	13-5
那除此之外	13-5
這個latent sementic 就是我們在十二點零所說的那一個l s a	13-5
那麼這是一個非常有有效的方法在這裡	13-5
那麼其實l s a 當初是為了這個發展出來的	13-5
那同樣的的也可以有很多其他的統計的模型我們後面也會說到	13-5
譬如說用h m m 來做也可以等等	13-5
你也可以再整合用不只一個model 來做hybrid combination combination 這都可以好	13-5
那底下呢我們就先來說第一個這個vector space model	13-6
這也是retrieval model 裡面最基礎	13-6
最常見	13-6
見那麼最簡單也	13-6
其實它也非常有效的	13-6
那喔這個的基本精神就是我	13-6
我為每一個每一個document d d 或者每一個query q	13-6
的representation 就是一一個vector	13-6
那也就是說我們這邊的剛才這邊的每每一個document 的representation 那個d	13-6
就是一個vector	13-6
每一個q 就是一個vector 那	13-6
那一個vector 是什麼呢	13-6
就是我們這邊的用這一些個每一個indexing feature	13-6
當成他的喔其實每一個type of feature 就有一個vector 什	13-6
什麼叫每一個type	13-6
譬如說我們們剛才講如果以word word 為單位的話呢	13-6
這個word 是一個type	13-6
所以我就為word 建一個vector	13-6
如果我是以三個phone 為三個phone 的phone sequence 為單位	13-6
那我也為它建一個個vector	13-6
那這個每每一個呢這叫做一個type of indexing feature	13-6
那麼我們舉例來講假設說我以word 為單位	13-6
我以我如果以word word 為單位的話呢	13-6
假設gates 在我的辭典裡面	13-6
所以gates 是一個word	13-6
那麼我我每一個word 我就有一個element	13-6
就有一個dimension	13-6
這是第一一個word 第二個word	13-6
一直到譬如說我有五萬個word	13-6
我就要五萬個dimension	13-6
每每一個word 裡面有一個東西有一個component	13-6
那這個時候呢呢假設這個gates 是這裡面的一個字的話	13-6
是一個word 的話我	13-6
我針對這個gate 會有一個數數字在這裡	13-6
等等那每一個word 都有一個	13-6
個那這樣構成一個vector	13-6
是我們講的這個以這個type j 如果這個j 是word 的話	13-6
我就有一個word	13-6
那我如果是以phone 為為為為這個element	13-6
然後我如果是以每三個phone 的	13-6
的一個phone sequence 當成一個element 的話呢	13-6
那就變成說	13-6
我譬如說我的phone 有六十個	13-6
phone 有六十個	13-6
那三個phone 連連起來的sequence	13-6
sequence 至少是有六十的三次方個	13-6
那這裡的每一個都有都有一個在這裡	13-6
譬如如說在這個case	13-6
這個ga 後面接ts	13-6
ts 那這是這三個phone 連起來	13-6
這是一個	13-6
然後呢譬如說這裡有一個	13-6
這三個phone 連起來這是一個等等等	13-6
那總共有這麼多個	13-6
個那每一個是一個element	13-6
那這樣我構成一個這樣的vector 等等好	13-6
是這樣的意思	13-6
所以呢所謂的的每一個每一個type of indexing feature	13-6
像這就是一一個type of indexing feature	13-6
這就是一個type of indexing feature	13-6
我都有一個vector vector	13-6
那這個vector 裡面面的每一個	13-6
我們說每一個這個element 我會有一個component 有一個數值啦那	13-6
那這個數值是什麼呢	13-6
就是這個z 的j t	13-6
那所以以呢這個j 就是我的type j	13-6
t 呢就是針對某一個對對某一個indexing term t	13-6
譬譬如說在這個case 這個個indexing term	13-6
我們就叫做t	13-6
所以這裡在這個case 而言我這個t 有譬如說五萬個	13-6
那這裡的每一個都是是一個t	13-6
所以每一個這邊的那個這個這個東西就這邊講的z j t	13-6
就是是這個z j t	13-6
那同理呢那麼在這個case 的話呢	13-6
這一個這三個phone sequence	13-6
這三個phone sequence 連起來的這個phone sequence 是	13-6
是我們這邊所謂的一個term t	13-6
那這裡的一個element 叫做z 的的j t 等等	13-6
好那如果是這樣子的話呢	13-6
那麼這個z j t 等於什麼呢	13-6
最重要的就是這一個count	13-6
也就是這個c t	13-6
c t 是什麼呢就是就是這個term 的count count in the document	13-6
如果這篇文章是在在講微軟什麼什麼裡面bill gates 出現了十次	13-6
那這個e 那個那個數字就是十	13-6
那你可可以想得到如果這篇文章是在講微軟是在講這個什麼麼的話	13-6
那很可能bill gates 會出現十次	13-6
windows 會出現現二十次	13-6
然後什麼dot net 會出現個十五次	13-6
microsoft 會出現一這個一百次等等	13-6
但是會有一大堆東西是沒有的	13-6
這是空的都沒沒有	13-6
好那麼因此呢這個feature vector vector	13-6
基本上就已經描述了我的這個這篇文章	13-6
它裡面會說些什麼什麼東西喔	13-6
等等所以呢你就就是把它的裡面的每每一個word 把它count 好	13-6
那就他的這個那就是我的這個痾這個ct 就是我的的frequency count 我	13-6
我或者是用這個word 或者是用這個phone sequence sequence 都一樣我這樣得到一堆count	13-6
那不同的人用的公式不太一樣	13-6
最多的人寫成這樣樣子	13-6
不過有的人不用log 有的人取log	13-6
取log 沒有什麼特別的道理	13-6
只是有人做實驗取log 比較好就是了	13-6
那你知道取log 比較好的原因是說因為它是一一個數目太大的時候他會把他壓下來	13-6
它不是linear 上去的	13-6
那那麼因此呢當你數目多的時候不要增加那麼快	13-6
它有這個好處	13-6
那麼麼這個有的實驗顯示這樣比較好	13-6
那還有的人喜歡加一個e 有的人不加	13-6
那加個e 那也也是類似情形	13-6
那你可以想像的是	13-6
多的還是照樣會多嘛	13-6
但是呢原來如如果只只出現一次log 之後log 一變成零	13-6
那這裡都至少加了一就不會變變成零了之類的	13-6
那也是有人做實驗他比較好	13-6
那有的人不要加所以這個不不一定	13-6
那不管怎樣呢這個東西有個專有名詞就叫做做term frequency	13-6
那你可可以顧名思義就知道就是指這一個term	13-6
這是所謂的一個term	13-6
這個term 的frequency	13-6
這個term 的frequency	13-6
那這是一一個非常重要的訊息說明這篇文章	13-6
這篇文章他哪一些個term 出現	13-6
哪一些個term 出現現表示這篇文章在說的是什麼東西	13-6
啊因為到時候我就是要要比對這些字有沒有比對這些word 嘛那	13-6
那麼因此呢這所謂的term frequency t f	13-6
那還有另外一個東西也很重要呢就是後面這這一項	13-6
那這個東西是什麼呢	13-6
這個n t 呢是指total number of term of documents in the database 它包括這個term term 的	13-6
我們以這個gates 而而而而為例的話呢	13-6
我整個網頁一百萬篇篇裡面	13-6
講微軟的講gates 的會有gates 這個字的	13-6
有一千篇的話	13-6
那這個n t 呢就是一千	13-6
那麼這個n 是什麼n 是total number number of documents	13-6
那麼因此呢n 是什麼假設我我總共是一百萬篇就是一百萬萬	13-6
那麼一百萬除以一千之後呢	13-6
那還有還有好多萬阿	13-6
那這個數字很大	13-6
痾對沒有錯	13-6
這這個數字很大就表示說這個很重要	13-6
那麼也就是說假設我我全部的是一百萬篇裡面	13-6
只有一千篇或者五百篇或者五十篇	13-6
有這個字的話	13-6
那顯然這個字出現顯示抓到這這些文章的能裡很強	13-6
所以呢這個是很重要的	13-6
那麼因此呢我用這個一除之後	13-6
那你可以反過來	13-6
來像我們之前提到過的例子	13-6
在這裡的話我們譬如說妳可以想到的	13-6
的的也是一個字阿	13-6
你如果的是這這裡的話	13-6
那在每一篇文章裡面都有	13-6
那這個就是沒有意義的字	13-6
你譬如說this	13-6
這些function word	13-6
或者at	13-6
這些些個word 在這裡都是沒有意意義的	13-6
每一篇文章裡面都有的	13-6
那這些東西呢你或者說是它的音	13-6
譬如說this 這	13-6
這個phone sequence 在這裡的話呢	13-6
就是沒有意義的	13-6
因為它每一個裡面都都會有	13-6
那麼那麼因此呢	13-6
如果是這一些個東西的話呢	13-6
呢那麼我在一百萬篇裡面每一百萬篇都都有	13-6
一百萬除以一百萬之後是一log 一就是零	13-6
零那這個就沒有了	13-6
啊這個值就變成零了	13-6
那那麼因此呢這個等於是在在說到底我的所有的我的整個database 裡面我用一個database 去算	13-6
我就會算出來說那麼這堛漕c 一個term 它的重要性	13-6
那麼麼如果說是每每一篇都會有的的話呢那那這個就是一了	13-6
那這個這個log 一就是零就是沒有重要性	13-6
那反過來如果在一百萬篇裡面只有十篇有的	13-6
那這個顯然很重要	13-6
你有了這個幾乎就是要找到那十篇去了	13-6
那因此呢那這就是所謂的這個那這個值就會很大	13-6
那這就是所謂的inverse document frequency	13-6
因為它把document 的數目做在分母母上面	13-6
所以是跟document 數目是相反的	13-6
所謂inverse document frequency 就是i d f	13-6
那麼i d f 非常清楚的告訴我們哪一個term 比較重要	13-6
那這個東西跟我們在十二點零所說的另外一個東西西意思是非常像的	13-6
你如如果記得的話其實在那埵酗個很類似的東西是entropy	13-6
我們在十二點零的的時候	13-6
我們數算這個的時候	13-6
這個term document 的的這一個term document 的matrix 裡面的那一個element 裡面	13-6
我也在數count	13-6
這是非常像的喔	13-6
非常像的我也在數count count	13-6
在數count 之後呢我再做一個entropy 的動作	13-6
這個normalize entropy 其實是相很像的意思	13-6
那麼麼我做normalize entropy 的結果也會使把這些個word 的效果降成很小很小	13-6
然後會把特別的word 的效果果拉的很高	13-6
好那意思是一樣的那	13-6
那只是有不同的作法	13-6
這個normalize entropy 是一個很好的作法	13-6
同樣的這裡的i d f 也是一個個這裡的i d f 也是一個重重要的很好的作法	13-6
所以這個是痾目的是非非常接近的	13-6
好那這兩個連起來就是所謂的tfidf	13-6
在information retrieval 的領域裡面這是一一個非常常用的詞	13-6
所謂的tfidf 就是指這兩個東西相乘	13-6
那一個個代表的是它的count	13-6
一個代表的是它的重要性	13-6
那這兩個一乘起來的結果就告訴我這個element 這個值應該要多少	13-6
好那如果這個有了的話	13-6
我現現在就是為每一個每一個document	13-6
我為每一個document 我都建了一個這個這個個vector	13-6
然後呢同樣的我也為query 建相同的一個vector	13-6
我今天user 輸入的說他他要找的是什麼	13-6
他要要找的是microsoft	13-6
那這個時候呢我也一樣為他建這個vector	13-6
不過現在user 很懶他只輸入了了一個字	13-6
就是microsoft	13-6
所以呢就是這個字出現了一次而已	13-6
別的什麼都沒有別的全全部都是零	13-6
但是這樣的user query 我也建一個相同的vector	13-6
那這個做法跟剛才一樣的作法	13-6
我也是是不是只算一個count 我也要算i d f	13-6
然後就是把這個t f i 都算出來之後呢得到一個數值	13-6
這是z j t 在這這裡	13-6
那這個個是我為user 所建的	13-6
user query 所建的	13-6
那user 也建了這個vector	13-6
我這這邊也建了這個vector	13-6
之後我這兩個vector 就可以做內積	13-6
因此呢我底下就可以做這個內內積	13-6
這裡的這個符號不對阿這個是電腦的問題	13-6
所以應該就是我就是為這這兩個vector 做內積	13-6
那做內積積之後其實再normalize 它的長度	13-6
就是這個這這個cosine 嘛你知道	13-6
就是這個這兩個vector 做內積	13-6
但是這個內積是是包含了它的vector 自己的大小	13-6
所以我要把大小normalize 掉	13-6
因為它也許這這篇文章長達十萬十萬個word	13-6
那裡面這個數目就很多啦	13-6
那這篇文章只有二十個word	13-6
這數目就很少啦啦所以這個大小顯然不對嘛	13-6
所以你要把它大小的normalize 掉	13-6
把vector 的大小normalize 掉之後	13-6
後剩下其實就是它的內積	13-6
也就是cosine theta 這個東東西	13-6
那這個值基本上是在一到負一之間嘛	13-6
那你可以知道他在在一附近的時候就是是他們最像的時候嘛等等	13-6
那零左右就是他們最不像的時候	13-6
那所以你只要用用這個方法來做的話這個	13-6
其實就是所謂d j q j j 就是q j 就是user 的query	13-6
user 輸入的說我要找microsoft	13-6
那這個時候呢user 的那個query 就是q j	13-6
我對這個對user 做的這個que 這個vector	13-6
那麼我每一篇文章所做的呢就是d j	13-6
我就分別都去做這個內內積	13-6
然後看誰的內積大就就是跟誰接近嘛	13-6
那你可以想像其實的結果是說	13-6
因為user 只輸入這個而已	13-6
大部分都是零的	13-6
這邊全部都是零	13-6
所以這個內積結果其實就是把這邊的microsoft	13-6
這邊如果這個字是microsoft 的話	13-6
這個不是零的那些文章就會抓出來	13-6
就會抓到microsoft microsoft 嘛阿	13-6
所以這個只要這樣內積一乘的話呢就會把你要的抓出來	13-6
那然後這個j 是我們們剛才講的type	13-6
所以呢我譬如說這個個word 是一個j	13-6
這個三個phone 連起來這也是一個j	13-6
我有好好多個j	13-6
我就把它們通通都每對每一種type 我都做一次次這個內積	13-6
都有一個分數	13-6
然後我更成上一個weight 把它全部加起來	13-6
那這樣子的話我就得到一個個total 的分數	13-6
那於是就知道user 要輸入的跟他跟誰比較像	13-6
那這個就是最基本的這個痾痾vector space model 的原理	13-6
那你在做文字的時候也是這麼做的	13-6
我們如果是是這個這個如果是文字的retrieval 的話呢	13-6
他也也也這樣做	13-6
那這就是retrieval 裡面最基本vector space model	13-6
那我們不同的是說文字的時候呢	13-6
他大概不需要用這個phone sequence 這種東西	13-6
他就用這個word 就好了嘛好	13-6
但它它可以有more than two words	13-6
譬如說你你應該是兩三個words 連起來是更好的term	13-6
譬如說bill gates	13-6
要bill 跟跟gate 連在一起那	13-6
那個bill 跟gate 連在一起的時候才是有意義的	13-6
否則一個bill 是很多bill	13-6
bill 但是呢有gates 的bill 那就是特別的	13-6
所以你可以把兩三個個words 連在一起	13-6
所以他還是有可以有很多個不同的term	13-6
但是它基本上文字只要要用word 來做就好了因	13-6
因為word 一定對	13-6
只有在語音的時候候才我要用像phone 這種東西來做	13-6
那在語音的時候除了用phone 這種東西來做做之外呢我其實還可以做什什麼呢	13-6
我這時候我的這個除了用count 之外	13-6
我還還可以用這個分recognition score 你辨識的分數啊	13-6
confidence measure 這個我們本來在十點零裡面講的	13-6
不過我們現在在十點零還沒講	13-6
所以呢不過你可以了解所謂confidence measure 意思就是我每一個辨識結果	13-6
我都可以以給他一個分數	13-6
當我辨識出來得到一個word sequence 的時候	13-6
我可以為每一個word 我每一個辨識結果給他一個個分數	13-6
譬如說在零到一之間	13-6
譬如說這個是零點九	13-6
這是零點七七這是零點三這個零點五	13-6
那給他一個分數的意思是是說我對於這個recognition 的結果的confidence 有多多少	13-6
那越接近於一的表示越可靠	13-6
大概是對的	13-6
那如果很低的表示說我雖然辨識出這個來我對他覺得他很可能是錯的	13-6
的那我可以用很多在辨識中間的各種的的information	13-6
來算出這樣子的所謂的confidence measure	13-6
那麼那麼等於說是我對每一個word 都可以給他一個我的confidence	13-6
那你可以把這一類的東西	13-6
不論是這一種種的confidence measure	13-6
或者是recognition score	13-6
放到這裡面來也就是你在數count 的時候	13-6
不光是數count	13-6
你每一個個count 可以把這些東西加起來	13-6
那他就不見得是出現一次不是一次他只有零點點三分	13-6
因為我我不太相信他	13-6
啊可以把這些算算進去	13-6
那這樣的話就變成語音的	13-6
所以語音可以用這個方式就可以這樣做了	13-6
那這這個所謂的vector space model 是是基本上的model 我們可以看成是這樣子的	13-6
大的vector 來這樣來算	13-6
並不表示你寫程式的時時候真的要這樣子寫	13-6
那為什麼呢因為你可以想像其實在真正程式在操作的時候	13-6
可能遠比這個要來的簡單	13-6
為什麼呢	13-6
因為user user 輸入他只輸入microsoft 一個字	13-6
所以以其實呢我所謂的這個user 這個q 的這個microsoft	13-6
就是只有這個element	13-6
別的沒有嘛	13-6
那因此呢當這個跟這個來做內積的時候	13-6
其實只是在算這個個的microsoft 裡面面的這個幾分然後這個跟這個個相乘就好了	13-6
別的根本就不要嘛	13-6
那麼因此呢你真的在寫程式的時候候你很可能是變成	13-6
我這邊雖然我每一篇文章的這個q 的	13-6
這個d 我要這樣建起來	13-6
來可是到時候我並不見得真的要做內積	13-6
我只要看user 輸入的那個是什麼之後我直接這個從那個element 去找這個element 就對了嘛	13-6
嘛所以呢真正做的時候不見得是是真的要有這麼多vector 在做內積就是了	13-6
好那那這個是基本的vector space model	13-6
那要再進一步改進的話呢怎麼樣做的更好	13-7
最常用的第一個就就是我們之前提到的這個relevance feedback	13-7
以也就是說你可以把第一次找到的結果feedback 回去	13-7
通常你第一次找到的最前面可能就是最重要的information	13-7
你把它feedback 回去那	13-7
那一個最直接的標準的作法就是像這樣子那	13-7
那這是什麼意思呢就是假設說	13-7
user 輸入一個microsoft 那	13-7
那麼你找到的你你你得到的文章呢你會從會從第一名排到第一百萬名	13-7
那你可以把最前面的譬如說十筆還是二十筆還是五十筆還是五筆	13-7
你把最前面的這個呢叫做叫做這個痾這個relevant doc doc 最接近的relevant document d r	13-7
然後你把這邊最不最不像的這堆呢叫做irrelevant	13-7
就是d 的irrelevant	13-7
那你如果這樣子的話呢你就可以把他們的那一些個的的d vector 加起來	13-7
然後呢加回原來的那個q 去	13-7
那什麼意思呢	13-7
這就是我們剛才講的user 通常很懶惰	13-7
當user 輸入一個microsoft 的時候	13-7
其實他要找的也許是bill gate	13-7
也許是windows windows	13-7
也許是dot net	13-7
也許是office 或者是什麼什麼	13-7
那麼但是是你現在只找了一個這一個他就只會找microsoft 的啦	13-7
如果裡面文章裡面面沒有microsoft 這個字的話你那些什麼什麼都都沒有用	13-7
那麼因此呢比較好的辦法是	13-7
你如果把把用microsoft 所找到的最前面的譬譬如說五筆十筆二十筆一百筆	13-7
這些東東西的vector 加回去	13-7
你把這些個vector 加起來	13-7
乘上一個weight 加回去	13-7
那麼於是就會把你雖然一開始user user 只輸入一個microsoft	13-7
你就會把bill gate 也加回去了	13-7
你會把windows 也加回去了	13-7
你會把這些跟他相關的都加進這個q 裡面	13-7
那麼因此呢你原來輸入的user 很懶惰他只輸輸入一個q	13-7
那個q 只有一個microsoft	13-7
但是呢你現在把那個最最前面的五筆十筆一百筆加回去之後	13-7
你就會把跟microsoft 相關的的很多word	13-7
都加回去你的q 就會比較多東西了	13-7
好那麼那你這邊用一個weight 來來來加回去	13-7
同樣呢你也可可以最不相關的去扣掉	13-7
那麼這個效果比較小前面那個比較大	13-7
你把這個最不相關的裡面這個裡面的這個這裡面的vector 大概跟你你要的是最不像	13-7
你可以把他扣掉啊	13-7
所以這個也可以用減的那麼乘上上某一個weight	13-7
那如果這樣的話呢我就可以把原來的q 變成一個新的q pron	13-7
那這個q pron 顯然會比原來q 好很很多	13-7
那這個就是這個所謂的這個relevance feedback	13-7
那這個做法是完全全自動的	13-7
所以這這是所謂blind	13-7
那你就直接每一次的時候他自動可以做甚至於幾個iteration	13-7
那麼這麼一來的話呢通常常就會把很多相關而user 沒有用的term 都加進來	13-7
那麼因此你會找到比較好的	13-7
那這一類的東的的作法呢有另另外一個名詞其實就是所謂的query expansion	13-7
就是你怎麼樣把原來的query 把它expend	13-7
因為user 懶惰所以user 用的query 是最簡單的	13-7
那你如何把它expend	13-7
那底下這裡講的是你還有另這另外外一個也常用的辦法就是你有一個term association	13-7
你如果為這裡的的所有的term 之間建立一個他們之間的association 的關係	13-7
誰跟誰是有關係的	13-7
因此呢你你的bill gates 跟microsoft 就是是有關係的	13-7
他們的association 是很高的	13-7
納麼因此呢當我user 輸入bill gates 的時候	13-7
我就把自動把microsoft 加進來	13-7
當我輸入microsoft 我就自動把bill gate 加進來	13-7
那這個個就是我在做這個term association	13-7
所以呢這個講個就是用term association 來term association association	13-7
來做這個query expansion	13-7
那這個的的的基本精神是說ok	13-7
那你就要有一個term association 的matrix	13-7
一個association matrix	13-7
說明明第i 個term 跟第j 個term 就是是ti 跟t j	13-7
他們的關係到底怎樣	13-7
那你其實有很多方法來算	13-7
那詳細的我想我們這裡不講	13-7
但是你基本上你可以猜的出出來	13-7
哪個term 跟哪個term 會有什麼樣樣的關係你是可以算出他們的關係來的	13-7
有很多種算法那	13-7
那這裡講一種最簡單的算法	13-7
就是假設對我這邊講的假	13-7
假設他們如果出現	13-7
在同一篇文章裡面越多次	13-7
就表示他們關係越高那	13-7
那bill gate 跟這個microsoft 老老是出現在同一篇文章裡面	13-7
他們的關係就高	13-7
microsoft 裡面老是會有software software 那	13-7
那他們的關係就高	13-7
microsoft 裡面老是會出現windows 他們們的關係就高	13-7
那反過來呢microsoft 跟譬如說跟george george bush 有沒有關係	13-7
可能很少	13-7
microsoft 跟賓拉登有沒有關係	13-7
可能根本就不會出現在同一篇文章裡面	13-7
那他們的關係就很低	13-7
所以以呢你最簡單的辦法就是我直接算他們有沒有出現在同一一篇文章裡面的算他們的這個frequency	13-7
那這個例子就是這這樣做	13-7
那裡面的我現在是如果算term i 跟term j 的話	13-7
就是t i 跟t j	13-7
那麼第i 個跟第j j 個他們的關係如何呢	13-7
就數他們出現在一起的次數	13-7
那麼fi 就是這個term i term t i 出現的文章的總數	13-7
f j 呢就是term t j 文章出現的總數	13-7
那麼f i j 呢就就是i 跟j 同時時出現在文章裡面的文章的總數	13-7
那如果這樣一算的話這個就是這樣的意意思	13-7
他就是個介於一到零之間的值	13-7
你看什麼時候是一	13-7
如果i 跟j 永遠出現在一起	13-7
譬如說bill gate 跟microsoft	13-7
假設他們永遠出現在一起	13-7
任任何一篇有了microsoft 就有bill gate 有了bill gate 就有microsoft 的話	13-7
那麼他們在總共有幾篇呢有一百篇	13-7
那fi 就是bill gate 一百篇	13-7
f j 是microsoft 也也是一百篇	13-7
f i j 是這兩個同時出現也也是一百篇	13-7
如果這樣的話呢最後這個是一百這個也是是一百	13-7
除起來就是一	13-7
所以如果這兩個永遠出現在在同一篇文章裡面的話	13-7
就得到一	13-7
反過來如果一個是microsoft 另外一個是bill gate 另外一個是賓拉登	13-7
好這這回完全沒有出現在一起	13-7
那這這個是零這個值就是零	13-7
所以他在介介於一跟零之間	13-7
那你有有了這個值之後你就可以做像上上面這類似的事情	13-7
不過我不用這個方法做我就直接把query 直接做expansion	13-7
就是我這個query 就改一一改	13-7
我凡是以現在輸入的是這個term 這個query 裡面就是在講這個	13-7
user 只有輸入這個字	13-7
那這這跟term 跟誰有關的	13-7
他跟這個的關係比較多	13-7
他跟這個的關係比較多我就把這這邊也加一點分數進來	13-7
乘某一個weight 的比例	13-7
這個weight 跟他們之間的這個相相關關相關度有關嘛	13-7
嘛如果他跟他關係比較大	13-7
我乘一個比較大的weight 放進來	13-7
那關係係比較小我乘一個個比較小的weight 放進來等等我就把一個相關相關的東西放進來我我就得到一個比較好的一個query	13-7
那就是query expansion	13-7
by term association	13-7
那把這些東西都做進來之後	13-7
那基本本上vector space model 是一個最簡單的model	13-7
那效果也也不錯	13-7
啊那麼你基本上來講你如果自己做個寫個程式來做這件事情的話你	13-7
你只要做這個效果就不錯了	13-7
那當然要真的要做的非常好像google 那樣	13-7
那有很多學問	13-7
那像google 那樣他得把全球球所有的網網頁	13-7
他要在全球各地設sever	13-7
把全球的網頁都都把他index 進來通通弄好之後	13-7
我可以瞬間找全球的東西	13-7
那當然那就很很有學問	13-7
但基本上你如果果光是講這個基本原理	13-7
其實是很簡單他只是這樣的而已	13-7
好那底下我們要講的是中文阿	13-8
阿那麼其實我底下要講的這一段就是我們在開學第一天給你看那那個demo 的時候	13-8
我們那個demo 系統怎麼做的	13-8
那基本上來講呢這個喔你記得我們那個時候就是喔我只要說我要找以	13-8
以色列阿拉法特	13-8
所有的跟以色列阿拉法特相關的的全部都可以出來	13-8
喔我只要找這個sars 疫情	13-8
喔所有跟跟sars 那些就可以出來	13-8
來那事實上效果是相當不錯的那	13-8
那個怎麼做的其實是喔我們就講在在底下這一段	13-8
段基本上主要的就是用syllable	13-8
阿那在中文而言我們最好的單位顯然是像這種subword unit	13-8
那是什麼呢是syllable	13-8
那最最好是幾個syllable 連在一起	13-8
就像呂秀秀蓮阿等等	13-8
那這樣用幾個syllable 幾個syllable 連在一起的就像這裡幾個phone 連在一起一樣	13-8
用這種東西來做的話呢你你就可以得到類似的效果就是我們這裡所講的東西ok	13-8
我們先停在這裡休息十分鐘	13-8
我們接下去講中文的怎麼做的那中文的怎麼做的我們要先說就是	13-8
中文其實本本身的喔文字的的information retrieval 也不一樣的	13-8
為什麼麼呢因為中文沒有word	13-8
中文是中文的文字是一堆字	13-8
那麼你並不知道哪裡是一個詞	13-8
你需要去斷詞才知道說OK	13-8
這是一個三字詞這是一個兩字字詞這是一個四字詞	13-8
這是一個單字詞	13-8
這是一個三等等你	13-8
你你得斷出來才知道哪裡是word	13-8
那麼因此呢你在做比對的時候	13-8
即使是即使是文字的文字的的IR	13-8
也文字的information retrieval	13-8
像我我們這邊所說的這一些	13-8
他也有它的需要克服的問題	13-8
因為在英文裡面你可以用這些word	13-8
用這些word 或者兩個word 三個word Bill Gates 要連在一起的時候才是一個是一個term	13-8
啊你把兩三個個word 連在一起他就代表比較怎麼樣的意思	13-8
september eleven	13-8
你要把這這兩個連在一起才代表某個意義等等	13-8
這個在中文的困難是是說	13-8
你現在根本本不知道詞在哪裡	13-8
然後你的斷詞不見得會對	13-8
那麼然後你可能重要的keyword 也一樣是OOV	13-8
所以根本就斷不出來阿	13-8
那我們這邊舉幾個例子	13-8
就是說其實那你怎麼辦那你直接比對字好不好	13-8
好直接比對字也有困難為	13-8
為什麼直接比對字也有困難因為我們辭的結構是非非常flexible 的	13-8
我們舉例來講你如果要找李登輝	13-8
但是在文章裡面他可能是李前總統豋輝	13-8
那它的這個李跟登輝中間拆得很遠	13-8
那你得知道拆的這麼遠其實還是同一個	13-8
你得知道這這個李跟這個登輝連起來就是你要找的李登輝	13-8
那另外呢我們在中文裡面是是很長的詞自動縮短成為短詞	13-8
選哪些字呢	13-8
選最有意義的代表表性的字	13-8
而這是人自己做的	13-8
不需要解釋	13-8
所以說譬如說北部第二高速公路那	13-8
那這裡面呢就用了一個北跟一個二一個高	13-8
這三個字最清楚代表這裡面的意思	13-8
所以就得到北二高	13-8
那麼因此你如果user 輸入我要找北二高的路況	13-8
那網站上是說北部第二高速公路目前車多	13-8
那你要知道那個北部第二高速公路就是這邊的北二高	13-8
那事實上呢那那那這三個字憑什麼是從這裡面挑這兩這三個字出來呢	13-8
那另外呢在中文裡面很多詞中間換掉一兩個字其實意思還並沒有改改變	13-8
那麼因此你輸入的是這個我要找的是這個	13-8
其實可能文章裡面是講那個	13-8
那是要你要得知道得找得到	13-8
那當然這個這個其他語言翻譯過來的詞就更難了	13-8
我的字可能都不一樣	13-8
我所以我要做字的比對也會出問題等等	13-8
那麼因此呢在在這個中文裡面	13-8
其實要做這些事情你你要斷詞你可能斷不對	13-8
然後可能會這這個搞不好	13-8
就算你那你就就直接比對字的話呢也也是有問題所以他有它的難度	13-8
那當然斷詞本身是一個重大的問題	13-8
那這是一個我們們常舉的例子	13-8
就說你如果有一個人要研究腦科他上網去找腦科	13-8
結果找到一大堆腦科	13-8
但是其實這些腦科都不是腦科	13-8
是電腦科學	13-8
那麼你問題就是其實你這個斷詞的時候你你沒有做好斷詞的關係	13-8
那有一個人他要寫一篇文章關於台灣的民間信仰土地公	13-8
他就上網去找土地公	13-8
結果找到一大堆	13-8
怎麼有這麼多土地公呢	13-8
而且這些土地公都有政策	13-8
策那然後你就知道喔不是的	13-8
其實它是因為是他斷詞應該是在這裡斷開來的	13-8
那這是中文的斷詞的問題	13-8
那這邊講的都還沒有包括語音	13-8
這光是文字的時候	13-8
所以中文的google	13-8
那它裡面有一堆中文的技術不是光是用英文的方法就可以做的	13-8
因為有這堆問題喔	13-8
所以呢那這個個有一堆問題有有有一堆方法去做的	13-8
那麼如果再加上語音的話	13-8
第一個問題就是語音辨識會錯嘛	13-8
所以語音辨識有一堆error	13-8
deletion or SUBSTITUTION insertion 的各種error 然後有OOV 嘛	13-8
那中文的OOV 是出奇的多	13-8
那你知道因為我們可以隨便湊成詞嘛隨便湊隨便湊成詞嘛	13-8
你譬如說這個終統	13-8
或者是個廢統阿	13-8
還是一個什麼你這個中文是因為英文每個字都是有意義的所以我們很容易把任任意幾個字兜起來變成我們所要的一個詞所以他的OOV 是 中文的OOV rate 是特別的高的	13-8
那麼而通常我們retrieval 你要找的那些個key phrase	13-8
常常就是OOV 嘛	13-8
那我要找一個這個賓拉登相關的	13-8
我要找一個這個這個喔這個這個新光三越	13-8
或者是我要找一個什麼鼎泰豐喔	13-8
這個都是OOV 嘛	13-8
所以呢這個是中中文裡裡面的很多困難的問題	13-8
那怎麼解決這個問題呢那我們後來發現最直接而單純的辦法就是用syllable	13-9
那麼為什麼是syllable 呢	13-9
那就跟我們之前講這個syllable 跟他們講的的phone 是很像	13-9
其實一個syllable 就是兩三個phone sequence 好	13-9
但是因為中文是一個每一個syllable 是一個字嘛	13-9
所以用syllable 是有很多好處的	13-9
但是呢不是光靠syllable 而是用很多層的syllable	13-9
這一堆我們叫做做這個overlapping syllable segment with length N	13-9
N 等於一的時候單一的syllable	13-9
譬如說這假設我有我辨辨識出來它的他的文章	13-9
或者是你的query 裡面	13-9
有這十個十個syllable 的話	13-9
那麼N 等於一就是每一個syllable 各自都是一個term	13-9
那麼你譬如說我今天如果如果我要找李登輝	13-9
那至少你在那裡有李有登有輝嘛	13-9
你如如果把每一個單一的syllable 都拿出來做indexing term 的話	13-9
至少有李有登有輝跟你這個是像的嘛	13-9
阿你就不一定要把它連起來看的話	13-9
單一的syllable 是有道理的	13-9
同樣呢你這裡北北部第二至少有北有二有高嗎阿	13-9
所以呢你這個這個我寫在這裡就是說	13-9
這個所有的word 都是compose by syllables	13-9
所以他基本上你如果用單一的syllable	13-9
至少你的OOV 都在裡面	13-9
那麼就像譬如說你你也許巴塞隆納翻成成巴瑟隆那翻成這樣子	13-9
但你如果用syllable 來看的話至少對了三個嘛	13-9
這三個syllable 是一樣的嘛	13-9
那那這裡的話至少syllable 會像嘛	13-9
那那所以呢你單一的syllable 是有這個好處的	13-9
我們說這個這個你你所有word 都是是syllable 所構成的	13-9
通常呢如果相關的word	13-9
常常會有一些syllable 是一樣的	13-9
那那所謂相關的word 會有一些syllable 是一樣的就像我們這邊的例子	13-9
就是像譬如說這個跟這個	13-9
他們是兩個不同的word	13-9
但是是耶	13-9
他們就是有一些syllable 是一樣的	13-9
就像這個跟這個	13-9
耶他就是有一些SYLLABLE 是一樣的	13-9
所以單一syllable 是有這個好處的	13-10
可是單一SYLLABLE 有個最大的問題就是造成ambiguity	13-10
因為很多同音字嘛	13-10
每一個syllable 有一大堆同音字	13-10
所以你如果光靠單一syllable 的話是沒用的	13-10
的因為你說我這邊有李有登有輝所以我可以找到李登輝嗎	13-10
但是登有一大堆堆登阿	13-10
輝有一大堆輝阿所以你會找出一大堆其他的什麼登什麼登	13-10
然後輝什麼輝什麼	13-10
那麼因此呢你不能光靠單一syllable	13-10
那怎麼辦呢	13-10
那就兩個syllable 啊	13-10
啊所以兩個syllable 就進來啦	13-10
就是譬如說這個一二跟二三跟三四	13-10
那就變成就好像	13-10
呂秀蓮	13-10
秀蓮副啊	13-10
蓮副總	13-10
副總統阿	13-10
這是兩個兩個或者三個三個	13-10
你如果是是兩個兩個就是呂秀秀秀蓮蓮副副總	13-10
總統	13-10
你如果兩兩這樣去抓的話兩兩做為一個term 的時候	13-10
那可以抓到一堆東西	13-10
那我你如果看這裡的話	13-10
譬如說你這邊可以抓到登輝	13-10
有登輝這兩個連在一起的話	13-10
那八成就是這個李登輝啦	13-10
啊你有二高抓到的話這個就是二高啦	13-10
所以呢你你如果兩兩兩連在一起的話實際上是是會抓到一堆的	13-10
那當然你也會抓到一堆error	13-10
抓到一堆noise	13-10
譬如說蓮副這個不曉得是什麼	13-10
副總這個不曉得副總你可能變成另外一堆副總總去了	13-10
等等	13-10
你會抓到一堆error 的那	13-10
那麼因此呢那你還有別的嘛	13-10
那麼因此呢就是我還有N 等於三	13-10
N 等於三就三個	13-10
一二三二三四三四五嘛	13-10
就變成呂秀蓮秀秀蓮副蓮副總副總統	13-10
那你這些東西通通都拿來做嘛好等等	13-10
你可以N 等於一等於二等於三等於四等於五那	13-10
那我們實驗結果是	13-10
N 等於二效果最好	13-10
那原因其實很簡單就是中文裡面最多重要的詞是雙字詞嘛	13-10
雙字詞的比例最高而且最重要要的詞常常都是雙字詞	13-10
所以N 等於二效果最好	13-10
但是光靠N 等於二是不夠的	13-10
N 等於於一是有幫助的	13-10
N 等於三也是有幫助的阿	13-10
他們都有助於你要把它們通通用進來	13-10
那用進來的方法就是我們這邊邊講的剛才的這個weight 嘛	13-10
你N 等於二有一個內積N 等於一也有一個內積N 等於三也有一個等等	13-10
那分別用不同的weight 加起來	13-10
這個就是我們這邊講的你overlapping syllable segment with length N	13-10
那這個時候呢就有一些poly selected word	13-10
就是就是不不只不只是單音的而是雙音三三音四音的詞	13-10
你都可以抓的到	13-10
然後呢中文詞裡面最多的是雙字詞所以雙音呢是最有效的	13-10
那麼你如果抓的的到的話譬如說副副總統	13-10
有了副總統這三個音的時候大概不是別的東西就是副總統	13-10
因為就是說你如果是這個多音節的word 的話	13-10
基本上幾乎就是同幾乎就是抓得到了	13-10
雙音還不一定如果超過雙音的話好	13-10
呂秀蓮你有這三個音的話	13-10
大概就是這個呂秀蓮會找到別人的機會不大等等	13-10
雙音還是會有很多	13-10
譬如說你知道譬如說香蕉跟這個相交	13-10
所以如果說雙音可能還是很多同音的	13-10
但是三音以上幾乎就是你要的	13-10
那這些呢就構成我們這邊講的這一系列就	13-10
就是overlapping syllable segment with length length N	13-10
N 等於一等於二等於三等於四五	13-10
那我們基本上是效果最好的是N 等於二	13-10
然後你可以加上一加上上三都可以加分	13-10
你只要weight 好	13-10
加上四跟五的時候太多noise 了	13-10
其實沒有什麼用了	13-10
加上四跟五大概不會再好多少	13-10
所以最主要是一二三	13-10
那還有另外一組呢就是syllable pair SEPARATE by M	13-10
就是你跳一個syllable	13-10
兩兩兩算	13-10
跳中間的一個	13-10
個譬如說中間如果M 等於一的話你就一三二四	13-10
一三二四三五的跳一個	13-10
跳兩個的話呢你就是一四二五這樣子	13-10
跳兩個跳一個跳兩個是幹麻的呢	13-10
那你可以想像的到	13-10
我這邊有一堆這種事情他就是跳了	13-10
譬如說北二高這是北部第二高	13-10
這個就是跳的	13-10
那同樣你這個這個中間這個syllable 不對	13-10
那你這邊跳出來是對的嘛	13-10
好你這邊可能這個賽這個是塞這兩個個不一樣	13-10
可是這邊可能跳過來就是對的嘛	13-10
所以你跳一跳之後事實上克服一些困難那	13-10
那同時呢那還有一個好好處就是說	13-10
我們的辨識裡面可能有錯	13-10
那譬如說你你如果有deletion 有insertion	13-10
這時候你錯錯掉一個的話你跳的也算的話反而會對	13-10
呂秀蓮你萬一這個秀字沒有辨識對	13-10
呂跟蓮你如果那邊可以抓到呂跟連搞不好就是同一個嘛所	13-10
所以這個是有幫助	13-10
那這個我們的實驗結果是這這個M 等於一是效果再加上來來可以增是加分的	13-10
M 等於二也可以再加一點分	13-10
M 等於三以上就大概加不上去了	13-10
那用了這一堆feature 之後	13-10
那這一些feature 的理由就我剛才講就是說在這一些裡面	13-10
那就有幫助了	13-10
可是這樣呢還是有很多的問題	13-11
一個最大的問題就是中文的syllable 本身是很難辨識的	13-11
因為中文的的syllable 的錯誤率非常高	13-11
他們的con 非常confused	13-11
為什麼呢你馬上想想到嘛	13-11
譬如說八跟搭是像的	13-11
八跟搭我耳朵聽都聽不清楚	13-11
那他跟他很像的是趴跟這個塌	13-11
那還有咖還有這個什麼喔這個很多很多那	13-11
那這一大堆其實都很像	13-11
那麼我我如果光是辨識這些syllable 的話呢其實錯誤是錯誤率是非常高的	13-11
那有阿同樣別的音也是一樣	13-11
譬如說晡跟督鋪跟禿跟哭的	13-11
都是非常像的	13-11
逼跟低	13-11
批跟踢跟key	13-11
阿都是非常像的	13-11
阿那你這一堆東西都很像	13-11
所以其實我們syllable 的正確率是不高的	13-11
syllable 正確率不高可是我們為什麼可以辨識對	13-11
是因為詞	13-11
那也就是說呢你雖然這個八很難辨識	13-11
可是你如果有一個巴比倫	13-11
那就是巴比倫嘛對不對	13-11
你你就自動雖然說這個八跟這個搭跟這個趴很像	13-11
但是這個詞呢你有了這些東西連起來那就是巴比倫不會是別的	13-11
所以所以你你這個時候其實我們們中中文的這些個syllable 單獨去辨識syllable 正確確率是不會高的	13-11
因為他們有很多問題	13-11
可是呢那我怎麼會對呢我是要是靠詞的	13-11
那也就是說我們在八點零那裡面所說的那個key lexicon	13-11
我的那個lexicon	13-11
我把lexicon 變成這個tree 之後	13-11
tree lexicon	13-11
那這樣的話呢我其實就自動自動的限制哪一些syllable 不會不會出現喔	13-11
這樣的話呢我才我可以得到很好的結果	13-11
可是這回不對啦我這回變成說是我要要抓syllable	13-11
對不對	13-11
我現在要用syllable 來做了	13-11
那syllable 這裡面錯很多怎麼辦呢	13-11
這如果錯很多的話就不對啦	13-11
那所以呢這個辦法就是這邊講的我用syllable LATTICE	13-11
用很多這個multiple syllable hypothesis	13-11
譬如說你現在辨識的這一串syllable	13-11
我每一個syllable 都把前五名放進來	13-11
因為它很可能不對	13-11
前五名裡面比較可能是有一個對的	13-11
就譬如說這五個這五個很像那	13-11
那我可能弄錯我選的第一名可能不對第三名才對	13-11
所以但是我如果選個前五名大概是正確就在裡面阿	13-11
所以我就可以把每一個syllable 我不是只取第一名	13-11
而是取譬如說前五名或者前十名	13-11
等等	13-11
那這樣我就變成一個這個syllable LATTICE	13-11
syllable align 的LATTICE	13-11
也就是說我其實就是align 好的	13-11
那可是如果這麼一來的話這個太多了嘛	13-11
那於是你譬如說兩個syllable 你這兩個也可以這兩個也可以這兩個也可以這兩個也可可以	13-11
這樣馬上就有這五個的話就有就有二十五種雙音的這個的太多了	13-11
那怎麼辦呢	13-11
那麼我們做syllable level 的utterance verification	13-11
這是我們十點零裡面講的東西	13-11
不過基本上意思就是說你可以verify 一次	13-11
就像我們之前講的confidence measure 一樣	13-11
你這邊辨識前十前五名或者前十名	13-11
你每一個都有一個分數	13-11
就是我的confidence 的分數	13-11
那如果分數really 低的話我就可把它拿掉	13-11
所以呢我可以有一個用一個這個喔我就可以把這個confidence score 低的東西先拿掉	13-11
所以雖然說我每一個都取前五名	13-11
譬如說我這裡畫成我每一個取前五名	13-11
但是呢我把它的這個分數低的先拿掉	13-11
所以藍色的都拿掉	13-11
於是剩下要考慮的就少了	13-11
那在這個剩下的裡面呢	13-11
我才來算他雙音哪什麼什麼的	13-11
所以呢那這個就是	13-11
我建lattice 把可能的前五名都放進來	13-11
但同時我做了verification	13-11
根據我的confidence score	13-11
把不可能的先拿掉	13-11
那之後呢我還可以多拿掉一點	13-11
拿掉哪些呢	13-11
譬如說我可以用文字的語料去train	13-11
你就會發現不是所有的雙音都會會出現	13-11
我們的syllable 有我們的syllable 有一千三百個	13-11
那雙音的話一千三百的平方	13-11
不是所有這麼多都會出現	13-11
那有些雙音是不會出現的	13-11
好因此那種就是所謂的低頻的term	13-11
那那些低頻的term 我們可以先拿掉	13-11
就知道那應該是錯的嘛	13-11
那那譬如說這個是啊這個是這個低頻的	13-11
就是低頻的term 那麼就是低低頻的雙音	13-11
那這些如果果這個我用我用一個大的語料庫train 出來都都知道他們這這個雙音不太會出現的話	13-11
那這個顯然應該裡面有有錯	13-11
我就可以把它拿掉	13-11
好那這個是用根據這個文字的語料料去train 的	13-11
把低頻的拿掉	13-11
那還有呢就是把I D F 低的	13-11
那是特別高頻的	13-11
譬如說這個這個I D F 就是我們之前講的	13-11
這個I D F	13-11
那這個這個這個如果分數低的話表示他這個term 沒有什麼意義	13-11
譬如說非常	13-11
這個雙音非常沒有什麼意義	13-11
因為它只是一個function word	13-11
那麼在每一篇文章裡面都可以有這個東西	13-11
他不代表什麼好	13-11
那你譬如說這個今天	13-11
這可能也不代表什麼	13-11
那這個也是你在每一篇文章裡面都可以有的	13-11
那這種就是I D F 非常低的word	13-11
那非常低的雙音	13-11
那我可以把這一些個這個I D F 非常低的那些個雙音我也拿掉	13-11
阿像這些	13-11
當我這些都拿掉之後我剩下這些就是reliable 的	13-11
而且正確的應該在裡面的	13-11
那用這個來做就會好很多等等	13-11
好那這個大概講了我們中文怎麼做的	13-11
痾你如果有興趣的話詳細的寫再蔗一篇裡面就是這個第二篇喔	13-11
這篇其實是非常完整的paper	13-11
裡面講的非常清楚阿裡面每一步怎麼做我們都講的很很清楚	13-11
你如果自己寫程式你even 可以做出這個來	13-11
這個是作得到這	13-11
這個都寫的非常清楚這篇是保證很好看的阿	13-11
這是講中文怎麼做的	13-11
那麼再下來的話呢我們來講一下其他的	13-12
剛才基本上是用這個喔講的是vector space model	13-12
但是當然我可以不用VECTOR space	13-12
那可以用別的	13-12
那第一個呢我用h m m	13-12
那麼用h m m 的話呢喔其實我們中文也一樣	13-12
也可以用syllable 來做這個h m m	13-12
那這個是怎樣呢	13-12
基本上就是我現在把user 的query	13-12
看成一個SEQUENCE of input observation	13-12
然後呢我每一篇文章看成是一個h m m	13-12
那你記得我們原來h m m 是幹麻的	13-12
譬如說我每一個我要辨識零到到九	13-12
我零有一個h m m	13-12
一有一個h m m	13-12
到九有一個h m m	13-12
然後呢這個聲音進來的是八	13-12
那這個聲音我把它放到每一個h m m 裡面去看他的分數	13-12
喔結果如果這個聲音是八的話呢	13-12
那個八的那個字的那個h m m 分數就會高	13-12
這個是我們原來h m m 的作法	13-12
這個h m m 是非常有用的東西我們可以拿來做很多別的事	13-12
譬如說在這裡	13-12
那我現在變成不是這樣子了	13-12
我現在是每一篇文章是一個h m m	13-12
每一個文件譬如說這個文件是一個h m m	13-12
這個文件是一個h m m	13-12
這個文件每一篇文件都是一個h m m	13-12
那user 的query 是一個observation	13-12
好所以呢user query Q	13-12
那裡面是一堆term T one T two 到T N	13-12
那這個是observation	13-12
那我現在要把這個observation 放進每一個h m m 裡面去看他的分數	13-12
跟那個完全一樣	13-12
那你這個user 的query 看成一堆term	13-12
那就是這邊的意思嘛	13-12
你可以用譬如說word	13-12
你可以用word	13-12
你可以用用phone	13-12
用phone sequence	13-12
這些東西就可以當成你可以所以在query 裡面找他的words	13-12
或找他的phone sequence 什麼什麼的這些當成他的term	13-12
然後呢那你用這個query 就變成一串term	13-12
那麼這串term 這一串的term 你把它放到這些h m m 裡面去	13-12
看他的分數	13-12
那這裡面有一點不同的地方	13-12
就是那在這裡做的時候	13-12
到目前為止他們用這個方法來做用h m m 來做這個retrieval 的話呢	13-12
大概每個h m m 都只有一個state 啊	13-12
應該說還沒有人想出來兩個state 怎麼做	13-12
所以都是一個state	13-12
所以呢在這上面的時候呢每一個有好多個state	13-12
那這邊呢每一個都	13-12
只有一個state	13-12
那這一個state 的時候呢這個放進去算他的機率怎麼算	13-12
我們在這裡的時候是怎麼算的呢	13-12
這個這個state 裡面機率呢我們把它變成一堆Gaussian 對不對	13-12
變成OK 這有一個Gaussian 這有一個Gaussian	13-12
我把它想像成是一堆Gaussian 的組合	13-12
那麼於是呢我用一堆Gaussian 來model	13-12
於是呢譬如說呢這個state 我就把它看成是這樣的一個一堆Gaussian	13-12
於是呢我這個observation 可以放到這個裡面去看	13-12
算他的分數	13-12
那我現在這個怎麼辦呢	13-12
這裡面是是這個喔phone 或者是word	13-12
或者是什麼那種syllable 這種東西的話呢	13-12
我怎麼辦呢	13-12
我這邊沒有辦法當成什麼Gaussian	13-12
但是我這可以什麼呢用N gram	13-12
所以呢我這個變成是用N gram	13-12
用N gram 來做成這個機率	13-12
所以這個方法這個這個妳基本上來看	13-12
他的整個的觀念跟上面是完全一樣的	13-12
只是呢其實我們剛好全部都反過來	13-12
你原來的這些個h m m 裡面的這每一個feature 都是acoustic signal	13-12
我現在的每一個feature 都是linguistic 的unit	13-12
都word 還是term 還是phone	13-12
都是DISCRETE	13-12
不像這邊是continue 的signal 的	13-12
原來這些feature 是acoustic signal	13-12
這邊都變成linguistic term	13-12
那原來這裡面用的我的N gram	13-12
在這裡跑跑到這個機率裡面來了	13-12
所以我的N gram 原來在這裡面的時候是來算那些個term 之間的關係的	13-12
我現在的N gram 變成h m m 裡面的東西了好	13-12
所以這個是倒過來	13-12
但是事實上所有的觀念都一樣	13-12
因此呢我現在可以用這篇文章我可以算這篇文章的N gram	13-12
於是這個於是這些term 進入這個N gram 就可以算他的N gram 機率	13-12
uni gram 就是這一個一個的機率	13-12
bi gram 就是兩兩相連的機率等等	13-12
我就可以算這些這個N gram 就當成這些東西阿等等	13-12
那基本觀念就是這樣	13-12
那就是這邊所說的就是說我把query Q 看成是一個一個sequence of of observation	13-12
也就是一堆term	13-12
然後呢每一個document D 呢看成是一個h m m	13-12
只是我現在H h m m 都只有一個state	13-12
那麼沒有人還沒有人想出來兩個以上state 會怎樣就是了	13-12
然後呢那這個h m m 裡面那一個state 裡面我沒辦法做那什麼Gaussian 什麼沒有這這種Gaussian 那怎麼辦呢	13-12
就是N gram	13-12
所以呢就	13-12
就是由N gram 來兜兜成的	13-12
在這裡面所舉的例子就是用uni gram 跟bi gram 喔	13-12
這個是用用這篇文章的用這篇文章的文的文字可以train 出他的uni gram 跟他他的bi gram	13-12
這個就是他的uni gram 跟他的bi gram	13-12
我就是用這篇文章train 的	13-12
那就是這個uni gram 跟這個bi gram	13-12
但是通常因為這一篇文章	13-12
文字量實在是很少	13-12
你train 出來的uni gram 跟bi gram 可能不夠好所以呢	13-12
你再加一個這個是用一個比較大用一個大的corpus	13-12
train 的比較標準的uni gram	13-12
跟bi gram 加在一起喔	13-12
所以呢我再加一個用這個用一個大個corpus 所train 的一個uni gram 跟一個bi gram	13-12
那就是後面這是given C 的這個就是這個大的corpus 所train 的uni gram 跟bi gram	13-12
等等你也可以有有有這個tri gram 什麼也可以	13-12
那其實你看uni gram 是什麼uni gram 也就相當於會不會出現某個term 的意思嘛	13-12
所以uni uni gram	13-12
gram 其實也就是在算這邊的每一個term 他出現的count 嘛	13-12
是一樣的	13-12
那bi gram 就等於是算他的sequence	13-12
對不對兩個term 連在一起出現的這個count	13-12
其實是一樣的東西	13-12
那如果是這樣的話呢我們以這個為例的話	13-12
我們這邊就是用了用了用了兩個uni gram	13-12
這個是這個文件自己的	13-12
這個是用一個corpus train 的	13-12
兩個bi gram	13-12
這個是文件自己的	13-12
然後呢這是corpus train 的	13-12
那我就變成這四個機率相加	13-12
各有一個weight	13-12
M one M two 這是他的weight	13-12
這四個機率相加這四個四個N gram	13-12
就好比這邊有四個Gaussian 是一樣的意思	13-12
我現在是變成是用N gram 來做就是了	13-12
那如果這樣的話我的retrieval 我也可以用M A P	13-12
那這個意思這個就是M A P 的意思	13-12
就是說我given 一個OBSERVATION	13-12
我的Q	13-12
Q 就是user 的輸入嘛	13-12
就跟user 的輸入聲音是一樣的	13-12
我user given given一個user Q	13-12
那我就可以去算說某一篇文章是relevant	13-12
這個意思R 就是relevant 的意思就是相關的	13-12
那麼那麼他的機率是多少	13-12
然後呢我對所有的每一篇文章都去算	13-12
那如果這樣的話呢我看哪一個最大	13-12
最大就是我的答案	13-12
這個就是MAP 嘛	13-12
所以呢我就可以用這個方式given 這個Q 咳	13-12
given 這個Q	13-12
那我就可以算每一篇文章的相的的可能的機率	13-12
那當然我不一定要只選只選最最大那一個	13-12
就這裡而言 我們可以選最大的一百篇	13-12
或者最大的五十篇等等	13-12
那這個就是這個就是我們平常講的這個retrieval 的方法	13-12
可以這樣子來作	13-12
那當你變成這個的時候呢一樣的就是MAP 的這個這個機機率我不會算	13-12
我把它倒過來	13-12
所以呢我把它倒過來變成	13-12
如果是講這篇文章的話那麼會看到這個Q 的機率	13-12
如果是這篇文章的話	13-12
我有這個N gram 在這裡	13-12
given 這個N gram	13-12
我會看到這個Q 的機率是多少	13-12
所以我就是算這個Q 用這個N gram 來算機率	13-12
就跟把它放到這裡面來算機率是一樣的	13-12
所以呢我只要倒過來之後我就可以算	13-12
given 這個N gram	13-12
那麼這個Q 的機率	13-12
那應該還要乘上這個機率	13-12
喔這個只是我們這個MAP 裡面你知道MAP 我們每次把它倒過來	13-12
然後其實應該還要除上一個機率不過那個機率不算	13-12
反正是一樣的所以可以不算	13-12
但是現在是這個機率也無法算	13-12
這個機率因為不知道是什麼	13-12
所以我們也不算就算這一個	13-12
如果算這個的話呢	13-12
那這個就變成MAXIMUM likelihood	13-12
因為這是一個likelihood function	13-12
把這個倒過來了	13-12
這個是MAP	13-12
這個變成MAXIMUM likelihood	13-12
當你變成這樣之後呢	13-12
那其實就是那這個其實就就是把那個Q 放到這個個D 裡面去	13-12
把這個Q 放到這個D 裡面去放到這個N gram 你算這個N gram 的意思	13-12
當你算他的N gram 那就是底下這個式子	13-12
底下這個式子沒什麼特別	13-12
就是在算uni gram 跟bi gram	13-12
也就是說我現在如果我現在如果是這個痾T one T two T 三T 四	13-12
的時候到T N	13-12
這是我的user query	13-12
變成一串term 的時候	13-12
那我第一步呢第一個要算他們這這個T one 的uni gram	13-12
那就是這一個這T one 的uni gram	13-12
他在這篇文章裡面自己的uni gram	13-12
以及在一個比較大的corpus 所train 的那一個uni gram	13-12
然後從二以後呢我前面這兩個是uni gram 這兩個是bi gram	13-12
二開始始呢我也算他的uni gram	13-12
我也算他的喔他的bi gram	13-12
也就是前面這個來的	13-12
所以呢二的時候我有他的uni gram	13-12
有他的bi gram	13-12
那那就是當N 等於二的時候	13-12
這兩個就是uni gram 然後這兩個就是bi gram 喔	13-12
那這是我的document 這是這篇文章自己的	13-12
這個是我整個的corpus 的bi gram	13-12
然後三的時候呢那當然三也有他的uni gram	13-12
也有他的bi gram 喔等等	13-12
那你可以再加tri gram 等等再一直加到N 就是了	13-12
那你這些個這四個N gram	13-12
你有他的weight	13-12
那這個weight 可是可以train 出來的	13-12
你用EM 也可以train 你用M C E 也可以train 喔	13-12
那當然要train 你要有一個標準答案	13-12
就是說這些是query 這些是找到哪些文章	13-12
那有個標準答案你就可以train 這些東西	13-12
那這些都都可以train 出來	13-12
那這就是用h m m 來做	13-12
那我們之前講的用syllable 一樣可以做這件事	13-12
我們都做過用syllable 來一樣建這個這個model	13-12
那這時候就是我每一個這裡的每一個term 變成一個單一的syllable	13-12
雙音兩個syllable 三個syllable 就	13-12
就在這裡	13-12
或者跳音都在這裡弄	13-12
那這樣的話呢我也可以做用syllable 來做這個這個h m m 的retrieval	13-12
那再來呢這個呢其實就是我們第十二點零講的latent SEMANTIC	13-13
那這一塊其實跟我們十二點零講的沒什麼不同是一樣的	13-13
那本來十二點零的那一招就是當初人家發明他的時候就是為了做這件事情用的	13-13
那那他的point 在哪裡呢	13-13
它主要的point 應該是這是所謂的concept matching 以別於所謂的term matching	13-13
這個這個意思是說就是我們之前講的	13-13
因為user 通常是懶惰的	13-13
user 只會講一兩個term	13-13
他要找的東西可能不是那一兩個term 所能描述的	13-13
就好像user 他就他就輸入一個Bill Gate	13-13
輸入一個Bill Gate	13-13
但是事實上他要找的東西裡面可能包含他要找的可能包包括microsoft	13-13
包括windows	13-13
包括這個什麼dot net	13-13
這個等等等等他有很多	13-13
甚至於有intel 啊什麼什麼	13-13
user 要找其實是很多這些東西	13-13
但是呢他可能只輸入一個Bill Gate	13-13
如果他只輸入個Bill Gate 的話你那文章裡面必須要有Bill Gate	13-13
否則就找不到	13-13
那這個就是所謂的term matching	13-13
因為我們之前講的都是這樣做的	13-13
你如果看這個的話	13-13
其實是在match 這個term	13-13
所以你如果沒有講到他的話	13-13
你沒有輸入那個term 他就是沒有嘛	13-13
你如果用前面的這些個都一樣	13-13
這些都是在term 做matching	13-13
包括vector space 這些都是用term 在做match	13-13
都是這個所謂的term matching	13-13
但是你現在如果改用改用LSA 來來來做的話	13-13
就是我們這邊講的所謂的latent SEMANTIC indexing	13-13
那這個其實就是我們十二點零所說的那一招	13-13
那也就是說我們十二點零的時候呢	13-13
我們把它把它變成這個這個這個term 跟document matrix 之後	13-13
你記得我們做了這個SINGULAR value decomposition 之後	13-13
轉成一個譬如說八百維的空間	13-13
在這裡面的每一維變成concept 了	13-13
所以這個時候當你每一每一維變成concept 之後	13-13
我現在其實在做的事情是把這些文章都轉到這這個空間來變成concept	13-13
那麼因此你如果記得的話	13-13
我們當時就有譬如說這個我的我的這個fold in	13-13
我可以把user 的query 當成一篇文章	13-13
把它放進來	13-13
於是呢我我把它放到這裡面來之後	13-13
我是在這八百維裡面	13-13
去看它是那一個concept	13-13
那這個時候它裡面會有出現哪些term 就不重要了	13-13
他的那個concept	13-13
很可能就是這一堆所構成的那個concept	13-13
那你裡面講的究竟是講的Bill Gate 還是講的microsoft 其實不重要了	13-13
他都會幫你map 到那個concept 去	13-13
所以呢他就有concept matching	13-13
他等於是針對concept 來做喔	13-13
這個是LSA 最大的好處	13-13
那這是我我們十二點零講的這堆東西	13-13
其實最早是用來做retrieval 之用的	13-13
那只是說這個後來人家拿來做language model 而已	13-13
那麼因此呢我們是可以回過頭來看	13-13
用這個的話	13-13
那就是一樣這個是跟我們十二點零講的完全一樣你就建一個term document matrix	13-13
然後做SINGULAR value decomposition	13-13
然後那你現在怎麼找	13-13
很簡單就你把user 那個query	13-13
把它當成是一篇新的文章把它放進來	13-13
把這個user query 當成一篇新的文章	13-13
把它放進這個空間來	13-13
然後他也有一個vector	13-13
於是呢你拿那個query 的vector 去跟所有的文件的vector 去做內積	13-13
那這樣你就可以找到它跟哪一篇最像	13-13
而這個像是在那個concept 的空間裡面像	13-13
不一定是term 像不像而是concept 像	13-13
所以這樣這個就答案就出來喔	13-13
那這個我想細節我們就不用講	13-13
你大概就知道這個就跟我們那個是完全一樣的	13-13
那麼再下來其實跟這個相關的有另外一堆	13-14
我們待會在說我們先把這個講完	13-14
就是這個keyword	13-14
keyword base 是另外一堆	13-14
那麼基本上呢你可以想像就是	13-14
我們說你如果一堆文字的文字的文件的話	13-14
我要有一套演算法去自動抽keyword	13-14
我如果有辦法自動抽keyword 的話	13-14
我就可以得到一堆keyword set	13-14
有了keyword set 之後我們就可以做keyword spotting	13-14
這個是在十點零裡面我現在還沒有講	13-14
我們盡快講到這個keyword spotting	13-14
那你知道所謂keyword spotting 就是user 講了講了一段話	13-14
我不要辨識整段話	13-14
我只要看它裡面有沒有我的keyword set 裡面的某一個keyword	13-14
我我如果有keyword set 裡面某一個keyword 我把它抓出來	13-14
那就是所謂的keyword spotting	13-14
所以呢我今天如果我有辦法把每一篇文件自動抽他的keyword	13-14
放在這個keyword set 裡面的話呢	13-14
我只要user 輸入什麼話我就去抓裡面的keyword	13-14
我抓得到的話	13-14
我就說他有這個keyword	13-14
那我就可以回去說耶剛才這些裡面是那些文章有這個keyword	13-14
那就找出來了喔	13-14
那這就是keyword base 的方法來做這件事	13-14
那這個自動抽keyword 是基本上是用文件文章的文字的比較容易抽keyword	13-15
你如果文字不容易的話如果是語音的話是比較難	13-15
語音的話呢比較常用的辦法是我去做個recognition	13-15
做了recognition 之後呢變成文字之後再來抽	13-15
但是這個時候OOV 啊什麼已經已經掉了	13-15
所以呢如何如果是語音的文件	13-15
要如何讓裡面的keyword 出來這是蠻有學問的事情	13-15
這可以做我們今天都有技術阿	13-15
但是呢我們不見得有時間來說它	13-15
但是你基本上可以想成就是我得要有好的辦法	13-15
我不是是直接做recognition	13-15
否則的話很可能OOV 全部都丟掉了	13-15
所以我要想辦法讓這裡面的keyword 能夠抓出來等等	13-15
底下這一章是在舉一個例子講	13-15
中文的文字怎麼抽KEYWORD	13-15
那麼中文的文字面跟西方語言最大的不同	13-15
就是你even 不知道詞在哪裡	13-15
那你怎麼知道keyword 在哪裡那	13-15
英文而言的話呢他的keyword 不會太難抽的原因是	13-15
第一個就是你有boundary	13-15
你所有的空白就是boundary	13-15
你已經知道這是一個空格這是一個空格所以這是一個word	13-15
這是一個word 每一word 都define 的很清楚	13-15
就是由這個boundary 所define 的	13-15
然後呢所有的專有名詞都有大寫	13-15
那麼所以呢通常的專有名詞都是大寫	13-15
有大寫的轉有名詞常常是keyword	13-15
所以你先把這個有專有有專有名詞的大寫抓出來啊	13-15
有大寫專有名詞抓出來那你八成那個keyword 已經抓到一堆	13-15
然後你再用什麼TFIDF 那些東西一算的話	13-15
你那keyword 就可以出來	13-15
那中文的問題是說你它是一串字	13-15
你不曉得詞在哪裡	13-15
然後一大堆OOV	13-15
所以你怎麼知道哪裡是一個詞	13-15
然後他該不該是一個keyword	13-15
這就比較難	13-15
那這有很多特別的方法我們這邊並不細說	13-15
這裡舉一個舉一個例子	13-15
那這個例子是說你怎麼知道這幾個word 應該是一個keyword 呢	13-15
你先要看它是不是一個完整個pattern	13-15
要看它是不是一個完整的pattern 都不容易	13-15
那在這個例子而言它是說OK	13-15
譬如說這個是呂秀蓮副總統	13-15
那麼你拿掉最後一個字	13-15
或者拿掉兩個字	13-15
呂秀蓮副總	13-15
或者者呂秀蓮副	13-15
你會發現他不會單獨出現	13-15
他出現的時候一定是在呂秀蓮副總統一起出現的等等	13-15
那這樣就表示應該是這整個才是一個term	13-15
或者是一個word	13-15
而不應該是拿掉一兩個字的等等	13-15
那就是這邊講的就是你這個W S 如果always appear as part of W 的話	13-15
或者你前面拿掉一個字	13-15
呂秀蓮副總統跟秀蓮副總統	13-15
那你沒有這個秀蓮副總統你每次就是一定有呂的	13-15
那這樣的話呢那這個時候呢就應該是一個完整個term	13-15
應該是這整個的等等	13-15
那另外呢你就是你除了這個之外你還要看後面會接什麼前	13-15
前面會接什麼	13-15
那麼也就是說如果它是一個完整的term 的話	13-15
後面應該會接不同的東西	13-15
那前面也會接不同的東西	13-15
那如果說他們後面老是接相同的喔	13-15
喔就是說如果你W 後面永遠接一個固定的東西的話	13-15
那他們可能是連在一起的	13-15
譬如說行政院長游錫坤	13-15
那你你如果發現他老是連在一起的話	13-15
他可能應該是連在一起才是一個term	13-15
那反過來當然會不一樣	13-15
譬如說自動化	13-15
或者說OK 合法化	13-15
合法化的那個化你會發現合法自己常常出現的話	13-15
那麼合法跟化就應該拆開來合法才是一個term	13-15
所以這裡面有一堆這類的問題	13-15
然後你看哪個是個term	13-15
然後你再來看他重不重要好等等	13-15
那這個這是中文的key term 的的keyword 裡面有一些相關這類的問題	13-15
好那那我們這邊先把這個大致先說到這哩	13-15
那這個是我現在這個十四點零的部份	13-15
那跟這個相關的有另外一件事情我們在這裡順便提一下	13-15
就是我們剛才講這個L S A	13-15
用這個十二點零的L S A 的時候我可以做這一類的concept matching	13-15
那跟這個很像的就是加機率	13-15
那這個在我們十二點零的時候我們就提過這件事	13-15
在十二點零的最後一頁的時候	13-15
我們說過就是就是像像這個十二點零都是在講把這些term	13-15
跟這些個文件之間	13-15
想辦法去找中間的那八百個concept	13-15
那其實呢這個方法不算是最好的	13-15
因為他用用一大堆matrix	13-15
但是它沒有機率的沒有太多機率的觀念	13-15
那到後來的時候就有了這個它就有一套全新的formulation 就是所謂的probabilistic L S A P L S A	13-15
那他就重新建一套完全是意思還是很像的我這邊就是所有的文件	13-15
那邊是所有的term term 或者所有的word 或者什麼	13-15
term 啦就是我們這邊講的所所有的term	13-15
或者所有的indexing element	13-15
那麼你中間是什麼	13-15
是你真正的topic	13-15
那你可以建這中間的的機率關係	13-15
因此呢對每一個文件而言呢	13-15
假設我這邊有八百個topic 的話	13-15
我每一個文件呢你可以分析說它是它是在講哪一個topic 的機率	13-15
都有一個機率	13-15
那對每一個topic 而言呢	13-15
每一個term 會出現有他的機率	13-15
那麼於是呢我這個時候某一個文件裡面會出現某一個term	13-15
不是在這裡數他出現幾次的count	13-15
而是算成一個這樣子的機率	13-15
就是這個文件他會講哪一個topic 的機率	13-15
以及在那個topic 裡面這個term 會出現的機率	13-15
然後呢我把所有的topic 加起來把八百個全部加起來	13-15
那這樣的話呢我可以拿來跟我真正數count 的時候	13-15
這個裡面的有多少我來比比看	13-15
那我可以maximize 這個likelihood function	13-15
做為來train E M 的model 的方法	13-15
所以呢我用這個我要maximize 這個東西來用E M 就可以train 出這個model 出來	13-15
那這個東西的的的精神跟我們之前講的第十二點零講的這個這些東西是非常像的	13-15
可是他這個時候呢我就完全用機率方法來model 之後呢	13-15
我就可以用機率的各種方法來做它	13-15
那麼這個效果呢terns out 是比L S A 還要好	13-15
那同樣我這個可以做這裡的我可以做這裡的的的	13-15
這個retrieval 完全可以拿來做嘛	13-15
因為你你現在這個user 輸入一個query	13-15
你也一樣可以把那個query 看成是一堆term	13-15
然後那你也可以因此可以算他是他會在哪一個topic 裡面的機率等等等等因此	13-15
此你的都可以這樣算所以呢這些東西可以完全拿來來做做我們這邊講的retrieval	13-15
那terns out 這個效果比L S A 通常是會好一些	13-15
這個到這裡呢相當於我現在把這個痾十三點零講完	13-15
十三點零我們這邊的這個reference 裡面	13-15
我們剛才講	13-15
因為information retrieval 是一個很大的領域	13-15
那最多的資訊來源應該是A C M 的這個special interest group on information retrieval 的這裡面	13-15
它每年有很多paper	13-15
那麼然後呢底下這一篇是就是用H M M 來做的原始paper	13-15
就是這個A C M sig I R 就是這個的就是這個	13-15
那底下這兩個都是這裡面的嘛	13-15
那咳那這個是用H M M 來做的	13-15
這個就是probability	13-15
用就是我剛才講的用probabilistic 來做的L S A 的	13-15
那這兩個都是在他的原始paper 都出現在A C M sig I R 的一九九九年	13-15
那當然你現在看的時候這兩個原始paper 不見得是最好看的	13-15
那在這個後面都還會有好多篇的paper	13-15
會寫的比較完整比較好看的	13-15
那你如果要看的話可以再從後面去找阿	13-15
這是講十三點零好	13-15
那底下呢我們就可以進入十四點零了	14-1
那麼十四點零我們講的呢是document	14-1
spoken document 有語音文件就是有語音的	14-1
也就是說你either 是multimedia 或者光是聲音	14-1
舉例來講電視新聞	14-1
或者是課程的錄影帶	14-1
或者是演講的錄影帶	14-1
或者是談話節目啊	14-1
這個啊訪談節目	14-1
電影啊	14-1
各種各樣這種東西的話呢	14-1
你都帶著語音的這種spoken document 那麼	14-1
understanding organization	14-1
這個reference 我待會再說	14-1
那麼這裡面的這幾章大概我們從前都都你都知道的	14-1
就是說凡是這些東西我們說譬如說演講的錄影帶啦	14-1
這個這個電視的訪談節目啦這些東西的話呢	14-1
旁邊都帶了語音	14-1
那這些語音其實講的話新聞啦這些語音所說的話其實是它的是它的keyword	14-1
那麼因此你如果根據語音大概就可以找到你要找的東西	14-1
那這一章也是我們講過啦	14-1
就是你這邊的這些東西我們用語音來來作為它的indexing	14-1
那user 輸入也有語音等等	14-1
那這時候最大的問題除了我們剛才講的那些之外呢	14-1
那有一個重要的問題就是你今天用google 去找很方便是因為	14-1
他找到兩百筆的時候這兩百筆就用文字呈現	14-1
列在那裡	14-1
螢幕上一看一目瞭然	14-1
也就是說你文字的document 是很容易呈現在螢幕上	14-1
很容易瀏覽的	14-1
因為這些文字document 是用人是人寫的	14-1
人寫的時候就已經自動分好段落	14-1
寫好標題	14-1
因此呢你很容易把這些段落標題呈現在螢幕上	14-1
所以你找到前一百篇你就把它呈現在螢幕上就好了	14-1
所以user 一眼就知道哪個他要哪個他不要	14-1
它就挑他要的	14-1
不要delete 掉	14-1
所以呢你用google 非常的方便	14-1
可是你如果是multimedia	14-1
新聞啦演講啦什麼訪談節目啦	14-1
它沒有辦法像這樣子	14-1
如果沒有人去把它寫成文字加好標題的話	14-1
就很難呈現在螢幕上	14-1
你找到一百篇怎麼辦	14-1
找到前兩百篇你沒有辦法呈現我不知道怎麼去想不知道道怎麼看嘛	14-1
因此呢你需要一些比較好的辦法	14-1
那這就是我們這裡所謂的這個spoken document 或者是multimedia document	14-2
你得要有一些understanding 跟organization 的方法	14-2
那包括哪些呢	14-2
就是我們這邊講的至少有這些東西	14-2
就是你需要呢第一個那堆聲音	14-2
你就是要靠聲音啦	14-2
就要從聲音裡面抓到O O V 的keyword	14-2
你你聲音裡面有一堆keyword	14-2
他很可能是O O V	14-2
不容易辨識出來	14-2
可是那些東西才是它最重要的東西你得抓到	14-2
包括裡面有人名組織名地名event 名等等	14-2
好那你把這些抓到之後你可你可以比較清楚他在說什麼	14-2
然後要切成小段	14-2
一個兩小時的演講	14-2
你必須把它切成譬如說五分鐘或者六分鐘或者兩分鐘的小段	14-2
之後你才能夠每一段每一段去找	14-2
然後我要看哪一段	14-2
否則的話你不能一次次看那麼多	14-2
那怎麼切成小段	14-2
我應該每一段有自己的中心主主題的	14-2
要根據他的concept	14-2
那就根據他在說什麼	14-2
你這邊都在說windows OK	14-2
那這個應該是一段	14-2
底下講到office 那應該是另外一段	14-2
底下講到Bill Gate 那是另外一段	14-2
喔你要根據這些concept 來分段	14-2
分了段之後	14-2
每一段裡面去抽他的主要的information	14-2
譬如說人事時地物	14-2
或者是其他的類似的這些keyword	14-2
或者這些這些構成的concept 裡面的最重要的部份是什麼	14-2
那通常是你這邊剛才妳這邊抽到的keyword 裡面之間的relationship	14-2
譬如說是咳那些keyword 咳那麼是發生在什麼地方是什麼人做了什麼事等等	14-2
那他們之間的relationship 就是這重要的重要的information	14-2
抓到這些東西之後你用它來做summary	14-2
做一個簡單的summary	14-2
做一個標題阿	14-2
做一個標題	14-2
有了標題之後user 可以用眼睛看	14-2
就又可以列在又又可以顯示在螢光幕上了	14-2
於是user 可以用眼睛看	14-2
看到他喜歡的可以去把那個summary 放出來看或者聽喔	14-2
咳那麼然後呢你可以把這裡的這裡的所有小段把它的topic 根據他的主題做organization	14-2
那這一些是什麼	14-2
這些其實就是我在咳開學第一堂課的那個demo 裡面所做的新聞	14-2
你如果記得的話我們的新聞就做了這件事	14-2
所以呢你可以把電視新聞裡面的咳電視新聞裡面充滿了新聞的事件以及人名地名這些東西	14-2
很多新聞的key term	14-2
那麼我們都能夠抓得到	14-2
然後我們可以把一個小時的新聞自動切成一則一則	14-2
每一則講不同就切得出來	14-2
然後抓裡面的information	14-2
為每一則新聞作一個summary	14-2
做一個title	14-2
於是你現在找到的時候	14-2
你要我要找以色列或者我要找賓賓拉登的時候	14-2
我出來一大堆新聞	14-2
他就是以它自動產生的標題	14-2
title列在那裡	14-2
我一看就知道了	14-2
然後我我甚至於根據他的topic 做好一個structure 啊	14-2
然後我就可以這個瀏覽了等等啊	14-2
那那一個系統就是講這些事情	14-2
那我們底下就稍微說一下這裡面每一件事情怎麼做	14-2
啊等等	14-2
那我想我們在這裡休息一下	14-2
休息十分鐘	14-2
我們剛剛講的就是底下這邊講的就其實就是這些事情怎麼做	14-2
那這個其實就是這個我們開學第一堂課給你看那個demo	14-2
做的新聞系統就是這樣子	14-2
就你要把新聞裡面所有的keyword 絕大多數是o o v	14-2
因為他們就是新聞事件的人名地名等等阿	14-2
要把那些能夠抓得出來因為那些是keyword	14-2
然後你要把那個根據它裡面講的東西	14-2
切一個小時新聞切成一則一則	14-2
那每一則因為它講的東西不一樣	14-2
把它切出來	14-2
然後在裡面去抓每一則裡面重要的事情人事時地物	14-2
這些key word 之間的關係	14-2
然後做summary	14-2
然後做title	14-2
所以每一則新聞後來就有title	14-2
然後就有summary	14-2
然後你再把它整個structure 變成一個好的structure	14-2
那麼因此我們在demo 裡面我說我要找以色列我要找這個阿拉法特我要找什麼的話	14-2
你馬上找到一堆新聞	14-2
那堆新聞我都有它的title	14-2
title 列在那裡所以我很容易看	14-2
那我選擇哪一則title 我可以先聽它的summary	14-2
聽完summary 我還要聽全文我才去聽全文	14-2
那它們之間可以自己構成一個structure	14-2
那我知道哪一類哪一類相關主題的都排在一起歐等等	14-2
等等	14-2
那這個就是底下就是我們講就是這些事情怎麼做	14-2
那這個的完整的paper 就是寫在這裡面的reference 二這一個	14-2
阿ㄟ等一下不對一是一這個阿	14-2
就是兩千零五年九月的這一篇	14-2
這篇是我寫的保證好看阿	14-2
那我們就把這裡面剛才講那些東西怎麼做的我都有說	14-2
然後包括我們那個系統是怎麼做的	14-2
都有在裡面歐	14-2
那所以完整reference 是這篇	14-2
那這底下第二篇呢是專門講這個語音的文件怎麼做summary	14-2
那這個是阿日本的furui	14-2
那應該是今天講語音文件的summary 裡面做的少數做的最多的	14-2
那麼那他做的語音文件都是日文的演講	14-2
他就是把他們所有的語音領域的所有的人的演講通通都錄音錄下來	14-2
然後都去做summary 阿	14-2
是這一篇	14-2
所以這個我想這是兩個相關的reference	14-2
那我們沒有多少時間我們很快的說一下	14-3
譬如說這個如何抽人名地名組織名	14-3
again 這個又用hidden markov model	14-3
這就是h m m 阿	14-3
只不過我現在這個現在指我現在用文字的阿	14-3
我在文字假設是一段文字	14-3
一段文字	14-3
一段文字我怎麼知道哪裡是人名哪裡是地名	14-3
那我就四個state	14-3
人名地名組織名其他的所有的字	14-3
這樣我用四個state	14-3
然後呢你進來假設我進來一大堆文字的話我就當成這一個一個的就是這個term	14-3
然後我就去跑這個hidden markov model	14-3
其實就是在跑什麼跑viterbi ok	14-3
所以呢你就變成ok 我有四個state 嘛	14-3
就是人名地名組織名跟所有其他的字	14-3
那人名後面可以接我搞不好是人名連續好幾個嘛這還是一個人名嘛對不對	14-3
然後人名可能後面接地名可能可以接其他的其他的字嘛	14-3
所以你這所有的state 都可以跳來跳去嘛	14-3
這就是所有的state 都可以互相跳	14-3
然後你就在這裡面跑viterbi	14-3
那當然你人名要有人名的機率	14-3
你有一個model 在這裡	14-3
算它的機率	14-3
那這時候這個機率也不會是我們講的幾個gaussian	14-3
而比較像我們剛才講的那種用n gram 什麼什麼等等來算的機率喔等等	14-3
那用這樣的方式呢我基本上是model base	14-3
可以來抓這些人名地名用h m m 來做	14-3
那然後呢rule base	14-3
所謂rule base 是說你有一堆rule	14-3
假設這個是人名的話	14-3
它後面可能會出現什麼譬如說先生	14-3
前面可能會出現什麼告訴	14-3
那裡面會出現什麼	14-3
那麼也許會譬如說這個是張	14-3
百家姓什麼什麼等等阿	14-3
你可用一堆rule 說它裡面的結構會有什麼特別	14-3
前面後面會有什麼特別	14-3
這是它們所謂的這個這個internal 跟external 的evidence	14-3
你根據internal external 的evidence 有一堆rule	14-3
來判斷它是	14-3
地名有地名的rule 阿等等	14-3
這是所謂rule base 的方法	14-3
那你通常是這個這個h m m base 跟這個rule base 加在一起	14-3
那但是這樣這個講主要是指文字的	14-3
如果一堆字的話可以用這個方式來抓	14-3
語音的話語音最大的問題不在這裡還在很多別的	14-3
那我們舉一個例子來講	14-3
那我們語音的時候我們多加這兩這三套方法	14-3
那這個呢是說你其實不容易抓到	14-3
很多很多keyword 很難抓的	14-3
譬如說你看這句話裡面遊戲橘子是一個什麼東西實在看不出來	14-3
我們人看都很難看	14-3
除非你知道它是一家做遊戲的公司否則的話你怎麼知道它是什麼	14-3
那麼在這裡你也看不太出來	14-3
可是你如果在一篇文章裡面在講遊戲橘子的話	14-3
講到後來你看到這個	14-3
ㄟ它有一個董事長阿	14-3
那就表示這應該是一家公司或者是一個集團	14-3
你再回去看像不像像	14-3
再回去看像不像像	14-3
所以這就是context information	14-3
也就是說你不是不是光在這裡看它前後左右	14-3
你去看一篇夠長的前後	14-3
然後如果這堆字連續出現的話	14-3
你一起去	14-3
那這個時候就容易看得出來它是不是什麼東西	14-3
那語音有個最大的問題就是o o v	14-3
o o v 怎麼辦	14-3
這是一個我們處理o o v 一個很重要的辦法	14-3
雖然很麻煩但是很有效	14-3
我們舉例來講	14-3
新聞它說納莉颱風重創花蓮縣壽豐鄉	14-3
但是納莉是一個o o v	14-3
所以它辨識成為哪裡	14-3
壽豐是一個o o v 所以它辨識成為那個壽豐	14-3
但是呢颱風重創花蓮縣這個是不是o o v	14-3
所以我會辨識正確	14-3
那因此當我辨識出來這堆word 的時候	14-3
我們剛才提過就是我每個word 可以有一個confidence measure	14-3
我每一個分數每一個word 可以給它一個分數	14-3
算這個confidence measure	14-3
譬如說在這個case 可能颱風是蠻準的	14-3
重創也許比較差一點因為這比較低頻的詞	14-3
花蓮是個專有名詞是很很容易進去跟颱風很容易兜到的	14-3
所以譬如這三個比較高	14-3
那這個呢這個哪裡雖然音可以很像	14-3
可是哪裡跟颱風的bigram trigram 兜不起來	14-3
所以這邊我可能就得到一個比較低的譬如說零點四	14-3
那這邊的壽豐跟這個前後都兜不起來	14-3
我的bigram trigram 都兜不起來	14-3
所以呢而這個本身也是低頻詞	14-3
所以這個可能也是低的	14-3
所以我如果可以把它的confidence measure 算出來的話	14-3
那你就會發現中間這三個比較對	14-3
這兩個可能有問題	14-3
於是我就把這三個拿去google	14-3
去google 就會把一大堆跟颱風跟重創跟花蓮有關的	14-3
一堆東西出來	14-3
那一大堆文字呢我就去做上面這些事情	14-3
包括我上下一起上下文一起看	14-3
包括我用這些rule base	14-3
我用這些h m m 去跑	14-3
那很可能你最後就會跑出一堆可能的keyword	14-3
那它裡面可能會有這個納莉可能會有這個壽豐	14-3
那這個時候你再回來看	14-3
我這邊分數低的這種東西	14-3
它是哪些個phone 拼起來的	14-3
那它的form 跟我找到的那些專有名詞裡面有沒有對的	14-3
你可能就會對到ok 那個納莉跟這個壽豐	14-3
於是這個納莉跟壽豐可以得到正確答案出來	14-3
這個是我們講的你o o v 是有辦法的	14-3
這是一個例子	14-3
這個o o v 是可以這樣出來的	14-3
那底下這個是講說這個multilevel multilayer 的viterbi	14-3
就是我們剛才就是就是主要就是說中文的	14-3
就是我們剛才就是就是主要就是說中文的很多專有名詞是一堆專有名詞拼起來	14-3
這個是地名	14-3
這個是人名	14-3
這個是一個	14-3
這個是地名這個是人名這個是一個這個建築物的名歐這個兜兜兜兜到最後變成一個專有名詞	14-3
那你這個時候怎麼辦	14-3
你如果用用剛才的這個	14-3
你如果用剛才的這個這個model 會怎樣	14-3
你要跑好多層	14-3
就是一層一層跑一層一層跑之後就會跑出來	14-3
那就是所謂的multilayer	14-3
那就是所謂的multilayer	14-3
這個是講抽專有名詞抽oov 等等	14-3
怎麼切	14-4
怎麼切一段一段	14-4
那麼切的基本的辦法就是說因為我們是做新聞新聞是可以這樣做	14-4
我先有一堆training 新聞	14-4
我先有一堆training 新聞	14-4
譬如說我找了十五萬則兩年的新聞	14-4
我每一則新聞可以用它的裡面的文字	14-4
t f i d f 那個vector 像剛剛那種vector	14-4
我就可以做做k means	14-4
做v q 什麼之類的東西	14-4
我就可以把它分群	14-4
我要分成兩百五十六個topic 或者五百一十二個topic 一千零二一千零二十四個topic 都可以分	14-4
就可以分分成一堆topic	14-4
每個topic 就是一群新聞	14-4
那每一群新聞就又變成一個我又變成hidden markov model 歐	14-4
個h m m 實在是一個非常有用的東西	14-4
那我現在又變成一群新聞	14-4
那麼變成一堆	14-4
那麼變成一堆state	14-4
我如果把這邊的十五萬則新聞分成一千零二十四群的話	14-4
我每一群	14-4
我每一群就是一個topic	14-4
這是第一群第二群等等等等	14-4
第二群	14-4
等等等等	14-4
我就變成有一千零二十四個topic 就是一千零二十個	14-4
我就變成有一千零二十四個topic 就是一千零二十個二十四個state 的一個大的hidden markov model	14-4
那於是我現在辨識出來的我我我辨識	14-4
那於是我現在辨識出來的我我我辨識出我六十小時的新聞	14-4
六十小時新聞我辨識出來就是一長串word	14-4
一長串word 沒關係	14-4
我就把它這些word 或者把它斷成句也可以	14-4
我斷斷句的方法很簡單	14-4
就是只要中間停頓夠長	14-4
只要中間停頓夠長	14-4
就把它斷句了	14-4
我可以把它斷句我也可以不斷句就用詞也可以	14-4
然後我就去跑這個viterbi	14-4
然後我就去跑這個viterbi	14-4
我這裡有一個一千零二十四個topic 的一個非常大的	14-4
我這裡有一個一千零二十四個topic 的一個非常大的hidden markov model	14-4
每一那麼每一個topic 可以回到繼續走講繼續講它自己的topic	14-4
那麼每一個topic 可以回到繼續走講繼續講它自己的topic	14-4
也可以我換一則新聞就講另一個topic	14-4
全部都可以跳	14-4
那如果這樣的話就就去跑viterbi 之後	14-4
那如果這樣的話就就去跑viterbi 之後	14-4
那我就可以把每一個word 或者每一個sentence 它屬於哪一個topic 分出來	14-4
那麼於是呢你應該同一則新聞	14-4
會在同一個這裡面一直跑	14-4
這時候一直繼續在這裡就同一則新聞	14-4
等到它跳出去的時候	14-4
就是換一則新聞那你就切開嘛	14-4
這是最基本的用h m m 來切	14-4
來切新聞的方法	14-4
那麼再來做summary	14-5
做summary 基本的辦法就是說你就把它	14-5
先把它翻譯成為word sequence	14-5
那也是一樣我們基本上就是word sequence 中間凡是這個	14-5
斷開比較長的	14-5
就算就算是斷句了	14-5
然後我就選重要的句子	14-5
選重要的句子兜起來就是summary	14-5
重要的句子怎麼選	14-5
用一些分數	14-5
這是舉一些例子	14-5
像這個就是t f i d f 你用t f i d f 是可以	14-5
算	14-5
每一句的t f i d f	14-5
跟整篇文新聞的t f i d f	14-5
像不像	14-5
這還是做個內積	14-5
看哪句跟整篇的最像	14-5
那那些跟整篇最像那幾句	14-5
拿出來譬如說這是一種方法	14-5
這是另一種算分數的方法等等我們現在有很多種算分數	14-5
然後你可以挑出比較好的句子	14-5
這就是做摘要	14-5
那做摘要其實詳細的最詳細是這個這個我剛剛講這個reference	14-5
這是他們	14-5
到目前為止應該做語音新聞摘語音文件摘要做的最好的應該是這個阿	14-5
這個是一個相當好的reference	14-5
那其實我們做的大概是學他的	14-5
那有一些不同的方法就是了	14-5
那怎麼怎麼做標題	14-6
這個標題是特別的另外一招	14-6
那就是說我用那因為我們你可以想像所有的	14-6
文字文件只要是人寫好的文件都有一個標題在那裡	14-6
所以你只要收集夠多的文字的人寫好的文件	14-6
它們都有人寫好的標題	14-6
你就可以train	14-6
人寫好的文件跟人寫好的標題之間的關係	14-6
那就我們新聞而言也是這樣子	14-6
我們拿十五萬則新聞	14-6
這十五萬則新聞都有	14-6
新聞記者或者是編輯寫好的	14-6
標題	14-6
於是我就可以train 它們之間的關係	14-6
如果這裡面有一個什麼詞的話	14-6
它不會出現在標題裡	14-6
還是它會出現在標題裡	14-6
哪些詞會出現在標題裡面是最關鍵的keyword	14-6
還有你的標題不是只有keyword	14-6
是有一些function word 把這些標題連起來	14-6
那這些個會把function這些會把這些keyword 連起來	14-6
構成標題這些是非常特別的一些word	14-6
那這些word 都是特別的	14-6
那這些機率都可以train 出來	14-6
然後標題裡面	14-6
通常的文字比較簡潔	14-6
所以它的n gram 不一樣它有它的n gram 等等	14-6
那麼於是你這些都train 好之後	14-6
現在新的一則新聞進來	14-6
我就把它辨識	14-6
辨識出來之後	14-6
我就學剛才這些東西	14-6
我就找裡面哪些可	14-6
可以做成標題的就把它做成標題歐等等	14-6
那這個就是做標題的方法	14-6
那再來怎麼做organization	14-7
那也是一樣	14-7
這個	14-7
如果記得我們那時候demo 裡面就有我就可以把新聞分成	14-7
譬如說這個	14-7
這個這個某一群是講什麼的某一群是講什麼的	14-7
然後如果這群新聞很多它可以分成下一層再分成很多群	14-7
恩	14-7
等等	14-7
那這怎麼做的呢	14-7
那這個就是我們剛剛講的probabilistic l s i	14-7
那個東西呢我可以分	14-7
可以分析出topic 來	14-7
我既然可以分析出topic 的話	14-8
那我就可以把這個	14-8
根據它的topic 來來來來分來來做這樣的organization	14-8
這樣就行啦	14-8
我這樣的話我這個就做出來	14-8
等等	14-8
那這整個structure 變成一個這樣子的圖	14-8
呃	14-8
分成四個level 就是	14-8
叫做term level	14-8
concept level	14-8
summary level	14-8
跟topic leve	14-8
那四個level 其實就是我們剛剛講的六件事情喔	14-8
這邊是在抽專有名詞	14-8
抽key word 阿	14-8
抽抽那些	14-8
那這是term level 你要先有這個term 才知道它在講什麼	14-8
有了term 你第二個就是concept level	14-8
你可以根據concept	14-8
來切一則一則	14-8
或者切成一小段一小段	14-8
切好之後你可以抓裡面的concept	14-8
當你concept 抓出來之後	14-8
就可以用這個concept	14-8
來做summary	14-8
跟做title	14-8
那這個時候就進入summary level	14-8
那你進入summary level 之後呢	14-8
那你title 可以看成是summary 裡面最精緻的部分	14-8
那麼	14-8
這個	14-8
但是有了title 其實title 通常已經告訴我它是什麼topic	14-8
所以這時候進入topic level	14-8
那我現在分析topic	14-8
等等	14-8
這是從bottom up 從底層往上	14-8
這樣一路往上做	14-8
你其實	14-8
這也是從上往下	14-8
你可以想像	14-8
底層做東西	14-8
有了底層才可以往上做	14-8
但是呢你底層有錯上面就跟著錯越錯越多	14-8
可是你上面這個大的比較大的方向抓的到之後	14-8
可以放到下面來	14-8
concept 抓到你可以可以回過頭來再回去看	14-8
如果知道這段是在講什麼東西再回去看的話	14-8
它會有什麼term	14-8
如果知道它在說什麼事情的話我再去看它	14-8
concept	14-8
其實是可以top down	14-8
所以這裡面是有這個一面bottom up 一面top dow	14-8
所以你可以有iteration 的關係	14-8
當這套都弄好之後	14-8
我現在做retrieval 就好做了	14-8
現在要找的東西都organize 出來	14-8
然後你都可以看到所有東	14-8
西	14-8
不至於說一大堆	14-8
我都沒有辦法聽沒有辦法看沒有辦法出現在螢幕上	14-8
這樣我就可以都呈現在螢幕上好等等	14-8
恩	14-8
那這段就是我們講的	14-8
呃	14-8
大概是最後一章ya 這是最後一章	14-8
所以這個是我想大概這個	14-8
呃	14-8
很快的go through 一下不過我想	14-8
呃	14-8
因為文章裡面寫的很清楚	14-8
所以你如果有興趣的話	14-8
這也是寫報告的好題材阿	14-8
那麼	14-8
這個	14-8
保證好看所以應該沒問題所以我們講快點沒關係	14-8
那所以呢我我們今天我就我改成今天我們先講十五點零	15-1
那麼這個啊如果今天可以把十五點零講完的話我們下週回到九點零	15-1
我想是這樣可能比較順序上可能會被對各位更準備期末報告可能更方便一點	15-1
那十五點零的這些refer 我們這些講的是什麼東西我們待會解釋	15-1
那這裡面的這個啊課本上有一小段在講就是這裡面	15-1
這個課本裡面講的一點	15-1
那大部分的內容其實也是一樣都是啊我選一些從大概九零年代中期到兩兩千年左右的一些個比較代表性的東西	15-1
那麼經過這麼多年下來應該是大家公認是一些有這個有代表性的有具體的都公認相當不錯的技術	15-1
我們拿它來說一下	15-1
做為example	15-1
雖然從那個以後到最近	15-1
還有很有非常豐富的paper 可以找得到	15-1
那那些應該都是很好的報告題材	15-1
所以我們今天先來講這個十五點零	15-1
那我們先說一下十五點零在講的啊我們這邊講最主要的一個基本的事情	15-1
就是所謂的mismatch	15-1
in 這個我我們的語音辯識到目前為止所有最成功的方法	15-1
都是以統計為基礎	15-1
那麼所謂統計為基礎其實就是我們這邊畫的這一塊	15-1
這一小塊其實就是我們之前一直在說的東西	15-1
那最前面這個就是當你的聲音進來的時候我先做這個我先做這個feature extraction	15-1
這就是我的front in七點零所說的	15-1
最後求出來的一西一些m f c c	15-1
那麼之後呢那這些acoustic model 就是我們四點零五點零所說的那些個tri phone 跟hidden markov model	15-1
然後呢這邊就是我們六點零所說的language model 等等	15-1
那麼我們可以回想一下就知道	15-1
hidden markov model 是完全統計式的	15-1
是一個完全以統計為基礎所建立的model	15-1
同樣的n gram language model 也完全是統計為主而建立的	15-1
那這些統計的基本精神是假設說	15-1
我用夠多的語音的data	15-1
然後呢它能夠幫我們描述所有的phone 所有的tri phone	15-1
它的統計特性怎樣	15-1
我有夠多的文字的data	15-1
它可以幫我們很清楚的描述這些文字的這個n gram 是怎樣的	15-1
然後我用這些為基礎製做我輸入的東西呢	15-1
我希望我輸入的聲音它的m f c c 的distribution	15-1
的m f c c 的這些所有的統計特性	15-1
是用這個來描述的所以可以用它來recognize 是什麼phone	15-1
它的文字之間的字詞句之間的關係是跟這個一樣的	15-1
所以可以用它來描述等等	15-1
那這個是統計方法的一個最基本的精神所在	15-1
可是事實上是不是這樣呢	15-1
我們必需了解不盡然是這樣	15-1
那麼當我真正進來的聲音它的m f c c 的長相	15-1
跟這邊所用的model 的train model 這些東西長得不太一樣的話呢	15-1
那就是所謂的mismatch	15-1
同樣呢當我進來的句子它的字詞之間的關係跟你這邊的不太一樣的話呢	15-1
那也就是所謂的mismatch	15-1
那是什麼情形呢	15-1
我們只要想我們在十一點零所說的speaker 的的speaker 的adapt adaptation 時候	15-1
我們說假設對speaker a 而言	15-1
它的ㄚ是這一群的	15-1
它的ㄨ是這一群的	15-1
那speaker b 就不見得還是這樣	15-1
它可能會更多一點	15-1
當我有一群人的時候呢這個ㄚ可能就擴大到這樣子	15-1
因為不同人聲音不太一樣而這個ㄨ可能就擴大到這樣子	15-1
於是呢它們的這個distribution 會會overlap	15-1
那這個時候怎麼辦我們做adaptation 這就是我們十一點零所說的	15-1
我現在看這個人的聲音是誰	15-1
他講話講起他輸入的聲音發現他的ㄨ在這裡	15-1
所以呢我就知道其實對這個人而言他的ㄨ在這裡	15-1
我這些就不要了	15-1
對這個人而言他的ㄚ在這裡我就在這裡	15-1
那於是呢我這些東西都可以拿掉	15-1
之後呢我就把它們又分開來了	15-1
這就是speaker adaptation 在說的事情	15-1
換句話說其實不同的人的統計特性都不一樣	15-1
那麼不同的人他的他的這個他的這個啊這些m f c c distribution 本來就不同	15-1
那這個這個adaptation 的例子其實就是在說	15-1
我的我我train 的是用speaker independent	15-1
譬如說五百個男生五百個女生	15-1
那train 出來的東的東的的tri phone 的model 是用這個這些train 的	15-1
進來的人進來的聲音不見得是他呀	15-1
那進來的聲音是另外一種	15-1
因此我還要再想辦法調到進來的那個	15-1
那我才能夠做得好	15-1
當你沒有調這件事的時候你進來的聲音	15-1
其實跟它是mismatch	15-1
那麼因此呢你會如果有mismatch 當然你到時候做出來就不會對	15-1
以language model 而言也是一樣的	15-1
那麼我們之前說過	15-1
各位的習題也做過	15-1
就是譬如說體育新聞跟財經新聞	15-1
它們的詞彙跟句型	15-1
它們詞跟詞之間的關係都是不一樣的	15-1
因此呢如果說是我用相同的我我即時我這邊想辦法讓所有可能的文字都拿來train	15-1
train 出各一個各種各種topic 都相關的的n gram	15-1
但是呢我今天這個講的人講的的是哪一個topic	15-1
他如果講的是某一件特別的事情的話搞不好你這個個n gram 就是不對的	15-1
那這個就是language model 的的mismatch 等等	15-1
那麼因此呢當我們在說這些個統計	15-1
以統計為基礎的時候	15-1
我們千萬不要以為統計那麼好	15-1
那麼統計是必須我們必須知道	15-1
盡信統計不如無統計	15-1
那麼統計本身是有很大的問題的	15-1
那一個一個很大的問題就是mismatch	15-1
那我現在在這一頁所說的是所有可能的mismatch	15-1
那我們先說最最基本的一個情形就是我的聲音不盡然是最乾淨的聲音	15-1
我的聲音可能被各種東西破壞	15-1
那麼我們舉例來講假設你是用你的手機	15-1
打電話給遠端的一個server 來操作什麼事情的話	15-1
你在你真正講的聲音是這個x n	15-1
不是那個x n	15-1
你講的x n 在這裡	15-1
但是呢在你打電話的時候	15-1
這個你你說話的同時	15-1
進入你的那個手機的麥克風的還有很多的雜訊	15-1
那麼這些雜訊一起進來我們姑且稱為n one	15-1
那麼之後呢這個聲音經過了傳送	15-1
經過了你的電話的傳送到了接受端	15-1
這個傳送中間會有各種各樣的破壞	15-1
最簡單的就是你電話本身一定會有channel 的distortion	15-1
那這個你馬上想得到	15-1
我們知道在你打手機的電話的時候你聽到你的朋友打的的聲音	15-1
那個聲音可以清楚到你知道他在說什麼話	15-1
你也可以判斷他是什麼人	15-1
但是你知道那個聲音跟他在你對面跟你講話是不一樣的	15-1
那他跟在你對面講話是不同的	15-1
這個部分其實就是經過了一些變化經過了一些distortion	15-1
那麼in addition 當然還有很多其它譬如說麥克風本身是一個distortion	15-1
因為你知道任何一個聲音通過麥克風的時候	15-1
麥克風本身是有他的frequency response 的	15-1
那麼在某一些frequency	15-1
它得到不同的gain 跟不同的face 等等	15-1
所以它整個的訊號對麥克風而言其實也是會造成任何一個麥克風都會對聲音造成失真的	15-1
這是所謂的麥克風distortion	15-1
那所謂的acoustic reception 是說	15-1
你事實上你對著對著手機講話的時候	15-1
你並並並沒有把你的手機的這個喇叭對準嘴巴而是在旁邊的	15-1
所以你的聲音其實是從正面出去而你收到的是側面進來的聲音	15-1
那也會造成一些影響等等	15-1
那所有的這些呢我們姑且都用一個convolution 的效應來描述它	15-1
那總之這個意思等於是說	15-1
我這邊不但是有noise 加進來而且還被破壞	15-1
那這個破壞我們通常把他用一個convolution 來描述	15-1
那麼也就是說呢我如果我進來的是x n 真正的聲音是x n	15-1
我現在加上了一個noise	15-1
那麼這個地方其實應該是我寫成成n one 的t	15-1
也沒錯啦這是time 這是continue time	15-1
不過我們們如果寫成discrete time 都是n 啦	15-1
取sample 的話n one 的n	15-1
之後呢我convolve with h n	15-1
我把這個跟h n 的convolution	15-1
拿來model 這個中間的這一堆	15-1
不管是因為電話傳送造成的問題	15-1
或者是麥克風的distortion	15-1
或者是其它的問題等等	15-1
那麼我們簡單的用數學來說的話呢	15-1
是一個這樣的convolution 關係	15-1
同樣的呢你電話在傳送中間不只是一個convolution 而已	15-1
你還會有別的noise 加進來	15-1
那不論是這個wireless 傳送的中間或者是怎樣	15-1
又經過了一些amplifier 把它放大的時候都等等	15-1
都可能增加noise	15-1
所以又有n two 的n	15-1
我又加進來	15-1
那這個是在這邊	15-1
所以我最後的y n 其實是這個	15-1
是這些東西	15-1
那跟我原來的這個這個才是y n	15-1
所以呢到了遠端	15-1
你的所收到的聲音這個y n	15-1
其實跟這個x n 已經有很大的差異	15-1
至少包括這個加上n one	15-1
然後convolve with h two h 然後再加上n two	15-1
那這裡面的n one 跟n two 呢我們叫做additive noise	15-1
就是它直接加上來的	15-1
這是additive noise 直接加上來的	15-1
那麼這個h n 這個呢我們稱之為convolutional noise	15-1
因為它其實是一個convolution 的process	15-1
那有一個有一個這樣子的東西	15-1
那麼因此呢這個當然跟原來的x n 是不一樣的	15-1
那麼因此呢我今天如果我的recognizer	15-1
我這邊所用的那些tri phone 的這些h m m	15-1
我都是用這樣的x n 來train 的	15-1
就算這個x n 就是跟這個training data 是一樣的	15-1
它的distribution 完全一樣	15-1
那我x n 也經過這些變化	15-1
也變了樣子	15-1
所以呢我的y n 跟h x n 已經不太一樣了	15-1
當然因此跟這個training 也就不一樣了	15-1
那更何況你這個x n 不見得就真的就它的它的distribution	15-1
它的統計特性	15-1
不見得會跟它一樣	15-1
那麼一個很單很簡單的解釋	15-1
就是說我的每一個h m m 每一個h m m	15-1
它的每一個state 不是都有一個distribution 嗎	15-1
我們說是這堆gaussian 所構成的	15-1
那麼這堆東西是用譬如說是用這些x n 來train 的	15-1
但是經但是我真的所收到的聲音是有這些東西	15-1
那麼這些n one n two 跟h 呢	15-1
老早已經把它破壞了	15-1
所以我真正的它長的不是這樣	15-1
所你用這個來做辨識是有問題的等等	15-1
那這個是一個mismatch 的例子	15-1
那這個mismatch 呢就是我們這邊所謂的mismatch in acoustic environment	15-1
那也就是說我的整個的acoustic environment	15-1
是是有mismatch 在這裡	15-1
那主要就是additive noise 跟convolution noise	15-1
那麼不斷地隨時在整個process 中間不斷有additive noise 來破壞它	15-1
也不斷有convolution noise 來破壞它等等	15-1
那所有的這些我們如何克服這些問題呢	15-1
那就是我們所謂的environmental robustness	15-1
我們希望我們用什麼方法	15-1
讓我整個的這個process 這個辨識的這個這個process	15-1
能夠對於這個這些environmental 的或者說是acoustic environment	15-1
的變化有更為robust 的能力	15-1
這是所謂的environment robustness	15-1
那事實上除了這個之外還有很多其它的mismatch	15-1
我們順便都在這邊說一下	15-1
其它的各種各樣的mismatch	15-1
那麼都是屬於training 跟recognition condition 之間的mismatch	15-1
所謂的training condition 就是說	15-1
我們的hidden markov model 以及我們的n gram 是由哪些東西train 的	15-1
但是呢我真正的recognize 的時候呢是這些東西進來	15-1
它跟這個不見得match	15-1
那我們說底下的這個mismatch 我們之前已經講過了	15-1
這個就是speaker 的不同你這邊用一千個人去train	15-1
可是現在的speaker 不是現在這邊一千個人的一個	15-1
而是是其它的任何一個	15-1
它搞不好跟這一千個人都不像	15-1
那麼因此呢你必須要做speaker adaptation	15-1
所以呢我們在十一點零所說的	15-1
這是我們十一點零所說的speaker adaptation	15-1
其實是speaker 特性的一種adaptation	15-1
啊speaker 特性的mismatch 的的的的問題	15-1
那除了speaker 不同之外呢	15-1
那還有一堆就是這個其它的acoustic condition	15-1
還有很多其它的acoustic condition 也不match	15-1
譬如說我們說speaking mode	15-1
所謂的speaking mode 這個我們中間提過	15-1
就是從read speech 一直走到spontaneous speech	15-1
這個你說的mode 不一樣就是不一樣的	15-1
那麼所謂的read speech 是說	15-1
給你一份稿	15-1
我完全照著那個稿很清晰的朗讀	15-1
叫做read speech	15-1
譬如說今天早上有一點雨我從家裡出發	15-1
一個字一個字唸的很清楚的	15-1
這個叫做read speech	15-1
那麼但是事實上呢我們通常說話很少情形是真的有一張稿讓你這樣子唸的	15-1
那麼比這個比較講得隨便一點的是所謂的prepared speech	15-1
prepared 是說你有準備	15-1
但是呢並沒有完全照著稿唸	15-1
那一個很代表性的例子就是廣播新聞裡面主播的播新聞	15-1
主播播報新聞的時候是有稿的	15-1
但他不是照著稿念他是照著稿說	15-1
稿上說有什麼新聞之後呢	15-1
那麼主播是照著那個稿說的	15-1
那他是有準備但是呢他並不是照著搞唸的	15-1
這個時候就不大一樣他他就會講成今天早上怎樣怎樣等等	15-1
那就會很多音就會連起來	15-1
就會比較比較流利	15-1
比較這個不會說是今天早上啊	15-1
他就會今天早上怎樣怎樣	15-1
那這個就是比較prepared 的	15-1
那再來的話如果是conversational	15-1
就是如果兩個人在對談	15-1
或者打電話	15-1
或者spontaneous 就是我們隨時都隨隨便自發性地在說的話的話呢	15-1
那就有更多的狀況	15-1
跟我們清晰朗讀聲音是不一樣的	15-1
那這種的越越往這邊走的越複雜	15-1
這裡面最大的最大的這個不同	15-1
那應該有兩個	15-1
一個是pronunciation variation	15-1
也就是說我的聲音會因為講得很流利而改變	15-1
那不論在從prepare 開始	15-1
到這個越越過去的話這種情形越嚴重	15-1
一個最簡單的例子我們剛才講的今天早上	15-1
這個今天早上我們會變成間早上	15-1
我會變成今天早上啊	15-1
譬如如說是今天早上	15-1
你可能會變成這這樣子	15-1
那這些就是說你你本來的的一個一個音	15-1
本來的某一些個音都會變調今天就變成間啊等等	15-1
那這些東西的話就是所謂的pronunciation variation	15-1
你的每一個音都可能會會這個因為說得流利而滑掉了	15-1
那另外一個最最重要的現象就是disfluency	15-1
也就是不流暢的現象	15-1
那disfluency 是說我們如果是如果是用唸的或者是prepare 的	15-1
你大概都可以講得很流利	15-1
可是你如果是平常在講話的話顯然中間隨時都會斷掉	15-1
欸我說這個啊啊	15-1
不是不是應該是那樣啊等等	15-1
你不斷地會有這種這種這個你你一個一個句子說說到一半我會換成另外個句子或者怎樣	15-1
或者我要重來一下	15-1
啊我說這邊欸不是這樣我我我要重新說一次這個是這樣啊等等	15-1
這一類的disfluency	15-1
是一個非常在自然的如果你拿平常的conversation 的錄音帶來聽的話	15-1
這種出現地非常多	15-1
那這些呢都造成因為我們train 的時候沒有train 這種東西啊	15-1
那這個都都是我的mismatch	15-1
這是對於speaking mode 的mismatch	15-1
那我train 的時候沒有train 這些東西但是我講進來的時候會有這些東西	15-1
會有這些東西	15-1
那再者呢就是speaking rate	15-1
就是講得快慢的問題	15-1
那嗯你你可以猜得到	15-1
因為我們的這個這個state 跳來跳去的這個這個process	15-1
我們在這裡用我的model 在這邊描述	15-1
但是呢如果說是我train 的時候講了一個速度	15-1
真的進來的聲音速度不一樣的話會會不match 啊	15-1
那麼這個時候這是這是speaking rate 的的mismatch	15-1
那麼這個時候啊不論是這邊是正常的速度所train 的	15-1
你講得比他慢或者講得比他快的話	15-1
都會有mismatch	15-1
然後呢這是口音跟方言	15-1
那你知道他如果是這個上海口音	15-1
或者台灣口音	15-1
雖然講的都是國語	15-1
它很多音就是會差	15-1
那這個呢就是口語方言的mismatch	15-1
還有emotional 的effect	15-1
那假設你有一點不高興	15-1
或者有一點生氣	15-1
或者有一點什麼的話你的聲音也會變等等	15-1
那這些都是屬於各種各樣的acoustic condition 的mismatch	15-1
那除了acoustic condition 的mismatch 之外呢	15-1
那還有什麼mismatch	15-1
詞典的mismatch	15-1
也就是說這個lexicon 也不見得準啊	15-1
那那這個這個詞典的問題呢	15-1
第一個就是o o v 對不對	15-1
我們說過在任何語言都有一堆o o v	15-1
有的多有的少	15-1
以中文而言o o v 是非常多的	15-1
因為我們不斷的創造新詞	15-1
這些新的詞都出現在日常生活之中	15-1
但是不見得在詞典裡面	15-1
因此呢你不斷的產生很多新的詞	15-1
你的詞典需要不斷地調	15-1
然後呢我們剛才講的pronunciation variation	15-1
這常常是一個辦法	15-1
就是在這裡調我的詞典	15-1
譬如說我們就可以讓今天這個詞	15-1
有兩個音	15-1
我們可以在我們本來在詞典裡面是說	15-1
ok 今天	15-1
他的發音是ㄐ 一 ㄣㄊ 一 ㄢ	15-1
但是我可以說他還有第二個發音就是間	15-1
那我如果把這個發音叫做間的話呢	15-1
於是我這個今天就會有兩個發音	15-1
我就在詞典裡面做兩個pronunciation	15-1
那這是一種處理這個pronunciation variation 的方法	15-1
那麼我們通常可以把這些可最常發生的pronunciation variation	15-1
放到詞典裡面去	15-1
但是這樣一來也會發生別的問題	15-1
就是當你碰到一個間的時候	15-1
你怎麼知道他是講中間的間	15-1
還是講今天呢	15-1
它要你要靠一些方法來判斷	15-1
譬如說n gram	15-1
如果前面是一個中的話那應該是中間	15-1
如果後面接早上的話呢	15-1
那應該是今天啊等等	15-1
那因此所那這些呢就是所就是我們講就是說	15-1
你可以做你的lexicon 可以做adaptation	15-1
那depends on 現在什麼狀況	15-1
我我詞典做某些變化	15-1
這個是這個是pronunciation variation 放在這個lexicon 裡面的情形	15-1
那當然還有language model 的adaptation	15-1
我們剛才也已經提到了	15-1
就是你的test domain 不一樣	15-1
它如果今天是在講某一個科學問題	15-1
譬如說醫學知識	15-1
跟他今天是在講這個譬如說這個美國的大選	15-1
或者現在是在講這個某一種什麼東西	15-1
當你講的東西不一樣的時候它這邊都不一樣我的n gram 應該都不一樣	15-1
你最好是用不同的n gram 來做它	15-1
這就是language model adaptation	15-1
所以這一些都是可能的mismatch 的狀況	15-1
那麼我們需要有各種方法來處理它	15-1
那這個就是我們這邊所說的各種的mismatch	15-1
那麼在統計式的技術裡面	15-1
mismatch 是我們必須要想辦法解決的問題	15-1
那這些都是非常重要的研究課題	15-1
你如果去查查reference 查paper 的話	15-1
你用這些keyword 進去找都可以找到很多相關的paper	15-1
那麼我們之前在十一點零講的是屬於speaker adaptation 是這一塊	15-1
我們在十二點零講的是language model adapt adaptation 的一小塊	15-1
那麼用嗯那個l s a 來做那是裡面的一種方法	15-1
那其實這裡面每樣的東西都可以找到很多東西	15-1
那我們現在的這個這一段啊	15-1
我們今天在講的是十五點零的這個嗯robustness for acoustic environment	15-1
是指這裡面的這一塊	15-1
也就是acoustic environment	15-1
也就是這邊有additive noise	15-1
這邊有convolutional noise	15-1
當我有這些東西的時候所造成的	15-1
additive 跟convolutional noise 所造成的environmental 的的mismatch 的時候	15-1
那我怎樣讓我的系統我的這個recognition	15-1
可以比較robust with respect to 這些的變化啊	15-1
所以呢我們這邊講的這個十五點零所說的	15-1
robustness 是指這一塊	15-1
那這一塊的嗯是今天的語音研究裡面相當大的一塊	15-1
那麼相當多的團隊的研究都集中在這裡	15-1
那麼因為這是一個很重要的問題	15-1
因為你隨時真的碰到的就是你在路上打打你的手機電話就是會有雜訊進來	15-1
就是會有這些東西	15-1
所以你這些東西必須要克服	15-1
所以這個是非常重要的問題而有非常多的研究在進行	15-1
嗯那也是一個很好的報告題材	15-1
所以我們今天提早到這邊先來說這樣子	15-1
讓你這個可以可以多思考一個這個可以做報告的topic	15-1
那底下我們講的就是這一塊啊	15-1
就是這個在additive noise	15-1
跟這個convolutional noise 的破壞之下	15-1
這邊的進來的聲音跟原來的training data 有很大的mismatch 的時候	15-1
那麼有些什麼方法來做	15-1
那麼基本上呢我們剛才提到	15-1
我這邊所講的這些	15-1
你從reference 裡就知道我們講的還是比較早的	15-1
就是已經過了五六年以上大概	15-1
或者是八九年	15-1
那麼已經算是嗯經過了這麼長時間的考驗	15-1
大家都公認他是道理的而且嗯有一定效果的	15-1
但是他們顯然不是最好的	15-1
因為又經過了這麼多年在最近幾年又有很多新的技術出來	15-1
那麼有很多新的東西都是可以做報告的題目	15-1
但是我們在課堂上講我們以這種比較經典式的	15-1
算是比較經典式的然後比較公認都不錯的方法我們以它為例子來說	15-1
那就這些而言呢我們大概可以用底下這些圖來解釋他是怎麼回事	15-1
那麼我們把用這條線這個虛線以上的是training 的時候	15-1
以下的是真的recognition 的時候	15-1
我training 的時候我用一堆聲音	15-1
抽它的m f c c 的feature	15-1
train 我的hidden markov model	15-1
train 我的tri phone 等等	15-1
然後就得到我的一堆tri phone 的model	15-1
所以這一塊其實就是上面的這一塊	15-1
就是這一塊	15-1
所以呢我就是把那基本上這一堆很可能是我的乾淨的聲音啊	15-1
我的clean 的乾淨的聲音	15-1
沒有經過這些noise 的破壞的乾淨的聲音	15-1
我抽它的feature 然後train 它的model	15-1
得到的tri phone 就長這樣	15-1
這就是我的acoustic model	15-1
那問題是我真正要要做recognition 的時候我進來的y n 跟它不一樣的	15-1
y n 是這種y n	15-1
是經過了additive noise 跟convolution noise 破壞以後的y n	15-1
那這個y n 呢跟它本來就不一樣的	15-1
所以我抽出來的m f c c 當然是不一樣的	15-1
那麼如果你直接用這個model 去去去辨識的話呢	15-1
顯然會錯很多	15-1
啊那麼因此怎麼辦呢	15-1
你如果直接把那個model 拿來去做辨識一定錯很多	15-1
因此我們有幾種可能的辦法	15-1
第一種是調model	15-1
雖然你原來的原始的聲音	15-1
乾乾淨淨的聲音train 出來train 出來的這堆tri phone 是這樣的	15-1
那麼我想辦法去調它	15-1
given 現在的聲音長得不一樣了	15-1
我如果有辦法根據現在的狀況	15-1
我馬上調我的model	15-1
把我的model 調成像現在的狀況來辦識	15-1
那這樣的話呢我就把我的lambda i 呢調成lambda i 的prime	15-1
讓他的特徵比較像這個	15-1
那這個意思也等於是說	15-1
我本來的本來的這個我們說我本來的model 是這樣的	15-1
那因為我現在進來的聲音不太一樣了	15-1
我發現其實我應該調過來一點	15-1
我得到一個這樣子的model	15-1
這樣子我把我的model 整個調了一下	15-1
這一類的方法我們稱為model based approach	15-1
它是調model 的	15-1
也就是我直接動這個model 上面的mean 或者covariance matrix 的等等東西	15-1
去直直接動它	15-1
我根據我現在的noise 等等東西我直接動它	15-1
這是model based approach	15-1
那第二類的話呢	15-1
我不動這個model	15-1
因為動這個model 不好動我動feature	15-1
那我可能想辦法讓我這個這個feature	15-1
跟原來那個feature 比較像	15-1
看我有什麼辦法讓我進來的東東西求的feature 能夠跟這個比較像	15-1
而不要跟他差太多	15-1
那還有一種情形是說我這個求feature 的方法能不能改變	15-1
我不是完全用m f c c 而是在用別的方法	15-1
使得我這個feature 求出來的feature 呢	15-1
這兩個比較像不要差那麼多	15-1
那這類方法呢我們稱為feature based approach	15-1
就是動這些東西的	15-1
那還有一種根本就是動這裡	15-1
那也就是說我希望我這個聲音最好能夠像它	15-1
我用什麼方法讓它的特性就像它	15-1
如果這樣的話我後面就不要動了	15-1
那這就是這這種我們就稱之為speech enhancement	15-1
那麼所謂enhancement 就是把想辦法把它這裡面的這些個破壞	15-1
不論是additive 還是convolutional 這些東西儘可能把它拿掉	15-1
讓這個東西呢跟它很接近	15-1
那這就是所謂的speech enhancement	15-1
那麼因此呢我可以在這裡做可以在這裡做也可以在這裡做	15-1
那這各有很多種方法	15-1
那當然你也可以猜得到他們之前其實也有可能可以加成	15-1
我可以一面那裡model 也動一動	15-1
這邊feature 我也動一動	15-1
這邊我也動一動可不可以有的時候也可以	15-1
當然有的時候也許它們你這邊已經動掉了之後這邊再動一次的話會會破壞	15-1
也不一定	15-1
那基本上呢它們是有可能可以加成的	15-1
好那底下呢我們講的就是分別以這三大類來說	15-1
然後我們講的只限於這一種	15-1
就是acoustic environment	15-1
就是這些的x n 變成y n	15-1
中間經過additive 跟這個convolution 的時候的破壞	15-1
所產產生的問題	15-1
然後呢我們分別以我們先說model based 的例子	15-1
然後再說feature based 例子	15-1
然後再說enhancement 的例子	15-1
那就是我們底下的這些譬如說這邊講的就是我們先從model 的的的例子來說	15-1
這是model based 的第一個example 然後呢這是第二個example	15-1
之後我們再來講的就是feature based	15-1
這些是feature based 的例子	15-1
然後再來我們講enhancement 的例子等等	15-1
所以這些就是嗯應該講是有五年或者十年左右的歷史的	15-1
比較最具有代表性的方法來說明這些東西	15-1
那他們的reference 分別我我講到哪裡會說	15-1
是哪哪一個reference	15-1
那麼但是我們剛講這些我們舉的都是比較早的	15-2
那在最近	15-2
五年或者嗯五六年之內	15-2
啊那還有非常非常多的	15-2
那很多都比這些好	15-2
但是我們不在這裡講是因為他們還沒有經過足夠的時間的考驗	15-2
到底哪一種真的可以被公認是好的	15-2
我們也許還要一段時間才知道	15-2
所以呢我們先不講那些	15-2
不過那些也都是很好的報告題材	15-2
你如果你如果去找reference 就會看到	15-2
好那我們先從第一個就是model based 來講	15-2
這個model based 就是我們剛才講的這個情形	15-2
就是我直接調我的model	15-2
直接調我的model 想辦法讓它比較接近這個聲音的特性	15-2
那這是怎麼回事呢	15-2
我們舉第一個例子就是所謂的parallel model combination	15-2
p m c	15-2
那這是一個非常成功的而且普遍使用在很多地方效果都不錯的方法	15-2
那它的基本的精神是怎樣呢	15-2
我們可以舉個例子來說	15-2
如果我的speaker 在講話的時候	15-2
講的這段話	15-2
從這裡開始	15-2
那其實它中間當然是會斷開的	15-2
我們知道我們的我們的speech 中間永遠有非常多的這個silence	15-2
因為我們人要換氣嘛	15-2
我我不可能一口氣一直講下去我一定會斷開來	15-2
然後呢再接下去講	15-2
再斷開來	15-2
然後再接下去講	15-2
我中間一定會不斷地有斷開來的地方	15-2
那麼因此呢我的noise 就在這裡	15-2
我一開機還沒開始說話的時候呢這邊有noise	15-2
我如果抓得到這些這些noise 的話	15-2
我可以用這個noise train 一個noise 的model	15-2
noise 也可以train 成一個像hidden markov model 一樣的東西	15-2
不管你是一個state 還是兩個state 幾個state	15-2
我就有一個noise model 在這裡	15-2
所以呢你你可以在我這個這個一開機我還沒開始講話	15-2
前面譬如說有零點一秒的時間是一個有零點一秒的時間是我沒有講話但是有noise	15-2
我就可以把這抽來train 一個model	15-2
然後呢我這邊真的真的聲音我原來有一大堆model	15-2
譬如說這是ㄚ的model	15-2
這個是ㄧ的model	15-2
這是ㄨ的model	15-2
對不對我有一整套的這是我原來的用clean speech 所train 的	15-2
沒有雜訊的clean speech 所所train 的model	15-2
於是我想辦法把這兩個結合起來	15-2
我把這兩個把把這個加上來	15-2
想辦法得到這一個這兩個結合的	15-2
於是呢那如果我有辦法能把這個跟這個model 結合起來的話	15-2
那我應該是等於是能夠辨識這種雜訊加上這個聲音的時候的	15-2
我等於是現場直接來來動	15-2
那麼等到這段話講完的時候我這邊開始有有有一堆silence 的時候	15-2
我現在又可以抽另外一堆雜訊	15-2
我把這個雜訊呢跟這個可能不一樣了	15-2
我把這堆雜訊再來重train 一個model	15-2
那這個model 就調過一次是根據這個調了	15-2
調完之後我再把它再加過來	15-2
因此呢我現在train 我要辨識這邊的聲音的時候呢	15-2
我就用這個這段聲音所train 的model	15-2
跟它的結合之後	15-2
來辨識這邊的	15-2
待會這邊又有一段silence	15-2
我又可以抓一堆model 抓一堆noise 來我又可以再train 一個noise model	15-2
那我再這樣調等等	15-2
這個是所謂的parallel model combination 的意思	15-2
就是我不斷讓它們平行地在做這個combination 的動作	15-2
我不斷隨時抓real time 抓最新的noise	15-2
然後train 最新的noise model	15-2
來反應當時的noise 長怎樣	15-2
把當時的noise 特性去加到這裡面加到這裡面來	15-2
讓它一起於是我這邊就是有等於是ㄚ加上noise 的ㄚ	15-2
這是ㄧ加上noise 的ㄧ	15-2
這是ㄨ加上noise 的ㄨ	15-2
那如果是這樣的話呢我用這個model 來當我用這些東西得到呢我就辨識這堆聲音	15-2
我這邊得到的就辨識這邊的聲音等等	15-2
那如果這樣的話呢我就可以嗯得到一個比較好的結果	15-2
這就是所謂的p m c	15-2
那這樣的精神基本上嗯你可以想到的是說	15-2
我主要的目的是在處理additive noise	15-2
它並沒有真的處理到這個沒有處理到這一塊	15-2
它處理到就是就是這個additive noise	15-2
等於是說我我現在想辦法把這個東西train 一個model	15-2
跟原來這個model 去結合等等	15-2
所以它處理的是additive noise	15-2
然後呢那它的基本的想法應該是說	15-2
最理想的recognition 是如果你的model 都是train with matched noisy speech	15-2
也就是說最理想的狀況是我如果知道我的noise 是這樣	15-2
我就把這些noise 加到ㄚㄧㄨ裡面去	15-2
然後用他們來train 在這種noise 情形之下的ㄚㄧㄨ的model	15-2
這個時候的model 是所謂的這個train with matched noisy speech	15-2
如果你如果真的有跟當時的noise 完全相同的noise	15-2
加到ㄚㄧㄨ的聲音裡面去	15-2
然後用它們來train 的model 來辨識在這種noise 下之下的ㄚㄧㄨ顯然是最好的	15-2
所以呢這些最好的recognition 呢是用這個完全match 的完全全一樣的noise 加上去	15-2
clean speech 那些speech 所train 的model	15-2
只是說這個是impossible	15-2
因為你的noise 千變萬化	15-2
對不對你不可能如果有一萬種noise condition	15-2
你不可能去train 一萬套model	15-2
所以這個其實你是做不到的	15-2
因此呢怎麼辦呢	15-2
就是我generate 一個real time 隨時generate 一個noise model from the noise	15-2
collect in the recognition environment	15-2
during silence period	15-2
就是whenever 有一個silence period 的時候	15-2
我就我隨時在做這個detection	15-2
那我whenever 知道有一個silence period	15-2
我就把這個noise 抓來	15-2
train 當時的noise model	15-2
然後呢我得到這個noise model 之候呢	15-2
去跟原來的clean speech model 去做結合啊	15-2
去做結合之後產生在這個noise 狀況之下的model	15-2
於是我就可以得到在這個noise 狀況之下的有noise 的ㄚ	15-2
有noise 的ㄧ	15-2
有noise 的ㄨ的model	15-2
然後用它來做recognition 等等	15-2
所以說它基本的想法是這樣的	15-2
但是這樣做有一個最重要的難題	15-2
就是我們的recognition 是都是m f c c 嘛	15-2
也就是所謂的cepstral domain	15-2
是m f c c	15-2
但是noise additive 不是不是在m f c c 上面是在哪裡	15-2
是在linear spectrum domain	15-2
或者是說time domain	15-2
也就是說你如果看這個的話	15-2
你就知道我的我是在time domain 相加	15-2
這個這個x n 對不對	15-2
就是這個x n 加上n 這是在time domain 相加	15-2
在time domain 相加的時候在m f c c 上面呢	15-2
它是一組怎麼樣的關係呢	15-2
是一個個非常複雜的關係	15-2
那我們得要handle 這個關係	15-2
那麼這個關係是什麼關係也許我們底下這個圖看得比較清楚	15-3
啊像這張圖	15-3
那麼這張圖所說的是真正的的m f c c 之間的關係	15-3
那你記得m f c c 是怎樣的	15-3
我們是這個經過一個f f t	15-3
得到的是一個spectrum	15-3
然後我在這上面用filter bank	15-3
一個個三角形的filter bank	15-3
得到一個值對不對	15-3
每一個三角形的filter bank 得到一個值	15-3
每一個三角形的filter bank 得到一個值	15-3
我再把這個值拿去做這個啊取它的energy	15-3
取它的這個這個power	15-3
這個絕對值的平方	15-3
然後再取log	15-3
當我這個都做完了之後呢我再做一次discrete cosine transform	15-3
這樣之後才得到m f c c	15-3
ok 這中間有這麼多個程序的	15-3
那我不只是frequency domain 而已	15-3
我還絕對值平方我還做log	15-3
我還做d c t	15-3
才是這些東西	15-3
那我現在要辨識的是這些東西啊	15-3
我現在辨識的譬如說這個model 是什麼model	15-3
我們說他譬如說這個state 是長怎樣的	15-3
這個state 是長這樣的	15-3
它裡裡面有有這些個gaussian	15-3
那它有這個mean	15-3
它有這個covariance 等等	15-3
這些是什麼都是都是in m f c c domain	15-3
對不對這都是三十九維的m f c c 在那裡面的東西	15-3
那麼這個noise 跟這個東西在time domain 是相加的	15-3
可是在這裡面到底是什麼關係呢	15-3
那就是我們這裡所說的這張圖所畫的	15-3
就說呢你你第一個你可以想像	15-3
我在time domain	15-3
我在這邊的時候是time domain 是x n	15-3
這是我的speech	15-3
noise 是n 的n	15-3
這兩個是相加的	15-3
time domain 是相加的	15-3
經過f f t 之後呢	15-3
在frequent domain 還是相加的	15-3
還是相加的可是呢我取絕對值平方之後呢	15-3
其實已經不完全是相加了對不對	15-3
你這邊可以想像成為我的是是這個這這兩個是相加的	15-3
我取了f f f t 之後這邊還是相加的	15-3
然後呢我取了絕對值平方之後呢	15-3
其實已經不完全是相加	15-3
不過我們仍然可以假設	15-3
非常接近相加	15-3
那就是所謂的linear power spectrum domain	15-3
所謂的linear power spectrum domain 就是指這一個	15-3
就是說我做了絕對值平方以後	15-3
它是相當於power spectrum 的的的power 的spectrum	15-3
那在這個時候呢我們仍然可以假設	15-3
雖然這已經是不完全正確了但是還是可以假設	15-3
這是所謂的linear power 的spectrum domain	15-3
它們還是相加的所以我這邊的符號裡面	15-3
s 是我的clean speech n 是我的noise	15-3
我仍然可以假設它們是相加的	15-3
可是再來我幹嘛呢	15-3
再來我就取log	15-3
一取log 鐵定不是相加了	15-3
那取log 之後呢就變成另外一種關係	15-3
那我這邊的的這個符號呢	15-3
這個這個x 跟n 代表的是clean speech 跟noise	15-3
那這個x 呢是真正的noisy 的speech	15-3
然後呢那這個是指它在剛才講的linear power spectrum domain	15-3
就是取了絕對值平方以後的這個東西	15-3
我們假設它是相加的話	15-3
我取了log 之後呢	15-3
就是做了這個log 之後	15-3
那做了這個log 之後呢	15-3
你至那我們現在的符號就是我右上右上角加一個l	15-3
代表這個是取了log 之後的	15-3
所以這個x 的l 就是這個x 取log	15-3
這個東西取log 就是這個	15-3
那個這個x 取log 就是x 的l	15-3
這個n 取log 之後呢就是n 的l	15-3
這就是這邊的這三個式子ok	15-3
所以呢我右上角加了這個l 呢就在這個domain	15-3
那這個domain 我們稱為log spectrum domain	15-3
取log 的叫做log spectrum domain	15-3
那在這裡的話呢我就右上角都都加了l	15-3
所以呢我的這個x 取了log 得到這個右上角的l	15-3
x 取了log 之後得到了這個x 的l	15-3
n 取了log 得到了這個東西	15-3
那如果是這樣的話	15-3
那它們這三個東西的關係在這個domain 是怎樣的呢	15-3
那你會發現它們的關係是這樣	15-3
也就是說你需要把這個clean speech 的在這個domain 的這個clean speech 跟noise 呢	15-3
分別先取exponential 才會回到這個domain 來	15-3
如果我把它們分別取exponent 分別取exponential 回來的話	15-3
這裡是相加的	15-3
所以它們分別取exponential 之後變成x 跟n	15-3
於是呢我就可以相加	15-3
這個加就是在這邊加	15-3
加完之後我再取log 回來	15-3
就是這個對不對	15-3
所以在這邊的時候這兩個只是這樣相加得到它的話我們假設它們是相加的了	15-3
在這邊不是相加	15-3
是要分別取exponential 回到這邊之後相加	15-3
然後再取log 回來	15-3
這個是在log spectrum domain	15-3
就是我取了這個時候	15-3
它們的關係是這個關係	15-3
那如果是這樣的話呢我現在還要再做一次discrete cosine transform 才會變成我的m f c c	15-3
還有一個這個勒	15-3
那這個更複雜這個其實是我們這邊很偷懶的就是寫一個c	15-3
這個c 的意思就是這個discrete discrete cosine transform 的意思	15-3
那這個是一個複雜的process 不過我們簡偷懶一下就這樣子寫	15-3
於是呢這樣得到的是cepstral domain	15-3
也就是m f c c domain	15-3
也就是這個m f c c 上面的東西	15-3
那我就在右上角寫一個c	15-3
那這個關係呢	15-3
那我們說這個就是這個log spectrum domain 上的的x	15-3
做一個cosine transform 得到c	15-3
這也是偷懶的寫法啦本來不是這麼這麼簡單	15-3
但你可以想像是一個cosine transform 的一個function of 這個啊	15-3
這樣子你把他想成是一個function	15-3
那我們是我這邊是簡寫就是寫成一個這個c 就是了	15-3
所以呢你把他經過cosine transform 得到這這這個東西	15-3
經過cosine transform 得到這個東西	15-3
同樣的這個x l	15-3
經過cosine transform 的話得到這個這個x c 就是這個	15-3
那這個noise 的linear 的log spectrum domain 的noise 呢	15-3
也是一樣經過cosine transform 得到這個東西	15-3
那麼因此就是這個式子	15-3
它這個log spectrum domain	15-3
要經過這個cosine transform 得到這個這個cepstral domain 的東西	15-3
那如果這邊關係是這樣的話呢	15-3
那它們之間的關係是什麼呢	15-3
是這個式子	15-3
那就更複雜啦	15-3
那就是你要先把這裡的signal 跟noise	15-3
分別先做inverse cosine transform	15-3
才可以對不對它們要先經過inverse cosine transform	15-3
得到這邊的這個那就是這兩個式子	15-3
所以呢我這兩個先經過inverse cosine transform 得到這邊的這個東西	15-3
然後呢在在這個地方之後我再分別取它們的exponential	15-3
回到這邊來	15-3
所以它們都經過inverse cosine transform	15-3
再經過exponential	15-3
才一路回回到這來	15-3
到這來才可以相加	15-3
加完之後呢我再取log 回來到這裡	15-3
再做一次cosine tansform才會到這裡對不對	15-3
所以呢我在這邊的時候呢是要這樣子做的	15-3
那麼那麼因此你可以想像到	15-3
我們在講這個幹嘛是在講這件事	15-3
我現在做recognition 的時候這些model	15-3
這些model 裡面的東西都是m f c c	15-3
都是在這裡的	15-3
那你這邊抽到的noise 我也可以做m f c c	15-3
也在這裡的	15-3
那你怎麼怎麼做怎麼做這兩個相加呢	15-3
你必須在這裡做	15-3
這裡不能做所以一定要轉回去轉到這邊來	15-3
在這裡做相加再轉回來啊	15-3
那這整個過程就是我們在這邊講的這個	15-3
noise model 跟你clean speech model 怎麼加呢我一定要回到那個domain 去加	15-3
那這個一個更清楚的圖呢就是底下這張圖講的	15-4
那這張圖所說的事情呢	15-4
那啊其實我現在畫成橫的了	15-4
不過意思跟剛才是一樣的	15-4
這個linear spectrum domain	15-4
哦我這邊是分成三段	15-4
右邊這個linear spectrum domain 就是剛才的上面這裡	15-4
然後中間的log spectrum domain	15-4
就是剛才的中間這裡	15-4
然後呢左邊的cepstral domain 呢	15-4
就是就是剛在的這裡	15-4
那麼因此呢我們剛才講說你要你要把它一路這樣轉回來轉到這邊來相加再transform 回來	15-4
那在我們這邊這張圖的話就一路這樣過去	15-4
在這邊相加再一路transform 回來	15-4
那這個情形其實講的跟剛才是一樣的	15-4
不過這邊稍微再再detail 一點	15-4
那你就知道這個關係其實不是那麼簡單	15-4
那舉例來講呢	15-4
我們剛才這是一個簡單的示意圖	15-4
我們說喔ok 你如果這邊在log spectrum domain	15-4
做一個cosine transform 變成cepstral domain	15-4
或者這邊做一個inverse cosine transform 變成這個domain	15-4
這個所謂cosine 跟inverse cosine tansform 真正的做的時候	15-4
是這個式子	15-4
那麼你真正要做的是它的每一個mean 跟covariance	15-4
也就是說我的譬如說ㄚ的model	15-4
裡面的每ㄚ的model 每一個state	15-4
每一個state 裡面的每一個mean	15-4
它有它的mean 跟它的covariance	15-4
那這個mean 跟這個covariance 怎麼轉	15-4
那這個mean 呢就是在我右上角寫一個c 表示是在這個cepstral domain	15-4
就是m f c c 的就是這些個clean speech 的hidden markov model	15-4
這些clean speech 的hidden markov model 上面那些m f c c 的以m f c c 為單位的mean 跟variance	15-4
你要怎麼轉到log spectrum domain 來	15-4
其實是做一個這樣子的東西	15-4
那這裡的c 是我們把cosine transform	15-4
寫成一個matrix 相乘	15-4
然後呢那其實是一個inverse 的那個matrix	15-4
你這樣呢可以把這個mean 轉到這個mean 來	15-4
那covariance 轉得更複雜是左邊右邊各乘一個這個cosine transform 的那個matrix	15-4
一個是有transpose 你要這樣子做的話呢你可以轉到這個來	15-4
那麼因此呢這個意思是說你原來train 好的這個ㄚ還是ㄧ還是ㄨ	15-4
裡面的這些個mean	15-4
用這個方式把它轉到這邊來	15-4
轉到這個log spectrum domain	15-4
然後covariance 轉過來	15-4
之後再進一步你要轉回到linear domain 的時候還要更複雜	15-4
那這個這個其實就是我們剛才的那個那個那個exponential 跟log	15-4
這邊寫的很簡單喔我這個只是取一個log	15-4
這邊取一個exponential	15-4
其實不是	15-4
你如果仔細去分析那個數學的話	15-4
是這個詳細數學我們這邊不講啊	15-4
不過你你你去看的話你去看reference 就知道了啊	15-4
那麼其實是蠻複雜的	15-4
你這個是log spectrum domain 的mean 跟covariance	15-4
那你要變成linear spectrum domain	15-4
那其實呢是嗯有更複雜這裡表示是我的mean 的第i 個component	15-4
那這是表示這個covariance matrix 第i j 個element	15-4
那我要做這樣子的這裡面是有exponential	15-4
但是這個exponential 遠比我們的剛才講得複雜嘛	15-4
我們這邊好像就是做個exponential 就可以過來	15-4
其實不是這樣	15-4
那是要這樣子的數學式子	15-4
那你這樣得到linear spectrum domain 的mean 跟covariance	15-4
那我的因此呢我原來train 好的clean speech 的model	15-4
我不是直直接用它	15-4
我經過這兩個transformation 之後	15-4
變成linear spectrum domain	15-4
其實我存在我的recognizer 裡面的model	15-4
是這個model	15-4
是這個model 因此我隨時可以把noise 加進來	15-4
那麼因此呢我我原來做recognition 的時候	15-4
我所有的這些model	15-4
我就是存m f c c	15-4
但是我現在不是	15-4
我現在是經過兩次的transformation 就是這邊講的這兩次transformation	15-4
把它變成另外一種東西	15-4
那我現在這上面的這個mean 跟covariance 呢	15-4
是用是用這個東西來存的	15-4
我存起來是在linear spectrum domain 來存的	15-4
我存的是這些model	15-4
而不是這個model	15-4
當我存的是這個model 的時候呢	15-4
這個domain 跟noise 是可以直接additive 可以相加的	15-4
所以呢我現在如果現場抽了這個noise 的話呢	15-4
我把這個noise 可以train 出我的model 來	15-4
那也一樣這個model 裡面有mean 跟covariance	15-4
那所以呢這個是我的clean speech 的model 的mean 跟covariance	15-4
這是我的noise 的mean 跟covariance	15-4
ok 所以這是noise 的h hidden markov model	15-4
我現在clean speech 我都以這種方式來存	15-4
所以這個是linear spectrum domain	15-4
這個是linear spectrum domain 所以可以相加	15-4
然後我這邊所train 出來的model	15-4
我也這個noise model 我也以這個domain 求出來	15-4
所以呢這兩個我都在這裡	15-4
所以可以用相加的	15-4
那因此我的mean 呢就是把clean speech 的mean 加上noise 的mean	15-4
covariance 就是clean speech 的covariance	15-4
加上noise 的covariance	15-4
那這邊你可以調一個參數就是g	15-4
你要讓你的noise 多大	15-4
那其實這個g 也是可以根據這個來來調的嘛	15-4
就是說你可以算這中間的s n ratio	15-4
你第一次可能比較難啦因第一次你不曉得我聲音多大	15-4
但在這裡的時候你就可以算兩個的s n ratio	15-4
就是這個這個這裡的noise 有多大	15-4
然後我的speech 的聲音大概有多大	15-4
你可以去根據這兩個的ratio	15-4
來求這個g 值	15-4
當你把這個g 做好之後	15-4
你就得到這個model 這個這兩個加在一起的mean 跟covariance	15-4
那到這個時候我等於把這裡面的每一個mean 跟covariance 都把noise 加進去了	15-4
於是呢我現在一路轉回去	15-4
我真正的recognition 要在這裡做嘛	15-4
我們recognition 必須是m f c c 的嘛	15-4
所以我一路再轉回去	15-4
那這個process 跟這個就是它的inverse 啊	15-4
那因為這個數學式比較複雜所以你看起來就好像複雜其實是一樣的嘛	15-4
你很容易看舉例來講像	15-4
像底下的這個式子就是上面這個式子的inverse	15-4
所以呢你要先把它除以mu i mu j 對不對	15-4
然後再加一這邊是減一嘛	15-4
然後再取log	15-4
於是就得到新的嘛	15-4
等等所以這個式子就是它的inverse	15-4
這個式子就是它的inverse 等等	15-4
所以你加好後之後再一路轉回來	15-4
那這個是它的inverse 這個是它的inverse	15-4
那麼因此呢這樣一路回來之後你再辨識	15-4
這個是這個p m  c 的基本精神是這麼做的	15-4
那當然這個這個這個數學蠻複雜的	15-4
所以呢從這邊走到這邊	15-4
你要real time 的一抽出來馬上train 出來一路做這麼多事	15-4
其實是不大容易的	15-4
所以這原始一九九五年那個年代他們做的時候	15-4
中間做了很多assumption 跟approximation	15-4
也就是做一些假設之後我讓這個數學簡化一點我才能夠real time 一起來做	15-4
但是既使經過了一堆assumption	15-4
把它做一堆approximation 之後	15-4
這個效果是不錯的嗯	15-4
這個其實中間我們都做過的所以我們都知道	15-4
我們做過的這樣子起來的話呢	15-4
你確實可以當場抓一堆noise	15-4
然後呢你經過你的transformation 之後	15-4
你在這邊把它組合	15-4
然後再回來啊	15-4
所以呢那那就是我們剛才講的就是你你真正的那些聲音的的這個這個clean speech 所train 的那些model	15-4
你並不以m f c c 的方式存在那裡	15-4
而是你一路轉回來	15-4
以這個linear spectrum domain 的的的以這些單位存在那裡	15-4
然後我隨時加noise	15-4
隨時過來在這邊做recognition	15-4
那這個也就是我們之前這張圖的意思	15-4
就是說你train 好的clean speech	15-4
你不這個是在cepstral domain	15-4
你要經過這就是嗯inverse cosine transform 這是exponential	15-4
經過這兩個	15-4
我要transform 到這個linear spectrum domain	15-4
在這邊你才可以跟它做combination	15-4
那我的noise 直接在這邊求出來就好了	15-4
noise 不必到這來	15-4
noise 直接在這邊做	15-4
啊那你直接在這邊做combination 之後再轉回來啊	15-4
所以這張圖所畫的	15-4
其實就是剛才這張圖的簡化嘛	15-4
啊就你在這邊做noise 然後跟它做結合之後再轉回來	15-4
在這邊做辨識嘛	15-4
那也就是我們這邊所講的意思	15-4
那麼也就是說呢你的這個因為你的這個recognition 你必須是cepstral domain	15-4
可是呢noise 跟signal 是additive 在哪個domain	15-4
linear spectrum domain 你才可以說它們是additive	15-4
而不是在cepstral domain	15-4
所以你必須把它們都transform 回到這個domain	15-4
然後去去相加	15-4
那麼通常我們就是調這兩個東西	15-4
就是mean 跟covariance	15-4
mean 跟也就是也就是這邊的這兩個	15-4
這兩個mean 跟covariance 我們可以調啊	15-4
就把這個noise 都加進去	15-4
那因此我用的就是一個noise model	15-4
跟clean speech model	15-4
這個嗯在在那個年代的時候一直有一個很大的疑問	15-4
我到底要train model 的時候到底要乾淨的聲音來train 還是用noise 的聲音來train	15-4
因為你如果用noise 聲音來train 的話	15-4
好像跟noise 比較接近比較match	15-4
但是在這個方法而言不是的	15-4
我是用用乾淨的聲音來train	15-4
因為我有了乾淨的聲音來train 之後	15-4
我可以加任何一種noise	15-4
我現場抽任何一種noise 加進來都可以	15-4
就可以得到任何一種noise 環境之下	15-4
那這是一個非常typical 的example 就是我們講的model based approach的做法	15-4
那這個的我想這個的啊這個都已經講過了嘛啊	15-4
它喔就我剛剛講的他用了一堆assumption 跟一堆approximation	15-4
來簡化中間的數學	15-4
然後使得我的這個整個process 可以real time 來操作	15-4
那這個就是所謂的parallel model combination	15-4
那詳細的內容	15-4
我想這是相當這算是相當經典的一個作品	15-4
雖然今天來講已經有很多方法比它好了	15-4
不過這個是相當值得學習的一個很好的想法	15-4
這個的的原始paper 是這一篇啊	15-4
我這個二的reference 就是在講在講這個	15-4
好那這個是model based 的第一個example	15-4
那底下我們講第二個example 呢	15-5
就是所謂的vector taylor series	15-5
另外一個example 就是也是model based approach 那是所謂的這個vector taylor series	15-5
那這個觀念其實跟剛才幾乎是一樣只是做法不同而已	15-5
也就是說它的想法還是這樣	15-5
那麼我在開機前的零點一秒或者什麼的我抽出noise 來我train 一個model	15-5
我隨時碰到silence 我都抽出noise 隨時train 新的model 然後跟clean speech model 去做結合	15-5
所以這個觀念跟剛才都一樣啊	15-5
那唯一不同是說它的做法不同	15-5
它覺得說你像剛才那個個方法太複雜了	15-5
它想一個比較好的辦法來解決剛才的我們剛才說你現在的你那些model 你要經過這麼在那個domain 去給它加加之後再這回來你real time 要做這麼多事情實在很複雜	15-5
那他想了一個比較簡單的辦法	15-5
那他用的辦法就是所謂的taylor series expansion 也就是這個這個vector taylor series 這個這個這個字的由來	15-5
這個taylor series expansion 是什麼呢	15-5
就是這個那這個是你從小就學微積分的時候就學過這個東西	15-5
那那個是one dimension 的function 的時候有這麼一個taylor series 泰勒展開式	15-5
那現在把那個再延伸變成n dimension 變成vector 就是了	15-5
如果這個是f 的function那我現在如果要求某一個x 的這個f of x	15-5
怎麼求呢這邊是說你如果要求某一個x 的那一點f of x 的話呢	15-5
那你如果知道c 的那一點的話可以這樣子來求	15-5
譬如說我如果知道c 在這裡的話	15-5
那麼f of c 我是知道的這一點是c 點是我知道的這一點是f of c 我是知道的	15-5
我就可以在這一點求它的斜率	15-5
然後呢那這個這個斜率呢就是就是這個這個d f d x 的c	15-5
所以呢這個斜率就是d f d x 的c	15-5
然後這一段呢就是x 減c	15-5
所以呢你如果把這個x 減c 乘上這個斜率的話呢	15-5
你就可以得到你把這個乘上這個斜率就得到得到這一段	15-5
所以呢你如果直接把所以你如果直接把f c 加上這個第一項的話呢	15-5
就等於是把這個f c 加上x 減c 乘上這個斜率就得到這一項所以你得到就是這一項	15-5
那得到這一項雖然不是原來的f of c 不是原來的f of x 但是接近嘛	15-5
那中間差的這個呢那就是誤差	15-5
那如果你可以求第二項的話會更好	15-5
第二項的話呢就是把這邊再求一個二次微分	15-5
然後跟這個的平方等等那就是第二項	15-5
那你如果做第二項的話呢這個東西會更接近一點做第三項就會再更接近一點你看你做能夠做幾項的話呢	15-5
其實就可以得到譬如說這個這個是n 次項的話呢你就會得到比較接近一個approximation	15-5
這是所謂的泰勒展開式你從前學微積分或者學數學一定學過這個泰勒展開式	15-5
那它現在怎麼辦它就用這個泰勒展開式進一步延伸為兩兩個function	15-5
我們這裡只是f of x 嘛他現在變成g of x y	15-5
我如果裡面有兩個variable 的話變成x 跟y 的話	15-5
我仍然可以這個其實z 等於g 的x y 就跟這邊的y 等於f of x 是一樣的我把它延伸變成有兩個變數的我把它延伸變成有兩個變數的	15-5
然後我再讓它的變數都變成vector	15-5
這變成有n dimension 的x y 有n dimension 的y z 有n dimension 的z	15-5
然後呢我再把它們變成random 的vector	15-5
這裡面每一個dimension 都是random variable 還是有這個關係的	15-5
當它變成random variable 之後我可以再假設它們都是gaussian	15-5
當它們都是gaussian 之後我這個式子就變成這個式子	15-5
那因此呢那這個式子是什麼這個式子其實是我們剛才的這個式子就是這個式子	15-5
換句話說它沒有要像我們剛才的這個paparallel model combination 我們是要從這裡一路轉轉轉一路轉轉轉回到這來在這邊相加再轉回來	15-5
這個計算量太大了他說呢我只要轉到這裡就好	15-5
在這裡的這個這個加法做這裡這裡的這一堆複雜的這個數學呢其實我就把它變成一個taylor series	15-5
所以呢它就我就不要從這裡我們剛才的parallel model combination 是要從這裡一路轉到這兒來然後再回來	15-5
那它等於觀念沒有改變他只是一個比較聰明的做數學的方法	15-5
他說我只要做到這裡就好了我在這裡做	15-5
這裡還是很很複雜啊沒有關係我這裡就用泰勒展開式來做	15-5
所以就是這個式子變成一個泰勒展開式	15-5
所以你你看這個式子其實就是我們現在的這個式子ok	15-5
所以這個式子就是我們剛才的這個式子是完全一樣的	15-5
那我現在是把這個式子裡面的這個這個當成x 這個當成y 然後這一堆function 就是g 的x y	15-5
當這個是x 這個是y 的時候g 的x y 這個就是z	15-5
因此呢它們在這裡相加的話呢是有一個複雜的數學關係是沒有錯	15-5
但是呢我就把它看成是z 等於g 的x y 變成這個式子	15-5
那我們剛才說這個式子的話呢我們把它看成是裡面有兩個variable 的f	15-5
因此呢我可以我也一樣可以對這兩個variable 分別做微分	15-5
然後我再讓兩個variable 分別都是vector	15-5
因為我現在要把譬如說它的mean 要放進來的話這是一個vector	15-5
這是這是一個vector 這是一個vector 變成n dimension 的vector	15-5
那我一樣可以把這個式子推出來	15-5
然後我真正推的時候它們其實不不只是vector 而且每一個dimension 上面的element 呢都是random variable	15-5
所以它們是random vector	15-5
那這上面每一個element 都是random variable 的時候呢我再假設它們是gaussian	15-5
如果是gaussian 的話呢這些微分都可以求	15-5
所以呢於是我這這個求法其實就是用這個泰勒展開式	15-5
但是我把這個數學帶到這裡這裡面來做	15-5
那如果它是gaussian 的話呢那它們的這個一次微分兩次微分這些東西呢這是一次微分兩次微分這些東西我都可以求得出來於是答案就出來了	15-5
簡單講就是這麼這麼意思這就是叫作vector taylor series 啊	15-5
所以呢我們說基本精神是跟p m c 是非常像的	15-5
也就是說呢我在我我我我隨時把這個noise 我隨時像那邊一樣的作法	15-5
我就是把這個silence 裡面的noise 抓到之後	15-5
我隨時產生noise 的model	15-5
之後呢我就跟我的clean speech 的model 去作結合	15-5
這個觀念是一樣的	15-5
不同的地方是說	15-5
我是直接在log spectrum domain 就直接作組合作加乘了	15-5
我不回到linear domain 去我直接在log spectrum domain 就直接加乘	15-5
那怎麼加呢用taylor series	15-5
就是我們剛才講的這個one d 的taylor series 把它generalize 變成兩個random variable 而且這兩個都是vector 都是random vector	15-5
那其實z 也是所以是是x y z 這三個都是n dimension 的random vector	15-5
然後呢你假設它們都是gaussian 它們的mean 跟covariance 都知道	15-5
當它們mean 跟covariance 都知道的時候	15-5
那麼我這個這個taylor series 就可以變成這樣子	15-5
於是我的這個新的mean 的這個組合的那個vector 那個gaussian 的mean 怎麼求可以用這個方法來求	15-5
那麼這個variance 怎麼求都可以用這個方法來求	15-5
ok 應該說這裡還沒有假設它們是gaussian	15-5
只要知道它們是只要知道它們是mean 跟covariance 就可以求了	15-5
那通常在這個時候呢我們在假設它們是gaussian 的話這個微分更好微	15-5
啊假設gaussian 是讓它們微分更容易微就是了	15-5
那這樣子算出來之後呢我就把這個關係代到剛才的那個linear spectrum domain 啊log spectrum domain 的關係來	15-5
把這個就當成是這個z 然後我這樣就照做於是我答案就出來了	15-5
那這個呢就是vector taylor series	15-5
那我想這個觀念很一樣所以我們就不需要多講只是一些數學不同而已	15-5
你如果有興趣看這些數學的話呢這個reference 在下面一篇	15-5
就是三的這個就是它當時的原始paper 是這一篇就是講這個東西	15-5
那這個的效果不錯那這個我們從前也做過的	15-5
那這個因為用了這個vector series 關係所以它的計算量沒有那麼大	15-5
那也有相當程度精確	15-5
所以事實上它這邊其實只做到兩次項只做到兩次項而已啊	15-5
但是事實上效果不錯計算量也小很多那所以呢得到不錯的效果	15-5
這個是一個後來使用地非常普遍大家都用它效果也不錯的方法這是	15-5
以上講的兩個例子是所謂的model based	15-5
那麼再來呢我們來看feature based	15-6
那麼什麼是feature based 呢我們稍為回憶一下	15-6
所謂feature based 是說我不動那個model 我來動feature	15-6
我希望讓我真實的環境裡面的有被破壞的訊號的feature	15-6
跟原來的clean speech feature 最好很像	15-6
啊我的目的就是讓它們的feature 像	15-6
所謂feature based 那怎麼做這件事呢	15-6
那麼這裡面feature based 最基本最常用的這個就是所謂所謂的cepstral mean subtraction 這個c m s	15-6
那這個方法是啊今天普遍用的最多的幾乎所有的語音系統都用這個	15-6
它的效果有相當好的效果嗯就是c m s	15-6
那它的意思講起來非常簡單	15-6
它原來的原來發明這個方法的時候是為了解決convolutional noise 的	15-6
什麼是convolutional noise 呢你記得我們講的是說你這個東西呢被經過一些個convolutional 的破壞不只是加上去的noise 而是經過convolution	15-6
經過convolution 之後破壞之後我如何能夠讓這個feature 得到feature 還是可以跟它很像呢	15-6
那這個主要的想法來自我們講m f c c 的時候我們在七點零就說過了	15-6
就是說在time domain 做convolution 其實在m f c c 的domain 呢就是相加啊	15-6
那這點我們從前就說過了因為你是你你是這個做了f f t 的關係	15-6
做了f f t 之後你你如果本來是convolution	15-6
我的x 跟h 有一個convolution 的關係	15-6
經過了f f 經過了f f t 到到frequency domain 是加是變成加法	15-6
所以呢就變成它的它的它們這兩個的transform 的加法啊不是是乘法	15-6
這經過fourier transform 之後是變成乘法	15-6
然後呢我這個時候再取一個log 的話呢就變成加法	15-6
對不對基本上是這樣子的關係	15-6
也就是說我在求m f c c 的過程之中我有我有一個這個fourier transform 的過程還有一個取log 的過程	15-6
那transform 的fourier transform 呢把這個convolution 把這個convolution 變成相乘	15-6
那取個log 呢把相乘變成相加	15-6
所以其實m f c c 裡面	15-6
如果我我原來是兩個相做convolution 的話呢	15-6
我到了m f c c 是變成相加的	15-6
既然是相加的話呢那你其實只要減掉就行了嘛	15-6
如果你有辦法知道那個h 是什麼的話	15-6
如果我有辦法知道h 是什麼的話呢我就y 減h 不就是x 嘛	15-6
那麼那你有沒有辦法知道h 是什麼呢	15-6
你可以假設大多數的convolutional noise 改變是比較慢的	15-6
什麼叫作改變比較慢呢	15-6
你如果想這張圖我們說主要的convolutional noise 主要來自譬如說電話	15-6
如果是我電話的這個變化的話呢	15-6
你可以想像我打手機的時候在零點一秒之內我移動不會太多	15-6
或者在零點一秒之內整個的transmission 環境不會改變網路環境不會改變太大	15-6
所以你可以在零零點一秒之內我算假設它是一個固定的等等	15-6
啊那當然換了零點一秒之後呢下一個零點一秒可能改	15-6
但是沒有關係啊我就是每隔零點一秒我算一次嘛	15-6
所以我如果你如果假設說我的這個h 是因為是我打的手機而我人在走動所以我我不斷地在變化的話	15-6
那我可以假設零點一秒之內我的這個值變化不大	15-6
而我每一個零點一秒估計一次新的h 值這樣我就可以減啦	15-6
啊那所以基本想法就是這樣子	15-6
所以當時原來的這個想法是來自這樣子的觀念	15-6
就是我的convolution noise 在time domain 在m f c c 的時候呢變成相加	15-6
所以我只要有辦法估計那個加的那個h 把它減掉就夠了	15-6
那我可以假設這個h 變化比較慢	15-6
譬如說零點一秒才才變一次我就零點一秒估計一次嘛	15-6
那這樣的話我就可以減掉我基本上可以得到比較好的	15-6
那那問題是這個h 怎麼求呢	15-6
在當時一個最簡單的想法就是說我先假設我的x 是zero mean	15-6
如果假設我的x 是zero mean 的話那我那個y 我就做個mean 不就是h	15-6
所以呢我就減掉那個mean 把它變成zero mean 就好了嘛	15-6
那這個觀念就是所謂的cepstral mean subtraction 你就是把的cepstral 的mean 減掉	15-6
講起來非常簡單的觀念你說欸為什麼是zero mean 沒有錯這個假設是有問題的	15-6
啊說老實話當時他的想法是這樣來的他說假設它是zero mean 所以呢你這個h 呢這個h 呢就是我y 的mean 嘛	15-6
對不對如果x 是zero mean 的話我這個y 上面取個mean 不就是h 嘛	15-6
那麼因此呢我就減掉y 的mean 就好了嘛	15-6
所以當時他是這樣想的	15-6
那麼那也因為這樣子的話呢我其實就是把我的這個x 變成zero mean 就好了	15-6
那麼這個mean 怎麼麼求呢	15-6
over 一個utterance 或者是類似的我們剛才講譬如說我零點一秒求一次嘛	15-6
那後來其實真正做的時候	15-6
我們有很種多作法	15-6
一種是說每一個utterance 你你講一句話有一個逗點	15-6
等於是有一個停頓嘛對不對	15-6
那你這一段裡面就求一個mean 通通減掉	15-6
這段裡面再重新求一個mean	15-6
這兩個mean 會不一樣你再減掉	15-6
這就是所謂的by utterance	15-6
嗯就是over 一個utterance 來做	15-6
那其實還有很多別的辦法像我們後來其實不見得要這樣做	15-6
而是怎麼做你就是你如果要要做這個frame 的時候我就是前面算多少frame 後面算多少frame	15-6
你在這一段裡面求一個mean	15-6
拿來減做中間這的相減	15-6
待會呢待會到這兒來的時候呢	15-6
我就在這個前後算一個mean	15-6
對不對我永遠在前後算一個mean	15-6
然後呢來算中間的那一個	15-6
那其實也不一定要前後各取一段我也可以完全取前面	15-6
譬如說我現在要算這個的時候	15-6
我就拿前面的這一段	15-6
求一個mean 來減它	15-6
然後算這個的時候我在前面算這一段	15-6
算一個mean 來減它等等都可以啊	15-6
所以你其實用哪一段來算mean 其實都可以做	15-6
然後效果都不錯啊	15-6
所以呢那當然有最好的啦	15-6
最好的你可以猜到應該是這樣子	15-6
就是說我這個這個我永遠在前面後面都取一段	15-6
隨著時間變動啊	15-6
那像這類的方法我就因為我就是把mean 減掉嘛	15-6
那就是所謂的cepstral mean subtraction	15-6
那當時的想法原來是為了convolutional noise	15-6
可是後來人家發現這個其實如果不是convolutional noise	15-6
而是additive noise 一樣有效	15-6
如果說我的根本沒有這個	15-6
我根本沒有這個convolution 過程	15-6
只是加沒有noise 的話	15-6
我只是加noise 欸結果也很有效	15-6
這有點奇怪為什麼呢	15-6
後來了解其實這很簡單	15-6
就是我把它們都變成zero mean 了	15-6
所以它們統計就會接近	15-6
換句話說你可以想我們講的	15-6
這邊的問題就是它們不match 嘛	15-6
這裡面的data 的統計特性	15-6
跟這邊的data 統計特性不一樣不match	15-6
既然不match 的話我有一個辦法	15-6
就是我把它通通通通減掉mean	15-6
變成zero mean	15-6
把它也通通減掉mean 都變成成zero mean 的話那這兩個就比較像了嘛	15-6
就是這樣子啊	15-6
也就是說我本來這堆的統計特性跟這一堆的統計特性都不一樣	15-6
那我至少把它們的mean 都變成零	15-6
當他們的mean 都變成零的時候就比較像了	15-6
所以你把它們的mean 變成一樣之後它就像	15-6
所以結果turns out 你即使不是convolutional 的	15-6
你是additive noise 一樣有效	15-6
所以呢這個這個東西會turns out 是一個很容易做	15-6
只要減一個mean 而已就把它變成zero mean	15-6
然後我training 也就先做了zero mean再去train 啊	15-6
那這樣的話呢都有很好的效果	15-6
那當然你如果從這個觀點來看的話呢	15-6
它是直接immune to convolutional noise	15-6
也就是說你如果跟任何東西去做convolution	15-6
得到的答案是不變的	15-6
對不對你想想就了解這點	15-6
也就是說你現在我的x n	15-6
如果跟任何東西去convolve	15-6
我做了做了c m s 之後	15-6
答案都是一樣的	15-6
因為你convolve 的那個東西都是加了一個加了一個東西	15-6
那你不管它原來是不是zero mean 啊	15-6
我都把它變成一個zero mean 的東西嘛	15-6
所以我我最最後的做法其實就是c m s 就是把它變成zero mean	15-6
對不對即使它原來不是mean 也不是zero mean 也沒有關係	15-6
即使它這個假設不正確	15-6
其實這個假設真的不正確	15-6
因為沒有理由它是zero mean	15-6
但是我就反正把它都把它變成zero mean 了之後	15-6
我的train model 也是用zero mean 去train 的	15-6
所以我就通通把它變成zero mean 了	15-6
那麼這個時候呢	15-6
那麼不管你在跟什麼東西做convolution	15-6
得到的都是那個zero mean 的東西	15-6
所以就等於是完全把所有的convolutional noise 都解決掉了	15-6
那麼因此我們今天在講的時候	15-6
通常絕大多數的語音系統	15-6
都做了這一步	15-6
都做了這一步之後呢	15-6
其實這個convolutional noise 就不是問題了	15-6
幾乎都已經除掉了	15-6
因為你那那一步就等於把這個拿掉了	15-6
所以這個convolutional noise 就不是大問題就都減掉了啊我只要做了這一步	15-6
所以c m s 是個非常有效的方法	15-6
那當然你也知道它不改變delta 跟delta delta	15-6
也就是說我總共有十三	15-6
三十九維嘛我這個只做十三維啊	15-6
對不對我那個vector 是三十九維的	15-6
前面十三維m f c c	15-6
這邊是一次微分這是兩次微分	15-6
那你變把它變成zero mean	15-6
做做這個減法	15-6
不改變它的一次微分跟兩次微分	15-6
所以這二十六個沒有動	15-6
但是這十三個我都把它變成zero mean	15-6
啊答案是這樣子	15-6
那這個效果就就是不錯的啊	15-6
那這是最簡單而且效果很好的方法所謂的c m s	15-6
那後來有人在想了進一步	15-6
他說當然你把它變成zero mean 不是很有道理啦	15-6
那我們其實可以做得更好	15-6
做更好的方法就是底下這個	15-6
所謂的signal bias removal	15-6
那這個bias removal 意思是說我這個h 我應該仔細算一下	15-6
不要只是假設它是zero mean	15-6
我怎麼仔細假設呢	15-6
我就是用maximum likelihood 的criteria 來求它	15-6
我given 一個model	15-6
然後given 某一個h 的值的話	15-6
我可以算在這個條件之下	15-6
我會看到這些個observation 的機率	15-6
然後我想辦法調這個h	15-6
來maximize 這個機率可以做嗯	15-6
那這個詳細我就邊不講了我會給你一個reference	15-6
那其它的觀念都很像啦它只是說我現在這個我不要用mean	15-6
這個zero mean 的假設太簡單了	15-6
我應該做更精確一點	15-6
怎麼做我就是用這個maximum likelihood 的方法	15-6
這個就是likelihood function 嘛	15-6
就是given 這個model given 這個model	15-6
然後given 假設我設了某一個h 的話	15-6
那麼在這個情形之下	15-6
我我要看到這個observation 的機率要最大	15-6
然後我去調這個h	15-6
哪一個h 讓這個機率最大我就用那個h	15-6
那這個的h 做出來的這個h 拿來減的話會更好	15-6
你就不要用zero mean 來減用這個來減	15-6
會得到更好	15-6
那這個怎麼求這個怎麼求的	15-6
用e m 啊	15-6
所以我們這個這個講完我們就要來講e m	15-6
所以e m 是到處都在用	15-6
我用e m 的話呢我可以得到這個東西	15-6
那麼因此呢我就可以這個經過幾個iteration	15-6
得到這個值	15-6
那這個效果會比這這個c m s 又好一點	15-6
這是所謂的signal bias removal	15-6
那你看顧名思義就知道	15-6
所謂的bias 就是這個這個h 嘛	15-6
那我把它拿掉嘛	15-6
啊其實就是一樣的意思	15-6
那這個的reference 在我上面的再下一篇	15-6
就是第四篇就是signal bias removal	15-6
啊有maximum likelihood	15-6
你如果詳細去看你就會知道它是怎麼做的	15-6
那這是一個很簡單的例子在說	15-6
我怎麼做它的feature based	15-6
這是feature based 的例子	15-6
那底下再來的下一個feature based 的例子呢	15-7
就是所謂的temporal filtering	15-7
什麼是temporal filtering 呢	15-7
我們原來的這是語音訊號	15-7
這是x n	15-7
這是一個一個的sample	15-7
我這個x n 我可以去做fourier transform	15-7
得到我的spectrum	15-7
然後呢我們說我們可以每一個frame	15-7
譬如說這兩百五十六個點換成五百一十二的點	15-7
我可以得到一個m f c c 的vector	15-7
譬如說這個是c one 這是c two 等等等等	15-7
總共三十九個	15-7
我待會呢譬如說shift 一百點	15-7
我可以得到下面一個c one c two 等等等等	15-7
這裡有三十九維	15-7
我待會再shift 一百點	15-7
我又可以得到一個等等等等	15-7
對不對那它說呢你如果是這樣的話我其實當這上面有noise 的時候	15-7
當我這個不管是convolutional 還是additive 我這個noise 加在這上面把它破壞了的話呢	15-7
就是把這個東西破壞了嘛	15-7
這個東西破壞了嘛	15-7
那我其實我也可以把譬如說這個c one 看成是一個signal	15-7
c two 看成是一個signal	15-7
於是呢我就會得到一個像這樣的	15-7
譬如說這是c one 的m	15-7
這個橫軸是m	15-7
這m 其實是我這個frame 的index	15-7
這是這個c one	15-7
這是這個c one	15-7
這是這個c one對不對	15-7
同理呢我c two 我也可以得到一個	15-7
c two 的m	15-7
那麼我可能是另外一個樣子的	15-7
這個是這是c two	15-7
這是下一個c two	15-7
這是下一個c two 等等	15-7
那麼因此呢我把這裡的我我我何必還要去看上面的noise 呢	15-7
我就看這個的就好了嘛我這樣總共有三十九個signal 在這裡	15-7
或者說我有十三個signal	15-7
對不對我有十三個signal	15-7
那每一個呢它其實變化地很慢	15-7
因為我這個是一百點	15-7
假設我每一百點取一個window 的話	15-7
我每一百點才會有一個點嘛	15-7
所以這個變化的很慢的c one c two	15-7
那我其實是在用這這堆東西在做recognition 嘛	15-7
那你可以想像如果這上面有noise	15-7
把這個破壞就是把這個破壞嘛你可以想像其實是這上面是有noise 嘛	15-7
是這上面是有noise 嘛	15-7
既然是這上面有noise 我可以在這上面做filter	15-7
想辦法filter 這上面的noise	15-7
那這就是所謂temporal filtering 的意思嗯	15-7
那麼他們稱之為temporal filtering 就是說	15-7
每一個component 在你的那個m f c c 那裡面	15-7
當成是一個signal	15-7
或者是它的time trajectory	15-7
它的paper 裡面有的時候叫作temporal filtering 他們稱之為這個time trajectory	15-7
當著time index 就是frame number progress	15-7
我現在這個這個這是frame number	15-7
每一個frame 每其實這每一個frame 相當於這邊一百點你如果是shift 一百點的話	15-7
所以我每一個frame 才有一個點	15-7
所以這邊的data 量比那邊少很多	15-7
這個data 量少很多	15-7
而且這個data 是真正的我的拿來做辨識的的參數	15-7
所以這是重要的東西	15-7
我在這上面做就好了	15-7
我不要去搞那個	15-7
這上面noise 太複雜了	15-7
我搞最上面的noise	15-7
那如果是這樣的話呢	15-7
我也可以把這個看做成是signal	15-7
我也一樣可以做fourier transform	15-7
那這個時候我所得到的東西呢	15-7
它也有這種它的spectrum	15-7
不過這個frequency 會很低頻	15-7
因為我變得很慢嘛	15-7
我我我上面是因為這個變化很快嘛	15-7
我這個sample 很多變化很快所以我變成一個很多高頻的東西	15-7
我現在是每一百點才有一個點啦	15-7
所以變化很慢所以我這個頻率很低	15-7
所以這是另外一種spectrum	15-7
跟這個是不一樣的	15-7
是這種很慢的signal	15-7
就它的這些個m f c c 的參數的frequency 的domain	15-7
那那個是什麼呢	15-7
叫作modulation frequency	15-7
也就是說就是我們剛講就是說你現在是把每一個feature 每一個m f c c 的coefficient	15-7
把每一個m f c c 的coefficient 看成是一個這個看成是一個signal	15-7
或者time trajectory	15-7
然後呢那這個signal 我也可以做fourier transform	15-7
得到它的frequency domain	15-7
這個domain 呢這邊的frequency 呢我們跟這種是不一樣的	15-7
跟這種是不一樣的這是另外一種frequency	15-7
那我們稱為modulation frequency	15-7
filtering	15-7
因為這上面的noise 其實就是破壞了這些值	15-7
那我就在這上面在這裡做做filtering	15-7
我就看在這個上面怎麼做filtering 我在這邊看它這邊長得怎樣	15-7
我在這上面做filter	15-7
那做怎麼樣的filter 呢	15-7
這個是最早的一個想法	15-7
當時發明這個的人他他說他做一個這樣filter	15-7
那這個filter 這個filter 是寫成z transform 的寫法	15-7
所以各位之中如果你對z transform 很熟的你就知道這是一個filter	15-7
那你不熟的話你也沒關係	15-7
你就知道我們是做一個filter	15-7
那這個filter 長怎樣呢	15-7
這樣的	15-7
那你會看到這個橫軸就是modulation frequency	15-7
也就是我是在這個frequency 上面去做filter	15-7
不是這裡	15-7
不是這裡我是在這裡做filter	15-7
我就是要filter 這上面的noise	15-7
然後呢你看它這個因為是很低頻的東西	15-7
不像那個很高頻喔這很低頻的你看最大的這個是十個hertz	15-7
這也是十個hertz 這裡是一百個hertz	15-7
這裡是一個hertz	15-7
所以呢他設計了一個這樣的filter 之後呢	15-7
大概只有在一個一個hertz 或者零點五個hertz	15-7
到十個hertz 左右	15-7
是pass band	15-7
讓這個讓這堆frequency 通過	15-7
超過十個hertz 以上就把它濾掉	15-7
同樣的低於零點五hertz 也把它濾掉了	15-7
他的理由是什麼呢他說這個就是我們的typical rate of change of the vocal tract shape	15-7
就你說話的時候我們嘴巴不會亂動啊	15-7
你我你你每一秒你你所謂十個hertz 什麼意思	15-7
十個hertz 就是一秒鐘變十次嘛	15-7
我一秒鐘如果有十個cycle 就是十個hertz 嘛	15-7
那你想我們其實人人說話你嘴你人說話是靠嘴巴動嘛你嘴巴不可能動那麼快啊	15-7
你最快譬如說一秒鐘動十次吧	15-7
如果一秒鐘動十次你也可以想像好比是一秒鐘講十個phone	15-7
這是那種連珠砲的像張小燕那種的人她才會一秒鐘講十個phone 嘛啊	15-7
那所以呢十十十這個十個hertz 就夠了嘛	15-7
那超過十個hertz 以上的	15-7
應該就不是人講話的聲音了	15-7
那就把它濾掉嘛	15-7
那就是noise	15-7
同樣呢你如果在零點五hertz 以下的話	15-7
那變成說你兩秒才發一個音	15-7
那大概也已也已經不是人講的啦	15-7
那那大概也是個noise 會把它濾掉啊	15-7
喔	15-7
所以他這是一個非常直覺的一個想法	15-7
就是我去這樣的設計一個filter	15-7
讓它呢大概在這個範圍之內譬如說在零點五hertz 到一個hertz 之間	15-7
這邊大概到十個herz 這邊這一段是它的pass band	15-7
然後呢再高頻的假設就不是人講的嘛	15-7
再低頻也不是人講的那這些都是都是noise 嘛	15-7
所以我至少這邊都可以濾掉嘛	15-7
那我他這一段的frequency 基本假設就是typical range of change of vocal tract shape	15-7
我們的嘴巴最多只能動那麼快嘛	15-7
ok	15-7
所以呢他就說因此我假設	15-7
那麼其它的更高的頻或者更低頻呢就是我的noise	15-7
於是我就設計這樣一個filter	15-7
那這樣的filter 呢同樣呢apply 對每一個signal 都一樣啊	15-7
它這邊我這邊畫的所畫就是跟我這邊畫的是一樣的意思啦	15-7
就是我現在的每一個	15-7
這是c one 的signal c two 的signal 到c 十三這十三個這十三維	15-7
我分別都當成是一個signal 都通過一個filter	15-7
啊	15-7
我都通過一個filter	15-7
然後得到一個新的	15-7
那這樣把這邊高頻跟低頻弄掉了	15-7
所以呢它等於是一個特別design 的temporal filter	15-7
把這個time trajectory 上面的這個noise 濾掉	15-7
那這樣做之後欸turns out 效果是有進步啊	15-7
結果這個這樣做之後呢我確實我就把我feature 變得比較好	15-7
那這個真的做是怎麼做	15-7
這一類的feature based 做法都是你要training 跟testing 一起做	15-7
也就是說你你現在會變得怎樣	15-7
我本來的training data 我就做這件事	15-7
我training data 裡面的那些個filter 的的那些的m f c c 我就做了一次剛才講的那個filtering	15-7
然後呢我的testing data 我也一樣做了那個	15-7
我當這兩邊都做了都做了這個時候它們就比較match 了	15-7
因為我的training data 跟新進來的recognition 要的聲音呢我這兩者的m f c c 我都做了這一步的動作	15-7
我都做了這一步動作都把它濾掉了	15-7
啊	15-7
因此它們比較match	15-7
而那些noise 都清掉了啊	15-7
那這樣的話呢它們效果欸是不錯啊	15-7
那這個是所謂的rasta	15-7
其實這個名子很奇怪因為它原這個全文是relative spectrum	15-7
那他就把這個很難拼嘛他就就隨便抓幾個字母就變成這個叫做rasta	15-7
不過因為這個這是他發明的所以他的paper 這樣寫	15-7
所以呢這個就是rasta 的基本想法就是這樣子	15-7
那詳細的我就不講了	15-7
那麼為什麼是這個他有他的道理	15-7
那他有一堆方法做出這些值來等等	15-7
那你這個如果有興趣的話	15-7
這個原始paper 是我上面的下一篇	15-7
第五篇啊rasta processing of speech 的這一個呢就是他當時的作法	15-7
那如果說是rasta 可以這樣想的話	15-8
那麼再下來呢	15-8
其實就有一堆這個temporal filtering	15-8
其它的temporal filtering 他說我為什麼要用你這個filter 呢	15-8
你這個filter 你這樣講其實有點不通	15-8
這個好像很直覺地用很直覺地在那邊想的	15-8
真的是那樣嘛沒有這個道理嘛	15-8
所以我應該用別的方法來求這個filter	15-8
什麼方法呢	15-8
就是data driven	15-8
我真的用一堆聲音去train train 那個filter	15-8
那這個就是所謂的data driven	15-8
就是我用data 我用一堆data 去求	15-8
看這個filter 到底應該長怎樣	15-8
那這就有很多種方法了	15-8
我們這邊舉幾個例子	15-8
這邊的這邊的是用p c a 的方法我們之前講過的p c a	15-8
也可以做其它的用l d a 等等所以我們底下會講l d a	15-8
那也可以再用別的	15-8
啊這是用l d a 的然後還可以用別的	15-8
那我們先來說這個p c a 的	15-8
那它的意思是說呢	15-8
就是我們剛才講你這個rasta 你這個filter 其實是很這個憑空亂想的你胡思亂想想一個辦法說	15-8
我在十個hertz 一個hertz 到十個hertz 之間這很沒道理啊	15-8
我為什麼不讓data 告訴我	15-8
我就用一堆training data 來train 這個filter	15-8
讓data 告訴我它應該長怎樣	15-8
那用data 告訴我有很多種方法	15-8
第一種方法就是用p c a	15-8
那你記得我們之前講p c a 是什麼	15-8
p c a 是說	15-8
假設這是c one 這是c two 這兩個軸	15-8
假設我的一堆data 散在這裡	15-8
是這樣的話	15-8
我在c one 的軸上看到它們的distribution 是這樣的一個distribution	15-8
在c two 上看到一個distribution 是一個這樣的distribution	15-8
但是這兩個distribution 都散得不夠開	15-8
我想辦法找另外一個軸	15-8
這個軸呢譬如說在這裡e one	15-8
在這個軸上呢我就發現它散得最開	15-8
這是一個散得最開的distribution	15-8
那因為這樣子的話呢我的這個散得最開我就把所有東西都都拉開了	15-8
我辨識比較方便	15-8
效果就會比較好	15-8
這是p c a 的精神	15-8
那這個意思是說我的每一個點	15-8
我找一個新的軸之後我每我的每一點都投影到這個軸上來	15-8
它們就散得比較開了	15-8
那怎麼投影呢就是我原來的這一點就是x 是在c one c two 上面	15-8
我現在投影上來呢就變成x 跟我這個e one 做內積	15-8
這兩個都是vector	15-8
我在做內積	15-8
內積的結果因為這個是一個unit vector	15-8
那我一做內積的話呢	15-8
我就把這些點投影上來	15-8
我得到這個上面	15-8
它散得就比較開	15-8
我就辨識得比較容易做得好	15-8
那他說我可以用相同的觀念來想這件事	15-8
因為你現在如果是這一個這個就是我們c one 的那個的signal 嘛	15-8
我c one 的那個signal	15-8
我要通過一個	15-8
我這個c one 的signal 我要通過一個filter	15-8
這是c one 的m	15-8
我要通過一個filter	15-8
那通過一個filter 是什麼這是一個convolution 的過程	15-8
那convolution 你你你只要熟悉convolution 你就知道	15-8
所謂convolution 就是把這些個分別乘上一個constant 加起來	15-8
那待會對不對	15-8
這些個值分別乘上con 它們加起來	15-8
然後呢得到一個值	15-8
然後再來把這些乘上con 加加	15-8
嗯這樣講	15-8
就是說你把譬如說這四個分別乘上一個constant 加起來	15-8
你得到一個值	15-8
待會我把這四個乘上constant 一個constant 加起來	15-8
得到一個值	15-8
待會我把這四個乘上一個constant 加起來	15-8
得到一個值	15-8
那那個就是convolution 也就是filtering	15-8
所以你其實就是把它們的這個分別乘上一個值加起來	15-8
分別乘上一個值加起來	15-8
分別乘上一個值加起來然後這個不斷的不斷的移動	15-8
這就是這句話在講的意思你的filtering 其實就是一個convolution	15-8
也就是weighted sum of a sequence of a 一個coefficient with length l slide alone f frame index	15-8
你	15-8
那我畫到這裡就是這個意思	15-8
你把這四點分別乘上某一些值加起來得到一個值	15-8
然後你下面這四個點分別乘上一個值加起來變成一個值	15-8
等等等等	15-8
那這些東西分別乘上一個值加起來這件事情	15-8
其實不就是一個內積嗎	15-8
其實就是一個內積嘛	15-8
就等於說是你這四個值跟一個e one 去做內積的意思得到一個值	15-8
待會再跟這個做內積	15-8
再跟這個做內積嘛	15-8
既然如此	15-8
我現在要它們我現在如果以p c a 的觀念來想的話	15-8
就是我希望它們最後出來的通過這個filter 之後出來的值能夠散得最開	15-8
那怎麼散得最開呢	15-8
那散得最開的方法我就是用p c a 來求	15-8
那因此呢這個意思是完全一樣的	15-8
我現在就是把譬如說如果我是這四個來求來相乘相加得到一個值	15-8
這四個相乘相加得到一個值	15-8
這四個相乘相加得到一個值這樣子的convolution 的話	15-8
那我就把這四個當成個四維的vector	15-8
這個四維的vector	15-8
那這堆四維的vector 就是這些的點嘛	15-8
那因此我就是有這些個以以剛才這個四維的點為例	15-8
這就是一個四維的點	15-8
那麼待會那個呢也是一個四維的點這也是一個四維的點	15-8
我就把這些四維的點通通通通拿在一起然後來做p c a	15-8
來找它的最好的那個軸	15-8
那個軸就是它的最大eigen value 的那個eigen vector	15-8
這是我們中間講過的p c a	15-8
那麼因此呢我現在就把它們	15-8
那這些四維的點通通投影到這上面來的話	15-8
那其實就是在做這件事情	15-8
那也就是我把這個東西最大的eigen value 那個eigen vector	15-8
當成我的	15-8
當成我的那個filter 的coefficient 就可以了	15-8
所以我如果用這個方法來做的話呢	15-8
我就我就是maximize the variance of the weighted sum	15-8
對不對	15-8
那麼因此呢我就可以把這個這個求filter 的這個這個問題	15-8
求filter 的問題就變成一個p c a 的問題	15-8
因為我的	15-8
我我我求filter 的問題其實是在求到底它們應該各乘上多少coefficient 去相加	15-8
我就等於是在求這個東西	15-8
那這個東西其實就是這個東西	15-8
所以呢我就把我只要把這些個每四個點每四個點每四個點我有一堆training data	15-8
我用training 的語音的這每四個點每四個點通通通通這個這個data 做出來變成一大堆點之後	15-8
求它的p c a	15-8
我得到最好的那個軸	15-8
那個軸上面的那個coefficient	15-8
其實就是我的filter	15-8
那這就是這邊講的意思	15-8
那麼因此我就是在v a 在maximize 它的這個通過這個filter 之後的variance	15-8
那	15-8
它的那那個filter 是其實什麼那個filter 其實就是這個第一個eigen vector	15-8
那如果這樣的話呢	15-8
做出來結果得到的filter	15-8
你如果把他圖畫出的話	15-8
跟這個有一點像但是不大一樣啊	15-8
基本上也是可以證實他這個原來的想法蠻好的因為他大概也得到的圖我這邊沒有畫出來啊	15-8
那他得到的圖大概也是	15-8
譬如說大概一個hertz 以上到十個hertz 之內	15-8
是很像這個情形	15-8
在十個hertz 以上也再把它濾掉	15-8
這邊也是把他濾掉這蠻像但是filter 長得不太一樣	15-8
但那個filter 是有道理的啊因為是data 告訴我的啊	15-8
因為那個那個filter 是我根據p c a 的data 告訴我這樣子比較好	15-8
所以我用那樣來做個filter	15-8
那這樣效果會比較好那就是用p c a 所所推的temporal filtering	15-8
好	15-8
那如果這個這個觀念可以了解的話呢	15-8
那麼我們就可以進入下一面個呢就是l d a 的觀念	15-9
那麼l d a 是跟p c a 蠻像的一個東西	15-9
但是它比p c a 更進一步	15-9
有更多的更多的想法	15-9
啊	15-9
什麼是l d a 呢	15-9
這個也是在很多課裡面都學過的	15-9
不不論是pattern recognition 還是這個machine learning 裡面大概都有這個東西	15-9
就是所謂的這個linear discriminative analysis	15-9
啊	15-9
那l d a 的基本精神跟這個有點像	15-9
但它比它更進一步	15-9
它的意思是說	15-9
假設假設我的	15-9
譬如說這堆點是ㄚ	15-9
然後呢我的這堆點是ㄨ	15-9
如果我知道這堆是ㄚ這堆是ㄨ的話	15-9
我現在這個軸是c one	15-9
這個軸是c two	15-9
你會發現呢ㄚ的distribution 是這樣	15-9
ㄨ的distribution 呢是這樣	15-9
overlapped 地很厲害	15-9
我在這裡切不開	15-9
那我如果用c two 來畫的話呢我發現呢	15-9
ㄚ的distribution 是這樣	15-9
那ㄨ的distribution 呢是這樣	15-9
也切不開	15-9
但是真的他們切不開嗎	15-9
其實我如果想辦法另外再找一個軸	15-9
我把它們儘可能找一個怎麼樣的軸才是能夠切得最開呢	15-9
那可能是在這裡	15-9
這個是e one	15-9
為什麼呢	15-9
你如果看看的話你會發現	15-9
譬如說呢	15-9
在這個情形之下呢	15-9
我的ㄨ到這兒來了	15-9
然後我的ㄚ呢是在這	15-9
那麼因此呢我原來在在在c one 或者c two 上面它們都overlapped 地相當厲害	15-9
因此你不管怎麼切都有很多error	15-9
可是我可能可以找到另外一個軸	15-9
它們能夠分得比較開	15-9
啊	15-9
那	15-9
那這樣的觀念就是所謂的l d a	15-9
就是我我仍然在做linear 的transformation	15-9
跟這個很像轉一個軸嘛	15-9
還是轉一個軸	15-9
但是我找的軸呢跟剛才不一樣	15-9
p c a 這裡我並不知道誰是ㄚ誰是ㄨ	15-9
我只知道有一堆data 我我把這堆data 把它拉得開一點而已	15-9
我不知道誰是ㄚ誰是ㄨ	15-9
但是在這裡的話呢我是假設我知道誰是ㄚ誰是ㄨ的話	15-9
我要把ㄚ跟ㄨ拆開來	15-9
我重新再找一個軸我讓ㄚ跟ㄨ能夠拆得開來	15-9
啊	15-9
那這個觀念就是所謂的l d a	15-9
ok 我們在這休息十分鐘好啦	15-9
那麼l d a 的基本精神跟這個是很像的	15-9
只是差一點點而已	15-9
我們現在應該只剩下	15-9
啊	15-9
我們來說一下期末報告啊	15-9
我們這門課期中考已經考過了然後嗯有兩個題目你suppose 也已經做了	15-9
那麼於是你只剩下一件事情就是期末報告	15-9
那我們的不過我們的期末報告佔分很重了佔百分之五十啊	15-9
所以也就是說你前面的期中考也好	15-9
那個習題也好	15-9
總共只有佔百分之五十而已	15-9
那期末報告是另外的百分之五十	15-9
所以啊你得要花點工夫來做你的報告	15-9
那麼我們的deadline 我現在是訂六月三十號	15-9
這個是校例所規定的期末考結束以後的三天	15-9
所以應該是盡量給你盡量給你最多的時間啊	15-9
校例規定的期末考是六月二十七	15-9
我們再加三天那這天是星期五	15-9
然後我們訂五點	15-9
下班時間	15-9
你交到這個電資學院大樓就是這棟樓的五三一	15-9
是我們的語音實驗室	15-9
電資學院大樓五三一啊	15-9
交給我們的任何一位助教	15-9
不論是宮嵊益還是還是許長文啊	15-9
就是在這個以前的任何時間都可以你最晚最晚是在這個時候為止	15-9
超過這個時間就算遲交就是了	15-9
這是我們講的deadline	15-9
那我想這樣應該有夠多時間了我希望你有夠多時間做它	15-9
是期末考校例規訂的考完之後還多三天	15-9
那麼我們可以可以做的這個style 應該是有	15-9
第一種是reading report	15-9
我不要做任何程式做任何實驗我就是讀paper 就夠了	15-9
我們講了那麼多paper	15-9
然後很多豐富的東西都可以都可以讀	15-9
讀了之後我可以寫一個心得報告我就純粹是讀paper 的也可以	15-9
當然你也可以做computer project	15-9
因為我們講的每一樣東西幾乎都可以寫程式	15-9
然後都可以做你要做的事	15-9
所以呢可以做computer project	15-9
你如果要做computer project 的話通常你需要data	15-9
那我們兩個習題已經給你不少data 了	15-9
我們包括在第一題裡面給你很聲音的data	15-9
第二題給你很多文字的data	15-9
所以呢那同樣這兩個習題也給你很多工具	15-9
包括第一題的這個h t k	15-9
跟第二題的這個s r i	15-9
它們都有非常多的工具可以用	15-9
那你如果要做的東西比這還多的時候	15-9
你可以找我們的助教說你要做的哪一種東西不夠	15-9
所以想要多要一些data 或多要一些工具	15-9
應該也是可能的	15-9
那助教會過來跟我商量說哪些東西可不可以給同學啊	15-9
那我舉個例子我們講的eigen voice	15-9
eigen voice 是可以做報告用的	15-9
可是你如果eigen voice 是可以很可以適合做報告的題目	15-9
可是如果你要做eigen voice 的話	15-9
你需要有夠多的不同的人都有夠多的data	15-9
那我們給你的data 大概沒辦法做eigen voice	15-9
啊	15-9
那麼因此呢在那個情形之下你就可以跟助教商量說我要做eigen voice 所以我的data 不夠多	15-9
那我會跟助教討論	15-9
拿一堆data 給你讓你可以做等等	15-9
啊	15-9
所以呢你如果要做computer project 的話	15-9
基本上從我們兩個習題給你的data 跟工具來著手	15-9
但是你如果覺得你的你的題目希望做更多東西	15-9
可以	15-9
啊	15-9
你跟助教商量	15-9
那助教覺得他有問題他可以來問我啊	15-9
然後當然第三種是combination	15-9
也就是說你都有一點	15-9
我讀了一些paper 之後	15-9
然後把那些東西弄清楚之後我來做一個程式	15-9
我兩個都做一點也可以	15-9
哦所以這三種都行	15-9
那然後呢我可以是這個啊嗯嗯這個team work	15-9
也就就是兩個人一組	15-9
可以啊	15-9
不要超過兩個人啊	15-9
兩個和尚挑水喝是剛剛好	15-9
但是如果三個和尚就會沒水喝	15-9
啊	15-9
所以呢就是	15-9
你如果要兩個人一起做是有它的好處就是說我們兩個人合作的話讀paper 的時候我讀一半你讀一半	15-9
那這樣的效果可能比一個人讀來得好我覺得是有道理的	15-9
或者說你讀paper 我來寫程式	15-9
啊	15-9
只要這兩個人可以密切合作的話	15-9
兩個人可以	15-9
但是不要超過兩個人	15-9
那這個時候你要註明每一個人的contribution 啊	15-9
就是說如如果我們兩個人合交一份報告的話	15-9
那麼你要裡面要註明清楚是誰我做哪裡他做哪裡	15-9
那這樣就可以	15-9
你如果沒有註明的話我就會以為是一個人護行另外一個人	15-9
因為可能是一個人把另外一個名字寫上去	15-9
啊	15-9
那這樣子就不行	15-9
所以你一定要註明說哪一部分是誰做的	15-9
那那不管是你既使做的是computer project 的話你還是會要交一個書面報告	15-9
就只是說你的因為你一定要有一個文字在描述你的程式嘛	15-9
所以只是說你如果是做computer project 的話你的書面報告會變得很簡單	15-9
這個少數幾頁就夠了	15-9
你的重點是那個程式了	15-9
啊	15-9
那你如果是reading report 的話你就你就完全是書面報告啊對不對	15-9
如果是computer project 的話你重點是那個程式	15-9
但是你還是要有書面報告	15-9
那不管怎樣呢這三種形態都有書面報告	15-9
只是書面報告多少的問題那書面報告交的時間就是這個時間	15-9
那你如果是computer project 裡面有computer 有程式的話	15-9
你可以交一個光碟或者交一個什麼	15-9
那當然你你如果想要demo 也可以啊	15-9
不是一定要	15-9
但是如果你覺得你寫的的程式適合demo 來呈現	15-9
那麼如果光是交一個交一個光碟不見得能夠感覺到它多麼有意思	15-9
demo 比較容易看得出來你可以說我要demo	15-9
如果要demo 的話呢就是在交報告的這個時候	15-9
跟助教登記說我要做demo	15-9
那到那個時候的話	15-9
我會來看demo	15-9
啊	15-9
所以呢這最後面的五十分的話是我自己改的	15-9
不是助教改的啊	15-9
那	15-9
這個所以這五十分的報告是我自己改的	15-9
包括如果你有demo 也是你是demo 給我看的不是給助教看的啊	15-9
那所以這個大概是關於期末報告這個你可以想的事情	15-9
那我們到目前為止已經講了好些東西了而後面還有很多東西都都可以做為期末告的的思考的方向	15-9
好這是關於報告	15-9
那我們們現在回過來說l d a	15-9
我們說l d a 的精神是	15-9
跟剛才p c a 不同的是	15-9
p c a 是我只是希望把他散開	15-9
l d a 是假設我知道這個是ㄨ這個是ㄚ	15-9
我希望把ㄨ嘎ㄨ跟ㄚ分開	15-9
那這是什麼意思呢什麼叫做把ㄨ跟ㄚ分開呢	15-9
那麼我們如果用比較精確的語言來講的話呢	15-9
是這樣講	15-9
如果說	15-9
這一堆是ㄨ這一堆是ㄚ	15-9
這就是分得很開的意思對不對	15-9
這樣我很容易區別	15-9
但是反過來呢	15-9
如果說	15-9
這一堆是ㄨ	15-9
然後這一堆是ㄚ	15-9
這就是分不開的意思對不對	15-9
那你想這個到底我們用什麼方式來說它們到底分得開還是分不開呢	15-9
那個最簡單的方法就是還是用我們的covariance matrix	15-9
我們譬如說	15-9
就這個而言	15-9
這一點它有它的一個mean	15-9
對不對	15-9
譬如說這是ㄨ的mean	15-9
那這個呢是ㄚ的mean	15-9
那除了mean 之外呢我還有什麼東西呢	15-9
就是它跟mean 散得多開	15-9
離mean 有多遠	15-9
那這個東西呢就是我們講的covariance matrix	15-9
u 的假設我用我用這個這個u 來co 代表covariance matrix	15-9
這一堆有它的covariance matrix	15-9
這就是它的散得多開的意思	15-9
怎麼講呢	15-9
那你如果仔細想的話	15-9
所謂covariance matrix 是什麼	15-9
這covariance matrix	15-9
它的每一個element是假設叫作u i j 好了	15-9
這它這是這個matrix element	15-9
它是什麼東西	15-9
它是這個嘛就是這個x 減x i 減掉x i 的mean	15-9
x j 減掉x j 的mean	15-9
就是第i 個這我這個都是vector	15-9
每一個vector 有第i 個component 跟第j 個component	15-9
這個x i 減掉x i 的mean 就是指它跟mean 的距離嘛	15-9
我分別減掉它的mean	15-9
這個x i 減掉x i 的mean 就是指它跟mean 的距離嘛	15-9
x j 減掉x j 的mean 是在j 上面它跟它的距離嘛 我都是在算它跟mean 的距離	15-9
然後呢	15-9
我再這兩個相乘求平均的這個東西就是u i j	15-9
這個matrix 裡面的每一個element u i j 就是這種東西	15-9
那你就可以想像得到我這個matrix 的對角線上的每一個呢	15-9
就是它的每一個component 自己的variance	15-9
這個你很容易想像就是我們平常講一個gaussian 的肥度	15-9
對不對	15-9
一個gaussian 這個是是mean	15-9
它的肥度就是它的variance	15-9
對不對	15-9
這個variance 就是當i 等於j 的時候	15-9
當i 等於j 的時候就是指對角線上面的這個東西的時候	15-9
就是指每一個component 它自己的那個散開的程度	15-9
所以呢這裡的在對角線上的每一個就是它散開的程度	15-9
那同理	15-9
不是對角線上的也其實也是一樣的意思	15-9
只不過我現在i 不等於j	15-9
是i 跟i 散開的是i 跟它的mean 以及j 跟它的mean 各自散開的程度的平均嘛	15-9
所以我變成用整個的matrix u i j	15-9
來描述這東西散得多開	15-9
那你如果這樣想的話呢	15-9
那你可以想得到我這邊的話呢	15-9
就這個圖而言它們這個散得不開這個比較緊這個比較緊	15-9
而這個呢就是散得很開	15-9
譬如說這個一個這麼大這散得很開別外一個呢又這麼大	15-9
那我希望得是變這種而不是變成這種	15-9
那我就希望每一個class	15-9
這是一個class 這是一個class	15-9
我希望每一個class 的u 呢盡量小	15-9
那那個class 就是我們這邊的u j 乘以weight w j	15-9
也就是說呢我現在我每一個class 每一個class 有它的mean	15-9
就是它的mean	15-9
有它的covariance matrix	15-9
哦	15-9
那這個mean 跟covariance 就是我這邊講的這兩這兩個東西	15-9
我每一個class 譬如說ㄨ譬如說這個是ㄚ這個是ㄧ	15-9
那麼這兩個每一個class 有它自己的mean 有它自己的covariance	15-9
那麼我希望呢它們都盡量	15-9
covariance 就是說它散從這個mean 散散開來的散開的程度	15-9
我希望它越小越好	15-9
那這就是所有的u j 的平均	15-9
那麼weight 是什麼呢w j 是什麼是number of samples	15-9
假設我這邊有一萬個點	15-9
這邊有十萬個點	15-9
那我就這個乘以一萬這個乘以十萬	15-9
或者說這個是乘以這個十一分之一這個乘以十一分之十	15-9
那這樣就表示說我我我同時把它們散開的程度跟這個data 的量weight 進去了	15-9
那這樣呢這個叫作within class the matrix the scalar matrix	15-9
就是指一個class 裡面到底它散得多開	15-9
我希望每一個class 自己不要散得開來最好越緊越緊好	15-9
所以我會希望這個u i j 的每一個element 越小越好	15-9
對不對	15-9
u i j  的每個element 越小越好就表示說我這個它的散開來的肥度越小	15-9
那就表示越緊	15-9
那這個是對每一個class 內部而言	15-9
我都希望它都能夠儘量收得很緊	15-9
這就叫作within class scalar matrix	15-9
但是還有一個東西呢就是between class	15-9
我還希望呢這個class 跟class 之間能夠散得開	15-9
對不對	15-9
我我不要這兩個雖然我我如果兩個排得很近的話還是會會混淆嘛	15-9
我希望它們儘量散得開	15-9
那這是怎樣呢那我就要有一個global mean	15-9
啊total mean 這裡有一個mu 是一個total mean	15-9
假設我有十個class	15-9
那這個十個class 的total mean 在這裡的話	15-9
那我就把這裡的每一個mean 相對於這個mean 來做一個那個covariance matrix	15-9
那那個呢就是這個	15-9
所以底下這個式子這個這個東西叫作between class 的scalar matrix	15-9
就是指class 跟class 之間我要它們這個分得開	15-9
那我就是把它們的mean 拿來作做一個covariance matrix	15-9
ok	15-9
換句話說	15-9
這個不好為什麼不好	15-9
一方面是因為每一個class 自己散得開	15-9
一方面是class 跟class 之間排得太緊了	15-9
譬如說白色的這個mean 在這這裡黃色的這個mean 在這裡	15-9
它們這兩個mean 太緊了	15-9
所以呢一方面是我的within class 在class 之內我盡量要收得緊	15-9
一方面呢我是要class 跟class 之間我要盡量拉得開	15-9
像這個的話就是我class 拉得開又收得緊	15-9
所以呢我有這個between class 的matrix	15-9
那這個其實是式子你看起來就知道它其實沒有什麼特別	15-9
就跟我這個covariance matrix 是一樣的意思	15-9
只不過我現在每一個就是各自的mean	15-9
每一個class 的mean 分別減掉那個total mean	15-9
那等於是這我們剛才是每一個點分別減掉它的mean 是一樣的嘛	15-9
啊我等於是每一個每一個class 的mean 去減掉它的total mean 之後	15-9
我來算這個covariance matrix	15-9
那然後呢我我用這個covariance matrix 來代表我的between class	15-9
那因此呢我變成是說我class 跟class 之間我要盡量散得開	15-9
而同一個class 裡面呢我要盡量收得緊	15-9
那相對於這個呢就是我同一個class 就沒有收緊就散開了	15-9
而class class 之間又又變得很緊所以這樣就是不好	15-9
這個是好的這個是不好的	15-9
啊	15-9
那麼因此我要怎麼做這件事情呢	15-9
我我有兩個matrix	15-9
我要within 要越小越好	15-9
between 的要越大越好	15-9
所以基本上我是要把這兩個來相除	15-9
就是把我要的東西是between class 的除以within class 的	15-9
我這個東西要越大越好	15-9
因為這個大就表示是說我class 跟class 之間分得開	15-9
所以呢這是越大越好	15-9
而這個東西呢要越小越好	15-9
因為這個就表示說我的一個class 裡面是收得越緊越好	15-9
那麼因此呢這兩個相除的話呢大就是可以最大嘛	15-9
我如果這個東西是maximum 的話呢	15-9
那就是表示是說這個越大越好	15-9
這個比這個這個越大越好這個越小越好嘛	15-9
或者說我的class 跟class 之間的距離相對於class 內部的距離是越大越好嘛	15-9
對不對	15-9
那你也可以解釋成這好比是s n ratio 一樣	15-9
這個是class 跟class 之間的距離讓我可以拆開signal 的	15-9
那這個呢是是noise	15-9
等於說是我每一個會把的noise 把它它的弄弄模糊的	15-9
所以這個有點像s n ratio 一樣的東西	15-9
所以我要的就是這個東西這個ratio maximum	15-9
就表示說這個這個最大這個最小	15-9
但是呢不能這樣做為什麼呢這兩個是matrix 啊	15-9
這是一個matrix 這也是一個matrix	15-9
matrix 哪能相除呢	15-9
不能相除啊	15-9
那怎麼辦	15-9
我們就加一個trace	15-9
trace 是什麼	15-9
你可以回去查你從前唸的線性代數裡面的matrix 就知道有一個trace	15-9
每一個matrix 我都可以做一個trace	15-9
做個trace 之後呢	15-9
其實是相當於它的eigen value 之和	15-9
那其實就代表它的total scattering	15-9
那麼換句話說	15-9
你可以想像我們的譬如說我們的covariance matrix 它的eigen value 其實是什麼	15-9
其實covariance matrix裡面的每一個eigen value 都代表它的那一個dimension 散開的程度嘛	15-9
就是我們p c a 講的嘛	15-9
它的散開的程度就是那個eigen value 嘛	15-9
所以我如果把eigen value 之和加起來是trace 的話	15-9
就代表它total 在每一個dimension 散開的程度	15-9
就變成一個單一的值啦	15-9
所以呢我這個matrix 我不能直接去求它不能直接去除它	15-9
而我必需這兩個matrix 都分別求它的trace	15-9
當我這個也求trace 這個也求trace 之後	15-9
分子就代表它的class 跟class 之間散得多開	15-9
分母就代表每一個class 內部散得多開	15-9
那你這個希望越大越好這個希望越小越好	15-9
所以你是這兩個的trace 來相除我要maximum	15-9
那在這個條件之下我現在要做的事情是什麼呢	15-9
我做的事情是要找一個軸	15-9
原來的這個c one c two 這個軸可能不好	15-9
我要找一組新的軸	15-9
那一組新的軸呢使得它們能夠散得最開	15-9
那這組軸怎麼找	15-9
就是用一組orthonormal basis 排成w one w two 等等	15-9
在k 個dimension	15-9
然後呢	15-9
那我如果用這個w one w two 這k 個這個orthonormal basis 做成一matrix	15-9
來做這個轉來做這個新的軸的話	15-9
那麼我的這個s 這這兩個這兩個這個scattering 的這個matrix 其實就是它的covariance matrix 會變成什麼呢	15-9
會變成前後各乘一個這個w 啊	15-9
所以這個數學式子是這樣來的	15-9
所以呢我現在我我真正要maximize 應該是這個嘛	15-9
我要maximize 的是這個嘛	15-9
但是呢我現在要轉一個軸	15-9
那個轉軸的那個matrix 呢就是w	15-9
那這個w 裡面的每一個column 就是一個basis	15-9
那當我用這個w 做個轉軸的時候呢	15-9
那你的那個s 呢就會變成左右各乘這個	15-9
這個s 也會變成左右各乘這個	15-9
所以我就變成說是我要左右各乘一個w 之後這是transpose 這邊沒有transpose 這是全部各乘之後求trace 相除	15-9
我要maximize 這個值	15-9
那我選這條所有的w 去找	15-9
哪一個w 讓它最大的那個w 就是我要的w	15-9
那用這個方式呢得到的答案就是我們講的linear discriminative analysis	15-9
那它的它跟p c a 有何不同	15-9
p c a 只是找出一些principle component 使得它在那些component 上面散得最開	15-9
但是p c a 並不知道誰是ㄚ誰是ㄨ	15-9
所以p c a 只是把它們散得最開並不知道誰是是ㄚ誰是ㄨ它並沒有把ㄚ跟ㄨ散開	15-9
但是l d a 是進一步	15-9
l d a 是知道這一堆是ㄨ這一堆是ㄚ然後所以我要把ㄨ跟ㄚ這兩個class 拆開來	15-9
所以呢l d a 使用的information 比p c a 要來得多	15-9
in general 在大多數狀況l d a 的效果比比p c a 更好	15-9
因此呢	15-9
所以呢我們l d a 呢是目的是要找出最最有具有鑑別力的	15-9
啊這個是鑑別力的意思嘛啊	15-9
我要找出最有最具有鑑別力的那些個dimension	15-9
然後呢把它這樣子拆開來	15-9
那如果是這樣子的話呢	15-9
那我就得到這個這個最好的結果	15-9
啊	15-9
那這叫做discriminative linear discriminative 就是我有鑑別力的一個分析	15-9
那麼當我這樣做的時候這個solution 是什麼solution 其實很簡單	15-9
我還是找eigen vector	15-9
不過什麼matrix 呢是這個matrix	15-9
就是這個within class matrix 的inverse 乘上between class matrix	15-9
這兩個相乘之後還是一個matrix	15-9
這個matrix 的eigen vector	15-9
然後找它最大的eigen value 的	15-9
那些個就是這些w	15-9
那麼因此呢跟剛才跟p c a 裡面一樣	15-9
那麼eigen value 最大的那個eigen vector 是w one	15-9
第二大是w two 等等	15-9
那結果我到時候在w one 上面呢我可以拆得最開	15-9
然後w two 上我拆得第二開等等	15-9
啊	15-9
那這就是所謂的l d a	15-9
好	15-10
有了l d a 之後那後面就一樣了我也一樣做這件事	15-10
這張圖其實跟我們上面講的那個圖是意思是一樣的	15-10
那麼跟我們那邊講的是一樣的	15-10
你所謂的這個temporal filter	15-10
就這個c one 變成一個signal 要做一個filter 這件事情呢	15-10
你可以想像成是	15-10
這個這個convolution 其實就是在把這幾個分別乘上一個coefficient 加起來嘛	15-10
所以你就是把這一堆這四個點這四個點這四個點都都當成是一個n dimension 的這些這些點	15-10
然後你在這上面去做l d a 我現在是做	15-10
剛才這邊是做p c a 我現在是可以做l d a 嘛	15-10
喔那等等	15-10
所以這邊圖畫的意思是一樣只是說我ok 我這個這個signal c one 當成是一個signal	15-10
然後呢你如果這三個一起乘上一個coefficient 的話呢	15-10
這相當於是output filter 之後的第一個output	15-10
你這三個乘上下一個的話呢是第二個output	15-10
這三個再乘上下一個是第三個output 等等所以這個其實就是在做convolution	15-10
那你如果這樣來看的話呢	15-10
我做l d a 跟剛才p c a 有何不同呢就是我要先分好class	15-10
假設我要辨識零到九的十個音的話	15-10
哪些是零的	15-10
我把那些個點點在這裡	15-10
哪些是一的我點在這裡	15-10
我通通都要分開來	15-10
我都要分好class 就是了	15-10
所以呢我的這些data 在做l d a 之前我要先分好class	15-10
哪些是一哪些是二哪些是三	15-10
然後它們分別知道class 之後	15-10
我就可以來做l d a	15-10
然後我把那個eigen value 最大的那個eigen vector 來做我的第一個軸	15-10
也就是做我的那個filter	15-10
那這樣得到的結果呢會比p c a 更好	15-10
那畫出來仍然是很像的	15-10
不過不太一樣就是了	15-10
那仍然是這樣大致在幾個hertz 之內的這個range 是最重要的	15-10
然後別的部分把它濾掉一些	15-10
那得到會更會得到更好的結果	15-10
那不論是p c a 或者l d a 來做的話呢	15-10
一個特點就是這裡的每一個filter 都不一樣	15-10
c one 有c one 的filter c two 有c two 的filter 每一個都不一樣因為它們各自有它們自己的的統計特性都不同	15-10
所以你得到的都不一樣就是了	15-10
那這個是這個用l d a 來做的temporal filter	15-10
那跟剛才p c a 唯一不同就是我用l d a	15-10
那跟這個很像的還有用m c e 去做的m c e 是我在九點零講完e m 之後要講的	15-10
這個我大概下週會講	15-10
那麼所以這個呢就留到後面再說	15-10
那這邊還有另外一個例子是說呢	15-11
這個filter bank 本身也可以optimize	15-11
那這個的意思是很像的	15-11
就是說我們原來講的這個	15-11
這個這個求m f c c 的時候用的三角形的filter 來做	15-11
排一堆三角形的filter 的時候是非常直覺而僵硬的沒什麼道理	15-11
我們說	15-11
譬如說這是這是一個這是一個filter 我們都是三角形的	15-11
憑什麼是三角形呢	15-11
沒有理由嘛	15-11
那我們為什麼不能說這個是一個某一種的filter	15-11
那第二個呢是另一種的filter	15-11
第三個是另一種的filter	15-11
可不可以呢可以啊	15-11
那因此呢這個filter 怎麼求	15-11
我也可以用data driven 來求啊	15-11
這是這邊講的data driven of filter bank	15-11
的意思就是這樣子	15-11
也就是說呢	15-11
我現在	15-11
譬如說我我我我的這個filter 是在求這一堆對不對	15-11
這一堆這個這個	15-11
這一堆component 到時候要用三角形嗎還是用別的呢	15-11
我如果有夠多的data	15-11
我拿夠多的聲音來求這堆frequency 的東西然後我可以用p c a 用l d a 都可以	15-11
然後我來找到底怎樣的filter 找才好	15-11
那麼結果它們可以找出來奇奇怪怪的形狀會更好會比三角形更好	15-11
那就是這邊所講的事情	15-11
你一樣的你就是假設這個是第一個filter 要做的這是第二個filter 要做的這是第三個filter 要做的	15-11
你就有夠多的data 的話我就可以用p c a l d a 或m c e 都可以	15-11
你用這些方法來做的話你都可以得到你想要的	15-11
那那這樣都可以得到比較好的feature 等等	15-11
那以上的這些就是嗯feature based 的一些例子	15-11
那嗯在reference 裡面的話我這邊所列的	15-11
哦第六個這個其實是一本課本	15-11
那只是講l d a 在這裡面可以查得到	15-11
那其實l d a 在很多課本都有	15-11
你在你找相關的pattern recognition 或者machine learning 很多都有l d a	15-11
我這裡只是列其中一本作為例子而已	15-11
那第七個這一篇其實就是在講一個找這個filter bank	15-11
啊我就這個filter bank 怎麼求	15-11
我不一定要三角形	15-11
我就可以讓它我我我以optimize 我的辨日辨識結果來看的話	15-11
我怎我我可以求裡面的東西的	15-11
這個是這是這個reference	15-11
那嗯其實剛才講的那些temporal filtering	15-11
嗯有一個新的reference 可以參考的	15-11
我還沒有寫上來	15-11
這個	15-11
就叫做optimization of temporal filter	15-11
類似這樣的名字	15-11
for robust features	15-11
之類的名字啊	15-11
這個是在同樣的這個期刊	15-11
就是i triple e transaction speech audio processing 的這個五月兩千零六剛剛出來的	15-11
那這篇就是把我們其實就是我們這邊講的好幾個都在這裡面	15-11
包括我們這邊講的用p c a 用l d a	15-11
啊用p c a 用l d a 用m c e 這些東西它它都有在裡面	15-11
那這個其實是我們實驗室的同學做的	15-11
所以保證很好看啊	15-11
ok 好那這樣的話我們把feature feature based 也講完了我們現在剩下最後一個那個這個enhancement 的例子	15-11
那麼我們留到下週就可以了我們就可以下週我們就回到講九點零的這個e m	15-11
好這樣子我們今天上課上到這裡	15-11
OK 我們上週這個十五點零還有最後一些最後一頁沒有講喔	15-12
最後一頁要講的就是那個 speech  enhancement 我們也舉一個例子	15-12
那就是這裡那所謂的 speech  enhancement 我們之前說過就是嗯	15-12
當你的這個真正所要 recognize 的的的的聲音跟你做 training 的時候的聲音不一樣不 match 的時候	15-12
那麼我們可以做 model  base 的部分我調 model	15-12
想辦法把 clean  speech 所 train 的 model 調成你的這個這個 testing 環境的 model 這是 model  base 的	15-12
我也可以做 feature  base 的那麼我想辦法把 feature 調成跟這個 feature 很像的這是 feature  base	15-12
那當然還有一種情形就是我根本就在 signal 這邊做 我想辦法就把這個 signal 弄得跟它比較像	15-12
那這就是所謂的 speech  enhancement	15-12
那 speech  enhancement 的發展歷史非常早	15-12
那麼早早在七零年代六零年代的時候已經是一個研究的題目	15-12
當時還沒有好好的做這個 speech  recognition	15-12
那就已經先有這個了因為當時的想法只是希望把聲音變的好聽一點	15-12
譬如說當你在車裡面或者在飛機上這個講話的時候聲音永遠都有干擾這個時候我如何把那個聲音雜訊濾掉弄得乾淨一點當時的想法是這樣	15-12
就讓它好聽就是了這是所謂的 speech  enhancement	15-12
但是到了 recognition 的時候其實發現其實不一定要再再好聽而且你如果把一個聲音弄乾淨一點的話其實對 recognition 也有幫助	15-12
所以呢 speech  enhancement 也就同樣可以用在這裡喔	15-12
所以這是一個相當有歷史的領域	15-12
那我們這邊舉的一個例子也是非常有歷史的例子	15-12
可能可以算是 speech  enhancement 裡面的最具代表性的一個例子	15-12
那所謂的 Spectrum  Subtraction 就是 s  s	15-12
那這個也可以算是少數這個壽命最長的語音技術	15-12
那麼它在七零年代一九七零年代的末期就已經非常成熟	15-12
但是呢一直到現在仍然廣泛的在使用	15-12
不但是廣泛的在使用而且不斷的有新的研究	15-12
所以呢一直到今天仍然每年都有很多篇 paper 在說這個可以怎麼做做的更好等等一直都是	15-12
到今天仍然是一個活躍的研究領域因為不斷有新的 paper 在說怎麼做喔	15-12
所以可以說是少數最長命的最長命的這個也算是最有代表性的技術之一所以我們稍微提一下	15-12
那這個觀念說穿了其實很簡單	15-12
那它的想法只是說	15-12
假假使我的語音訊號在這	15-12
那麼雜訊是什麼呢雜訊是 noise 一般是奇奇怪怪的加在這個上面	15-12
那麼加在上面之後把整個訊號都弄亂了而這個訊號常常是每一瞬間永遠不一樣	15-12
對不對你這一段的 noise 跟這一段的 noise 是不一樣的	15-12
你你真的要把它的 noise 去減掉還不知如何減法因為你其實根本就是 noise 跟 signal 混在一起	15-12
你要把它扣掉拿掉這這不知怎麼拿	15-12
那麼但是呢事實上有一個很自然的方法就是你如果做一個 fourier  transform 到 frequency  domain 去的話	15-12
到 frequency  domain 的時候我的我的訊號仍然每一瞬間是不一樣譬如說我這個取的跟這個取的是不一樣的	15-12
這個訊號的 fourier  transform 跟這個是不一樣的可是 noise 通常是很像的	15-12
所以呢我如果我如果 noise 取出來是某一種樣子的話	15-12
我下一瞬間的 noise 可能還是差不多的	15-12
也就是說 noise 本身在 frequency  domain 的變化不會那麼快喔	15-12
這個是一個這個 noise 絕大多數的 noise 都有這麼一個特性	15-12
也就是說雖然 signal 在在這個 frame 跟這個 frame 是可能是差很大	15-12
我的變化是相當多的它的它的 fourier  TRANSFORM 也是差很多的	15-12
可是 noise 在這一瞬間跟這一瞬間你如果	15-12
你看 time  domain 仍然差很多可是你如果做 fourier  transform 過來的話呢可能差不多	15-12
喔因為這個原因呢所以就想出這個辦法	15-12
就是我到 frequency  domain 去相減	15-12
那也就是說呢你你今天這個我們說話的時候我們知道我們永遠是有一堆斷開來的 silence	15-12
換句話說我在講話的時候	15-12
我絕對不是連珠砲一直說個不停而是我這一段說完之後我會停下來 either 聽對方講話 or 是我要換口氣或者怎樣	15-12
有一段是空的之後才會有下一段	15-12
那麼講完之後我又有一段是空的才會有下一段	15-12
那麼當你在中間這段空的時候這邊顯然都是 noise	15-12
同樣的一開機我還沒講話之前開機的這邊也是 noise	15-12
所以我只要在這個地方取 noise	15-12
然後來做 fourier  transform 的話我得到這個 noise 的 spectrum	15-12
那麼因此呢我可以在這邊呢就拿這個當成是 noise 來減把它減掉	15-12
基本上精神就是這樣子而已	15-12
那麼因此呢譬如說我在一開機還沒說話之前有零點一秒的的空檔我就把那零點一秒的空檔拿來做 fourier  transform	15-12
那我就知道 noise 長怎樣那我可以假設這段時間的 noise 都是這樣	15-12
雖然不完全是但是至少是像的所以我這一這一小段裡面我就都把它的全部都做 fourier  transform 都把它減掉	15-12
之後呢就得到一個比較接近於真實聲音的東西	15-12
然後等到這邊也有空下來之後我重新取新的 noise	15-12
取新的 noise 到這邊再來減新的 noise 喔	15-12
那我這邊又可以取新的 noise 我這邊又可以減新的 noise	15-12
因此呢我就拿這一段來減這邊拿這一段來減這邊	15-12
喔基本上想法是這樣子	15-12
那如果是這樣子的話呢你大致上可以把	15-12
當然不會 exactly 一樣啦 noise 當然也是在變啦不能說這邊的 noise 跟這邊的完全一樣是不會的	15-12
但是呢它變化的比較少	15-12
在 time  domain 變化得多在 frequency  domain 變化得少所以我以用它來當它來減的話呢是可以清掉不少 noise	15-12
讓這個聲音好聽不少	15-12
那這個想法就是所謂的 spectrum  subtraction 你看他名字就知道了	15-12
就是我到 frequency  domain 去在它的 spectrum 上面相減	15-12
那真的做的式子是稍微複雜一點是底下這個式子	15-12
那它的意思是說呢我們每一個 frequency 去減	15-12
這個就是 y 就是 y 就是我 signal 加 noise 當然 noise 也在這上面也加了在這上面嘛	15-12
noise 跟這個一起加的嘛所以這個地方得到的是 signal 加 noise 這就是所謂的 y	15-12
但是呢我在這邊所得到的呢就是所謂的 n	15-12
所以呢我把 y 跟 n 都做 fourier  transform	15-12
然後呢在 frequency  domain omega 這個地方去 y 去減 n	15-12
基本上就應該得到比較接近原來的 clean  speech 這就是我這邊用的 x 的估計值 x  hat	15-12
ok 那這個就是它的這個基本的最基本的想法其實就是這樣子而已	15-12
也就是說我的 clean  speech 是這樣	15-12
這個是我的 clean  speech 就是 x 的 t	15-12
那我加上一堆 noise 那我假設是 n	15-12
於是呢我就把 y 減掉 n 得到我的 x 的 hat 喔	15-12
但是呢這個時候要稍微小心一下就是我們真正做的時候呢這個 y 的 transform	15-12
你如果做出來很可能會變成這樣這是 omega 我的 frequency	15-12
那我我 y 做出來我們知道它其實有大有小嘛	15-12
如果這個是我的 y 的 omega	15-12
就是我的這個這個 i 就是第 i 個 frame 喔	15-12
這個 i 是指第 i 個 frame 的的那個做出來的這個這個 noisy  speech  y  of  omega	15-12
但是呢我減我這個這個我拿前面的這個這段這個還沒有開始講話之前剛開機的這個這零點一秒的這個 noise 拿來做的 transform 呢搞不好是像這樣	15-12
譬如說是	15-12
那這個呢我們如果說這個是這個 n  of  omega	15-12
那就是這個	15-12
那這時候你就會發現呢因為 n  of  omega 如果這樣 y 如果是這樣的話呢有的地方 y 比起 n 來並不大	15-12
當你像這一像這個 frequency	15-12
y 比 n 大很多的時候你可以相信如果把 y 減掉 n 之後大概把這個 noise 去掉不少是會比較好的	15-12
可是你如果像在這個地方的話像這個地方的話呢你的你的 signal 並沒有比 noise 大多少	15-12
noise 這個地方滿大的在這些 frequency 上 noise 滿大的而而 signal 滿小的	15-12
你這個時候再減還還可靠嗎這個時候可能非常糟糕	15-12
也就是說在在這些個 frequency 裡面它的 signal 比 noise 沒有大多少甚至於比 noise 還要小或者怎麼樣	15-12
那你這時候還要去減一減的話減出來是八成是不對的	15-12
因此呢怎麼辦	15-12
那它的辦法就是說我取一個原來那個 y 乘上某一個 alpha 這個 alpha 是一個要選的參數譬如說零點幾	15-12
你如果是減起來大於零大於它的原來的零點幾倍的話我就用減的	15-12
可是如果小於它的零點幾的話我就不減了我就把原來那個乘上零點幾就算了	15-12
我們舉例來講假設這是零點三的話	15-12
那麼我拿拿這個大的乘以零點三之後	15-12
凡是減了之後比那個零點三大都沒有問題我都用減的	15-12
所以夠所以這個地方只要夠大的話我都用都用這個來減	15-12
可是如果你這個一減之後比原來的零點三還要小的話	15-12
那就表示這個這個 noise 已經大到一個程度這個地方這些個 frequency 就已經非常不可靠那我就不要減了	15-12
我就把原來那個 y 乘上那個 alpha 就當成我的訊號了喔	15-12
好所以呢它就有有這兩種做法	15-12
那麼 depends  on 我有這個 alpha 就是它所謂的 flooring 就是說我有一個這個下限	15-12
我減起來要比這個下限大的我才我才減如果比這個下限小我就不減了	15-12
那這個是這個是這個所謂 Spectrum  Subtraction 一個最基本的一個一個公式就是這個東西	15-12
那在在這個情形之下呢那還有一個問題就是說那你這個 noise 要怎麼算	15-12
我們剛才講說 OK 我就是拿前面基本精神就是我拿前面這一段當成這邊的拿這邊這一段當成這邊的	15-12
不過一個比較好的辦法呢我也是用一個 interpolation	15-12
也就是說我如果是這個 n 的 i 跟新的中間新的跟舊的有一個 interpolation 用一個 beta	15-12
那 beta 也是一個參數	15-12
換句話說我如果前一個 frame 我是用這個 i 減一在減的話	15-12
那麼我新的假設我現在得到一個新的 n 我不是直接用它的 n 而是用前面的來做 interpolate	15-12
換句話說如果我一開始剛開機的時候我用這邊得到某一個去減	15-12
怎麼減我得到那個也是 averaging  over  m 個 frame  of  locally  detected  silence  parts	15-12
也就是說我不是拿最後一個而是假設說我剛開機有零點一秒還沒開始講話	15-12
這零點一秒我其實得到了譬如說十個 frame 我是拿這個十個 frame 做平均拿來用	15-12
那麼於是我用在這裡是沒有錯	15-12
等到這邊我開始又得到新的 noise 的時候呢	15-12
我新的 noise 平均得到新的東西呢我不是直接它來減它而是這個東西再跟前面呢來做一個 interpolation	15-12
那就是這邊的式子	15-12
所以呢這個這個 i 的 n 這個東西呢是最新 detect 到的 frame	15-12
我現在在 frame  i 應該要用的是這個 noise	15-12
但是呢我不直接用它我跟之前的做個 interpolation	15-12
那這個 beta 也是一個重要的參數要選的	15-12
那麼這樣之後呢你這樣的 noise 比較穩定一點我用這個東西來減它	15-12
喔那那這個就是這個這兩個式子其實就是 Spectrum  Subtraction 最基本的公式	15-12
那這裡面其實有很多的學問就是怎麼怎麼求這個 alpha 跟怎麼求這個 beta	15-12
那麼在什麼樣的 noise 狀況之下你怎麼樣子判斷這個 alpha 應該怎麼調	15-12
那麼 alpha 跟 beta 最好是可以隨著 noise 的狀況隨著 signal 的 noise  threshold 你要能夠調它	15-12
然後呢再進一步這兩個公式也可以調也是都是可以動的	15-12
那麼這兩個公式如果動一動都可以變的更好等等喔	15-12
那當然這些東西我們講起來呢都講起來這麼簡單那其實呢這邊相減呢只是他的	15-12
這個你知道 fourier  transform 都是有 amplitude 跟 phase 的	15-12
所以它是有一個 amplitude 你 transform 的時候是有一個 amplitude 有一個 phase 嘛	15-12
還有一個 phase	15-12
那真正的比較好的做法是	15-12
我減 amplitude 但是 phase 我用原來的喔他們通常的做法是這樣子所以呢	15-12
就是 transform  back  to 當你這個減完之後就這個地方減了減完之後把它 transform  back 到原來的那個	15-12
但是呢這個用原來的 phase 就是 phase phase 我沒辦法動它我就用原來的原來 y 有什麼 phase 就用 y 的那個 phase	15-12
然後一個 frame 一個 frame 轉轉回去你就可以得到一個有進步的聲音喔這就是所謂的 Spectrum  Subtraction	15-12
那在多數 case 都是有進步的	15-12
但是你如果 s  n  ratio 很低的話	15-12
這個 noise 很高 signal 很低 s  n  ration 很很差的話這個效果就不會好了因為你變成大多數都是這個狀況就不會好了	15-12
所以這個是但是它有在很多時候都有相當好的效果喔	15-12
那麼那麼很多時候你如果減不對的話會產生 musical  noise	15-12
什麼叫 musical  noise 呢就是你如果剛好譬如說	15-12
這裡的 noise 剛好 locally 有一個比較高的 pick	15-12
那你它減它之後喔不對比較有一個低的它剛好有一個有一個低的一個有有一個低的 valley	15-12
那這個低的部分你你有的時候你你在這邊你沒有辦法沒有辦法除掉那個低的部分	15-12
你就把它一減之後呢你那邊就多出一個高的出來那麼多出一個高的東西出來那個東西變成一個很奇怪的一個聲音	15-12
變成一個某一個 frequency 有一個很奇怪的聲音被被減出來那個聲音聽起來像像某一個 SIGNAL  frequency 在那個地方	15-12
那是所謂的 musical  noise	15-12
那麼在很多時候你如果這個 alpha 跟 beta 調的不好的話會跑出這種東西出來喔	15-12
那那那他們有很多的研究如何如何做的更好也也是很多時候是為了要消除這個 musical  noise 喔等等	15-12
那這個我想這個是簡單的解釋這個就是 Spectrum  Subtraction	15-12
那它的它的好處就是除了一方面可以 for  listening  purpose 一方面是 for  recognition  purpose	15-12
也就是說那麼其實早在七零年代這個辦法就已經出來了	15-12
那在當時是只是為了 for  listening  purpose 就是為了好聽的	15-12
那麼你在汽車裡面你在飛機裡面你在這個比較吵雜的環境之下的聲音你都可以加一個這個 process	15-12
都可以把它雜訊清掉相當程度變的比較好聽一點	15-12
那到了後來有 speech  recognition 的時候人家發現其實拿這個當成 recognition 的 front  end 先減掉之後再做也會比較好所以呢這個也可以做 recognition  purpose 喔	15-12
所以這個是 enhancement  speech  enhancement 的這個一個最代表性的例子	15-12
那我們剛才提過這是最少數壽命最長的技術	15-12
它在七零年代就已經出來了但是呢一直不斷有人在做研究因為不斷的可以把這個最簡單的方法做各種調改變	15-12
呃最簡單的就是怎麼樣調這個 alpha 跟 beta	15-12
再複雜是怎麼把這個式子再做一點改變喔這個式子這邊都可以做一點改變譬如說這邊加上一個 alpha 次方加上一個 a 次方	15-12
讓它在 a 次方的地方相減或者是怎樣有很很多這個手腳可以動	15-12
那一直到今天每年仍然有非常多的 paper 再說這個怎麼做會比較好等等	15-12
所以它活到現在而且還活的很好是少數壽命非常長的這個代表性的一個技術	15-12
所以我們拿它來提一提作為這個 speech  enhancement 的代表性的作品	15-12
那這個就是我們前面所說的這個這裡你如果直接在 signal 這邊做的話也是有東西可以做的	15-12
那他的原我列了一個原始 paper 就是這裡的最後最後第二篇喔	15-12
這個其實就是 Spectrum  Subtraction 的原始 paper 早在一九七九年所以是二十將近三十年前喔	15-12
但是這個技術活到到今天仍然活的很好喔那麼這個	15-12
那你如果去找 reference 的話每一年都可以找到很多 paper 我這個只是最早的最原始的那一篇喔	15-12
那另外像這個第九還有一篇這篇我現在就不講了那你如果有興趣可以自己去看這個也滿有意思的它用一個所謂 union  model	15-12
它把它的這個把它這個這個 Spectrum 分成一段一段的	15-12
因為很多時候這一段 noise 很厲害這一段 noise 不厲害所以你可以分成一段一段去處理哦等等	15-12
那這個我想這一類的 paper 很多那麼在這個領域是今天語音研究非常非常熱鬧的一個大領域	15-12
那麼你每年可以看到幾百甚至於是幾千篇 paper 都在搞這堆東西的喔	15-12
那麼我們這邊講的算是比較早比較早比較呃五六年以上然後比較這個嗯經過時間考驗大家都認定還算是不錯的東西	15-12
那新的很多很多那這個都是這個領域都是非常好的做報告的題材如果你對這個領域有興趣的話	15-12
所以我們大概說一說好十五點零說到這裡	15-12
那麼底下呢	16-1
我們要進入最後還沒有講的兩樣東西	16-1
就是呃這個dialogue 跟這個wireless	16-1
喔那我們要在剩下的兩兩堂課裡把它講完	16-1
ok 我們先在這裡停十分鐘	16-1
休息十分鐘	16-1
應該是十六十七吧可能	16-1
ya ok 我們底下會講十六點零spoken dialogue	16-1
ok 好我們來講這個十六點零spoken dialogue	16-1
那dialogue 是另外一塊也就是說	16-1
嗯它需要有喔有這個speech 的understanding recognition	16-1
然後這個dialogue 的manager 然後要能夠跟user 對話等等	16-1
那這整個的東西是蠻複雜的	16-1
那麼在呃一直到現在其實並沒有一個統一的統一的理論架構	16-1
說是怎麼樣來model 怎麼樣來分析這一塊	16-1
這個喔在dia 在dialogue 裡面倒底怎麼樣子能夠做最完整的分析的學理的模型	16-1
或者說是這個喔其實一直都沒有	16-1
那因此呢各家有它各家的做法跟說法	16-1
那麼都不太一樣	16-1
那麼並沒有哪一個是大家所公認的	16-1
那我底下所講的主要是根據這本書	16-1
就是這本課本所說的第十七章	16-1
為大部分的內容的來源	16-1
那有一小部分來自這裡	16-1
那這些基本上可以講是比較公認、大家比較可以接受都可以接受那一塊	16-1
那不過也因為是要大家都接受所以呢它裡面理論沒有沒有太多	16-1
比較是實務經驗比較多	16-1
就是真正在做這些東西的時候要考慮哪些事情喔	16-1
比較比較多的是實際的經驗	16-1
那它的理論架構主要就是這一頁所說的	16-1
這一套機率模型	16-1
這一套機率模型是多數人都覺得蠻好的	16-1
問題只是這套模型並不表示真的可以做	16-1
也就是說這裡面很多機率到底怎麼算的	16-1
怎麼train 出來的其實不見得有統一的說法	16-1
那麼每一個每一家可能說得不太一樣喔	16-1
不過這套基本上架構是這一套是大家所共同了解的	16-1
所以底下我們講就是這一套	16-1
那基本上這張圖是我們之前就已經說過的	16-1
也就是說你user 要跟系統對話	16-1
那麼這個系統顯然要先recognized	16-1
然後understand 你的speech	16-1
然後呢這個系統跟這個本身要有一個manager 來manage 這個dialogue	16-1
那麼當user 說了什麼你應該怎麼反應	16-1
user 怎麼樣做user 講了什麼你應該怎麼做等等喔	16-1
它有一堆這個操作的部分等等	16-1
那底下我們就來說這些東西	16-1
那這個基本架構是這樣	16-2
就是你可以想像dialogue 的核心看成是這個式子	16-2
這個式子裡面s n 是什麼所謂的discourse semantics	16-2
也就是dialogue state	16-2
那這是什麼意思呢	16-2
基本上來講喔這也是不同的課本不同的文章裡面他們用的名詞不一樣	16-2
他們有的人叫做discourse semantics	16-2
有的人叫dialogue state	16-2
意思是差不多的	16-2
也就是你的dialogue 進行到哪裡了	16-2
到目前為止獲得些什麼	16-2
舉例來講	16-2
如果說是一個買飛機票的dialogue 系統的話	16-2
你馬上要知道	16-2
這系統要知道的是你要的這個出發的地點是哪裡	16-2
目的地是哪裡	16-2
然後你要的是哪一定航空公司	16-2
你要的是這個呃幾月幾號出發	16-2
你要的是經濟艙還是商務艙等等	16-2
這些個就是user 要specify 他的user 的目的的	16-2
這些東西呢我們就稱為譬如說define 某一種state	16-2
所謂dialogue state 就是ok	16-2
出發地知道了沒有知道了	16-2
那麼目的地知道了沒有知道了	16-2
航空公司知道了沒有知道了	16-2
那麼這個哪一天出發知道了沒有還不知道	16-2
然後要經濟艙還是商務艙知道了沒有還不知道	16-2
ok 哪些知道了哪些不知道	16-2
然後它怎麼樣子去那麼還不知道的東西去問user 啊等等	16-2
那這些就是說到目前為止	16-2
user 的目的	16-2
user 要做的事情	16-2
你系統知道到什麼程度	16-2
這是所謂的dialogue state	16-2
那通常那也是你dialogue 最核心的semantics	16-2
就是說你要知道他的哪一句話是指他的目的地	16-2
哪一句話是指他的出發點	16-2
哪一地方哪一句話是指他的行班等等喔	16-2
那你要知道他的那就是所謂的semantics	16-2
所以你一路走過來前後文累積起來	16-2
我的semantics 知道到什麼程度	16-2
這叫做discourse semantics	16-2
那是在什麼時候呢是在第n 個term 的時候	16-2
所以是有sub n	16-2
所謂的term 你知道只是一個這個譬如說下棋的時候	16-2
這一步我下下一步你下	16-2
或者打牌的時候這次我出再下次你出	16-2
這就是所謂的term	16-2
所以在我們來講所謂的dialogue term	16-2
就是指這個user 說說一段話的	16-2
那就是一個term	16-2
機器在做一件什麼事情那是一個term	16-2
user 在說一段話機器再說一段話這各是一個term	16-2
所以呢s n 就是指在第n 個term 的時候	16-2
我的dialogue state 到哪裡	16-2
或者說我所了解的 user 的semantics 到哪裡	16-2
那那這個x n 呢	16-2
這個s 是指speech input	16-2
在第n 個dialogue term 的時候	16-2
那進來的聲音是x n	16-2
那因此呢	16-2
你在時在時間等於n 的時候就是第n 個term 的時候	16-2
進來的聲音是x n 的時候呢	16-2
你要根據n 減一的時候我知道多少東西	16-2
那麼於是我來判斷我下一步action 要怎麼走	16-2
a n 是指系統的action	16-2
系統action 包括系統去做事	16-2
譬如說他要訂飛機票	16-2
他去查哪一天有什麼航班	16-2
他要去這個或者他要講什麼話來問user 等等都算	16-2
就是系統的action 就是a n 喔	16-2
那麼包括你的手機的或者是網路的action 等等在內	16-2
在第n 個term 的時候ok	16-2
所以呢基本上這個意思等於是說	16-2
那麼在時間在第n 個term 的時候進來一個聲音是x n	16-2
那那個時候我要根據到n 減一為止的時候的所有的semantics 跟state 的知識	16-2
累積到這裡為止	16-2
我來判斷那麼我下一步應該做什麼action	16-2
那你可以把這個想像成是一個given 這個的一個機率的估計	16-2
然後呢我要找哪一個action 是機率最大的	16-2
那suppose 那就是最好的action	16-2
所以我的goal 就是要我希望這個系統的每一次都take right action after each dialogue term	16-2
每一次這term 的時候呢系統該動作都做一個對的action	16-2
然後最後可以complete task 喔	16-2
successfully finally	16-2
那上面這個式子怎麼辦呢	16-2
那麼他們說ok 我可以拆成這三個機率相乘	16-2
這樣拆是沒問題啦喔	16-2
問題只是說這每個機率之後怎麼train 這是一個很大的問題就是了	16-2
那你拆成哪三個呢	16-2
那第一個呢就是說這個given 我在第n 個term	16-2
我given n 減一的state knowledge	16-2
然後第n 個聲音進來的時候呢	16-2
我判斷的也許不是這個而是什麼	16-2
先要判斷這個這個f n	16-2
f n 是什麼呢是semantic interpretation of the 這個input speech x n	16-2
我不只是要做speech recognition	16-2
而且我要知道它的意思是什麼	16-2
譬如說如果你系統在跟他講買飛機票	16-2
他說我要去紐約	16-2
如果這樣的話呢	16-2
你不光是說我要去紐約這幾個做recognition 而已	16-2
你要知道它的意思是指他的目的地	16-2
是到一個city 去	16-2
那個city 的名字叫做紐約	16-2
那這個就是所謂的semantic interpretation	16-2
你要了解它的意思是指目的地	16-2
然後那個紐約是個city name	16-2
喔你要知道這些東西	16-2
所以這是一個那你要做就是speech recognition and understanding	16-2
那得到這個東西	16-2
那這個其實寫在底下喔	16-2
就是說這個機率其實就是就是這個我要convert x n	16-2
to some semantic interpretation f n	16-2
given 我前面到目前為止n 減一為止我的這個dialogue state	16-2
那這個要做的事情其實就是speech recognition understanding	16-2
也就是我們前面這張圖裡面畫的這一塊	16-2
那這塊就是要把這x n	16-2
變成某一種f n	16-2
就是它的semantics 這個semantics concept	16-2
然後呢有了這個之後呢	16-2
我要根據現在我得到這個semantic interpretation f n	16-2
跟之前的s n 減一	16-2
去得到下一個s n	16-2
我新的dialogue state 是什麼	16-2
譬如說他說我我我的目的地我要到紐約去	16-2
那於是呢剛才的n 減一那裡	16-2
我可能知道出發的地點我知道了時間	16-2
知道了user 的名字	16-2
但是我不知道他去哪裡	16-2
那這回他說他去紐約了你知道了	16-2
因此呢我就把這個加入我系統所累積的知識	16-2
也就是系統所累積的關於user 的目的	16-2
於是我就得到一個新的dialogue state	16-2
在這個新的dialogue state 裡面呢	16-2
我的目的地那個空格填進去了是紐約	16-2
那就由x n 減一變成s n 了	16-2
所以這就是所謂的discourse analysis	16-2
那也就是說呢你就是把把n 減一的時候的state	16-2
或者說semantics	16-2
或者說是你累積的knowledgement	16-2
convert 到x n	16-2
然後呢於是你就有一個新的dialogue state	16-2
given 所有的past 可能的f n	16-2
那麼於是呢那這塊是什麼呢	16-2
就是所謂的discourse analysis	16-2
也就是我們圖上的這一塊	16-2
你是要根據過去所累積的	16-2
過去每一句話每一個term 裡面user 說的累積起來	16-2
他已經說過他的姓名	16-2
他已經說過他要搭哪一家公司	16-2
他已經說過他出發的地點	16-2
他還沒有說目的地	16-2
這回你說目的地了	16-2
於是我就可以把剛才所累積的東西再加進去	16-2
我又多了一塊又填進去了等等	16-2
所以這個是discourse analysis	16-2
就是這個	16-2
那這個時候呢你可能f n 不只一個	16-2
也就是說你的understanding 可能不是唯一的答案	16-2
你可能有好幾個	16-2
舉例來講假設是買火車票	16-2
他說我要買火車票我要去台中	16-2
但是呢台中跟台東是很像的	16-2
所以呢你在這裡可能不能判斷他到底是台中還是台東	16-2
可能台中台東各有百分之五十的機率	16-2
如果這樣的話我這兩個都要考慮	16-2
都要放進來喔等等	16-2
那麼因此呢你就會有不只一個f n 在這裡	16-2
因為f n 是不能判不能確定一定對的	16-2
所以你可能要把好幾個f n 通通可能通通算在這通通加起來	16-2
喔所以呢這邊會有一個summation	16-2
之後呢那後面這這個機率呢就是	16-2
我現在已經知道我到這裡了	16-2
user 意思到這裡了	16-2
那user 想要做到這裡了	16-2
於是呢我現在下一步應該做什麼事	16-2
那這個呢就是dialogue manager 要做的事情	16-2
也就是也就是說那這塊呢就是我們剛才的這張投影片這塊	16-2
也就是說我現在已經知道累積了這麼多知識了	16-2
user 又進來這個畫了我都知道了	16-2
那這回下一步要做什麼事	16-2
它要決定下一步	16-2
所以這個就是dialogue manager 要做的事情	16-2
那它呢就是select the most suitable action	16-2
你要決定下面做什麼事	16-2
喔given 這些東西	16-2
那於是呢你在失在這個所有的a n 跟s n 裡面	16-2
它如果是某一某一個s n	16-2
它應該是某一種action 裡面我去找	16-2
看看哪一個a n 最好的那一個a n 就是這個	16-2
那這樣的這一個呢就把這個機率拆成這三塊	16-2
那這三塊其實就是這邊寫的這三行	16-2
那這三個呢就是這個機率就是我們這邊講的這個speech understanding recognition understanding	16-2
然後這邊講這塊就是我們這邊講的這個喔dialogue discourse analysis	16-2
然後呢這邊的這個呢	16-2
就是dialogue management	16-2
就是這個	16-2
所以呢這三塊就是分別由剛才這三個機率來代表嘛	16-2
那當然這邊還有一塊我們沒有講	16-2
那就是要做你要決定user 這個系統要對user 說什麼話	16-2
那可能要造個句子出來	16-2
然後做語音合成	16-2
那這塊我們現在姑且不說	16-2
所以我們講的就是這三塊	16-2
就是剛才的這三個機率或者這三行講的東西	16-2
好那到這裡為止這一堆是比較完整的這個學理上的說法	16-2
那底下的多半都是一些比較經驗式的	16-3
譬如人家真的在做是怎麼做的	16-3
那麼第一個呢就是你怎麼把那個dialogue define 它的structure	16-3
你define 它的structure 之後比較容易去分析它	16-3
那這個structure define 呢就有很多種	16-3
有人用term 有人用這個initiative response pair	16-3
有的人用act	16-3
有的用sub dialogue 喔	16-3
那我們姑且說一下譬如說什麼是term	16-3
我們剛才已經講過了	16-3
就是某一個人不管是user 還是系統	16-3
它講了一段話	16-3
一個uninterrupted 的stream of speech	16-3
他講那段話	16-3
那麼從一個一個人或者那個user 來的	16-3
那就是一個term	16-3
於是就會有用term 為單位	16-3
來把一個這個dialogue 的結構分析出來	16-3
那這個時候我們就會有speaking term 跟back channel term	16-3
有的term 是convey 明顯的訊息	16-3
有些沒有	16-3
它只是一個acknowledgement	16-3
譬如說ok yes	16-3
yes	16-3
你如果光是這樣的話這只是一種back channel	16-3
並沒有提供太多	16-3
這也是一個term	16-3
但有的時候term 是有豐富的意思的	16-3
這所謂的term	16-3
那另外一種說法呢就是說我把它不是用term 來分	16-3
是用initiative 跟response 來分	16-3
那不管是user 還是系統	16-3
會initiative 某一樣東西	16-3
然後user 去然後另外一方去回答	16-3
那這是initiative 跟res response	16-3
那這兩個跟term 不太一樣	16-3
譬如說一個term 可以同時包括兩個	16-3
那譬如說這個系統說你要的是星期五嗎	16-3
user 說是的星期五我要去美國	16-3
他可能除了是的星期五	16-3
這個是一個response	16-3
但是他如果說要去美國	16-3
那這個時候已經是下一個initiative	16-3
那麼因此呢這個這個你如果把它分成initiative 跟response 不是完全就是一個term	16-3
不一樣	16-3
那這裡面我們很早就說過你有分成system initiative 由系統來主導像填一個表一樣	16-3
但這個時候就是很僵硬的	16-3
那你可以user initiative 完全用user 來主導	16-3
不過這個是非常難的	16-3
我們今天比較希望做到中間這種就是mix initiative	16-3
也就是兩者都有一點	16-3
通常是盡可能由系統來主導	16-3
系統來主導的話是我比較容易來處理的	16-3
譬如說系統問說你要到哪裡去	16-3
我說去紐約	16-3
你要從哪裡出發從台北	16-3
系統一路問那user 一路回答像填一張表一樣	16-3
這個是最容易的	16-3
但是這樣會很僵硬	16-3
所以呢如果user 沒有照系統問的問去回答的話呢	16-3
只要user 講的是在那個範圍之內	16-3
最好系統也能夠處理	16-3
那這樣的話呢就是mix initiative	16-3
那最多的人分析dialogue 是用所謂acts 來分析	16-3
那麼這個speech acts 或者dialogue acts	16-3
這個是喔非常多的文章裡面它們的做法	16-3
就是用acts 來分析	16-3
什麼是acts	16-3
acts 就是user 的spea 這個speaker 的goal 或者intention	16-3
他倒底要幹嘛的	16-3
我們把它define 成一個act	16-3
那基本上來講它是regardless of the detailed linguistic form	16-3
也就是說你可一個act 裡面可以有很多linguistic form	16-3
我們用底下例子來看比較清楚	16-3
啊譬如說這個有有很多系統一開一開始就是說may i help you	16-3
或者說喔查號台你好	16-3
或者說這個您早這是什麼什麼什麼喔	16-3
那這些呢其實都是相同的意思	16-3
就是conversation opening	16-3
那不是真的在講這個話而只是opening 的意思	16-3
那這個就是一個act	16-3
那這個呢就是它的linguistic form	16-3
那有的時候呢像系統說there are three flies from taipei	16-3
那這個其實這個是一種offer	16-3
那這個offer 是一種acts	16-3
這個系統在offer 給user 說有什麼有什麼有什麼	16-3
那裡面linguistic form 很多有各種各樣	16-3
你可以說是有一種是assert	16-3
這是一種act	16-3
那reassert 是另外一種	16-3
no i say tuesday	16-3
那這是reassert	16-3
那也可以是information request	16-3
啊譬如說when does it depart 等等	16-3
那這邊的這裡的這些東西	16-3
括號裡面的就是所謂的linguistic form	16-3
然後這外面這個就是所謂的act	16-3
所以呢通常很多人把他的dialogue 系統define 成為一堆acts	16-3
譬如說你如果要去買飛機票	16-3
他可能總共define 二十五種acts	16-3
那就是說在user 在跟系統對話過程中間	16-3
所有的說的話不外乎分成這二十五類	16-3
那每一類呢就是一個act	16-3
然後你再去看那一類裡面他的是什麼	16-3
那你每一類可以每一個act 可以自己有自己的model	16-3
因為在那個act 之下	16-3
它要說的是它會說是那些	16-3
所以你acoustic model language model 都可以在每一個act 不一樣	16-3
那你如果是ok 查天氣	16-3
可能有查天氣的你可以define 成有十六種act 等等等等這樣子	16-3
那剛才那另外一種說法它可以把acts 分成forward looking 跟backward looking	16-3
剛才講的這些都是forward looking 也就是向前發展	16-3
向前發展的act	16-3
還可以向後來這backward looking 的	16-3
就是向後的譬如說confirmation yes	16-3
或者是ok but	16-3
我又加一點別的東西	16-3
那接受一部分	16-3
或者reject	16-3
或者說呢不清楚重說一次喔等等等等	16-3
這些東西屬於back backward	16-3
也就是說它並沒有向前發展新的東西	16-3
而是主要是在confirm 或者是回應原來的東西的	16-3
那基本上來講你可以想像一個act 可以對應到千千萬萬個不同的linguistic form	16-3
因為你要講某一件事情	16-3
你後面可能有很多種講法啊	16-3
所以呢你的linguistic form 可以有非常多	16-3
但是反過來也有的時候有好多種不同的這個呃同一個linguistic form 可能有不同的act	16-3
譬如說ok	16-3
至少有二種	16-3
一種是confirmation 的ok	16-3
一種呢是request ok 喔	16-3
如果你是講ok 的話好像應該是在問對方	16-3
要求對方confirmation	16-3
ok 那就表示是confirmation	16-3
所以你也很可能一個linguistic form 這是一個linguistic form 對應到二個不同的act	16-3
那這裡面的act 有的是task independent	16-3
不管什麼的dialogue 都可以用的	16-3
有的是task dependent	16-3
depend on 那個dialogue 幹什麼的	16-3
舉例來講	16-3
這個conversation opening	16-3
這個是independent	16-3
任何一個task 都可以由這個開頭	16-3
你說yes	16-3
這個大概也是任何一個或者說what did you say	16-3
這大概可以任何一個application 都可以用的	16-3
那就是task independent	16-3
反過來呢你說ok	16-3
你說there are three flies from taipei 這顯然是買飛機票的	16-3
如果是其它的聲音就是不同的了	16-3
所以這像這個情形的話呢這種買飛機票的就是專門針對買飛機票的裡面的一個act	16-3
所以這個就是task dependent	16-3
那當然這樣你當你盡可能分得那麼細的act 之後	16-3
有的act 是task independent 有的是dependent	16-3
於是呢你那些independent 就可以拿來在所有task 都拿來用	16-3
然後呢你只是在不同的task 做不同的task dependent 的東西等等	16-3
那這樣的話你可以就比較容易	16-3
那麼於是呢我的我要分析我要model 我要train model 等等	16-3
我都可以design 我都可以根據每一個act 來做	16-3
那他們很多的時候譬如說我買飛機票我define 二十五個act	16-3
於是所有的話只要在這二十五個act 之內	16-3
我depend on 在那裡我就用哪一個act model 去分析它等等	16-3
那還有一種人呢他是把它叫做分成sub dialogue	16-3
就是譬如說買飛機票的話呢	16-3
一個是要問目的地的	16-3
一個是要問出發地出發時間的	16-3
等等等等	16-3
你把它分成一個一個的小的task	16-3
那每一個呢就有一個小的sub dialogue 來處理	16-3
於是就變成一堆sub dialogue	16-3
這樣也可以	16-3
這個跟act 不太一樣喔	16-3
這是一個完整的小dialogue	16-3
然後每一個完整的小dialogue 他們連起來就構成一個大的dialogue	16-3
喔等等這是這樣的情形	16-3
所以你有各種不同的做法	16-3
那這是講我們怎麼看這個dialogue 的structure	16-3
那怎麼看這個language understanding 呢這有很多種做法	16-4
所謂understanding 你要抓到它的意思	16-4
那這裡面呢我們舉其中一個最常用的最簡單的做法	16-4
就是所謂semantic class	16-4
它就是有一個entity 然後有一堆attribute 或者叫做slots	16-4
舉一個最簡單的例子譬如說買飛機票	16-4
那你就是系統要了解user 要的到底是哪一個flight	16-4
那user 要的東西呢我們叫做一個就是所謂一個entity	16-4
就是這個flight	16-4
那這個entity 裡面呢就有一堆這個所謂的attribute	16-4
譬如說呢如果就flight 而言	16-4
哪一家航空公司聯合航空	16-4
出發地點是舊金山	16-4
目的地是boston	16-4
時間是哪幾月幾號等等等等	16-4
這每一個呢就是所謂一個attribute 或者一個slot	16-4
那這這理得東西呢就是所謂filler	16-4
所以你的所謂的slot and filler 就是每一個slot 有一個filler 填進去	16-4
那如果這樣來看的話這就是我們所謂簡單的一個semantic frame	16-4
那你就變成說是在這個dialogue 過程中就是要把這些東西一路填過去	16-4
如果user 說了這個	16-4
你就知道這個是填在這裡	16-4
user 說了那個就填在這裡等等	16-4
那然後哪些還沒說你就要去問user 哪些你要的是什麼	16-4
那這樣的話呢就是一個比較容易一個非常簡單的方式來做到這樣的事情	16-4
那如果是這樣子把它一個一個的attribute 或者一個一個的slot 填進去的話呢	16-4
填進去之後也就假設你understand 這個是這個	16-4
所以它講的這個呢是指一個date	16-4
它講這個是指填那個表但這個這是一種簡單的understanding 的方法	16-4
那一個比較完整的方法呢應該是做這個所謂sentence parsing	16-4
或著context free grammar	16-4
那這個在一般的language understanding 裡面他們很很常用的	16-4
那這個我們舉一個很簡單的例子來說大概是什麼意思	16-4
他他要有一個所謂的grammar	16-4
就是一個文法	16-4
那這種文法是稱之為context free grammar	16-4
c f g 也就是我並沒有特別規定	16-4
它的context 要是什麼的	16-4
那這些grammar 有很多種型態	16-4
最基本的型態像這樣	16-4
它就是所謂的一個rewrite rule	16-4
譬如說一個sentence	16-4
你可以把它rewrite 成為一個名詞片語noun phrase 後面接一個動詞片語	16-4
那什麼是動詞片語呢	16-4
動詞片語它說呢是可以是一群v cluster 一群動詞	16-4
後面接介系詞片語	16-4
這個p p 是介係介係詞片語	16-4
那什麼是v cluster 呢	16-4
就是一群動詞的結合呢他說v cluster 可以是一個would like to	16-4
然後呢後面接一個動詞	16-4
那這個動詞是什麼呢可以是譬如說是可以是go 可以是fly 等等	16-4
那介係詞片語是什麼呢是一個介係詞後面接一個名詞片語	16-4
那那個介係詞呢可以是to	16-4
那那個名詞片語呢可以是個名詞	16-4
那個名詞可以是boston 等等等等	16-4
那這樣你最後就可以得到說假設user 說這句話	16-4
i would like to fly to boston	16-4
而這句話已經被recognize 出來的話	16-4
那麼你可以根據這如果這句話被正確recognize 出來你可以根據這堆rule	16-4
可以把它分析出來哦它是這樣的一個結構	16-4
那你這個結構知道的時候	16-4
你其實就已經知道它的意思了	16-4
它的意思應該是說這是它的動詞	16-4
他的這個是它的介係詞片語用來形容它的動詞	16-4
所以呢它的fly 是to boston 的	16-4
那這個是這個user 要做的事等等等等喔	16-4
所以呢當你這個這個整個句子結構分析出來的時候你大概就得到一個好的understanding	16-4
那這個通常是這樣的c f g 呢	16-4
是可以跟n gram 結合的	16-4
我們知道n gram 只講local relationship	16-4
n gram 只是說這個word 後面接不接這個word	16-4
或者這兩個word 後面接不接這個word	16-4
它只管這個local relationship	16-4
而沒有管整個句子的structure	16-4
而這個c f g 呢是管整個句子structure	16-4
所以它跟n gram 基本上是互補的	16-4
所以可以整合	16-4
那n gram 管local relationship	16-4
而沒有管semantics	16-4
那當然你也可以反過來在這裡面加機率	16-4
那這裡面加機率有很多很多種方法	16-4
你可以把這些rule 給它機率	16-4
也可以把這裡面放機率進去等等喔	16-4
那麼各有不同的做法	16-4
那基本上呢這個都是常用的情形	16-4
那這樣是不是真的可以解決我們的問題呢不盡然	16-5
你如果用剛才講的sentence parsing	16-5
來分析user 所講的話的話	16-5
用這樣子的東西來分析user 要講的話其實是不太容易	16-5
常常有很多困難的地方	16-5
第一個就是人講的話不一定合乎文法	16-5
我們寫文章的時候可能每一句都合乎文法	16-5
但嘴巴講的時候不見得	16-5
不合乎文法的話呢你那文法可能不work	16-5
第二個你有很多辨識的錯誤	16-5
你的辨視錯誤都是你的文法不通	16-5
那都會都可能讓你解不解不了	16-5
再來呢很大一個原因是人講的話常常充滿了很多的unnecessary detail	16-5
也就是一堆irrelevant words	16-5
我們舉例來講假設說你要去哪裡to boston	16-5
其實只要講這二個字to boston 就很清楚了	16-5
可是這個user 可能說i am going to boston i need to be in boston tomorrow	16-5
這一堆話都是廢話	16-5
就是所謂的unnecessary detail	16-5
那這些什麼i am going to 啊這些什麼i need to 什麼都是irrelevant word	16-5
都是不相關的word	16-5
那有的時候user 會在裡面講一堆good mourning 什麼什麼這是greeting	16-5
這種都是沒用的	16-5
那然後呢通常對個given act 呢會有無限多的linguistic form	16-5
譬如說你想這句話其實只是在講to boston	16-5
但這樣的話可以千變萬化可以講很多很多種	16-5
那你怎麼去handle 它這麼多種	16-5
那還有呢就是很多時候你user 其實是一面講話一面想的	16-5
他在跟系統對話所以一面講一面想	16-5
譬如說他說嗯just a minute	16-5
i wish to i wish to go to boston	16-5
那這裡面就有很多東西譬如說just a minute 這是幹嘛的這其實就是所謂的hesitation	16-5
喔等於等於說你沒有在講東西嘛	16-5
i wish to i wish to 這什麼這就是repetition	16-5
這種都是沒有意義的	16-5
還有呢repair 是說呢ok	16-5
ㄜ我明天呃不是明天我後天去	16-5
你中間做了一個repair 的動作那等等	16-5
那這些東西都都會讓你這整個東西變得很複雜	16-5
那這些東西就是我們講的spontaneous speech problem	16-5
就是說你在一個人在很自發性的跟系統講話的話	16-5
我們其實人會講很多很很難處理的東西像	16-5
嗯咳嗽聲喔等等這些都是很麻煩的	16-5
這些東西都很難處理	16-5
那如何對付這些問題呢	16-5
一般講起來一個比較簡單的辦法是這樣子	16-5
雖然它的效果也是有限的但是它可以解決一部分的問題	16-5
就是robust parsing	16-5
什麼叫robust parsing 呢	16-5
就是它把這剛才這個grammar 再切得更小更小	16-5
我們剛才講得這個太大了	16-5
這個這個要分析整個句子然後裡面很複雜	16-5
你搞不好你這邊這邊都不能夠分析	16-5
那怎麼辦	16-5
把它切成很小很小	16-5
你把它變成一個particular item in a very limit domain 喔	16-5
變成一個很小的domain 裡面一個特別的東西	16-5
你用很小的grammar	16-5
舉例來講譬如說我要講這個買飛機票	16-5
它的目的地呢欸就是一個介係詞後面接一個city name	16-5
那這個介係詞可以是to 可以是for	16-5
city name 就是boston	16-5
就是所有的city name	16-5
那你就把剛才那個問題reduce 到這個問題	16-5
然後呢所有的東西都當成是filler	16-5
就這一個小的task 而言	16-5
它只管一件事情就是目的地	16-5
那於是它只有一個文法就是這樣子	16-5
那這個這個介係詞不過就只有這三種	16-5
然後呢所有的city name 都在那裡	16-5
那你就是要去這個其實就是做key word spotting	16-5
我就去spot 說有沒有這幾個字	16-5
有沒有一個city name	16-5
如果它們接在一起那就是這個	16-5
那如果是這樣的話呢我就是這樣子之後我變成一個非常小的grammar	16-5
然後其它東西都當成是filler	16-5
那就專門專門處理這一塊	16-5
那如果是這樣的話呢我可能有很多不同的小grammar	16-5
同時操作	16-5
你譬如說在講目的地的有一個很小的grammar	16-5
在講出發地的也有一個很小的grammar	16-5
在講時間也有一個啊等等等等	16-5
因為很多很多小的grammar	16-5
那你現在user 這句話在講哪一個你不知道	16-5
就可以同時啟動	16-5
你很多小的grammar 同時啟動	16-5
然後呢depend on 它你發現它在哪裡	16-5
這個keyword spotting 抓到哪一個等等	16-5
所以keyword spotting 在這裡是很有用的	16-5
那這個觀念其實有另外一種說法也很像就是concept n gram	16-5
以剛才這個例子而言其實就是像to 啊for 啊這些個介係詞就是一個代表direction 的一個concept	16-5
然後這些city name 呢就是一堆代表地點的另外一個concept	16-5
於是呢在這個這個concept 之後會加另外一個concept	16-5
這是一個concept 的bi gram 喔	16-5
所以你可以用concept 的觀念來來解釋的話呢	16-5
你也可以把它們看成是cluster based n gram 嘛	16-5
這就是這堆word 變成一個word class	16-5
這堆city name 的word 變成一個word class	16-5
那這樣變成word class 的一個n gram 也可以喔	16-5
所以這個觀念是很像的depend on 你怎麼去想這件事情	16-5
那當然你要做understanding 的時候呢你也可以分成兩個stage 來做	16-5
那麼一面做先做完recognition	16-5
變成一堆字	16-5
或者keyword spot 變成一堆字之後	16-5
後面再做semantic parsing 或者robust parsing	16-5
但是也有的人就把它合在一起	16-5
就一步到位	16-5
就是把這二件事情合在一起	16-5
一次做完也可以	16-5
所以你看我這個圖裡面畫的時候	16-5
我就畫成一塊recognition and understanding	16-5
那有的人會把它變成一塊recognition 後面快一塊understanding	16-5
但有的人就兩個他就一步完成的	16-5
所以你可以是拆拆開成兩塊	16-5
也可以是一塊	16-5
那再來呢這個是講這個這個discourse analysis	16-6
就是講我們剛才大部分都還在講這一塊	16-6
那我們現在來講一下這個	16-6
這個要注意些什麼地方呢	16-6
你可以想像的是說	16-6
你可以把user 的user 在說tomorrow	16-6
所謂tomorrow 是什麼	16-6
是幾月幾號	16-6
因為你如果買飛機票那飛機票database 不會說tomorrow 怎樣	16-6
它會說幾月幾號有哪些航班幾月幾號有哪些航班	16-6
所以這類的就是屬於相對的expression	16-6
它說next week 所謂next week 是指幾幾號到幾號喔	16-6
你要把他們講的每樣東西你要把他convert 到正確的	16-6
譬如說he he 是指誰	16-6
那你都可能在前後文之中你在前後對話中間你會知道這些東西	16-6
你要把他們對應過去	16-6
自動inference	16-6
也就是說呢你要知道哪些information 還沒有講	16-6
然後沒有講的是指什麼	16-6
舉例來講譬如說這個如果user 問你how many flights in the morning	16-6
欸什麼叫how many flights in the morning	16-6
那顯然他前面已經說過了我要從台北去紐約	16-6
如果是這樣的話顯然是指從台北到紐約的how many flights in the morning	16-6
那它講到底是指哪一天	16-6
前面一定大一定講過哪一天	16-6
所以呢也就是說你在現在在講某句話的時候你要跟前面很多話堆兜起來嘛	16-6
那也是我們之前講的這個地方	16-6
你要把之前的所有知道的東西跟現在的一路兜起來	16-6
得到一個新的	16-6
所以你把這個這個user 講的話一路兜起來	16-6
那也就是我們這邊所畫的d discourse	16-6
你就是不斷的把之前講過的話累積起來	16-6
大概知道在講什麼	16-6
那然後呢你就可以判斷說ok	16-6
他現在講的意思是什麼	16-6
所以當user 說how many flights in the in the morning	16-6
顯然是指我們剛才已經說過的從哪裡到哪裡哪一天都講在裡面	16-6
然後呢你要做inconsistency 跟ambiguity detection	16-6
你可能中間有不對的	16-6
譬如說他說這個ㄜ我要到台中	16-6
待會兒他說欸台東有什麼地方有什麼旅館	16-6
那台中跟台東不對的呢	16-6
你就表示說你中間有一個辨識錯誤	16-6
你要中間有inconsistency	16-6
那麼你要去重新去confirm 到底是台中還是台東	16-6
那然後呢你當然你就最常用的方法就是我們剛剛講的就是用所謂dialogue state	16-6
或者semantic slots	16-6
那所謂semantic slots 就是剛才講的這個case 嘛	16-6
就是你這樣你用一堆slot	16-6
它們你都一個個填進去	16-6
那每填好一件事情就是一個state	16-6
那麼因此呢你就等於是一個用一個這個呃states 的方法	16-6
來來隨時紀錄我現在隨時紀錄現在user 要的是什麼	16-6
一路填進來	16-6
那至於dialogue management 呢我們再講一下	16-6
dialogue management 是指這一塊啊	16-6
這一塊	16-6
那也就是這一塊	16-6
假設你知道state 是什什麼的話	16-6
那麼你下個action 是什麼	16-6
那這個時候呢有很多種情形	16-6
基本上來講它的任務就是要如何control dialogue flow	16-6
你讓這個dialogue 能夠順利的進行	16-6
然後想辦法跟user 互動	16-6
然後呢看看下一步應該做什麼事	16-6
譬如說你發現還有哪些哪些東西沒有填進來	16-6
你要去問user 還有哪些沒有填的要問它	16-6
哪個東西需要confirm 不太對了	16-6
你要去跟他confirm	16-6
然後呢你這個如果有inconsistency	16-6
你要跟它clarify 喔	16-6
哪些是還空的slot 你要把它填起來等等	16-6
這樣一步一步朝向整個task 完成	16-6
那這中間你可能還要做的就是想辦法optimize accuracy 跟efficiency	16-6
所謂optimize accuracy 譬如說你是不是應該要去confirm 一下	16-6
提高accuracy	16-6
但是一再confirm 的話會很煩	16-6
我也許為了efficiency 我不要confirm 那麼多次	16-6
為了user friendly	16-6
讓user 比較喜歡一點	16-6
你可能也不要那麼囉嗦喔等等	16-6
如何optimize 這個東西	16-6
這都是在management 要做的東西	16-6
那這有很多種做法	16-6
那譬如說以這個例子的話呢它有一個所謂的dialogue grammar	16-6
那你可以看成一件一件事情它說什麼東西做完該做什麼什麼做完該做什麼	16-6
然後呢你有一個像類似finite state machines 的方法來操作它	16-6
譬如說這是一個sub dialogue	16-6
先是一個opening	16-6
open 之後呢下步是個sub dialogue	16-6
去問你的目的地要到哪裡去	16-6
問完問完問完之後去確認一下	16-6
那塊填好沒有	16-6
沒有填好啊欸那就再從來	16-6
填好了之後呢再下去	16-6
那這時候我下個可能是要問說從哪裡出發	16-6
再看填好沒有	16-6
沒有填好再重來等等	16-6
那你可以從這邊open 就直接往這邊走也可以你可以寫一個grammar	16-6
讓它在不同的sub dialogue 裡面跳過來跳過去喔	16-6
那麼你也許這還沒有講完	16-6
欸但是他的那句話顯示我已經到這來了	16-6
那也也許讓他可以這樣	16-6
也許不讓他可以這樣喔等等	16-6
所以你可以在中間可以操作讓他怎麼進行	16-6
那另外一種做法所謂plan based	16-6
這個plan based 的意思是人工智慧裡面的一個專有名詞啊	16-6
那我們這裡不細說	16-6
基本上就是用plan 的方法來規劃怎麼樣進行這個dialogue	16-6
那基本上這裡面很難的就是如果是mix initiative	16-6
我們剛剛講如果user 他自己說一些話	16-6
你要能夠處理的話	16-6
他不完全answer 你的	16-6
他自己說一些話你要能夠處理這個時候就比較難	16-6
那這個時候呢一個系統通常我們怎麼measure 它的performance	16-6
dialogue 的performance measure一向是一個本身是一個研究課題	16-6
因為不知道怎麼measure 它	16-6
那麼最基本的做法你有兩類	16-6
internal 的跟overall 的	16-6
internal 你可以量它的word error rate	16-6
但通常error rate 不太準	16-6
我們要的可能是slot accuracy	16-6
所謂slot accuracy 就是說你的understanding 對不對	16-6
以剛才這個例子為例的話	16-6
這邊總共有五個slot	16-6
那麼對了幾個	16-6
中間倒底word error 如何沒有關係	16-6
你說他說我要到san francisco 去	16-6
他講了什麼話word word error rate 多少沒有關係	16-6
重要的是這個san francisco 有沒有對	16-6
所以呢如果這個對就是對嘛	16-6
這就是所謂的slot accuracy	16-6
所以呢你如果比較可靠的算法	16-6
是算這個slot accuracy	16-6
那但是還有更重要的就是overall 的performance	16-6
譬如說success rate	16-6
就是說你這個一百個人打電話進來	16-6
你倒底它的飛飛機票買成而且買對的有幾次	16-6
有多少人掛了電話就是很生氣的電話掛了就走了	16-6
那有多少次等等	16-6
這就是所謂的success rate	16-6
那這個是我的accuracy 的measure	16-6
但是呢你如果為了要success 很可能讓它一再的confirm	16-6
於是呢變成term 很多	16-6
這也不好	16-6
所以你要算average number of terms	16-6
所有的success 的task 裡面它用幾個term 完成的	16-6
那當然越少的term 越好	16-6
所以呢這就有efficient 這是屬於efficiency 等等	16-6
那這些都是比較常用的performance	16-6
底下這個圖是講一下他們通常用的architecture	16-7
這裡兩個例子這個是所謂的galaxy	16-7
galaxy 是m i t 所提出來的那一套	16-7
那這個也有個中文名字叫做銀河	16-7
因為他們的那個galaxy 有中文版就是銀河	16-7
那基本上這個galaxy	16-7
那這個是a t n t 的這個版本	16-7
galaxy 基本上你可以看到它就是用一堆server	16-7
那它把server 分成兩類	16-7
一類叫做human language technology server	16-7
這個是domain independent	16-7
這個做recognition	16-7
這個做understanding	16-7
喔這個是nature language understanding 這個是task with speech 合成的	16-7
這些東西呢是independence of task domain	16-7
那另外呢有一堆是domain server	16-7
是針對domain 的	16-7
譬如說買飛機票的這是一個domain	16-7
問餐館的這是一個domain	16-7
問氣象這是一個domain 等等等等	16-7
那麼於是呢你對每一個domain server	16-7
針對某一個task	16-7
提供所有東西給它讓它去做這個	16-7
舉例來講你如果買飛機票的話	16-7
你要把city name 你要把航空公司的名稱給它放在它的辭典裡面	16-7
於是你就用這個做辨識的時候呢	16-7
就就會辨識你要的這些東西	16-7
那同樣的呢	16-7
那麼如果這個時候系統要跟user 說什麼樣的航班什麼	16-7
那些東西我給了這個t t s	16-7
他就可以講航班等等	16-7
那這樣明顯的一個好處就是說你把它分成domain dependent 跟domain independent server	16-7
那如果是這樣的話呢	16-7
那麼我現在增加新的domain	16-7
可能這邊都可以用相同的東西	16-7
只是針對那個domain 我有新的辭典	16-7
我有新的language model 喔等等	16-7
那於是在那個domain 裡面我就用那套這樣子	16-7
那這個是它的基本的這個精神	16-7
那當然user 在這端	16-7
透過網路它不管是用電話p d a 什麼什麼都可以	16-7
等等那這樣做的話我可以有比較高的portability 去different task	16-7
所謂portability 就是我做好一個買飛機票的dialogue 之後	16-7
我能不能把它很快把它轉成問天氣的	16-7
我能不能很快把它很快轉成問一個city 裡面的tour 等等	16-7
那這個轉把一個task 做好之後轉到另一個task 去的	16-7
這是所謂的portability	16-7
那越難轉就是越不portable	16-7
ㄜ那麼你把它拆開來之後是比較容易portable	16-7
那這個也是今天的dialogue 仍然存在的問題就是	16-7
絕大多數的dialogue 系統都不是那麼portable	16-7
你它如果是在city 裡面問問路的話	16-7
你要把它變成問天氣是很難的	16-7
所以呢你要要port 到另一個去	16-7
結果你每次都要重做一個那是很累的	16-7
所以這仍然是今天一個很重要的問題如何解決	16-7
右邊這個圖是a t n t 的其實它你仔細看它的精神是很像的	16-7
他們只是每一個單位都喜歡有一個自己的就是了	16-7
像他這邊的這一堆像a s r 的server t t s server 就是這個嘛	16-7
一樣的嘛	16-7
就是每一個他有一個server	16-7
然後呢他的server 這邊有一個所謂server provider 的這個interface	16-7
然後這邊有user 他user 在上面就是這邊這邊了	16-7
user 在上面	16-7
於是就有user 的a p i	16-7
於是中間把它兜起來等等等等	16-7
所以這個跟這個其實觀念是很像的	16-7
好關於這個dialogue 我們說到這裡	16-7
那麼再下來我們底下要要進入的呢	17-1
是這個十七點零	17-1
就是這個distributed speech recognition	17-1
我現在要要在整個的無線環境裡面做這件事了	17-1
那麼這個時候呢	17-1
喔哦對我剛才漏了我大概有提到一下	17-1
就是這個這個dialogue 這裡面我講的內容大致上是根據這個第一個喔	17-1
就是這本書的第十十七章	17-1
這個是大概講得算是大家所公認的dialogue 的核心的東西大概是這些	17-1
我們基本上以這個這個為主	17-1
那那事實上我剛剛講過就是每一個單位做的每一個dialogue 系統他們都有他們自己那一套	17-1
那麼不太有一個共同的這個學理基礎或是什麼啊	17-1
那你去看每篇paper 都很有一套都講得很有道理	17-1
啊底下這些reference 大概是那樣的東西	17-1
那這個都可以參考	17-1
那其實你如果看近這邊是比較早早一點的	17-1
你近一點的還有更多	17-1
只是說每一種我們也不能講那個就是可以做為很好的基本架構	17-1
因為每一套每一家都不太一樣	17-1
好那麼關於這個這個這個distributed speech recognition 跟wireless	17-1
呢我們現在就是要把這個wireless 環境放進來	17-1
換句話說這是我們在二點零的時候所畫的圖	17-1
一個recognition 有這麼複雜	17-1
從front end processing	17-1
到這個acoustic model language model 然後辭典然後這個再做search	17-1
這麼多東西	17-1
你真的都能夠放在手機裡面嗎	17-1
當然不一定	17-1
但是呢不放在手機裡面也沒關係	17-1
通常我們可以把這個東西拆開來	17-1
拆成一小塊放在手機裡面	17-1
然後另外一大塊呢放在server 裡面	17-1
因為你絕大多多數要做的事情其實是要跟server 幹嘛幹麻	17-1
你是要跟sever communicate	17-1
所以那你何不把這些複雜的東西都放在server 裡面去	17-1
那user 只要做一些手機上只要做一些簡單的東西就好了	17-1
譬如說我在在做的front end processing	17-1
就是判斷一下哪裡是speech 哪裡是noise	17-1
然後把m f c c 求出來	17-1
那些m f c c 呢就變成feature vector	17-1
我就把它裝裝在pocket 裡面送到server 去	17-1
那後面所有事情都讓server 做	17-1
像這類的想法	17-1
那這就是我們講的所謂的這個把speech recognition 呢把它partition	17-1
拆成兩半	17-1
分分別放在client 跟server 裡面	17-1
就變成client server 的架構	17-1
那這樣好處當然就是說我的這個client 的負擔很輕	17-1
反正事情都在那裡做	17-1
但有個很大的壞處你可以想像就是我把整個的wireless network 包到我的系統裡面來了	17-1
這裡等於是說把整個的wireless 塞到這中間	17-1
所以呢我的系統裡包括整個的wireless	17-1
而wireless 有一堆wireless 問題	17-1
譬如說它有error	17-1
它的pocket 會丟掉	17-1
它的bandwidth 問題等等那堆問題就都跑進來了	17-1
那這個就是distributed speech recognition 所要考慮的問題	17-1
我必需考慮到這裡面的問題	17-1
但它的好處是我可以有非常多的user	17-1
共用相同的server	17-1
那為了得到這一點我們就要就有不同的做法來做這些事情	17-1
ok 我們進入最後這一段	17-1
那麼我們說這個distribute distribute speech ecognition 的目的是要把recognition 拆成一半	17-1
拆成二半然後把它放到wireless 網路裡面去	17-1
那這最大的問題就是我們把wireless 包到裡面來了	17-1
變成為變成為這個這個把wireless 包到我們系統裡面了	17-2
所以呢wireless 問題都跑進來	17-2
那基本上有什麼問題呢	17-2
就是wireless bandwidth 非常有限	17-2
所以它的它能夠送的bit rate 非常有限	17-2
然後它可能也是隨時變化的	17-2
它有很多error	17-2
然後呢它的這個甚至於還是birth error	17-2
所以一堆error 在一起	17-2
那都造成recognition 的問題	17-2
那為了克服這個問題怎麼做法呢	17-2
基本上開始想的問題有這三種	17-2
就是所謂client only client server 跟server only	17-2
所謂client only 是說呢我就把everything 都放在sever ㄜclient 這端	17-2
我把從feature feature extraction 到recognition	17-2
全部都放在我的手機上面或者放在我的p d a 裡面	17-2
那是有人這樣子做的	17-2
那他那個p d a 裡面就可以辨識所有東西啊等等等等	17-2
然後呢我辨識完之後才變成辨識結果變成這個零跟一送出去	17-2
那這樣的好處最明顯的好處就是我的speech recognition 會變成independent of wireless	17-2
因為wireless 的後面的是	17-2
我recognition 在這裡面完成了所以呢我可以完全自己掌握	17-2
不受wireless 的影響	17-2
我的recognition 完全不受wireless 影響這是它的好處	17-2
可是當然有個明顯的壞處	17-2
就是說我的limitation by computation requirements for handheld device	17-2
你這個手機或者p d a 它的memory 它的計算量都是有限的	17-2
因此你必需要把你的recognition 去配合那個	17-2
通常它要把language model 切的很小	17-2
它要把辭典收小	17-2
它要把什麼東西search space 變小才可以做等等喔	17-2
比較麻煩	17-2
這是它明顯的一個麻煩	17-2
所以呢我雖然全部都在client 上做	17-2
好處是很明顯的	17-2
但是呢它有它的麻煩	17-2
那比較多的人想的是這個就client server model	17-2
就是我們剛才說的我把它切成二半	17-2
我feature extraction 在這裡做	17-2
這個m f c c 抽出來之後	17-2
我就把這些m f c c 呢想辦法做compression	17-2
變成一堆零跟一	17-2
就塞到那個package 裡面去	17-2
是透過wireless	17-2
到了終端那邊就是server 呢	17-2
才來我的feature 把feature recovery 回來那裡之後我來那裡做recognition	17-2
application 都在那裡做	17-2
那這樣子的話呢我就一半在client	17-2
一小半就是feature extraction 在client	17-2
大部分的recognition 在server	17-2
明顯的好處是我把它的computation requirement 切成兩半	17-2
那這部份小適合放在client	17-2
這部份大適合放在server	17-2
而且呢我省了很多bandwidth	17-2
因為我把feature 抽出來之後m f c c 我可以做compression 剩下很好的bits 來傳送	17-2
所以我省了很多bandwidth 或者bit rate 的空間	17-2
但是它有一個最大的問題就是跟已有的手機的電話不相容	17-2
也就是說你今天的手機電話	17-2
沒有裡面沒有幫你充m f c c 阿	17-2
那那它們是抽另完一堆的feature	17-2
那你知道我們的這個我們打電話的時候我們也是一樣從聲音裡面抽一堆feature 然後拿去傳送	17-2
但是我們那種feature 是所謂這個perceptual efficient	17-2
我們打電話的時候所抽的feature 是為了聽的方便	17-2
所以呢你是抽那些feature 之後我可以recover 成為一些聲音	17-2
那些聲音其實已經跟原意差很多了	17-2
但是我聽起來還很像	17-2
所以呢所以呢這個這個喔我們平常的打電話所用的是這種feature	17-2
那m f c c 不是的	17-2
m f c c 是for recognition purpose	17-2
就是說做recognition 的時候很適合	17-2
可是呢它並不是並不是perceptual 的efficient	17-2
所以你如果是這個你今天的手機裡面打電話的時候	17-2
手機它也把聲音抽feature	17-2
它抽的是另外一堆feature	17-2
那堆feature 呢它們解回來的聲音跟原音不像但是聽起來像	17-2
那麼那那因此呢跟我們要的m f c c 是不一樣的	17-2
所以呢你要這樣做的話呢除非你可以想像我的手機得要有二個二個抽二種feature	17-2
難道說我打電話的時候用這種	17-2
然後我如果是上網去幹嘛用語音輸入又是另外一種嗎	17-2
啊那這是一個很大的問題	17-2
所以呢基本上就是說跟跟現有的手機的電話並不相容	17-2
你除非你現有的手機通話你得要有二套	17-2
上網用語音上網去辨識的時候是另外一套	17-2
打電話是這一套什麼什麼的很麻煩	17-2
那這裡面有一個最大的問題就是說呢	17-2
m f c c 是並不能夠recover 成為原來的語音的	17-2
m f c c 只是為了辨識好	17-2
所以呢m f c c 有沒有辦法恢復成為原來的聲音聽起來像的呢	17-2
基本上是不容易做	17-2
那麼到了最近幾年人家很努力所以有人做出比較像的	17-2
那你也不是完全用m f c c 來做	17-2
而是另外先求pitch	17-2
你如果pitch 也求出來也一起送過去的話呢	17-2
m f c c 再加上pitch 是有辦法recover 到跟原音比較像的	17-2
於是有人說那這樣子的話那我們就把這個手機現在在用的那套不要了	17-2
啊我們就都用m f c c	17-2
那麼於是呢這個都用m f c c 的話呢	17-2
那打電話的人也用m f c c 然後他到那端他用pitch 來recover 成原來的聲音	17-2
還是可以聽	17-2
但問題是人家手機公司不願意這樣做啊	17-2
人家手機每年每年年產量已經幾億台幾億支	17-2
那人家裡面的晶片人家不是做這個	17-2
人家不用m f c c 啊喔	17-2
所以就是基本上有這個問題就是不相容	17-2
那為了要相容呢就有人想另外一招就是server only	17-2
那我就用現成的手機	17-2
這就是現成的手機就好了	17-2
那就用他們這種perceptually efficient 的的feature 抽出來	17-2
就用他們的手機的網路傳	17-2
只是到了接收端呢我再把它解回來	17-2
這個時候你打打手機電話的人就這樣子去聽啊	17-2
聽起來就跟原音很像	17-2
可是我們如果拿那個解回來的聲音再來抽feature	17-2
再做recognition 的話呢	17-2
這個問題就是我的正確率變得很差	17-2
那原因就是我們剛才講過	17-2
我們打手機的電話的時候它抽的feature 是另外一堆feature	17-2
那堆feature 可以解回原來的聲音	17-2
你在接收端是可以解回原來的聲音	17-2
你用耳朵聽起來是很像的	17-2
可是只是耳朵聽起來像而已它其實跟原音差很多了	17-2
所以你拿那個去求m f c c 的話	17-2
做recognition 正確率差很多	17-2
那你不然怎麼辦呢那我就另外一個辦法我想辦法從這種feature 裡我能不能抽出m f c c 出來	17-2
這是有人在想的問題	17-2
就這邊講我要找出recognition efficient 的feature	17-2
從perceptually efficient feature 裡面找出來	17-2
也就是說你你現在傳的是這些feature	17-2
那你這些feature 裡面能不能對應到m f c c	17-2
能不能找出這個對應關係	17-2
如果可以的話	17-2
我就直接從這個找出m f c c 出來然後recognition	17-2
那麼問題是到目前為止這個並沒有非常好的答案就是了	17-2
那這樣不管怎樣它的好處就是跟手機電話完全相容嘛	17-2
所以任何人買它的手機就可以用嘛	17-2
那只是我接我的server 要做一些手腳嘛	17-2
那問題是這個怎麼能夠做的好	17-2
所以這是所謂server only 我都在server 做	17-2
那這個就是一般人用手機	17-2
一般人用的手機網路就好了	17-2
那所以呢這有這三種做法	17-2
那今天來講大部分的努力是大部分的研究是這種最多	17-2
這種也有一些啊	17-2
這二種大概都有就是了	17-2
那不過這種多一點	17-2
就這種而言這client server 的怎麼辦	17-3
你你的m f c c 怎麼做compression 呢	17-3
那最常用的就是底下這張圖就是所謂的split vector quantization	17-3
你譬如說我們有十三個m f c c	17-3
我這十三個m f c c 可能包括一個energy	17-3
那我就兩兩做一個v q 兩兩做一個v q	17-3
於是我就可以得到分別去有optimize bit allocation	17-3
譬如說這個比較重要我給它八個bit	17-3
做一個八個bit v q	17-3
這個呢我做七個bit 七個bit v q	17-3
這個做六個bit v q 這個做五個bit v q 等等	17-3
那你因為兩兩的v q 是只有兩個	17-3
是很容易做計算量很少	17-3
在手機就直接做這個v q 是很容易做的	17-3
而這樣一來這個八個bit 這個七個bit 什麼什麼的話我這個bit 數目可以變得很少	17-3
所以呢我在傳送的時候我把這些v q 的參數再放到packet 裡面去傳送	17-3
那我的error 也很少喔	17-3
那麼需要的bit rate 也很少	17-3
所以這個是今天多數做這個領域的人所用的方法	17-3
所謂的split 意思就是說我不是這十三個一起做一個v q	17-3
你也可以十三個一起做一個v q 啦不過如果十三個一起做一個v q 的話你計算量就很大了	17-3
那個v q 就會計算量很大	17-3
這樣你拆成很多小的話呢	17-3
就是所謂split v q	17-3
你就是這樣的話你每個計算量都很小	17-3
那這個是這個喔最常用的辦法	17-3
然後呢你要想辦去做error protection	17-3
因為這些東西錯的話麻煩就比較大了	17-3
你想辦法做error protection 就是你在你把這些bit 放到packet 裡面去的時候	17-3
你要加各種的error control 的方法	17-3
那麼你可以想像你不同的bit	17-3
重要性可能不同	17-3
有的可能比較重要有的可能不重要	17-3
所以呢你最好是讓它不同的參數	17-3
有不同的或者不同的bits 給它不同的error 這個correction 或detection 的方法	17-3
那讓它比較重要的bit 可以保護得很好阿	17-3
然後比較不重要的bit 我就比較隨便一點	17-3
那然後呢阿很重要一個問題你還是一樣要跟現有的無線網路的平台相容嘛	17-3
就是你不能自己做一個說我怎麼怎麼怎麼做	17-3
那人家現在的無線網路人家根本不做這件事的話	17-3
你塞不進去嘛	17-3
所以呢這都是跟現這個是engineering problem	17-3
就是說你必需跟很很所有的現有的平台要能夠相容塞得進去	17-3
那還有一點就是說呢你你不要把它想得這個全部都錯的	17-3
那我今天如果是這個錯的話	17-3
我也許只要想辦法再再我如果做error detection	17-3
知道是這個錯	17-3
別的都還是對的	17-3
那我只有處理這個錯就好	17-3
等等這就是你如果能夠知道error 發生在哪裡	17-3
那是最好的	17-3
那然後呢這個另外一招就是所謂error concealment	17-3
就是你怎麼樣子你知道它錯了你怎麼樣把它補回來	17-3
你把它補回來然後做一個很像的貼上去	17-3
看起來好像沒有錯一樣	17-3
這所謂error concealment	17-3
那一個最簡單的辦法這是講裡面最簡單的辦法就是說外插法啦	17-3
外插法就是說我今天有假設我現在有這一個一個的feature	17-3
feature vector	17-3
這樣子送過來	17-3
這是t	17-3
t 加一t 加二等等	17-3
這是一堆一堆feature	17-3
我今天如果發現這個錯掉了怎麼辦	17-3
我如果這個錯掉的話呢那我可以拿前面這個你可以想前面這個可能跟它最像	17-3
所以拿前面這個去	17-3
然後呢前面這幾個可能也都比較像	17-3
光用這一個也許不太可靠這裡面也許也有錯啊	17-3
所以我就拿前面這幾個喔	17-3
所以像這個case 就是說我拿t 減k t 等於一到l	17-3
我就拿前面的l 個	17-3
做一個平均	17-3
然後呢那畢竟還是最近這個最可能嘛	17-3
所以我就用跟最近的t 減一這個	17-3
來做一個interpolation	17-3
所以這個乘上beta 這個乘上一減beta	17-3
那這樣的話呢拿拿這個跟拿這堆一起做個interpolation 之後呢	17-3
來取代這個	17-3
那這是一個很簡單的做法	17-3
當然效果不見得很好	17-3
那當然你也可以做內插的	17-3
這是外插這是內插	17-3
內插就是說你也可以拿後面的嘛	17-3
你如果可以把後面這些東西也弄進來幾個	17-3
然後呢一起做	17-3
那這個就是所謂內插	17-3
那你也可以用別的方法等等	17-3
那最好就是說你不要當成這整個vector 都錯了	17-3
不要都當成這整個都錯了	17-3
而是這個如果你知道是這個錯	17-3
你知道是這個錯我別的就都算對	17-3
我別的都算對	17-3
我只是這個用這個跟這個對不對	17-3
喔我只是用這個來算這個	17-3
那別的我都知道是對的話是最好	17-3
但是當然這個就你要用更多的error control 的bits 放在裡面才知道說是這個錯別的是對	17-3
那那這個就是最基本的想法	17-3
那當然你真的詳細做有很多學問就是你怎麼樣去管這個error	17-3
你從頭從頭就開始怎麼樣處理到最後	17-3
然後怎麼樣讓這個error 的效果能最小等等	17-3
我想我們就不多說下去了	17-3
那麼之後我們現在要進入最後一段就是十八點零	18-1
哦對了要講一下這這這一部分的reference 其實很難講	18-1
因為這塊是最新的領域	18-1
那麼在大概從九九年以後才陸陸續續有不少paper	18-1
那麼倒底哪一種才是講得最對的	18-1
其實現在還不知道	18-1
哪一塊最後會變就像我們剛才講的這個倒底哪一種會變成final solution 我們也不知道	18-1
所以呢哪一些paper 才適合作reference 其實也很難講	18-1
所以我這邊是隨便列列列幾個example	18-1
那你自己可以去找那每篇說法都不一樣	18-1
那麼都值得參考	18-1
但是我們並不能講哪一個真的是最好的reference	18-1
好那我們進入十八啊最後一段	18-1
那阿我其實有兩個reference	18-1
怎麼這個是舊的版	18-1
這是舊版還是有一個新版	18-1
這個是舊版	18-1
阿你如果上網去看的話這個新版應該是有兩個reference	18-1
我有加一個新的reference	18-1
啊應該是是一個很合適做為報告參考的一個reference	18-1
就是i triple e signal processing magazine	18-1
的九月二千零五	18-1
喔i triple e signal processing magazine 的九月二千零五它這個special issue	18-1
on speech	18-1
那裡面大約有十篇左右的paper	18-1
那大概代表這個現階段這個時候大概是兩千零五年就是去年嘛的一些最重要的最重要的方向裡面的大部分	18-1
最重要的方向也許有二十個它裡面有十個了差不多這樣子	18-1
所以是相當好的一個reference	18-1
那裡面的很多篇都蠻好看的	18-1
蠻值得看的喔	18-1
所以那個是你如果上上網的話應該有第二個reference 這個版本是舊的	18-1
那這邊我想我們都已經說過了就是語音未來的最大的機會應該是在網路環境之下	18-1
讓user 可以用語音上網	18-1
那麼啊這ㄎ我們以前也講過	18-1
我們現在可以看一下其實大部分的東西我們都已經說到了	18-1
包括speaker 我們在十一點零講的阿	18-1
這個robustness 我們在十四十四還是十五的時候講的	18-1
keyword spotting 今天講的啊	18-1
然後呢language model 這個什麼這個我們在十二點零啊什麼講的	18-1
那麼understanding 我們今天今天有講	18-1
text to speech 是我們沒有講的	18-1
然後dialogue 今天有講喔	18-1
然後這個是上週有講	18-1
這個是今天有講等等喔	18-1
所以我們到目前為止大概把大部分圖都cover 進來了	18-1
這也是一張舊的舊版的圖	18-1
啊如果是新版應該還要多一點	18-1
那這邊要講的是說我們這個領域一個最大的特點	18-1
應該是說我們這個語音這個領域是非常interdisciplinary	18-1
橫況非常多的領域跟非常多的知識	18-1
那麼你如果把語音處理想成是只是訊號處理在處理語音的訊號的話	18-1
那就不對了	18-1
那麼我們你可以看到我們是在處理非常多的知識	18-1
那除了訊號處理之外呢	18-1
這個是認知科學的就是人的聽覺perception 的分析	18-1
那這是語音學	18-1
這是語言學	18-1
自然語言的分析	18-1
人工智慧	18-1
pattern recognition	18-1
統計學	18-1
information theory	18-1
detection detection theory 我們今天講的	18-1
那麼數位通訊	18-1
information retrieval	18-1
無線通訊網路等等喔	18-1
我們cover 非常多的不同的東西	18-1
那也就是說它是一個非常跨領域的	18-1
我們把各個領域的專長的知識我們都學進來	18-1
放到這個領域裡面來	18-1
那麼也因此這個領域的entry barrier 是比較高的	18-1
也就是你要進來是稍微難度大一點	18-1
但是也因此你進來以後裡面的空間是相當開闊的	18-1
那麼那這也是為什麼它到今天的發展還沒有那麼成熟	18-1
因為它是有它的難度	18-1
那麼它得要把這麼這麼多東西整合起來是不太容易	18-1
那麼也因此是現在唯一的missing link	18-1
在整個的chain 裡面這我們在第一堂課就說過	18-1
整個的chain 其實就是上面這張圖	18-1
你從sever 透過網路一直wireless 到最後	18-1
大部分東西都已經成熟了缺的是什麼缺的就是語音	18-1
那麼因此呢這是一個這個整個的chain 裡面唯一的missing link	18-1
我們這塊也是很我們這裡清楚的是把很多的數學模型跟programming 整合在一起的	18-1
那麼因此呢這個啊你可以發現所有的數學都變得非常的豐富多采多姿	18-1
而所有的programming 也不再枯躁也是千變萬化	18-1
喔那就是因為programming 跟數學模型有非常好的結合在我們這裡	18-1
再來我們要講的是說呢這個語音是我們所有的人每天最常用的最自然的communication 的media	18-1
那麼而且它有無限多的variety	18-1
我們知道每一個人每天都說好多好多話	18-1
所以這個語音的的的使用是無窮無盡	18-1
他的量無限大	18-1
然後就我們做語音的人而言天下沒有兩句話是一樣的喔	18-1
那麼既使是同一句話讓同一個人說兩次	18-1
你知道它的wave form 一定不一樣	18-1
它的m f c c 也是完全不一樣的	18-1
所以呢天下沒有兩句相同的話	18-1
所以它的變化度之大	18-1
它裡面的學問之豐富是可以想像的	18-1
那我們說全世界至少有四千種以上不同的語言	18-1
那大量使用的至少一百種以上	18-1
所以可以可以發揮的空間是非常多的	18-1
那我們相信這個未來語音的時代是早晚要來	18-1
而且越來越近	18-1
尤其是後pc 時代	18-1
那麼wireless 足夠發展之後	18-1
我們這個新的平台裡面語音可能是無可避免的	18-1
那麼因此呢語音的機會跟未來的challenge 是這個無可限量	18-1
這個limit by your imagination	18-1
也就是說這是看你怎麼想你覺得想到哪裡就想到哪裡恩喔好	18-1
那呃應該是這是應該是最後一頁吧	18-1
ya	18-1
ok 好那最後我保留幾分鐘我要講一下就是在這門課其實我在開學第一週應該講過	18-1
我這門課的目的除了告訴你怎麼做語音之外	18-1
我其實還有另外一個重要的目的	18-1
就是希望透過這門課讓各位在一個學期裡面	18-1
體會到做學問的基本的一些方法跟principle	18-1
那最最簡單的來講就是說因為這是一個全對多數的人而言這是一個全新的領域	18-1
所以你從開頭是完全不知道的進入一個全新的領域	18-1
但是那你你最後進入的是研究的課題	18-1
所以你從從完全不知道	18-1
到到到這個學最基本的knowledge	18-1
到後面進入各個研究的領域到最後看人家所有的paper 都可以做的到	18-1
那這是一個很好的經驗讓你可以體會這個從最basic 從最basic 開始	18-1
到到research 其實這條路並不長	18-1
那麼任何一個領域都是一樣的	18-1
那麼這個領域其實比比別人走還要長一點	18-1
那麼此外呢	18-1
那麼這是一個很很好的領域讓你練習我們所謂的unstructured knowledge	18-1
也就是說在這門課裡面我們講的所有東西並不見得有一本很好的教科書	18-1
讓你看完第一章以後可以讀第二章	18-1
讀完第二章可以讀第三章	18-1
那所有東西是非常零散的散在不同地方的	18-1
但是呢你其實這個必須要在這邊讀一點那邊讀一點	18-1
然後這邊看到一堆看不懂的東西	18-1
那邊又看到一點看不看不懂的東西	18-1
那這個這個情形其實就是你離開校園之後每一個人所進入的世界就是一個這樣的世界	18-1
因為沒有人會把你所需要學好所需要的知識寫成一本教科書讓你第一章讀完讀第二章沒有這種事的	18-1
那你碰到的就是一個unstructured 的knowledge 的世界	18-1
而這個那我們這門課是一個很好的經驗讓你體會	18-1
其實當這個knowledge 沒有那個structure 好的時候	18-1
沒有什麼難的我們一樣都可以做的到	18-1
那麼然後呢我想在我們這裡面呢	18-1
也很讓你體會到譬如說我希望讓你體會到的一個就是我們做學問要見樹也見林	18-1
那你如果每週都有在上這門課你可能會體會這一點	18-1
我每一週講的東西永遠讓你感覺到整個領域整個世界	18-1
但是同時我們也在每一棵樹上下功夫	18-1
所以重要的樹一定看的很清楚	18-1
仔細看它的每一棵葉子	18-1
可是我們隨時都會退出來看整個的林象	18-1
所以整個的領域的環境是很清楚的	18-1
那我覺得這是做學問很重要的一點	18-1
那麼做學問的人很容易犯一個毛病就是掉到一個洞裡面去	18-1
那我們隨時都要從洞裡面跳出來	18-1
再看整個的環境	18-1
那此外呢另外另外一個很重要的是	18-1
那麼這個我在開學第一週也講過的就是	18-1
我們一定要能迎頭做學問一定要能迎頭趕上	18-1
什麼叫迎頭趕上	18-1
就是說這個這個學問的發展永遠很快	18-1
我如果從頭開始念	18-1
念一點這個念一點這個之後呢	18-1
人家已經又發展了這麼多	18-1
你念一點這個念一點這個之後人家又已經發展了這麼多	18-1
這樣你永遠追不上	18-1
所以怎麼辦呢	18-1
所以就是要迎頭趕上	18-1
什麼叫迎頭趕上呢	18-1
就是說我要知道哪裡是基本的重要的	18-1
我就看那些	18-1
我就讀那些	18-1
當這些基本的讀完之後我就要立刻跳到最前面來	18-1
看這些有什麼	18-1
然後那這裡面我如果發現有問題再回去找reference 就好了	18-1
但是基本上我要不斷的跳到最前面去看	18-1
那在這門課裡面	18-1
我們從九點零以後其實都在給各位練習這件事情	18-1
就是我們前面到八點零為止是basic	18-1
等於在八點零為止我們是在做這些事情	18-1
但是從九點零以後我們就直接跳到最前面來	18-1
每一個topic 我們都在看最前面有一些什麼東西	18-1
然後人家怎麼做的	18-1
那其實你會發現你如果都有認真看這些東西的話你後面再看	18-1
所有的研究課題你都可以看	18-1
你確實可以做到迎迎頭趕上	18-1
那麼當然這裡面也包括就是說這個另外一個常常說的就是selective reading	18-1
網路上的東西千千萬萬	18-1
圖書館裡面的東西千千萬萬	18-1
那麼我永遠不知道哪些是我該念的	18-1
那麼應該說每一個人需要念的東西是不一樣的	18-1
每一個人要走的路都不同	18-1
每一個人喜歡的東西都不一樣	18-1
所以每個人得做你自己的selection	18-1
那麼怎麼做這個selection	18-1
那麼最好的辦法就是見樹然後去見林	18-1
當你當你看到林之後	18-1
你可以選擇你喜歡的樹	18-1
那麼因此呢那其實在我這門課裡給你這樣的一個機會所以最後	18-1
要交報告其實就是做這件事	18-1
那麼這個等於說你在瞭解整個環境之後你可能可以看到一個你喜歡的東西	18-1
你去選擇你要做的報告的題目	18-1
然後選擇你要喜歡的reading 的topic	18-1
然後可以做你的報告等等	18-1
那這些這個事實上是一個很好的練習	18-1
讓你體會到怎樣做selective reading	18-1
那麼每個人離開校園之後你所碰到的一個世界	18-1
就是必須要能夠好好做一個selective reading 的世界	18-1
那麼你就必須要自己選擇你要做的事情	18-1
那麼這個期末要交報告這件事情	18-1
那麼我的說法就是學問是做出來的	18-1
不是念出來的	18-1
也就是說你如果光是念念念的話	18-1
很可能會覺得那些東西是很空的	18-1
過了幾個月又忘光了	18-1
那得去做	18-1
那所謂的做就是吸收消化reorganize 它	18-1
restructure 它變成你自己的東西	18-1
那就是寫一個報告	18-1
包括也許是寫一個程式或者是怎樣	18-1
這都是一種reorganize restructure 它	18-1
那這個時候就變成你的學問	18-1
所以要做才能夠真的掌握學問	18-1
那麼交報告是一個辦法來做一次喔	18-1
所以我想這是這一門課另外一些目的	18-1
那麼利用這個機會告訴各位	18-1
那我想那麼我們時間應該也到這裡	18-1
那麼這個喔我們因為期中考以後不再考試	18-1
所以呢這個期中考以後還會坐在這聽應該算是很不容易的	18-1
那麼你如果聽到最後一堂課我想是非常難得	18-1
這個跟各位算是非常有緣份阿	18-1
那希望有機會將來還可以再續前緣啊	18-1
好我們這門課到這裡結束謝謝各位	18-1
