ok 好各位早	1-0
這門課是數位語音處理阿	1-0
那麼呃我們這個學期這門課有點不一樣就是我們同時錄音錄影	1-0
給各位看到我們有在錄影也在錄音阿	1-0
那這是兩個目的	1-0
第一個目的是這門課我們這個學期準備錄下來就做成網路課程所以將來是可以在網路上上課的	1-0
那麼我們這個學期是把他錄下來	1-0
呃下個學期以後就可能可以在網路上	1-0
那麼另外一個目的是我們同時錄所有的聲音我們就要拿來做語音的實驗呃	1-0
那麼因此呢這個我們這次錄的所有東西都是我們未來實驗的素材	1-0
好那麼一開始呢我們先解釋一下這門課在做什麼	1-0
我們這門課所講的事情就是語音處理	1-0
那麼你如果看這張圖	1-1
你如果呃修過訊號處理相關的課你就會發現這門課這個東西你是很熟悉的	1-1
那麼那麼事實上我們所謂的語音處理就是指我們人的語言說話的時候的聲音	1-1
然後我們想辦法來做處理	1-1
那麼你知道我們說話的人說話的語言的聲音是在空氣中的疏密波	1-1
那這個疏密波經過我們的麥克風	1-1
我們就把它轉成電的訊號也就是呃電的訊號也就是我們非常熟悉的一堆這樣子的 waveform	1-1
那這個東西就是我們所謂的 x  of  t	1-1
那麼在很多時候最有效的處理他的方法	1-1
我們希望用它來做各種各樣的事情	1-1
那麼最有效的方法是把它變成數位式的用 computer 或者是晶片或者是程式或者相關的東西來做處理	1-1
但是光是這樣子不是那麼容易處理最有效的方法就是取它的 sample	1-1
也就把它變成一個一個的 sample	1-1
於是它就變成一個一個的 real  number	1-1
它就變成一系列的 real  number 那就是我們說的 x  of  n	1-1
那麼這個過程就是我們這邊所說的 sampling	1-1
也就是我把 x  of  t  每隔一個很短的時間取一個 sample 就變成 x  of  n	1-1
這時候變成一系列的 real  number 之後就像這樣	1-1
於是呢我就可以寫一個程式來操作它	1-1
那麼這個程式可以在 computer 裡面可以在晶片裡面可以在任何一個能夠處理的元件上面就做	1-1
於是得到我要的事情	1-1
那麼這個是什麼呢這是 low  pass  filter	1-1
你如果修過相關的 sampling theory 的課的話你可能就知道這個的目的是避免發生 aliasing 的現象	1-1
所謂的 anti  aliasing  filter	1-1
那如果你沒有修過那樣的課不那麼了解它的原理的話其實也沒有關係	1-1
那麼事實上呃我們後面不會再碰到這個	1-1
阿我們直接把它看成它就取成這個 x  of  n 就對了	1-1
那事實上我們今天任何一台 p  c 裡面都有一個這樣子一張這樣的卡片	1-1
在做這麼一件事	1-1
那裡面都含包含了這個東西都包含了這個東西	1-1
所以我們事實上任何一台 p  c 都可以做這樣的語音處理的工作	1-1
那麼這樣子	1-1
那這是我們最簡單的解釋我們講的語音處理是這麼一回事	1-1
那麼跟你的其修的其他的訊號處理的課處理其他的訊號有何不同	1-1
基本上是一樣的	1-1
唯一不同的是我們只處理與人的語言的聲音	1-1
人的語言的聲音有何不同我寫在這樣	1-1
因為它是人說的話	1-1
所以它帶著人的語言的知識跟訊息	1-1
那是什麼呢	1-1
簡單的說就是它有我們人說的話	1-1
那麼舉例來說呢就這段話而言假設我們說的是	1-1
今天天氣很好	1-1
那你知道其實這是一個詞這是一個詞	1-1
那它是來形容它今天的天氣的	1-1
那麼很好好是一個形容詞很是一個複詞它是形容它的	1-1
那事實上呢這個呢是一個主詞這是一個補語	1-1
那麼它是在描述這件事情的	1-1
或者說呢它是描述這件事情的阿等等	1-1
那麼因此呢這個訊這個訊號的的上層其實是這些東西	1-1
譬如說它是在講這個它是在講這個它是在講這個它是在講這個	1-1
那這裡還有這兩個字呢它可能是在這邊是在這邊這邊是在這邊阿等等	1-1
那麼因此呢你如果把語音訊號處理想成就是一般的訊號處理光是在處理這堆東西的話那其實是不對的	1-1
因為我們這東西的背後是這些東西	1-1
那這是它的上層的訊息這它下層的信號	1-1
那麼這就是我們所說的阿我們其實是帶著人的訊息的	1-1
這些訊息簡單的講就是字詞片語句子觀念	1-1
那這就我們在講的東西	1-1
那麼阿我們這邊這門課裡面所有的這個 powerpoint 上的字我們儘可能都用英文這樣你可以跟所有的文獻都對應	1-1
不過我們常常會用中文為例來說因為我們的語言是中文	1-1
那麼在英文而言這個代表的是字母	1-1
譬如說這個 o 是一個 character	1-1
這個呢代表一個 word 像這就是一個 word 這是在英文的時候	1-1
在中文的時候一般的習慣是把我們的一個字叫做 character ok	1-1
所以我們如果這邊講的是 character 如果是對中文而言是指一個字	1-1
那麼那個 word 是什麼呢 word 是一個詞	1-1
這個呢我們叫做一個 word 是詞 ok	1-1
可是這些我們講到是中文還是英講到這個 word 跟 character 後面會一再看到的時候我們是指這個	1-1
這個英文而言 character 就是一個字母word 就是你所熟悉的這就是一個 word	1-1
中文的話我們講的 character 是指這裡的一個字 word 是指一個詞	1-1
那這些字阿詞阿片語阿句子這像這些的名詞片語這是一個 phrase  ok 然後這就是一個 sentence 就是一個句子阿	1-1
那他所描述的是某一些個 concept 這些東西就是我們所說的 linguist  knowledge 或是 human  information	1-1
當你這樣子看的時候你就知道我們講的語音訊號其實是雙層的	1-1
那這兩層之間是有密切的關係的	1-1
那這兩個關係呢我們可以用底下這張圖來講	1-1
就是說好我這邊可以先說就是我可以是兩層嘛你可以看成底下的這一層我現在畫在上面就是這個 acoustic  signal  level	1-1
那也就是這個這個聲學訊號的這個層次你我我們所 observe 到的我們所收到的是這個東西	1-1
但它所帶的是裡面隱藏在背後的或者它在裡面的上層的訊息呢是這個 linguistic  level 或者說是 symbolic  level	1-1
因為在這個的話我們是用 symbol 來代表而不是 signal	1-1
那我們其實這兩層在一起的所以我們真正處理的時候呢是這兩層一起處理是 double  level 的雙層的 information 然後它們是互相的 interact 的	1-1
那我底上那張圖在說這件事	1-2
譬如說我這邊畫的是太簡單了真正的訊號是像這樣的今天的天氣非常好	1-2
我如果拿裡面的一小段這一點點來看的話呢它就是這樣一個個的 sample 這是我們剛才說這一堆的 real  number	1-2
那麼經過 sampling 之後就變成這樣	1-2
那麼你如果以為說我們處理訊號就是處理這個的話那就不對了因為還有另外一層	1-2
另外一層呢我們稱之為語言的知識 linguistic  knowledge	1-2
那麼它最具體的語言知識呈現的方式那就是一個是詞典告訴我那些音構成那些詞一個是文法告訴我它們詞跟詞之間可以怎麼銜接它們的關係	1-2
所以詞典會告訴我說今天是一個詞的是一個詞天氣是一個詞這些都是詞這是詞典告訴我那些個聲音湊成那些詞	1-2
文法告訴我說這些那些詞是可以互相兜起來它們的關係	1-2
因此最後我得到一個這樣的關係這個是這個是我的這個語言的知識	1-2
那我真正處理的時候是這兩個合在一起的我兩個一起來處理得到我的結果	1-2
那麼當然我的是用一堆程式一堆演算來做的那它可以是在 computer 裡面可以在晶片上或在其他任何的元件上處理	1-2
好這個大概最簡單的解釋語音處理跟別的訊號處理的有何不同	1-2
那麼在底下呢我們再回過頭來說語音語音處理最主要的 application 是什麼我們撿最粗的分法兩大應用的領域	1-2
第一個呢就是可以做語音的 coding 也就是把它 digitize 把它數位化然後 compression 把它壓縮到最小	1-2
那麼這個最直接的 application 就是我們今天打的所有的電話	1-2
那麼任何人你打電話其實都是在做這件事你都是把你的這個呃聲音我們我們今天打的電話不管你是在這個用手機還是用這個家裡的電話是一樣的	1-2
我們以手機為例你說的聲音進入你的手機之後的第一步就是 sample 之後變成這個 real  number	1-2
它這個處理呢就想辦法把它 compress 把它數位化之後 compress 到 minimum  number  of  bits	1-2
然後我就傳送到對方去對方的手機呢把它解回來然後得到它聽到的聲音	1-2
這個聽到的聲音跟原來的聲音一定有一點不一樣所以我們加一個 head	1-2
那麼這個時候我們考慮重要的因素包括我的 bit  rate 是多少也就是 per  bits  per  second 每一秒鐘的聲音到底需要多少個 bits	1-2
然後呢 recover  quality 這個聲音絕對不會跟這個一樣但是我希望它愈像愈好耳朵聽起來愈像愈好	1-2
那麼多像呢這叫做 recover  quality	1-2
然後呢這中間的演算的過程不管是這邊這個 compress 還是這邊這個解回來我都希望它的計算量它所需要的 memory 這些東西呢都能夠符合我的可行性我要裝在那個小小的手機裡面能夠做的到等等我是我所要的考慮的這幾個因素	1-2
那那這樣的情形就是我們講的這個語音處理的一個最直接的應用	1-2
那麼各位今天每天都在用	1-2
那麼我們也看到各種各樣的產品外面都在賣	1-2
那麼也同樣繼續不斷的在進步很多人在發展相關的技術等等	1-2
這是今天非常熟悉的一個 application	1-2
那麼第二個 application 是我們認為未來最大潛力的一個 application	1-2
那個今天也許還沒有看到真正的應用	1-2
或者說很少很少	1-2
但是未來呢可能會很多很多	1-2
那這個呢就是我們稱之為這個人跟網路之間的 interaction 的 voice  interface	1-2
那麼這個呢我們底下來解釋一下	1-3
那麼我們了解呢我們未來的網路世界其實今天的網路世界已經是這樣	1-3
未來會更豐富的上面就是掛滿了所謂的 content	1-3
這個 content 可以是所有我們想得到的所需要的知識跟各種的資訊活動	1-3
譬如說 real  time 的 information	1-3
像氣象啦航班啦股市啦體育啦等等	1-3
知識的典藏數位圖書館啦虛擬博物館啦等等	1-3
我們日常工作的業務譬如說做生意啦這個等等	1-3
我們平常的工作環境譬如說發 email 啦譬如說這個各種 intelligent  agent 啦電傳會議啦遠距教學啦阿網路課程啦等等	1-3
那同樣呢還有一堆是這是這些我們都可以稱做是 public 的是大家一起 access 的	1-3
那這個呢這個不是這是 private	1-3
任何 private 的個人的或者是公司的或者某一個團體的他們的東西都可以放到網路去	1-3
那麼裡面也許是個人的紀錄這個記事本也許是公司的 database 或者是家裡的遊戲什麼娛樂什麼什麼都可以	1-3
那麼你要只給我自己用也可以阿等等這是 private	1-3
所以這些東西呢我們可以說它就是整合我們全世界所有的人的所有的知識所有的資訊的活動所有的 service 全球性的	1-3
那麼所有這些東西呢你今天上網所看到的 content 仍然是以文件文字為主大部分是文字的	1-3
但是我們可以相信未來會愈來愈多的都是多媒體的	1-3
那也就是說呢阿他們可能都會有影音訊號在裡面	1-3
那麼因為多媒體是一個最 attractive 形式	1-3
只要是多媒體常常都帶著聲音常常都帶著語音	1-3
那麼最簡單的例子譬如說電視新聞譬如說網路的課程那譬如說這個電視上的一個談話性的節目譬如說這個電影有旁白那等等	1-3
那麼某某人的演講的錄影帶阿等等那都是屬於我們這邊講的東西	1-3
那麼凡是這些東西的話你會發現只要旁邊帶著聲音的話那麼語音的訊息通常是告訴我們這個multimedia 裡面的最重要的核心的觀念它的主題它的 concept 這就是在這裡	1-3
那麼我們舉一個簡單的例子假設你看電視新聞	1-3
那麼剛好是呂秀蓮副總統在講話	1-3
我現在如果把畫面關掉只聽聲音的話	1-3
我大概可以知道完全的新聞是什麼只是沒有看到畫面而已	1-3
可是我我如果把聲音關掉只看畫面的話那你知道就比較有限了你除了知道她在講話之外可能不知道她到底在那則新聞是什麼	1-3
那也就是說在這樣的環境之下很可能聲音所帶的 information 其實是比畫面更清楚的描述這個東西的主題觀念的	1-3
因此呢如果在我們要上網找這些東西的時候當網路上很多很多這些東西我們要搜尋的時候那等等那麼它聲音常常是它的關鍵	1-3
那另外一方面當然我們了解這個無線通訊技術的進步讓我們的終端設備徹底多元化	1-4
那麼換句話說我們現在手上可以用的是不再只是一個 notebook 或者一台 p  c	1-4
我們可以是所有其他的東西手機或者其他的手持的設備 p  d  a 等等	1-4
或者車上的電子設備或者是未來其他的 handfree handfree 就是我最好不要再用手拿了	1-4
那麼我可以不要拿	1-4
那麼舉例來講呢也許是在這上面裝了幾隻麥克風或者你在開車的時候是在車子的前面裝幾個東西	1-4
那這樣的話我可能手都不要拿東西了阿這所謂的 handfree 的 interface	1-4
或者是家電坐在客廳裡面我也許也不要用這些東西阿等等	1-4
其他的可穿戴的未來的任何可可穿可戴然後可以隨手攜帶的各種東西	1-4
那你可以想像譬如說裝在眼鏡上裝在像手表一樣或者一個東西可以插在口袋裡什麼什麼阿這些都有可能	1-4
那這些是我們所謂未來的可能的 user  interface 或者 user  terminal	1-4
那它們有一個共同的特徵就是輕薄短小那麼無所不在你可能根本看不到阿	1-4
這個 ubiquitous 意思就是說無所不在你等於是說在到處都都充斥著這些東西那麼你其實可以阿不感覺它的存在而它真正在發揮功效	1-4
那麼也因此這樣你可能沒看到它有它	1-4
那麼這也就是我們所說的我想各位都知道的也就是我們離開慢慢離開 p  c 時代進入所謂的後 p  c 時代	1-4
在九零年代 p  c 時代的時候 p  c 是資訊世界的這個核心所有東西都是用 p  c 來工作	1-4
那麼到了今天的話呢 p  c 角色慢慢淡化	1-4
而這些各種各樣的各種各樣的終端設備變成新的主流	1-4
那麼阿正如你的手機一樣	1-4
那你知道這個手機事實上不會取代你家的電話也不會取代這個辦公桌上的電話	1-4
當你回到家裡的時候你還是用那隻電話來打當你到辦公室你還是用那隻電話來打所以它不會被取代	1-4
但是你用的最多的電話號碼是你身上的那一隻	1-4
同理那麼未來你的 p  c 放在你的辦公桌上的或者書桌上的那台不會被取代	1-4
但你手上的而你手上的這些東西很可能會慢慢取代這個它它的角色會愈來愈重等等	1-4
那當你到了這個階段的時候我們就會就會發現	1-4
那麼我們今天用 p  c 來上網最方便的是鍵盤跟滑鼠	1-4
但是呢到了那樣的環境之下呢鍵盤跟滑鼠將不再方便	1-4
為什麼因為它愈縮愈小愈來愈方便	1-4
你隨身攜帶什麼的話呢我們人的手指頭不會縮小	1-4
所以當你小到一個程度的時候人的手指頭不會再有有利	1-4
同樣呢你也不見得還方便有一個有一個鍵盤可以讓我操作	1-4
那你說其實我今天手機我也有鍵盤我也這樣在操作阿	1-4
那是因為你的手機今天的功能很少在那些小功能情形之下我這樣就可以了	1-4
但是你如果是要它做到像你今天用 p  c 來上網一樣的複雜的或者更豐富的功能的話呢他就不再方便了	1-4
但是我們 really 需要這些東西讓我們可以隨時隨地 anytime  anywhere 跟網路相連	1-4
那怎麼辦呢我們需要有更方便的介面或者 interface	1-5
那麼語音是 one  of  the  few 它是最方便的可以隨時隨地	1-5
那麼任何一種介面都不管你是手機還是 pda 還是回到電腦或者是在汽車裡或者是在用的其他東西語音永遠可以用	1-5
那當然語音可能不是唯一的	1-5
那麼呃可能還有少數其他的我們到時候才知道	1-5
舉例來講今天的 p  d  a 上面那隻筆是相當好的	1-5
那那個應該會跟語音同樣都是未來非常重要的介面等等	1-5
好有了這個之後呢我們可以想像的是一個這樣子的環境	1-5
這張圖其實這上面的網路就是剛才的那個網路就是這個網路	1-5
那剛才底下畫這麼多不同的 content 那麼現在呢就是底下這一個這就剛才底下的各種 content	1-5
那我現在 user 怎麼辦 user 用各種各樣的終端設備就我剛才講的從手機到 p  d  a 到其他的各種車上家電的什麼什麼各種各樣當然也包括 p  c	1-5
你要用這個來access 這些網路的東西	1-5
那你怎麼做 well 那麼一個方式是借助語音的方法	1-5
用語音來下指令用語音來做瀏覽用語音來做搜尋你要的資訊	1-5
如果你找到東西是都是文字的而我沒有一個夠大的畫面讓我看的話呢那我可以讓它用語音合成講給我聽	1-5
那我跟網路互動怎麼辦呢用對話阿等等這些都是我們所謂的 voice  access  application  task	1-5
那這些 application  task 怎麼做到它呢就要用我們的語音技術阿	1-5
那麼也就是說呢我們當我們由 p  c 時代進入後 p  c 時代的時候	1-5
我們原來在 p  c 時代的時候那個時候做語音的人也在想一堆 p  c 的應用	1-5
那是什麼呢	1-5
p  c 用的最豐富的就是文件的處理最多數的人用 p  c 是在處理他的文件	1-5
那麼計算什麼那是比較少數人在做的	1-5
那文件處理的話語音最大的用處是什麼	1-5
那就是譬如說可以語音輸入嘛我不要用打的嘛等等所以在九零年代很多人做語音做語音的都是在希希望說我可以不要打然後我就可以用語音輸入	1-5
但是到了兩千年以後那麼大家想的不是這個了而是如何用語音來做上網的所有的工作阿	1-5
那就是我們這邊所講的這個網路 based 的 information 跟 service  access 來的 application 阿等等	1-5
好那麼有了這個了解之後那麼我們可以想像阿未來有一天我們的這個上網的這個環境可能是一個這樣子的	1-6
那就是在一個我們剛才講的在 wireless 跟 multimedia 的的世代裡面	1-6
因為是 wireless 所以我們這邊都是一些小的各種各樣的終備設備透過 wireless 來達成	1-6
所以呢那我的這些東西事實上我都語音是一個非常重要的介面	1-6
那麼雖然可能還有別的阿那麼我都可以用語音來做我做我的這個 access 介面	1-6
而這邊呢我網路上所掛的東西呢我們剛才提到呢可能都是多媒體的	1-6
它旁邊附的聲音它不不見得附了文字	1-6
因此呢譬如說某一個課程某一小時的課程或者是某一段新聞或者什麼	1-6
它並沒有一段文字講它所有的東西	1-6
那它的聲音都在那裡這時候呢我們就要靠聲音來找這些東西	1-6
這邊呢用聲音來輸入阿這些底下講的就是用這個 voice 這個 speech  instruction	1-6
這邊呢是做 speech  instruction	1-6
而這邊呢你的 multimedia 東西呢是它的 key  concept 它的觀念它所描述主題呢是用也是用它聲音訊息來描述	1-6
這個就變成用聲音來找聲音以別於我們今天其實是用文字來找文字	1-6
譬如你如果是用 google 來搜尋的話你輸入你要找的東西這個是在這邊	1-6
所以你這邊是輸入什麼文字	1-6
那 google 是幹嘛呢就因為網路上那邊也是文字所以它就在做文字跟文字的比對然後找到相關的網頁給你阿這是 google 在做的事	1-6
那麼未來很可能呢你這邊變成輸入的是聲音那邊的也是以聲音為為這個 reference 於是用聲音去找聲音	1-6
那就是我這邊所說的今天這個網路的 access 是以這個 text  based 為主是用文字來做的	1-6
但是呢未來所有的文字的角色其實都可以用聲音來取代	1-6
那這個時候呢我們今天上網其實還有一個很重要的就是人跟網路在互動	1-6
當你 google 這邊輸入某一個你的 instruction 之後它找到這個做了文字比對之後找到相關的網頁給你	1-6
那你這邊就可以開始選說這個我要那個我不要你做一些選擇的動作	1-6
那它又會給你一些你要東西它再顯示出來	1-6
那顯示出來之後它可能會一些選項看你要選什麼你就用點選	1-6
你點選之後呢它又會顯現出來它可能會告訴你要個什麼東西你就去填等等	1-6
所以這個是你跟網路的互動	1-6
那麼互動怎麼辦呢這互動都可以用這個 spoken  dialogue 也就是口語的對話來完成	1-6
因此你可以跟它對話嘛	1-6
你可以說怎樣怎樣怎樣	1-6
那麼這個阿不只是你輸輸入的 keyword 可以用聲音來輸入	1-6
你如果要點選可以用聲音來點選要填可以用聲音來填	1-6
填空格	1-6
那如果這邊的找到的東西是文字的話我 always 可以用語音合成合成聲音來給我聽等等	1-6
要我做什麼事它都可以用用聲音告訴我要做要我做什麼事	1-6
所以都可以用聲音來做到	1-6
那麼因此呢我們就說這個是所謂 spoken  dialogue 也就是口語的對話	1-6
所以你今天上網其實這是一個互這個互動的動作這個互動呢我們可以用口語對話來達成	1-6
那這個時候呢我們原來像 google 在那邊做聲音跟做文字跟文字比對呢我現在就變成聲音跟聲音的比對那就是我們所謂的這個 spoken  document  retrieval	1-6
所謂 spoken  document 意思就是這些個多媒體的東西都帶著聲音的	1-6
我我我們就拿它的聲音來做搜尋跟比對	1-6
那那些聲音我們稱之為 spoken  document	1-6
那麼底下呢我們可以稍微再解釋一下這邊所講的東西	1-7
這裡面最重要這兩塊嘛一個聲音的搜尋一個是口語的對話	1-7
那麼所謂的聲音的搜尋這剛才其實已經講了就是今天的 google 就是文字跟文字在比對	1-7
你輸入一些文字它想辦法去找到這些文字跟這裡面的文字的關係然後相關的它幫你找出來	1-7
那未來呢你的 instruction 可能變成聲音	1-7
而這上面的東西也很可能變成一大堆 multimedia 東西裡面帶著聲音帶著聲音	1-7
因此呢你可能要從用這些聲音去找這些聲音	1-7
那這個是一種那有的時候呢還有一些網頁上呢它沒有聲音它就是文字那你要也要有本領從這個聲音呢找到文字	1-7
反過來呢我如果回到家我可能還是用我的 p  c 我還是輸入文字我如果輸入文字的話呢我也要能夠找到這些帶著聲音的東西	1-7
因此呢我除了原來文字對文字之外我現在多了三種聲音找聲音聲音找文字文字找聲音	1-7
那這些凡是有聲音的好就是我們這邊所要講所說的事情阿	1-7
那麼我不這我另外一個說法叫做 voice  based  information  retrieval	1-7
information  retrieval 就是在搜尋這些東西那我現在是用聲音在做搜尋	1-7
另外一個呢就是口語對話	1-8
這是一個非常 typical 的口語對話的一個一個這個圖	1-8
那麼所謂的口語對話呢	1-8
這個呃我們舉一個簡單的例子譬如說呢假設某一個人今天出門他去開會	1-8
開完會回來的路上他坐在捷運裡面他就拿他的手機來跟他的辦公室裡面的 p  c 來對話	1-8
那麼假設他的辦公室 p  c 裡面有一個軟體	1-8
這個軟體呢我們稱之為 dialogue  made  server	1-8
他就是在跟它的 user 在做對話的在他的辦公室的軟體裡面	1-8
這個目的是什麼呢是讀 email	1-8
那麼因此呢這是他的 email 的信箱那麼他從 internet 收 email 進來	1-8
於是呢他就可以打一個電話給他的 p  c 給他的辦公室的 p  c 說我現在要讀我的 email	1-8
那麼於是呢這個這個 dialogue  manager 呢就了解說他要讀他的 email 之後呢它的第一個問題是你告訴我你的 password	1-8
那麼因此呢這個 user 就要透過電話用聲音告訴它說我的我的 password 是什麼	1-8
那這個 dialogue  manager 要要判斷說到底你是不是該開這個信箱的人	1-8
那這時候 password 應該包括兩個部分第一個是你要知道正確的 password	1-8
那第二個是你要從你講的聲你講的 password 裡面的聲音的特徵來判斷你是不是那個人	1-8
那如果是的話那它就會說 ok 好那麼我現在把信箱打開了	1-8
於是呢這個 user 就說你告訴告訴我今天新進來的 email 有幾封	1-8
它就說有八十五封	1-8
那他就說欸有沒有某某人寄來的	1-8
它就再從那裡面查一查它說有那個人寄來了兩封	1-8
那你這個再問它說那麼你把這兩封 email 的寄的時間跟 subject 念給我聽	1-8
於是它就把這兩封的時間跟 subject 念念出來	1-8
於是你說喔我要聽第二封	1-8
你把第二封念給我聽它就把第二封念給你聽阿等等	1-8
那這個 process 呢就是一個 user 跟這個遠方的一個 p  c 做對話來讀 email 的過程	1-8
這樣東西其實今天早就有啦阿那麼只是它就得好不好的問題而已	1-8
那麼這樣的一個程序裡面那你就會發現其實跟你今天坐在你的 pc 前面開你的信箱讀 email 是完全一樣的	1-8
只是我們剛才所說的我的這個 text  based 的工作換成這個 voice  based 阿是用聲音來做而已	1-8
那麼這裡面那麼阿其實這也是 client  server 的一個一個例子	1-8
我們知道呢這個是 client 那麼這端是 server 我們是 client 跟 server 中間透過網路在進行	1-8
那這裡面有那些事情呢當然你 user 說的話它要能夠辨識而且還要了解阿	1-8
所謂的了解的意思是說他說我要看我的 email	1-8
你不是去辨認出來說看我的 email 這幾個字	1-8
辨認這幾個字沒有什麼用	1-8
那真正需要的是他知道了解你是要看信箱	1-8
所以它知道你要打開信箱	1-8
那打開信箱的先決條件就是要知道你的 password 等等	1-8
所以呢除了 recognition 還要 understanding	1-8
然後呢當它知道要做什麼事它要用語音來告訴你因為它它跟你唯一的來往是聲音所以呢它要要知道該說什麼話然後用語音合成講講給你聽	1-8
那麼除此之外呢要做這個對話的工作呢有一個最重要的核心就是所所謂 dialogue  manager	1-8
它在 handle 這個 user 跟它的對話	1-8
進來什麼聽到什麼該繼續說什麼到什麼時候該做什麼動作什麼時候該把信箱打開什麼時候該讀那一封信等等這個是它的工作所以它是在這個等於是主導整個的對話的流程	1-8
那這個 discourse 什麼呢這個是隨時記錄一路對話下來所記錄下來的知識	1-8
譬如說當你說到 ok 現在把第二封念給我聽	1-8
什麼第二封當然是指剛才打開的信箱裡面的某某人所寄的那個的第二封	1-8
那也就是一路講話的過程你一路把他講的話記下來了	1-8
那你就知道他講的是那個那個東西我們稱之為 discourse  context 阿這個是你知道這是環境的意思這是說話的時候的前後文的意思阿	1-8
好這個是我們講這個對話的這個基本的	1-8
那麼這樣的話我們大概描述了剛才這個圖裡面的對話的這一塊跟搜尋的這一塊	1-8
那麼有了這些之後呢我們 more  or  less 就可以辦到我們剛才的這個情形	1-8
那麼然後在這個情形之下呢那麼我們可以講另外一件事情就是這個我們其實大家都了解的我們今天有兩個重要的網路	1-9
它們正在快速的 merge 成為一個在快速的收斂整合之中	1-9
那第一個網路呢就是所謂的 P  S  T  N public  switch  telephone  network 也就是所有的電話網路	1-9
我們今天電話網路其實其實是包括兩個不過已經早已是一個了	1-9
一個就是你家裡的有線的電話	1-9
一個是你的手機的無線的電話	1-9
那它們早就連在一起所以呢這是一個所謂的 P  S  T  N	1-9
那麼你任何人只要有一隻手機辦好門號或者有一隻電話接上那條線你就可以跟任何全世界的電話都可以通這是 P  S  T  N	1-9
另外一個網路就是 internet 那麼你只要連上這個你的 p  c 就可以跟所有的 service 相連上所有的網站等等	1-9
那麼在早年這是兩個完全不同網路這裡面走的是 data 這裡面走的是聲音	1-9
但是今天你知道這兩個網路已經高度的整合在一起	1-9
所以你在這裡用手機什麼你也可以常常可以拿到看到聽到這裡面的東西	1-9
那在這裡你用這個電這個可能也常常可以打電話等等喔所以其實它們已經在高度的整合了	1-9
那為什麼會整合呢我們底下很簡單的解釋	1-9
就這邊而言它最吸引人就是它隨時隨地都可以跟網路相連嘛	1-9
我隨時隨地都可以打電話這個多好阿	1-9
因為它這麼好所以這邊的 user 早年不可以這樣子做所以他們就會說我們也要	1-9
於是呢這邊的 service  provider 需要想辦法 provide 這樣呢可以隨時隨地跟網路相連 service 給他們	1-9
那同樣呢這裡還有一個好處就是因為我只要講路嘛	1-9
我們畢竟語音這個語音那這個 voice 是我們人最方便最自然的 interaction 的 interface	1-9
你這個只要講話就好你這個還要還要什麼什麼一堆這個鍵盤那什麼有有夠麻煩	1-9
所以呢當然他們都會說我也要這樣子阿	1-9
那麼因此呢這些這邊的好處呢這邊都會希望要有嘛	1-9
因此呢這這邊的 voice 這個 service  provider 跟這個網路 service  service  provider 都希望把這樣的功能提供給他們	1-9
那反過來這邊最吸引人是什麼呢	1-9
這個 internet 最吸引人就它的網頁	1-9
它有豐富的全世界的所有東西的網網站在上面	1-9
所以呢它有最豐富的 content	1-9
那麼我們知道網網這個 internet 最吸引人的東西就上面掛掛滿了東西所以你要什麼他們都有	1-9
那麼既然是這是它最大好處呢這邊的 user 就會說我也要那些阿	1-9
他說他也要那些的話呢那他們的 service  provider 就就得想辦法提供那些東西讓他們也有	1-9
那麼於是呢這就自然造成這兩個必須整合	1-9
那我們看到它們確實快速整合	1-9
那還有一個很簡單理由就是說我們 internet 為什麼吸引人就是因為它上面有 content	1-9
content 就是人的訊息	1-9
而人的訊息最自然的傳遞的方式就是語音嘛阿	1-9
因此呢這它們自動連在一起的情形	1-9
那那麼它們自動 merge 在一起的時候呢你就可以想到其實這就我們這門課所講的的這個第二個重要的 application	1-9
就是其實就是我以 web 為基礎的各種 application 我都希望是有用聲音可以 enable 的它的 access 方法	1-9
那這就我們這邊所說的這件事	1-10
那麼如果是這樣講的話呢那你可以想像其實這裡講大部分都剛才都已經說過了	1-10
這邊主要講的是說我們真正要達到這件事情其實是一個非常複雜的過程	1-10
從網站這邊 server 上面的各種資訊的技術到 internet 的技術到各種網路的技術到最後無線通訊的技術到最後語音的技術	1-10
這整個是一個非常長的恩的 chain 阿是一個這個呃是一個 technology 的 chain 這一路傳過來	1-10
這樣一路走過來這這整個整合起才能夠做到	1-10
那我今天來看的話事實上是大部分技術都已經非常成熟或者接近成熟	1-10
不論是 server 端的資訊技術或者是 internet 技術或者網路技術無線東西大概都已經接近成熟	1-10
唯一的還沒有真正成熟的是語音的部分阿	1-10
好那麼因為這樣的關係呢我們這章講的大部分我們都已經說過了阿	1-10
那主要應該就是呃	1-10
因此呢阿就這邊了阿那麼所以最主要的應該是說呢這樣的關係所以我們會期待未來語音世界裡面的網路世界就跟今天一樣除了語音的角色會大幅的增加	1-10
你上網搜尋可能是用語音的你你在網路上瀏覽可能是用語音的今天的那些大的網站入口將來可能是語音的網站入口	1-10
那麼有的人說我將來的 web 就是一個語音的 web 等等	1-10
那就我們這邊所說的我們希望的是各種 web 上的 application 都是用語音來做 access	1-10
或者說我們需要有非常豐富的各種用語音來操作的 tool 跟 application 等等	1-10
那底下這句話所說的就是我剛才前一頁所說的就是你這整個需要一個非常長的 technology  chain	1-10
就是你要從這頭一路連過來到這頭	1-10
這麼長的 chain 裡面那麼其實唯一的還欠缺的 missing  link 就是語音的 interface	1-10
也就是說我們剛才講這些都已經成熟或者是半成熟都接近成熟唯一不成熟就是這邊	1-10
所以這是一個最重要的 missing  link	1-10
只要這部分完成那麼它是一個非常關鍵性的角色	1-10
好那麼有了這個的時的話呢我們大概可以了解說我們這邊這個呃所說的東西跟你在其他的課裡面所學到相關的東西的關係	1-11
那麼你可以想像我們未來是一個網路世界	1-11
這個網路裡面呢任何兩點都是一堆零跟一在跑來跑去不管通透過光纖透過無線或者什麼什麼	1-11
那那些呢就是我們所謂的通訊技術	1-11
那然後呢我的網路呢把它們都全部連起來就全部跑來跑去	1-11
那這個呢就是我們講的網路技術	1-11
那在這裡面的每一點裡面所相所連上去的 computer 或者 server 或者它上面有網站有各種各樣的 database 有各各種各樣處理的資訊那就是我們講的資訊處理的技術	1-11
那同樣呢你如果要讓這些無線的部分能夠跑的好包括通過衛星阿包括這個車上可以連起來啦等等	1-11
那這是靠什麼這是無線通訊的技術	1-11
然後呢我現在要把各種聲光影像都放進來讓這裡面呢有聲有色那是什麼呢那是多媒體的技術	1-11
那我們這邊講的就是這裡面的語音技術讓我們可以用語音來做這邊所有的事情	1-11
ok 那麼到這裡呢我們大概其實是在解釋剛才講的就是在這個網路環境之下語音做為我們這個這個上網的一個非常重要的一個 interface	1-11
而這個 interface 呢我們回過頭來看就是剛才這張圖講的	1-11
一面 user 這端可能用語音來輸入或者幹嘛	1-11
一面 content 那端它可能是靠語音來做它的 index	1-11
用語音來代表它的主題它的內容	1-11
那麼因此我的搜尋可能是用語音去找語音等等	1-11
那就是我們講的這個呃之前的	1-11
這裡我們說有兩個主要的 application	1-11
第一個是打電話第二個是未來的網路世界的語音的 interface	1-11
那這兩個呢你也可以說這個是今天的 application 這個是明日的 application	1-11
這個是今天已經在賣的大家會做的那麼繼續在進步但是這個有已經都看的到的	1-11
那這個呢是明日的我們現在還沒有真的看到	1-11
雖然裡面的很多 component 的技術其實都已經接近成熟已經可以用了	1-11
但是真正的整體的 application 我們似乎還沒有真的看到	1-11
所以這個可以說是明日的	1-11
那麼我們這門課所講的主要是底下的這個第二個這個明日的	1-11
那麼今日的這部分呢那麼事實上在很多的通訊課裡面都會講到	1-11
那麼因此呢我在這門課裡面這部分幾乎就只是說到今天為止說到這裡為止我們大概就不太說這部分了阿我們的主題是底下那部分	1-11
好那麼到這裡為止我們大致說明了這個呃這門課我們未來要講的最主要的內容	1-12
那麼再下來在在要進入這門課的 outline 之前我們應該要做個 demo	1-12
我不曉得助教在不在還是在外面	1-12
他們好像不在了	1-12
如果他們跑跑掉了的話	1-12
如果跑掉我們就先先往下走了阿	1-12
這是阿我們我這門課的到時候這個學期的內容應該是說	1-12
我們一面做這個這個這個這個 theoretical 的的	1-12
我們內容包括理論的部分也包括實際的部分喔我們這兩個部分都會 cover	1-12
然後呢我們會從 fundamental 開始最後	1-12
前半學期是講 fundamental 後半學期是進入這個研究的課題我們都會接觸到	1-12
因此呢我們大概前半學期是 fundamental	1-12
那麼這些其實就是我們剛才講的那些語音處理裡面的這個核心的部分	1-12
那我們每個都會夠深入的把它說清楚	1-12
這大概要花半個學期的時間	1-12
那麼後半個學期我們就進入各種相關的研究課題	1-12
那麼正如我們之前所說的這一些那麼你可以想像它會有很多可以做研究的的內容阿	1-12
有不同的題目不同的方向	1-12
那麼這些呢我們就在後面的這半裡面會說到阿	1-12
所以呢你如果看我的課程大綱上面的列的那些東西的話你大概就一半是前面一半是後面的	1-12
那麼我們這門課沒有教科書	1-13
那麼原因是不存在一本書 cover 到我們這邊想要講的這些東西能夠 cover 那麼好的	1-13
那麼我基本上呢就是用這四本參考書	1-13
這大概是到目前這個領域大概最要 cover 這些東西最合理的是這四個參考書	1-13
那麼呃我簡單解釋一下就是呃	1-13
ok 我們助教進來那就過來吧我們	1-13
那麼這個第一本應該是到目前為止最新的兩千零一年的	1-13
那麼呃這本書的作者這三位都是微軟的語音研究部門的人	1-13
因此呢可以說是這本書所 cover 的是這個呃微軟的這個語音技術的文化	1-13
那麼第二本呢這個作者是這個 I  B  M 的當年語音 group 的 leader 阿後來到 john  hopkin 當教授	1-13
所以他這個比較代表的是這個 I  B  M 的文化	1-13
第三本的作者呢都是過去在 A  T  and  T 發展語音的核心人物	1-13
所以這個代表 A  T  and  T 的文化	1-13
那麼第四本是比較沒有那麼大的一個代表性不過是它的這本書寫的滿好所以這四本各有它的好處	1-13
第一本的好處是最新不過也已經有五年了	1-13
那它 cover 的 cover 到所有重要的東西	1-13
但是呢它的這個呃太多了很多很多	1-13
然後每一樣東西當然沒有說那麼清楚所以這是它的好處跟壞處	1-13
那我們會用的最多因為它 cover 最豐富然後這個用的最多	1-13
這本呢是講的很好但但缺點它不是一本書它只是他的演講稿收集起來	1-13
所以呢有點不那麼好看	1-13
那這個呢是稍微早了一點已經十多年了	1-13
不過裡面有些東西是講的相當經典的我們會引用裡面的一些東西	1-13
那這本的話呢是呃唯一的可能是寫的最平易近人最好看的	1-13
但是它缺點是它只講了 basic 的部分再進一步的研究它都沒有了阿	1-13
因此呢這個每一本都有它的缺點都有它可可取之處所以我就是用這四本做為 reference	1-13
那麼阿你並我並沒有建議你去買任何一本	1-13
那我們都要求圖書館做為這個 reference	1-13
這個在那個 reserve 那塊裡面你只要去看可以找得到	1-13
那麼我們會講說這段是 reference 那一本阿	1-13
那麼我通常會用它的第一個作者來說是那一本譬如說如果我如果說是這個這一本的話我就會說是在這本書等等	1-13
所以當我 reference 那一段的時候你只要去圖書館把那段 co 來就可以了喔	1-13
所以呢你基本上是可以不用買任何一本書就用它它當教科當參考書就可以了	1-13
另外我們會有很多參考文獻會在課堂上提供	1-13
ok 好我們現在來來做這個 demo 好了
呃我們今天 demo 幾樣這個東西
那麼一個前面幾個是我們早年所做的一些東西阿大概都在九零年代的時候做過的
那麼現在看起來是像 toy 一樣只是一個玩具阿
那麼你可以感覺說但是你大概感覺我們在講是些什麼東西這門課會學的是些什麼東西
那麼阿最後會給你看的是真實的我們呃現在在做的東西
我們做的第一個 demo 是 dictation 
也就是我剛才講在九零年代的做語音的人就想的就是我要用 p  c 嘛所以我就是用語音輸入嘛那最好我就是不要不用鍵盤打了我是就用嘴巴講就好了
這語音輸入所以呢那時候我們稱之為聽寫就是 dictation 
那我們現在所 demo 這個是我們在一九九六年做的喔
剛好是十年以前了阿
那麼在那個年代是認為這個很重要
不過我們九六年以後就沒有再進一步去改進它
因為我們知道慢慢離開 p  c 時代它不重要了
那麼所以你今天看十年前的東西十年之內沒有改進它所以現在看是只是一個玩具而已阿
這個是在 category 它的它的這個麥克風的輸入的狀況
今天天氣非常好
早上我到學校去
遇見了我的老師
老師和我說
我的期中考
考的非常好
國立台灣大學
電機資訊學院
電機工程學系
有一門課
叫做數位語音處理
 ya  ok 這個是九六年的技術了不過你可以了了解這個看起來大概是這樣阿 ok 哈哈哈哈哈哈哈
ok 好這是第一個 demo 
那麼第二個 demo 是語音合成
這也這個是我們九八年做的阿
呃語音合成也就是說你給給你任何一段文字那你要把它的聲音轉成聲音念出來
那我們這個用的是這個 corpus  based 
也就是說我的呃找一位播音員他念了十多小時的文字的文字跟聲音之後
分析所有的文字跟聲音之間的對應關係
然後你今天跟我任何一段文字我就配合那個關係把它的聲音做出來你就聽到它的聲音
那這個的效果呃事實也是一樣就說非常 depends  on 你的那個你的那個那個 database 要夠好
像人家現在做比這個好很多是因為他有幾百小時的聲音就把各種聲音的變化都在裡面了所以呢聽起來會更好聽
那現在我們那個時候因為在學校做我們不太可能有那麼多的 data 
所以我們只有十十個小時左右所以聽起來就比較沒有那麼好聽
好我們來聽一下
 ok 好這是第二個 demo 是這個語音合成
底下給各位看第三個 demo 是對話系統
這個是一個非常簡單的對話系統我我們是在一九九九年做的阿
這個對話系統是這個呃一零四查號台
但是呢一零四查號台是不容易做因為你要查千千萬萬個 user 
所以呢我們只查這個呃金融機構
全台北市金融機構是的電話號碼總共兩千多個
就查那兩千多個
那麼
嗯我要找土地銀行
請問您是不是查土銀
是的
土銀本行請播二三六一三零二零
嗯我聽不清楚耶可以再說一次嗎
謝謝使用再見
老師要 demo 嗎
 ok 我來試試看阿
開始
再來一次吧
查號台您好請問您要查那個金融機構呢
嗯我想查台灣銀行
台灣銀行本行請播二三一四六六九九
阿我聽不太清楚耶你能不能重講一次
台灣銀行本行請播二三一四六六九九
阿有沒有其他的分行還是分公司還是那一個部門的電話呢
請問您是查本企南企還是淡企
好吧那我就查南企好了
請問您是不是查南企
對的
南企只有信託部請撥二五零四三一零八
你是說二五零三一四零八嗎
請播二五零四三一零八
好那我能不能再找一下另外一個中國信託
請播二五零四三一零八
好吧謝謝
 ok 好這是這個呃對話系統阿
好底下我們剛才講的這這都是屬於九零年代的比較早期的
那我們等於是就做一個 example 你大概了解我們在做這些東西
那底下看的一個 demo 是我們現在在做的或者是說過去幾年之內在做的東西
這個可以跳過去了這一頁前面這幾頁其實我們剛才都說了
 ok 好
這一頁好了對那
阿我們在做怎樣的事情這是一個現在在做的一個研究的例子就是說
阿假設你現在上網看假設網站上有所有的所有的新聞
問題是說
那麼多的東西阿這個
如果你今天上網會發現那麼方便是因為所有的文件都是文字
你一眼看到它段落分明標題清楚
所以一目了然你馬上知道那個是你要那個是你不要
你就可以選擇你就可以 delete 掉你不要的東西
可是如果是都是變成多媒體的話其實是很難看的
因為你它沒有標題它沒有分段嘛
然後一大堆每一堆你都它都是影音訊息
你要從頭聽到尾是很累很累的事情
所以呢我們就是需要針對它的語音裡面所說的內涵概念主題來加以自動的了解跟重組
然後呢我們給它做自動的切割
譬如說它是一小時的新聞我們把它自動切成一則一則的
根據它的內涵來切割
然後每一小段有它的中心主題就每一則新聞有它的主題
根據它的內容來主題來分類
然後呢抽取每一小段裡面的專有名詞未知詞
然後呢我們來這個判斷它裡面的人事時地物
然後呢來做自動的摘要以及標題的設定
於是呢你的 user 就可以根據它的標題來選擇他要的東西
然後可以自己自己這個聽
呃先聽摘要看要不要聽確定要聽再聽全文阿等等
那我們底下來 ok 往下去
這一樣啦再下一頁
那所以呢我們這邊講其實是這樣子喔就是說我的聲音裡面是有很多專有名詞
是是真正描述這個的核心就是一些 key  term 專有名詞可能都只有聲音
那我要抓得到
那根據這個這是在 term  level 進入這 concept  level 根據它 concept 來分分成小段
抽它的人事時地物進入 summary  level 做它的 summary 做它的 title 
有了標題之後呢我其實進入了 topic  level 分析它的主題阿等等
所以我們總共有這麼多東西分成這個
 term  level  concept  level  summary  level 跟 topic  level 來做這些事
好下下底下
那在等一下在分主題的時候你會看到我們是這樣做的
我們現在做的這個例子是新聞阿
那麼電視新聞那電視新聞的話呢我們就是把所有的新聞那麼同樣的一則類似的主題的變成一群類似主題變成一群
然後把它呈現在一個 two  d 的平面上
如果這群的內容很多我們可以展開到下一群
所以這樣我得到一個 two  d 的這個 two  d 的一個 tree  structure 或者是一個多層的 map 
那這樣的話呢就比較容易瀏覽
好底下
我們來舉一個例子先看一下一則新聞聽起來是怎樣的
霹靂遊俠男主角李麥克大衛霍索夫由於長年酗酒住進了勒戒所
大衛霍索夫的發言人茱蒂凱斯今天證實曾經主演霹靂遊俠海灘遊龍等熱門影集的大衛霍索夫已經住進了知名的貝帝福特中心希望改善自己長年酗酒的問題
上週八卦媒體國家尋問報報出了大衛霍索夫差點死亡的消息使得這名男星的健康問題成了媒體關注的焦點
與李麥克搭配的伙計霹靂車是當年霹靂遊俠影集的最大賣點
大衛霍索夫最近正積極為新版的霹靂遊俠電視影集催生希望再創演藝事業第二春
 ok 這是一則新聞四十秒
那麼可是如果你不喜歡這則新聞就會覺得這實在有夠長有夠 boring 
你如果給我聽的話我只要三句就夠了
好底下我們就給你一個三句的 summary 
八零年代紅極一時的電視影集霹靂遊俠男主角李麥克大衛霍索夫由於長年酗酒住進了勒戒所
大衛霍索夫最近正積極為新版的霹靂遊俠電視影集催生
這是三句阿你可以看到它是從這邊前面抽了兩句後面一句
那這三句所加起來它的 concept 
大致是跟原來的 concept 比較最接近的三句
那麼這樣的話呢就是可以節省時間至少你一聽就知道你要不要聽了
但是如果不喜歡這個人說唉呀你叫我聽三句也是 boring 
其實你如果給我一個標題的話我一眼看到我就知道那個我不要我把它 delete 就好了
好我們就給它一個標題阿
這是自動產生的標題
那麼你可以想像要做自動的這個這個是自動產生的 summary 的話我們除了要辨識正確還要掌握它裡面的 concept 才能夠兜出一個最接近的 concept 
那要產生這個標題呢是更不容易的事你要注意到這邊的這些都是專有名詞是詞典裡面所沒有的
這些音為什麼會兜成這個字這是不容易做的這些都專有名詞
然後我整個句子是相當通順的那這都很難做到
那阿但是當然有了標題之後好處是說我不喜歡我根本不要看我根本就 delete 掉呃
好這個是簡單的一個例子說明我們怎麼做好
底下我們來看
呃底下我們把剛才的這個標題跟呃摘要做在一個 database 上面這個 database 呢是有是在兩千零二年到兩千零三年錄的
那麼總共是一百二十小時七千則新聞
那麼都做同樣的事情
然後呢我們現在把它這個自動分類阿這個自動分類成為像國外政治阿國內財經阿等等等等分成這麼多類
那麼每一類有那些新聞呢我們可以看一下我們點一個譬如說國外政治
那每這裡面這麼多它都這是當時錄的時間
然後呢它們的這都是我們自動產生的標題這是阿自動產生標題
你如果用眼睛看一看大概可以發現它們大致都是國外的政治新聞
然後裡面大概都有一些專有名詞那大致去讀起來大致是都通順的阿
那我們稍微瀏覽一下就知道很多很多阿 ok 好我們回去
我們來看其中的一則譬如說我們每一則是它你如果按它的那個標題就可以聽全文按它的 summary 就可以聽它的 summary 我們按一個例子
譬如說這個這則半島電視台
九一一恐怖攻擊兩週年卡達半島電視台今天播出的蓋達組織首腦賓拉登同時半島電視台也播出了薩瓦里的錄音帶
 ya 這個是這則的 summary 那如果你要聽全文就可以再按就是了我們就不聽了
那這樣的話你可以看到雖然是很多但是那那個 list 很長很長阿要去你要去找一則你要聽的也很難找阿
那麼因此怎麼辦呢那麼我們就做另外一個 map 
那這就是國際新聞的 map 
那麼我們每每一個群就是用它的 keyword 這讓裡面的 keyword 來做為 label 
你大概了解它是幹嘛的
譬如說就這群而言你可以看的出來就像檢查人員聯合國安理會喔毀滅性的武器什麼等等
那你可以知道這就是這就聯合國安理會的那個武器檢查的 topic 
那像這個是什麼呢你一看出來就是自殺炸彈阿巴勒斯坦阿這個約旦河以色列阿這個就是中東的等等等等
那我們如果這這是很大的一群我們覺得太多我們可以點到下一層去
那就是剛才的剛才的那一塊我現在分成這麼多變成九個
那你可以看到譬如譬如說這個的話呢這就是巴格達嘛有一個伊拉克阿自殺炸彈什麼巴士受傷什麼什麼
那這邊是什麼呢是中東和平嘛美國的鮑威爾約旦埃及的和平路線阿什麼什麼什麼
那這邊呢就是阿拉法特阿什麼什麼什麼等等
好那你現在如果要知道這裡面是些什麼新聞呢我可以再按進去
那麼這個時候其實是這一塊這個時候已經是我們那個 tree 的底最底層的 leaf  leaf  node 了它裡面包含這些新聞
那你看這些新聞的標題大概都是屬於這個以色列阿拉法特他們之間的這些事情呃
那我們也要在這裡聽也可以我們可以舉個例子我假設聽這裡面這一則我們聽一個 summary 
以巴緊張情勢升高
巴勒斯坦激進分子在以色列境內發動了一連串的自殺炸彈攻擊事件之後
以色列間對巴勒斯坦領袖阿拉法特的總部發動了危險攻勢
這是兩千零二年九月二十號什麼的新聞阿
 ok 好
那麼這個是這個我們稱為是這個 top  down  browsing 也就是說我現在是從這個 top 一直往下走然後一路看下去看它整個的這個來來瀏覽整個的新聞
那其實我不一定要 top  down 我我也可以 bottom  up 
如果說是 user 心裡想說我要聽什麼新聞的話
好我們這底下來做這個 bottom  up 的這個搜尋
那麼我們現在可以輸入一個 user 想要說的話好
請幫我找以色列與阿拉法特
 ok 這邊是找出來的一堆新聞同樣的呢我們也都有它自動產生的標題在這裡
所以呢你可以看到它是什麼新聞大概都有以色列阿有阿拉法特啦或者是跟中東這個以阿糾紛有關的都在這裡
那假設說 user 看到阿我要的是這一則阿拉法特反對以色列什麼什麼包圍條件什麼東西
我要的這則其它的其實是我不想要的
那這樣的話我就他就不需要這個從頭去去一則去他他就直接從這個 link 到剛才那個主題新聞
那就回到了剛才這個 map 
那我們剛才這就這就其實就我們剛才聽到的那一則新聞
那很清楚的它你仔細看看這個內容就是完全都是跟它非常接近的相關的主題那也就是剛才裡面的這一塊
那麼所以呢如果真的要聽這個我就可以從這裡一路走去聽相關的
如果不夠的話我就上一層那那上一層的話呢剛才是在這裡
剛才是一塊那現在你可以聽這個旁邊跟這個相關的所以這邊有約旦河啦這邊中東和平等等可以聽這些
如果還不夠的話可以再往上走一層
再往上走一層那就是我們剛才看到全部的國際新聞阿等等
那這樣的話呢我現在這個我們稱之為 bottom  up 的 browsing 就是你從這個 tree 的底部往上去走所看到的東西阿 
ok 好阿我們的 demo 就到這裡為止阿
阿喔對喔還有一個我忘掉了 ya 我們還有一個要 demo 的
這個是另外一個就是說呢你你了解就是說我們這個阿你今天如果從 google 去找一個什麼東西它馬上給你一大堆網頁的時候它是雖然有很多但是因為是文字的所以你很容易一眼就知道說喔那些是我要那我就選我要的就好
可你如果是這種東西的話你我沒辦法選吶我得一個個去聽阿
即使是一個去聽聽那個聽那個那個摘要也很累阿
所以有沒有更好的方法呢那我們也有一個那就是你可以我們自動把當場選出來的新聞就做一個 tree  structure 
這樣你就可以看得更清楚阿好我們來做一個
請幫我找美國白宮
好這個是輸入這個要找美國白宮
那美國白宮相關的那這就是所有裡面有美國白宮的白宮做了很多奇奇怪怪的事情阿所以有很多很多阿
那麼你在這裡也許沒看到白宮是因為它的主題可能是別的嘛譬如說布希呼籲聯合國什麼什麼它裡面就有白宮啦阿等等等等
所以這個非常雜嘛很難看嘛那我們同時就為它做一個 tree 
在這個 tree 裡面我們就看到大概譬如說第一層有幾個 node 一個是美國一個是華府一個是白宮一個是法國
啊那麼這裡面呢我們也可以要再美國底下還有布希華航台灣伊拉克什麼
那我們來看一個譬如說伊拉克好了
伊拉克會是什麼呢
喔在美國白宮裡面的伊拉克底下喔它又會有這個跟跟這個中東阿跟德國阿什麼聯合國有關等等等等
那那麼這個時候呢我們就可以看到說 ok 你如果要找的是是這個伊拉克的話我們其實這個可以這個找的到你要的是什麼
那如果真的要這個的話呢我們也可以輸入這個伊拉克做為第二個 keyword 於是我就可以再進一步的找就可以增加再再查尋一次
那現在查的呢就變成是美國白宮再加伊拉克
那我現在我們再來看一看這邊講的都是跟伊拉克有關阿的白宮等等的阿
那這邊呢這是新的一個 tree 就變成這個有有美國跟白宮然後裡面這些東西阿
 ok 好我想這個是一個呃我們最最新在做的一個研究大概是這樣好我想我們的 demo 就到這裡為止
我們回到剛才課程的部分好了	1-13
ok 好好謝謝那麼我們最後還有一點點把它講完阿那就是呃我們這門課的教材都在網站上	1-13
我這邊是因為第一第一學期開的時候說是每一週上課前阿那現在是我們全套教材都在這裡你上網就查到了	1-13
那這個網址是我們實驗室的網址那麼你在上去你可以看那個 download 阿它裡面有一欄叫做 download	1-13
你從那裡面進去就會看到這門課的的從那裡面進去你可以看到所有的整個學期的教材	1-13
我們這門課適合的年級是只要是電機系資訊系三年級以上大概你的 background 就已經具備了就可以聽了阿	1-13
那麼當然如果是你是研究所的話那任何一個相關的所我當時還沒有列完譬如那時候還沒有網媒所現在有網媒所等等阿	1-13
那這相關的所應該都可以那麼呃這個程度沒有那麼深所以基本上是碩一同學是非常合適的	1-13
那麼這是合適年級我們這跟目的大概已經說過了就是我們當然最基本的目的希望我們同學能夠進入一個這麼一個新的領域的所需要的基本知識	1-13
那同時呢我們也可以說一下就是這門課事實上我們這個領域所講的事情其實是用一大堆的數學模型來做這些事情那這些數學模型的 solution 呢就是程式	1-13
那當然這個程式是寫在那裡呢寫在硬體上面不管它是一台 computer 還是某一種晶片或者是 component 等等	1-13
所以呢這我們這門課講的東西其實是這三個部分我們用一堆數學模型來解這個問題然後這個數學模型是用程式來獲得答案然後呢它用硬體再做到	1-13
不過我們這門課真正在接觸的是上面這兩個我們並不會去接觸硬體雖然你都可以把它們做在硬體裡面等等	1-13
那這門課的內容你可以發現是一個非常好的 example 說明這兩者之間的關係那麼你如果過去覺得寫程式很枯燥的話你會發現我們學過所有的數學都可以拿來用	1-13
那麼有了這些數學之後我的程式是千變萬化無所不能	1-13
反過來呢你如果過去覺得那些工數阿什麼學了太多數學有點頭大那麼你會發現其實那些數學不一定要真的你自己去解因為我們都用程式	1-13
常常借助很多的 iteration 就得到答案我不需要去解它阿	1-13
那麼因此呢有了程式數學可以簡單而有趣阿所以呢這個是我們這門課另外在做的一件事就是希望你能了了解這個數學模型跟軟體程式怎麼樣相輔相成	1-13
那這門課有另外一個目的就是這個讓你學習進入一個新的領域由基礎進入研究的歷程	1-13
那麼事實上我們任何人都可能會不斷的接觸一個新的領域因為我們這個整個的環境每天在變化	1-13
那新的領域怎麼怎麼進來呢那那這個我們可以想像在你離開校園之後你所碰到的都是一個這樣的環境	1-13
你很可能隨時接觸一個新的領域而你碰到知識都是 unstructured	1-13
所謂 unstructured 的意思是說這個阿我們所接觸的東西並沒有人幫你整理好變成一份教材	1-13
如果你修大一大二的必修課的時候它們就是 structure 的因為它有一本教科書	1-13
你念了第一章就可以念第二章第二章懂了就可以念第三章	1-13
那麼都 structure 好了但是你未來所碰到知識都不是這樣	1-13
那麼因為沒有人有功夫花那麼大功夫幫你把它寫成教科書變成那麼好念	1-13
到那個時候呢那個已經不是最重要的了那那你要知道是最新最新一定沒有 structure	1-13
那沒有 structure 就會怎樣呢 ok 它有一篇文章在說這個可你讀起來有一大堆不懂的這堆都不懂的話呢其實要去讀另外一本書裡面的那那些地方	1-13
可你讀那個又有一堆不懂的那這堆東西呢你又要去讀另外一本書裡面的另外一篇文章裡面的什麼地方	1-13
那裡又有一堆不懂的這堆不懂的話呢需要去讀這個才會懂阿等等	1-13
那就是我們所謂的 unstructured 的 knowledge	1-13
那事實上你如果離開學校進入你的工作環境什麼你所碰到都是這樣	1-13
你我們每天碰到新的知識都是這種 unstructured 那我覺得我們同學非常需要學習的是這樣的一個 unstructured 的知識的的環境	1-13
然後你如何進入一個那樣的環境之後由基礎然後進入研究	1-13
那我們剛才提到我們這一個學期裡面那麼就會讓你從這個從這個基礎走到走到這個研究阿	1-13
那麼雖然是那那我們譬如說我們說我們的這個教科書是非常嗯我們的教教科書是這個我們每每每一本書用裡面東一點西一點那也是一個 unstructured 阿	1-13
那麼這是一個很好很好的一個教學的機會讓我們同學體會你怎麼樣進入一個新領域然後怎麼去接觸這些東西之後慢慢形成你自己的知識基礎然後可以做研究阿	1-13
那這是我們這門課的另外一個目的那我們陸陸續續會讓你看到譬如說你如何能夠我們做學問很要避免的是說你鑽到一個洞裡面去鑽了半天但是沒有看到全全局	1-13
那麼你怎麼知道那個那個那個洞合適適合花那麼大功夫去鑽呢阿等等	1-13
那我們就又要強調怎麼樣子從從全面的知識到一個這個專門的知識阿等等	1-13
那這些都在這門課裡面我們會看到	1-13
同樣呢我們這門課也會讓你體會怎麼叫做這是我通常說的這個迎頭趕上阿	1-13
因為我們讀書永遠是沒有知識發展那麼快所以呢如果說是有那麼多知識我今天很努力的去讀了裡面的三本書	1-13
可是你讀了這三本書的同時人家出來了十本書那麼你很很努力的讀了兩篇 paper 人家又出來了三三十三百篇 paper 你永遠追不上阿	1-13
所以要怎麼要怎麼做呢那我所謂的這個這個迎頭趕上就是先把一些基礎的東西了解完了之後就直接跳到最前面去看人家最新的東西	1-13
發現有需要再 reference 回來	1-13
然後不斷的再跟著最前面走才行那這所所謂的迎頭趕上	1-13
那我在這邊其實就是在做這麼一件事	1-13
那麼你可以發現我們在開頭的時候是在做基礎的工作之後到這邊就很快就會跳到最前面去阿	1-13
那麼讓你在一個學期就可以感覺到其實我們可以要進入任何新領域都是做到而且可以在很短的時間裡面做得到	1-13
阿我希望我們的同學上這門課也可以有這樣子的收穫	1-13
我們的成積有一次期中考期中考的範圍大概就是 basic 的部分大概佔三十五	1-13
我們有兩個 homework 加起來百分之十五	1-13
之後期末就是要做個大報告大概佔百分之五十	1-13
好那麼期期末報告是可以是寫程式的做一個 project 也可以是讀 paper 的完全是呃呃讀理論 paper 也可以阿也可以兩者都有	1-13
然後這個報期末報告可以是每一個人自己寫一份也可以兩個人合寫一份等等啊這應該都是可以的 ok 好	1-13
我們這個第一堂課的目的應該是讓你知道我們這門課幹嘛抱歉我們己經超過了時間	1-13
我們這個有沒有我想第一門課目的讓你確定下一堂課還要不要在這裡阿有沒有你要你要決定一下下堂課還還要不要在這裡的的的有些什麼問題我們漏掉沒有說的有沒有	1-13
如果沒有的話我們在這裡休息十分鐘 ok	1-13
我們把這個現階段的核心技術是些什麼大概有一個簡單的簡介	1-14
那麼第一個就是就是語音辨識 speech  recognition	1-15
那麼最簡單的想法把語音辨識看成就是一個標準的 pattern  recognition 的 problem	1-15
那麼任何一個聲音進來譬如說我們要辨識零到九的十個音的話任何一個位置的聲音進來我們第一個從裡面抽一些特徵	1-15
那麼抽一些特徵之後呢把這個特徵做成我們所謂的這個特徵向量 feature  vector	1-15
然後這個 feature  vector 就得到一系列的 feature  vector	1-15
那這是什麼意思呢我們簡單的講起來大概是這麼一回事兒就是假設我的一個位置的聲音進來他是這種東西	1-15
那麼我們取裡面的一段這裡面有很多很多的 sample 就變成一堆的 real  number	1-15
我用經過一堆演算法把它算成一系列的 feature 特徵參數那構成一個 vector	1-15
那待會兒呢我這個 window 移過來的時候這又是一堆 data 我再做一次演算我就得到下一個 feature  vector	1-15
那這一堆就描述這一堆訊號待會兒呢我再搬過來我再得到下一個等等	1-15
於是呢我就得到一系列的這個 feature  vector  sequence 一個 vector 的 sequence	1-15
那個用這個 vector 來代表這個訊號	1-15
那麼有了這個之後呢我就想辦法去跟 reference  pattern 去做比對	1-15
假設我是辨識零到九的十個聲音的話	1-15
那麼零有零的 pattern 一有一的 pattern 二有二的 pattern 等等	1-15
那麼這些 pattern 怎麼來的是用訓練的聲音也就是說我們顯然需要先告訴機器什麼是零嘛	1-15
你總得說個零啊零啊零啊很多次的零他把那些個零想辦法建成一個 pattern	1-15
那也是一樣這些是已經知道這些聲音是零的我把已經知道是零的聲音都這樣建成一系列的 vector 之後	1-15
把它建成一個零的 pattern 一有一的 pattern 二有二的 pattern 等等	1-15
於是你聽到未知的聲音進來我也得到一系列的 vector 就跟這個 pattern 來比看看它像誰我就做個決定它是誰就這樣子	1-15
那這裡可能要解釋一下的為什麼要算這個特徵參數求這個 feature	1-15
那麼簡單的解釋就是其實我們這邊畫的太簡單這裡面的 wave  form 多的不得了哦你如果這個呃你如果坐在最前面的同學你如果看我們現在錄的音你就知道我們說話這個裡面有多麼複雜哦	1-15
那那麼複雜的聲音其實你不太容易去比的那就我們而言事實上天下沒有兩段聲音是一樣的	1-15
那麼同樣的人說同樣一句話說兩次他的 wave  form 鐵定是不一樣的	1-15
所以你如果光看這個 wave  form 看不出來他在做什麼你要辨識他是非常難的因此呢我們通常想辦法簡化把這麼多複雜的 wave  form 我把它 reduce 到一個少數的代表他的特徵的參數	1-15
那這樣這個才比較容易做比對	1-15
那從前有有人做這樣的比喻這是非常好的比喻他說你如果在路上看到一個人我要判斷他是男的還是女的我其實就是看兩樣事情第一個他的頭髮第二他的衣服	1-15
那根據這兩樣我就判斷他是男是女當然偶爾會錯誤但是正確率是很高的	1-15
那這個時候因此呢你如果用一張照片把他整個的拍下來有多少多少個點你根據那個點你反而看不出來他是男的還是女的	1-15
跟你如果去看他的頭髮跟衣服那你反而容易判斷	1-15
那同樣的情形你如果把整個照片拍下來有多少個點在上面的話就好像這個	1-15
而頭髮跟衣服就是兩個特徵參數那容易判斷他是男還是女是一樣的意思	1-15
那這些個最簡單的解釋我們怎麼做 speech  recognition 但是這個情形是很簡單因為我們只有零到九十個聲音嘛	1-15
如果不是只有零到九而是什麼我們平常說話你知道我們是任何音後面可以接任何音	1-15
那麼就我們的呃以我們講國語而言我們常用詞可能是六萬個詞	1-15
如果有六萬個常用詞的話那麼你第一個詞就有六萬種可能第二個詞也有六萬種可能第三個詞也有六萬種可能我講一句話有十個詞的話就是有六萬的十次方種	1-15
就不太可能用剛才的那個方法剛才那個講起來太簡單了	1-15
這個假設零到九十個音可以這樣子做那我現在如果是一個連續的聲音裡面有任何可能的音的話怎麼辦	1-16
那複雜很多那就是我們所講的這個大字彙連續語音	1-16
他進來一段聲音很進來一段聲音然後呢我每一個字每一個詞都可能是有六萬種可能	1-16
那這個怎麼辦那我們通常是用這個辦法這張圖是用英文為例所以我們講的是英文英文也是差不多的	1-16
英文常用詞可能也是六萬所以你也是假設第一個字可能是六萬個	1-16
這個第一個字有六萬種可能這個也是六萬種可能所以你這個字是非常不容易處理的那這個時候要怎麼辦呢我們通常的辦法是這樣假設要處理英文	1-16
假設我輸入這句話是 this  is  speech	1-16
那這時候怎麼辦呢我事實上是先把英文的所有的基本的單位音分析出來	1-16
比如說在英文的基本單位音裡面這是一個基本單位音這是一個基本單位音等等等等等等	1-16
因此呢我總共的基本單位音沒那麼多	1-16
這個可能是數十個或者是數百個就這麼多而已	1-16
那麼因此呢我的每一個基本的單位音看成是剛才上一頁的那些零跟一的 pattern	1-16
那我把這些基本單位音都建成了 pattern 之後那麼我現在進來一串聲音我就去跟那些剛才的 pattern 來比對	1-16
那麼那些呢我把他呃稱之為 acoustic  model	1-16
所以呢所謂的 acoustic  model 其實就是現在只有這一隻這一隻好像沒電了耶我所所謂的 acoustic  model 其實就是這一堆基本的單位音他們的這一堆基本的單位音他們的這個 pattern	1-16
那麼因此呢我就可以跟它比對	1-16
那這些基本單位音的 pattern 怎麼來的當然還是要用這個聲音去訓練出來的經過訓練程序訓練出他們的 model 來	1-16
那麼這個 corpora 這個字是我們常用的其實就是我們翻成中文就叫語料就是語言的或是語音的 data  base	1-16
我用一個 data  base 裡面有各種各樣的聲音的 data  base 我訓練出這些基本單位音的 pattern	1-16
於是呢我的第一件事你可以想像的是我的聲音進來經過一些這 front  end 的 processing 得到 feature  vector 就是我這邊所畫的東西	1-16
那麼我等於是在喔把這些轉成這些 vector 那就是這一塊的所謂的 front  end  signal  processing 在做的事	1-16
然後得到我這個一串的 feature  vector 之後呢	1-16
我現在第一步就是跟這些基本單位的 pattern 來比對	1-16
假設我得到說他是這一堆單位音	1-16
再來呢我有一個辭典這個辭典告訴我說這三個音拼起來是這個字這兩個音拼起來是這個字這幾個音拼起來是這個字	1-16
那這樣呢我就可以猜說他可能是 this  is  speech	1-16
這樣想起來好像容易其實這樣講實在是太簡化了問題	1-16
為什麼呢因為事實上你可以想像這個音跟這個音是很像的跟這個音也是很像	1-16
那麼因此呢我辨識的時候我不太容易判斷第一個音真的是這個音我很可能會覺得說第一個音很可能是這個音也很可能是這個音也很可能是這個音	1-16
第一個音至少有這三種可能那第二個音呢可能是這個音也可能是另外一個音也可能是另外一個音所以他也有三種可能	1-16
第三個音呢雖然是好像是這個音但是他也很可能是這個音他也有點像是這個音等等	1-16
所以你可以想像其實我不是那麼單純說哦前面是這三個音所以就是這個字不是這樣子而是我很可能這有很多可能音這有很多可能音這有很多可能音那他們兜起來可以兜成很多不同字	1-16
所以到底是哪個字我不知道啦因此怎麼辦咧我後面要用一個所謂的 language  model 那就是這個	1-16
那 language  model 是什麼呢他告訴我說哪些個字連起來是比較像一個句子	1-16
那麼他這樣連可能是一個字這樣連可能是另外一個字這樣連可能是另外一個字那麼因此呢我到底什麼字接什麼字可以有很多很多種我在這裡面找出一個連串的 sequence 來而他的機率最高最像一句話	1-16
譬如說在句首我先看這個字在句首的機率如果前面有這個字的話後面會接這個字的機率前面有這兩個字後面再接這個字的機率等等	1-16
那這樣乘起來就變成是這些個字連起來是一句話的機率我們用這個方式來算然後找一個字串是這個機率最高的	1-16
那麼這個的過程怎麼做呢那個其實我們這個東西是用另外一個 text  corpora 這也是一個 corpus 不過這個是文字的這個是聲音的	1-16
那這是文字的 data  base 譬如說上網去抓一千萬個網頁下來那裡面就有十億個字的文章於是你就可以用這個去計算譬如說這個字在句首的機率是多少然後前面是這個字的時候後面會接這個字的機率是多少都可以算的出來	1-16
那因此我就得到這些東西那這些個機率存起來就是一個我們稱之為 language  model 語言模型	1-16
就存在這裡那麼因此呢我現在就根據這個語言模形去算所有可能存在的字串裡面誰的機率最高那那個就是我的答案	1-16
那這裡面呢這種 language  model 我們通常有很多種我們後面會詳細說這邊簡單的舉例譬如說呢我們叫做如果是前面是這個字後面接這個字的機率是多少這個叫做 bigram 前面兩個字後面接這個的機率是多少叫做 trigram 就是兩兩相連跟三三相連的機率	1-16
就好像這個呢是 bigram 的 probability 這就是 trigram 的 probability 等等那這就是我們的 language  model 那他是用一個文字的大的 data  base 所 train 出來的	1-16
那麼這是一個簡單解釋說明呢這個大概是這麼做的不過這個要詳細做的好呢這裡面有非常多的學問所以我們其實前面的半個學期都在講這個 exactly 怎麼做	1-16
我們在這個一點零我們在二點零其實還會再詳細再稍微詳細一點的說的他們大概是什麼	1-16
然後到了這個三點零也是那麼到了四點零開始四點零五點零我們詳細說這個 acoustic  model 怎麼做	1-16
就是這些個基本單位音的 pattern 怎麼做六點零我們講 language  model 怎麼做七點零我們講這個 front  end 怎麼做八點零我們講這塊怎麼做所以我們到八點零才會全部講完這張圖	1-16
好這是一個簡單的解釋那麼今天我們來講這個 speech  recognition 的話呢大概有很多種可能的 application	1-17
那麼舉例來講呢你如果是 voice  comment 你的網路的一些 comment 或是 instruction 的話搞不好你只好辨識一個 word 就好了辨識他是什麼詞就好了	1-17
另外一種呢是 keyword  spotting 也就是說我只要 define 一組 keyword  set	1-17
然後進來的聲音我只要看裡面有沒有他就好了	1-17
換句話說你今天 speaker 講的這句話其實只有這個是 keyword 我只要抓他有沒有講這個就好了	1-17
其他的我就不管了這些都是垃圾我根本不要管	1-17
舉個例子來講譬如說我們剛才 demo 裡面的那個查號台他說我要查土地銀行	1-17
其實土地銀行就是 keyword 他把所有的銀行變成 keyword 你要判斷他有沒有土地銀行	1-17
你說什麼我要找啊你給我幹麻其實沒有關係的這些都是垃圾就抓他的 keyword 在哪裡就這樣子這我們稱之為 keyword  spotting	1-17
當然還有一種呢就是你要輸入大量的的時候就變成是這個大字彙的連續語音你要輸入比較長的文字	1-17
那我們可想必一個很可能的因應就是所謂的 remote  dictation 也就是說譬如說我今天去開會開完會在回來的路上我在路上坐在捷運裡面我就可以打開我的手機就開始說我今天早上幹麻幹麻幹麻我把我的這個這個會議報告寫用聲音寫完之後他都傳到我的電腦裡面去了	1-17
等我回到辦公室打開他已經辨識好了有幾個錯字我改一改那就可以用了喔等等	1-17
這我們叫做 remote dictation	1-17
那所謂的 automatic  transmission  transcription 是說自動的把他轉寫成為譬如說會議紀錄或是什麼東西我就錄音到時候就直接把他轉成文字了等等	1-17
那麼今天的問題包括呃當然最正確的辨識是 speaker  dependent 就是針對一個人的聲音去訓練他的聲音來辨識的話他的正確率一定是最高的	1-17
那你最理想的是說這個系統可以聽所有人的聲音那叫做 speaker  independent 但是今天你要做 speaker  independent 正確率是沒有這麼高的	1-17
那麼比較好的辦法呢是 speaker  adaptive 也就是說你是你跟他說了幾句話之後他開始學你的聲音然後正確率愈來愈高這是 speaker  adaptive	1-17
你剛才聽到的我們前面那幾個 demo 都是屬於 speaker  independent 所以我們如果換 speaker 都可以但是有些正確率沒那麼高了	1-17
那然後這裡面還有很多的問題是今天都沒有解決的譬如說這個 reception 就是說你現在必須要一個麥克風對著嘴巴嘛	1-17
我如果是用手機的話呢我是在這裡那麼其實他沒有對到我的嘴巴所以呢會丟掉很多訊號之後我這個訊號會變的比較微弱會出很多問題	1-17
background  noise 我拿著手機在路上走會有很多雜訊變化什麼的我如何克服這些問題	1-17
我如果是經過手機電話傳送的話呢經過其他的變化如何處理	1-17
那還有呢是清晰朗讀的聲音呢還是自發性的聲音 spontaneous 我平常講話的時候跟你朗讀一句話是不一樣的	1-17
你剛才聽的那個 demo 裡面我們說今天的天氣非常好	1-17
那這個是清晰朗讀的聲音	1-17
但是我如果平常講話不是這樣講我講今天天氣非常好今天天氣非常好你如果仔細聽一下的話呢我的今天天氣的今天已經不是今天了我是唸成尖的	1-17
今天天氣	1-17
我們人是聰明到你根本不會感覺今天唸成尖因為我們就知道你講的就是今天天氣好	1-17
可是對機器而言他聽到的是尖那你得要讓他知道這個尖其實是今天等等	1-17
那在這樣子當你今天唸成尖的這種就是所謂的 spontaneous  speech 我們平常日常講話其實是這種很多音滑來滑去都有變化的	1-17
然後呢 conversational  speech 是說我在對話之中那也是非常 spontaneous 我們對話之中還有很多特別的特別的這個符號或語言等等這些都是不容易處理的	1-17
那你清晰朗讀他的文字是最容易處理的那所以呢我們今天講起來語音辨識仍有一堆問題尚未解決	1-17
再來語音合成剛剛也聽到一個 demo 這個語音合成那語音合成基本的原理都一樣雖然作法技術千變萬化	1-18
基本上就這三塊第一塊的目的是你進來一段文字我要查辭典以及有其他的規則告訴我這段文字到底是相對於什麼音	1-18
你知道是什麼音之後並不是把把那些音拼起來就可以了你如果直接知道他是什麼音把他音直接拼起來的話你聽起來就會發現那是一堆聽不出來在說什麼的話	1-18
那麼最主要的原因我們人說話的時候其實是有另外一種東西在裡面這種東西叫做 prosody	1-18
那麼什麼 prosody 是呢這個不是那麼容易解釋但是用我們最常用的語言來講所謂的 prosody 就是我們聲音裡面的抑揚頓挫	1-18
也就是說	1-18
也就是說第一個我們說話的時候是聲調會有高高低低的	1-18
那麼在我們國語而言那麼每一個字有一二三四聲就是他的抑揚頓挫	1-18
那麼每一個句子其實也有他的句調他的高低的	1-18
那麼對於其他的語言譬如說英文法文他們也一樣有他們的抑揚頓挫只是說他不見得代表不同的字他只是代表其他的意思	1-18
譬如說 how  are  you  today 跟 how  are  you  today 都可以他當然他有他的抑揚頓挫的你如果變成 how  are  you  today 那就沒有了	1-18
所以呢我們不管是哪一種語言都一樣他有他的抑揚	1-18
頓挫呢就是說我們在講話中間其實是每一個音有長有短中間會有大頓有小頓這個頓也有長有短這些所有的音的長長短短的拼湊起來的那些變化就是所謂的頓挫	1-18
那除此之外當然還包括什麼還有譬如說這個這個重音有的地方會有重音有的地方會有能量的高低變化這一類的東西就是所謂的 prosody	1-18
那你要語音合成就是要把這些 prosody 做到那就像人的聲音	1-18
如果沒有做到這些 prosody 就會變成 how  are  you  today 那就什麼都不是了	1-18
那麼因此呢怎麼樣做一個 prosody 變成最大的難題那麼絕大多數的這個語音合成就是拿最大的功夫在做一個好的 prosody  model	1-18
因此你給我一段文字我就有一個 model 告訴我他的 prosody 應該是怎樣的	1-18
然後根據那個所要的 prosody 再去拼我要的聲音	1-18
然後把你所要的抑揚頓挫通通都調上去聽起來就像	1-18
那這個是語音合成最基本的原理那我們稱之為 text to  speech 就是把任何的 text 變成 speech	1-18
那他的目的你當然可以想到在網路的時代的話呢你的 email 要他幫你讀那麼網頁上的文字要他念讀給我聽等等那麼關鍵的問題就是這個 prosody  model 就這個 model	1-18
等等	1-18
那麼早年的方法呢最常用的是所謂的基本單位音我就是有一個 database 把基本單位音都存在這裡	1-18
然後你今天告訴我是什麼音湊成為把他拼起來要有什麼 prosody 在這邊調調出來就好了這是早年的辦法	1-18
那今天的辦法呢比較是屬於 corpus  based	1-18
所謂 corpus  based 就是我們剛才講的我們用一個 database 一個播音員念了幾十小時的聲音所有聲音都在裡面了	1-18
你要你要怎麼樣的聲音你要他怎麼樣的抑揚怎麼樣的頓挫他都在裡面了所以呢你只要把他你就把他拼起來然後你找到如果他沒有你要的抑揚頓挫你就調他的抑揚頓挫把他拼起來就是所謂的 corpus  based	1-18
你剛才所聽的那個 demo 就是 corpus  based 的 corpus  based 的一個好處是說我的 unit 不再是一定是基本單位是可以是 non  uniform	1-18
那麼舉例來講如果我現在要講台灣大學的話那麼不盡然要把台啊灣啊大啊一個一個拼起來而是說如果那個語量裡面那個人本來就說到台灣大學我就把那整個台灣大學拿來用	1-18
那絕對是比你把他拼的來的好等等所以他的 unit 不再是 unit 他是 non  uniform 的 unit 等等好這是語音合成	1-18
那再來呢語音的 understanding 了解我們講的語音辨識是只是把辨識只是把聲音 transcribe 變成 word  sequence 並沒有了解他是什麼意思	1-19
譬如說你說今天天氣好他辨識出來今天天氣好	1-19
你剛剛那個 demo 今天天氣好他只是知道這些字這些聲音相對今天天氣好他有沒有知道你在說什麼話沒有所以 recogniition 的話只是在做一個對應	1-19
只是在做對應把那個聲音 transcribe 成為一個字串至於他是什麼意思我們沒有去管他	1-19
但是如果你要了解他是什麼意思的話呢這就是 understanding 就是要了解他的你的 speaker  intention 什麼時候需要了解我們剛才講的對話系統你說我要看看我的信箱	1-19
那他不是要知辨識說哦看看信箱他不是這個意思他是要知道這個 speaker 是說我要打開我那個信箱的意思	1-19
所以他就知道哦要打開信箱那你就要先問他 password 所以呢如果你是像那樣的應用的話你就需要知道這個 understanding 要 understand 這個 user  intention	1-19
那同樣呢我們剛才的那個 demo 就是這個查號台那也是一個要 understand 我說我要找這個台灣銀行的電話號碼那麼他其實是必須要知道我是要找電話號碼喔他不是把電話號碼四個字辨識出來沒有用的他要知道是要查的是電話號碼	1-19
然後他知道你要查的是台灣銀行等等那這個 understanding 比起 recognize 要難很多你是可以想像的	1-19
因為他難很多所以很多時候我們做的其實是一個非常 finite  limited 的 domain 跟 task	1-19
就像剛才那個查號台我只是查電話號碼你跟他講電話號碼以外的任何事情他都不知道可是你如果跟他講電話號碼理論上我們希望他能夠幫我們做到查電話號碼的工作	1-19
那這個就是 limited  domain 的情形通常使用的方法呢可以你辨識出來可以分析他的文法結構他的句子的意思	1-19
那麼另外一個辦法就是用統計的辦法去分析會怎樣那像你剛才那個那個查號臺其實就是用統計的方法	1-19
那怎麼做的呢簡單的解釋就是我們把中華電信一零四查號台的錄音帶拿來用那個錄音帶去分析然後 train 所以呢用裡面的統計方法算出來這是所謂的 corpus  based	1-19
那通常用常用的辦法是用所謂的 key  phrase 跟 concept 舉例來講我們剛才說的幫我查一下台灣銀行的電話號碼	1-19
其實真正的 key  phrase 就是查一下台灣銀行的電話號碼那這三個東西就是我們剛才講的 key  word	1-19
你就是預先 define 好了這些 keyword 我就是要在聲音裡面去找他有沒有這個	1-19
至於這其他這些東西都無所謂啦這都是垃圾他隨便說什麼都可以啦所以剛才可以說你幫我找一下什麼這隨便亂講都可以	1-19
那麼這些 key  key  phrase 找到之後他們分別在對應到某種 concept 那麼譬如說這個所謂查一下其實只是說他是要問一個問題	1-19
然後呢電話號碼只是說他是要問電話號碼的意思至於台灣銀行是我真正要問的 target 等等他分別對應到的 concept 等等	1-19
所以呢我這個聲音進來的話呢你剛才那個 demo 其實就是這樣子他先辨識出是哪些個 syllable 就是我們的單音啊我們的這個單音節叫做 syllable 我們的中文國語裡面每個字都是一個 syllable	1-19
我辨識出一大堆可能的 syllable 之後我從這裡面去找有沒有我要的 key  phrase 他代表什麼 concept 等等再去看他是什麼意思等等	1-19
這是一個相當難的問題我們以後陸續還會說到我們現在這邊都是簡單的介紹一下詳細的後面都會說到	1-19
那再來呢 speaker  verification 你要確認他是誰我們剛才的 de 例子提到過就是如果你要開信箱他的一個重要的問題就是確認這個 speaker 確實是該開的了信箱的那個人	1-20
這就要問你 password 是什麼等等這就是 speaker  verification 就是要 verify 那個 speaker 就是他所 claim 的那個人	1-20
那你需要的 application 那他的那就是說在網路上有很多 application 都是需要知道他是誰就需要 verify	1-20
那我們今天講的 verification 有兩大類第一類是 text  dependent 叫他講某一個話叫他講某一句話規定好他就要講那一句話 speaker 還有一種是 text  independent 隨便你說什麼都可以所以呢就是這兩大兩大類隨便你說什麼都可以判斷他是誰還是不行	1-20
那我們今天其實 verification 不能做到百分之一百還是會有誤差所以我們常常會跟其他的 verification 結合起來像我們剛才講的例子他說你告訴我你的 password 那這個就是跟 password 結合	1-20
因為你本來輸入電腦的那個 password 也是一個 verification 那我等於把那個 verification 再加上聲音的 verification 兩個做在一起	1-20
那基本的原理很簡單都是這樣的就是每一個 user  train 一個他自己的 model 所以我現在說什麼話的話呢我也是在那裡面抽出他的 feature 來跟這個是一樣的	1-20
不過這裡面的特徵參數可能不一樣這個是我這邊是代表誰的聲音我抽出他的feature 之後為每一個人都建一個他的 model	1-20
那麼因此呢你現在就可以跟他的 model 來比較看看他跟他到底像不像是還是不是等等這就是 speaker 的 verification	1-20
我們以上講的這幾個是是最核心的技術包括 speaker 的 verification 包括 understanding 包括合成包括辨識等等	1-20
那麼再下來的呢是比較整合性的不是一種東西而是跟很多東西結合起來變成一種整合性的運用	1-21
譬如說這個是搜尋這個我們剛才上堂已經說過了就是你今天 google 搜尋是給我一段文字給我一些文字我去跟網頁上的文字做文字跟文字的比對	1-21
那麼但是我今天呢我的 instruction 可能是聲音網頁上東西可能是聲音所以我可能是聲音跟聲音的比對或者用聲音要去找文字或者用文字去找聲音等等	1-21
那這個我們剛才說過了我們不多說這個對話我們剛才也已經說過了那麼這也是個很重要的這個整合性的那麼我們舉例來講這個喔	1-22
剛才已經舉過例了所以不用舉例那麼應該說是我們所有的人跟網路的互動最可能的方式就是用這樣口語的對話來達成	1-22
那麼這個時候我需要東西呢包括語音的辨識還有 understanding 所以我你看我這邊我畫的是 recognize 加上 understanding	1-22
也就是當你說我要打開信箱的時候我不是要辨識打開信箱那四個字那是這邊做的事我真的要做的是要 understand 他是要開信箱的這件事	1-22
所以我要有 understanding 然後呢我顯然要造句來合成	1-22
現在都已經做的很好那麼打電話買飛機票的情形呢你可以想像是前一張表	1-22
出發地點是哪裡	1-22
ok 台北目的地是哪裡紐約等等	1-22
那就好像前一張表一樣然後由系統一一去問 user	1-22
每一個問題 user 回答就填進來	1-22
當表填完的時候我就可以買飛機票了	1-22
那麼因此他可以說請問你要去哪裡紐約	1-22
請問你要從哪裡出發台北	1-22
請問你要哪一家航空公司長榮那請問你要哪一天出發星期天	1-22
這樣子那就那這種情形就是由系統來主導就是所謂的 system  initiative	1-22
這是最穩的方法	1-22
但是呢 user 可能會很不喜歡這樣叫我一個一個去填這些問題	1-22
user 會希望一個比較自然的那最自然的是所謂的 user  initiative	1-22
完全由 user 來主導	1-22
就好像你打電話給一個旅行社你就說欸我明天要到紐約去啊你怎麼怎麼他就跟你講然後你就可以買到飛機票	1-22
這樣是最好的不過這樣子的 user  initiative 的對話系統今天並不存在因為那個太難了	1-22
那麼我們今天比較作得到的是屬於中間的所謂 mixed  initiative	1-22
就是基本上由系統來主導但是 user 不一定要完全照系統叫你說什麼就說什麼	1-22
而是可以說一點別的系統說請問你到哪裡去我說啊我明天要從台北到紐約	1-22
你講的跟他講的不太一樣但是他還是都知道台北到紐約明天那這樣的話呢就是我們所謂的 user  initiative	1-22
今天技術大概最多做到這樣就是 mixed  initiation 你想要真的跟 user  initiative 一樣就是跟你任何一個 user 打電話到旅行社一樣	1-22
你跟他說什麼啊我要幹麻幹麻他都跟你講這種技術現在還不存在	1-22
那麼這樣的 dialogue 同時我們也常常需要 worry 一些最重要的事情	1-22
第一個就是 reliability 嘛你跟他講什麼要買飛機票要買對啊不能買一張錯的飛機票	1-22
那麼這個是非常重要的問題要獲得 reliability 最好的方法就是用 confirmation	1-22
不斷的跟他 confirm	1-22
那麼你剛才的那個查號台你會發現他會問你你要的是土銀嗎	1-22
就是因為那個土銀土地銀行他沒有確認對不對	1-22
有點懷疑所以他就問請問你是要的是土銀嗎那這就是 confirmation	1-22
那麼 confirmation 是幫助 RELIABLE 很好的方法	1-22
但是多了呢也會很 boring 你可以想像我最 RELIABLE 的方法就是每一次都 confirm 一次請問你要到哪裡去紐約請問你是去紐約嗎是的	1-22
請問你從哪裡出發台北請問你是台北出發嗎是的你在這樣一個個下來當然會對啦不過這個實在是太 boring 了	1-22
所以你要有一個辦法讓他	1-22
那就是那因為那個字非常不 efficient 嘛你如何兼顧 RELIABLE 又 efficient	1-22
這是 dialogue 今天不能解決的很難的問題	1-22
那當然要做到這個就跟這有關就是這個 manager 如何	1-22
如何來 control 這個 flow 這個整個 dialogue 的流程	1-22
到底怎麼樣的譬如說我們剛才講他說請問你到哪裡去他說我明天早上要去紐約	1-22
他已經把多講了好些東西了	1-22
那如果系統能夠知道的話那最好啊那我就跟著這個往下走啊等等	1-22
那如何去根據 user 說什麼我隨時管這個流程讓我的這個更 efficient	1-22
這些都是這個相關的問題那我們通常最基本的兩個參數來描述他好不好的	1-22
一個就是 success  rate 也就是你要買飛機票最後真的買對了而且買成功了的成功率是多少	1-22
還是最後人家受不了把電話掛掉的機率有多少那這個就是 success  rate	1-22
那麼另外一個就是 average  number  of  dialogue  turn	1-22
也就是所謂的你所謂的這個 turn 就是就像下棋一樣我走一步叫做一個 turn 他再走一步叫做一個 turn	1-22
那對話就是我講一段他講一段這分別是一個 turn 那到底我從頭到尾到底把飛機票買成的時候到底用了多少的 turn	1-22
那 turn 的數目是愈少表示愈 efficient  turn 數目愈多就愈有問題所以呢通常這兩個是最常用的	1-22
這個 turn 的數目用來描述 efficiency 那個 success  rate 用來描述 reliability 等等好這個是對話系統	1-22
那再來呢是這個 Spoken  Document  Understanding  and  Organization	1-23
這個其實最簡單的例子就是我們剛才的最後那個 demo 就是譬如說你的新聞	1-23
那個網路上的新聞那麼多我每天回到家裡多少個電視新聞台那麼多新聞	1-23
打開都是我不要看的轉來轉去都是我不要看的怎麼辦我希望把	1-23
根據他的聲音去了解他的內容然後 organize 它	1-23
那麼我們剛才已經講過了就是這個因為他不像文字的可以一目暸然清清楚楚你全部都混在一起	1-23
你的要從頭聽到尾要花太多時間所以呢我們想辦法把他自動切割自動做 SUMMARY 自動建 title	1-23
然後呢我可以很方便的搜尋等等這就是這一塊所說的事情	1-23
那再來呢就是這個我們其實還有另外一個問題就是我們這個世界其實是一個多語言的世界	1-24
其實我們每天講的話都不是一種語言而是多語言等等	1-24
那我們也許休息十分鐘吧在這裡休息十分鐘好了	1-24
ok 我們來講這一個多元的問題	1-24
就是我們今天事實上是一個多元的社會	1-24
每一個人講的語言都不只一種	1-24
那我們也很容易把不同的語言混在一起說	1-24
那麼這裡面有很多種問題第一個所謂的 code  switching	1-24
這是語言學上面的說法就是我們常常會把兩種語言或是兩種以上的語言 switch 來 switch 去	1-24
第一種情形就是像這個人人都用 computer 家家都上 internet	1-24
這個其實是一句中文但是中文的某一些詞彙我們換成了英文	1-24
那其實你如果我的課就是一個這樣的形式那我講的是中文我裡面隨時都把這個 term 換成英文哪	1-24
那麼我們現在錄的音其實就是我要做一個 case 就是這個 case 我們是中英夾雜的語言如何能夠處理的好	1-24
那其實我們上課還有另一個現象就是剛才我們講的 spontaneous	1-24
就是很自發性的所以我中間隨時都會很多音滑來滑去這個現象都會在這裡發生的	1-24
那另外還有一種可能是其實我們很可能整句換成另外一種語言了	1-24
譬如說準備好了嗎 Lets  go	1-24
那你就變成了換一種語言	1-24
那你也可以準備好了嗎 lets 行	1-24
那就變成那就變成那就變成另外一種語言這個這個其實都可能發生啦	1-24
那麼這一類的就是你怎麼 handle 這些 code  switching	1-24
那另外呢還有一種情形就是其實是網路的 information 是多語言	1-24
因為你可以想像成我們的這個	1-24
呃這是我們網路是全球性的網路所以呢有全球性的 CONTENT	1-24
跟全球性的都是多語言的 user	1-24
舉例來講呢我今天如果用國語去輸入我要找什麼什麼東西的話呢	1-24
我希望不只是中文的網頁能夠過來英文相關的也來日文相關的也來對不	1-24
我最好是各種橫跨各種不同語言的網路的 information 都能獲得	1-24
雖然我的 input 是一種語言等等	1-24
那這是另外一種問題	1-24
那再來呢就是方言跟口音	1-24
那麼就台灣而言考慮的方言口音比較單純	1-24
就是台這個閩南語客家語那還好	1-24
那你如果想華文地區那就很多種	1-24
有這個根據他們語言學家的說法是所謂八大方言還是十大方言語系呀就是	1-24
你譬如說這個所謂的這個吳語	1-24
什麼是吳語呢就是	1-24
你大概可以猜的出來就是江浙一帶的	1-24
上海話南京話寧波話這個浙江話什麼鎮江話的揚州話什麼什麼	1-24
這一類的是屬於吳語	1-24
這是他們一個語系那有很多語系嘛有什麼譬如說	1-24
山東話或者是這個四川話這個都各有各的	1-24
那麼這個語言學家有另外一個說法喔他說全	1-24
整個中國大陸的方言是數百種到數千種	1-24
depends  on 這你怎麼去分他	1-24
誰跟誰算是同一種還是算不同種	1-24
那麼那麼你如果去分成十大方言語系還是八大方言語系的話其實每一個語系等於是一種不同的 language	1-24
那他們用的辭彙他們的發音什麼句型都有不同	1-24
那你可以想像呢並不是我們說秦始皇書同文	1-24
文字都統一嘛其實也不盡然因為很多音不是文字那個漢字可以描述的嘛對不	1-24
就像台語也不是漢字就可以寫清楚的	1-24
有一些音就是漢字沒有的	1-24
廣東話也可以有他們廣東話寫的小說	1-24
裡面就有很多奇奇怪怪的字他來描述廣東話一樣的	1-24
那麼因此呢就他們的說法其實整個華文語系	1-24
整個中國大陸的話是相當於很多種不同的 language	1-24
那麼我們為什麼都說他是一個中文其他都叫做方言呢	1-24
這是 political  reason	1-24
那麼覺得好像是應該是一種語言不同的方言其實他們是不同的語言等等	1-24
那同樣你在方言之間也會 code  switching 嘛	1-24
我們譬如說國台語中間是或者國語客語之間閩南語跟台語之間有 switch 的現象嘛	1-24
那我們也可以國台英嘛	1-24
對不對我們平常很可能講的話裡面有夾雜著英文夾雜著台語這種都會嘛	1-24
那麼因此呢你怎麼樣 handle 這一類的問題	1-24
那還有呢你可以想像成就是像像這個	1-24
在大陸上他有這麼多種方言那他怎麼辦呢他們另一個辦法就是說	1-24
我以這個這個哦他們叫做普通話啦我們叫做國語啦	1-24
我以這個國語為主其他的呢只是有口音	1-24
所以呢譬如說你如果要能夠處理上海話要處理四川話是難度太高了	1-24
因為你要處理這麼多種語言	1-24
那你也許就是一種就是普通話或者國語	1-24
但是呢他可以帶四川口音可以帶上海口音	1-24
但是他就是國語或是普通話只是有不同口音而已	1-24
這是所謂的口音跟方言那這也是一個夠夠麻煩的問題	1-24
那這是講華文的世界如果不限於華文的世界講全球的話呢	1-24
那語言學上通常通常他們叫做 global  language 跟 local  language	1-24
什麼叫 global  language 就是全球各地都有人在講的語言就是 global  language	1-24
那麼除了我們所了解的像英文法文這種都是 global  language 之外當然中文也是 global  language	1-24
一般而言在華文世界裡面所講的語言裡面被認為是 global  language 的	1-24
就是全球各地都有人在講的 global  language 呢至少除了國語或者普通話以外	1-24
還有兩個一個是廣東話一個是閩南話	1-24
廣東話也是全球各地都有人說的閩南話也是全球各地都有人說	1-24
所以至少是這三種被一般被認為是 global  language	1-24
那什麼是 local  language 就是說這個語言的人就限於某一個 local 的地區的	1-24
那 local  language 非常多	1-24
那麼全世界有多少 language 呢據語言學家的說法是	1-24
至少是四千到六千就是	1-24
這樣的數字	1-24
那麼那麼也有很多的 local  language 是	1-24
他們叫做 endangered  language 也就是這個語言	1-24
說的人已經越來越少越來越少慢慢有消失的危險	1-24
那麼 endangered  language 很多以千為單位阿	1-24
那麼你可以看到因為這個世界的文化是慢慢趨向於統一的語言所以呢這個這個喔	1-24
我們舉一個最最容易想像的例子這個有一個 endangered  language 就是這個滿州話	1-24
那麼這個在滿清入關的時候統一了整個大中國地區	1-24
但是他們的自己被漢化所以他們的語言慢慢消失	1-24
據說現在能夠說滿州話的人只有在大興安嶺裡面的幾百個獵人	1-24
他們講滿州話其他的人都不會了	1-24
那同樣的其實這樣的 endangered  language 是很多的	1-24
台灣也有很多好幾種原住民的語言都是屬於 endangered  language	1-24
那麼你知道這個日月潭旁邊有一個有一個原住民的的村落	1-24
不記得他們是這個族還是這個族	1-24
那他們的語言就是一樣的就是說的人已經非常少而且在減少中	1-24
那這一類的都是屬於 endangered  LANGUAGE	1-24
那麼做數位語音的人有另外一群人他們在想的是這樣的問題就是如何用數位技術	1-24
想辦法把這些個 eddangered  language 保存下來甚而進於進一步來推廣來教育等等	1-24
讓這些語言能夠繼續讓所有的了不起的文化都能夠保存喔	1-24
那麼這個是講不同的語言有不同的考慮的問題	1-24
那不管怎樣呢我們這邊所說的是語音的技術的話呢這個技術大概可以	1-24
切開來有所謂的 language  dependent 跟 language  independent	1-24
我們剛才提到的一些東西裡面有些不管是做中文做英文做德文是一樣的	1-24
那種呢就是 language  independent 的技術	1-24
但是也有一些是不同語言會不一樣的那就是 language  dependent 的技術	1-24
我們舉一個例子就國語裡面我們說過就是我們的抑揚頓挫是包含了聲調	1-24
那麼聲調代表不同的意思所以呢你說這個對不對	1-24
你說這個把跟你說這個爸是兩件是兩個不同的事意思是完全不一樣的只要聲調不同就不同了	1-24
這因此我們中文叫做 tone  language 就是	1-24
他是有聲調的不同的聲調代表不同的字不同的意思	1-24
那麼英文不同英文是沒有英文是也有這個 tone 是永遠是有但是它呢它並不是 tone  language 因為	1-24
不同的聲調並沒有代表不同的字我們講 How  are  you  today 跟 How  are  you  today 都可以	1-24
那麼因此呢在英文的 recognition 裡面呢他們不太講究這些東西	1-24
所以英文的 recognition 裡面他們常常沒有去分辨音調變化的高低	1-24
但是講中文的時候我們就必須要講究這個因為他代表不同的意思	1-24
因此如何去分這些聲調這就是一個 language  dependent 的 technology	1-24
因為他們做英文法文德文他們都不做這個但是做中文要做的這就是 language  dependent 技術等等	1-24
那如何讓你同時能夠 handle 不同的語言呢	1-24
一個最重要的原則就是 share  acoustic  units	1-24
就是不同的語言裡面它的 union 有點不太一樣	1-24
但是我們想辦法讓它盡量可以共有的共有阿	1-24
舉例來講國語跟台語大概有百分之七十的 units 是可以共用的	1-24
但是有另外百分之三十是不一樣的	1-24
那你如果所以你如果變成百分之一百三十倍的 units 的話呢就可以同時包含國語跟台語	1-24
於是國台語就可以同時 switch 來 switch 去大概就就可以了	1-24
那同樣呢中英文也可以這樣子喔	1-24
那麼現階段他們大概做的最多的大概是用二三十種語言	1-24
的共用的 union	1-24
大概是一種語言的 union	1-24
大概是五六倍的話	1-24
大概就可以把二三十種語言的所有的 union 都用進來等等	1-24
那這類是屬於這個我們講多語言的問題	1-24
好最後一個就是 DIS  DISTRIBUTION  recognition	1-25
也就是說你現在假設我們要講的是用一個手機	1-25
用一個 p d a 要一個很小的東西掛在身上什麼的話	1-25
那麼一點點能夠處理所有的事情嗎	1-25
當然不一定不過沒有關係我們當然不見得要把 every  thing 都在這裡處理	1-25
因為我們可以透過網路有 client 跟 server 嘛阿	1-25
所以這個最重要就是要用這個 client 跟 server 的觀念之後把一大堆東西丟到網路那邊去	1-25
換句話說我們要做的很多事情其實是在網路那裡	1-25
你要上網查什麼東西你要幹麻其實都在那裡嘛	1-25
那因此你如果要跟他對話他要跟我對話也是在那個那裡跟我對話嘛	1-25
所以那個對話東西也可以放在網路嘛放在那一端嘛	1-25
所以你可以把大部分的東西放在 server	1-25
只有最少數的部分放在我的 client	1-25
這樣的話呢我其實我真正的語音系統其實是包括了整個網路	1-25
那於是我就分散在整個網路裡面我有很多的 user	1-25
可能共用同一個 server 等等	1-25
像這一類的情形呢我們就稱之為 distribute  speech  recognition  D  S  R	1-25
那麼他是在一個 wireless 環境之下那麼常常是這樣子的一個考慮	1-25
麼在這樣子的情形之下呢怎麼做呢	1-25
這張圖就是我們剛才的	1-25
一點零第二張圖就講一個大字彙語音辨識有這麼多東西	1-25
這是 front  end 的的的處理他的那些 vector 的呃等等	1-25
這個是他的 acoustic  model 就是那些基本單位音的 unit 這是 language  model 等等等等這些東西	1-25
那有的人說我把這整個全部放在 server	1-25
我這邊只是把聲音送過去而已這樣也有	1-25
那如果這樣我就把這個全部都丟到 server 去	1-25
然後呢我這邊聲音反正就一樣就全部都跟打電話一樣就全部送到那邊再去處理	1-25
這是一種	1-25
那也有很多人士說我不要那樣的而是我把它拆開來	1-25
把這個語音辨識系統拆切開來像我這邊所畫的這樣這是一種切法	1-25
這是 example  partition	1-25
這樣切法這樣切的話呢	1-25
我就把這個 front  end  processing 放在這裡	1-25
因為這個 front  end  processing 也就是做這件事	1-25
把這個變成這個把這個變成這個這樣做這件事情是不難做	1-25
我可以放在這裡做	1-25
那好處是這樣做之後我這邊就剩下的就是這些個 vector	1-25
我就把這些個 vector 做成 packet 送過去就好了	1-25
那如果是這樣的話呢	1-25
我這個所需要的 band  width 傳送的問題什麼就可以少很多	1-25
到那邊的時候我再解回來再開始做所有我要做的問題	1-25
那這一類的就是所謂的 DISTRIBUTION  speech  recognition  D  S  R	1-25
那這裡面最大的問題應該是說你如何	1-26
當現在整個網路在我的系統裡面哪網路有一堆網路的問題要能夠解決	1-26
那舉例來講這是我們很簡單的說	1-26
如果是無線網路裡面的話呢	1-26
有 link  level  transport  level 跟 application  level	1-26
我現在如果做語音在上面做語音的話是在這裡做	1-26
是屬於 application  level 上面的一些的 core  technology	1-26
可是呢我這邊有 link  level 有 LINK  LEVEL 的問題	1-26
那麼 TRANSPORT  level 有 TRANSPORT  level 的問題	1-26
舉例來講譬如說 link  level 有什麼問題	1-26
他會這是傳輸這個我想你在通訊的課程裡會有我這邊並不去說他	1-26
你在傳送中間會有各種各樣的問題把你訊號破壞	1-26
阿他們稱之為 FADING	1-26
有各種各樣的 noise	1-26
那麼他有不同的特徵	1-26
然後會把我的訊號破壞	1-26
然後我的訊號的 level 可以變化很大一會兒很強一會兒弱	1-26
嗯你打手機電話你很清楚這一點	1-26
然後呢我的雜訊可高可低	1-26
還有呢我的 band  width	1-26
喔對還有這個因素就造成我的 error  rate 很高	1-26
會有很多 error 你知道我們今天你的手機電話大概	1-26
你的手機電話大概這個	1-26
錯誤率大概是	1-26
幾百分之一嘛幾百個 bit 會錯一個	1-26
那麼還會發生所謂的 BURSTING  error 就是所謂的一錯錯一堆	1-26
一堆都錯在一起嗯這些問題	1-26
那同樣的呢你還有 band  width 問題你手機電話的時候 band  width 非常有限	1-26
那你學通訊就知道 band  width 有限就使得我的 bit  rate 有限	1-26
我 band  width 也可能是 dynamic 隨時變化	1-26
我的 bit  rate 也隨時變化	1-26
這些就是 link  level 的問題	1-26
那到了 transport  level 的話呢	1-26
那麼我們平常你如果是用 internet 寫 email 的話呢	1-26
所用的所謂的 T  C  P  I  P 的話呢	1-26
是發生 error 沒有關係	1-26
因為我永遠可以要求他	1-26
我只要發現有 error 我就要求 re  transmission	1-26
重送一次	1-26
問題就是有一點 delay 但是我寫 email 呀什麼的有一點 delay 是沒有關係	1-26
我們通常通常透過網路送個什麼檔案給別人 delay 個幾秒鐘都沒有關係的	1-26
可是你如果現在是打手機的電話操作什麼東西的話你可能需要的是 real  time	1-26
那你如果要需要 real  time 沒有 delay 的話呢你可能就需要用 U  D  P	1-26
那你學網路就知道呢 U  D  P 的話呢他是可以沒有 delay	1-26
但是呢你如果發生錯誤怎麼辦	1-26
我就是把那個 packet 丟掉了	1-26
所以呢我的 packet 會有丟掉一堆 lost 會會有一堆 packet  lost	1-26
然後呢然後我的這個 packet 呢會 out  of  sequence	1-26
我收到然後 packet  SEQUENCE 跟原來 sequence 是不對的等等	1-26
那這些問題都是我們在這個 link  level 跟 transport  level 產生的問題	1-26
那我們講的語音技術雖然是寫在這個地方顯然是被這些影響	1-27
那這就是這個 D  S  R 需要克服的問題	1-27
那可能怎麼做呢那麼可能的做法包括	1-27
譬如說所謂的 packet  re  sequencing	1-27
嗯就是說你的這個呃	1-27
我收到的 packet 之後我要把他重新 sequence 一次	1-27
所謂 error  concealment 是說當你發現哪裡有 error 之後你想辦法做一個假的訊號	1-27
假設他跟真的一樣把他貼進去補起來這個叫 error  CONCEALMENT	1-27
這些東西呢	1-27
你基本上可以想像成是	1-27
發展一個叫做 convergence  functionality 就是有的人這樣講啦	1-27
這個圖跟剛才有何不同你如果看的話	1-27
如果你如果看的話剛才這圖上面就是 core  technology 就是這些東西	1-27
那我現在呢是把這個再拆成兩半	1-27
所以下一張圖是這邊拆成兩半	1-27
我co co 這個 core  technology 在上面	1-27
我底下多了這個叫做 convergence  functionality	1-27
那他的功能就是我剛才講的這些	1-27
packet  re  sequencing 或是 error  concealment 這類的事情	1-27
他的目的我們比較抽象來講就是增加了這一個保護層	1-27
使得	1-27
我的這個 link  level 跟 transport  LEVEL 這邊的這一堆問題呀	1-27
變成是 transparent  to  core  technology	1-27
讓我的從上面的 technology 來看的話呢	1-27
他們好像什麼問題都沒有一樣	1-27
他所有的問題在這邊都把他吸收掉了	1-27
對不對這邊所有的問題這邊都吸收掉了之後呢我這邊看起來是沒有問題的	1-27
或者是說這是一個保護層哪	1-27
他保護層把他保護的好好的	1-27
你這邊的各種變化呢都到這裡為止被檔掉	1-27
這邊看起來是乾乾淨淨的	1-27
嗯這是所謂的 convergence  functionality 的意思	1-27
那當然即使是這樣還是會有些 error 會跑上來	1-27
有 error 跑上來那怎麼辦呢你上面一定要更 robust	1-27
所以想辦法要有一些更 robust 的方法的 core  technology 來對付這些 error	1-27
你最常用的辦法是像 verification 嘛	1-27
像這個我們後面都會再說到	1-27
好那這是一點零我們說到這裡	1-27
一點零是有 reference	1-27
嗯我來 check 一下他是	1-27
這邊不曉得有沒有二點零	1-27
如果這個 REFERENCE 應該是在	1-27
嗯有沒有二點零呀	1-27
可能助教沒有放他認為今天只教一點零是沒錯 ok	1-27
那如果沒在這裡的話你只要去到網站上看在二點零的	1-27
二點零的這個	1-27
標題的那一頁裡面就有他上面就有他 REFERENCE  for 一點零	1-27
嗯所以我們今天這樣子講完一點零之後你其實可以看一點零的 REFERENCE	1-27
那 REFERENCE 有兩篇	1-27
一篇是就是在講我們剛才講的這些東西的	1-27
在網路環境之下這些技術的一些基本東西的概念等等	1-27
那那篇是第一篇保證很好念	1-27
因為那是我寫的	1-27
那第二篇是在講 dialogue	1-27
就是嗯是 M  I  T 的	1-27
教授所寫的就是 dialogue	1-27
口語對話的一些最基本的知識	1-27
是滿好看的嗯所以我想那兩篇應該都可以看	1-27
這是我們第一週的一點零的 REFERENCE	1-27
好我們現在上到這裡	1-27
我們今天是二點零喀	2-1
那麼二點零要講的事情其實是	2-1
呃我們一點零裡面講過一張圖阿	2-1
我已經他們有個 mouse 所以我剛才已經	2-1
如果記得的話我們上週有一張圖在講這個大字彙的語音辨識	2-1
那張圖呃裡面有好多塊東西	2-1
那我們其實今天是把那張圖裡面的幾塊重要東西簡單的說一下他的基本的原理	2-1
那呃第一個要講的是 hidden  markov  model	2-1
那麼之後我們底下要講的是	2-1
這個怎麼樣前面求 feature	2-1
然後我們會講後面的 language  model 等等	2-1
那麼我們主要講這三塊	2-1
那麼這三塊是我們在一點零上週有過那麼一張圖裡面的三個重要的部份	2-1
那這三塊我們今天講的也只是個非常簡單的簡介	2-1
這個讓各位有一點初步的 feeling 那個是在幹什麼的	2-1
那個那麼等到後面我們還有陸續好幾周都要繼續在講這三塊	2-1
比如說我們後面的四點零五點零還是在講這個 hidden  markov  model	2-1
然後六點零會講 language  model	2-1
七點零會講那個 feature	2-1
那都是在這裡	2-1
然後八點零在講怎麼作 search	2-1
所以我們一直到八點零講完都還在講這堆東西	2-1
那今天這裡只是一些最基本的觀念	2-1
我們把一些基本東西先說一次這樣你比較容易有概念他們是什麼	2-1
那第一個我們先說得是 hidden  markov  model	2-1
那這個呢應該可以算是今天所有的作語音的研究裡最主要的最常用的 model	2-1
也是最成功的 model	2-1
雖然他不是唯一的	2-1
那也有人用其他的	2-1
不過這個到目前為止仍然是最成功的用的最普遍的	2-1
所以呢我們來說一下	2-1
那麼 hidden  markov  model 是什麼呢	2-1
你基本上可以想像成是一個我們把每一個想要辨識的聲音都建一個 model	2-1
舉例來講呢假設我們用最簡單的例子來講	2-1
我現在要辨識零到九的十個聲音	2-1
那我就為每一個聲音建一個 model	2-1
這個 model 就長得像這個樣子	2-1
那麼於是我就會零有一個 model 一有一個 model 二有一個 model 一直到到九	2-1
總共呢就有十個 model	2-1
那麼每一個 model 是怎麼樣的呢	2-1
有這麼多個 state  n 個 state	2-1
那麼舉例來說假設零	2-1
這是一個為零所建的 model 的話	2-1
那他的第一個 state 第二個 state 分別代表我發那個零的時候的聲音訊號變化的狀態	2-1
舉例來講呢	2-1
那這第一個 state	2-1
很可能是我在發這個零的時候那個了	2-1
那個了我這個還沒有發出來的那個時候那個了那個時候那個就是可能是第一個 model	2-1
第一個 state 所描述的	2-1
第二個 state 可能是了發出來了	2-1
不過那個了可能後面帶著了一	2-1
那個了一的時候可能是第二個 state 等等	2-1
你到後面第三第四個 state 可能是一	2-1
零裡面那個了一一的音	2-1
然後等等等等到最後一個音可能是	2-1
最後一個 state 可能是零的那個登登的音等等	2-1
那這樣的話那我這個聲音一路慢慢跳過去呢	2-1
我就描述了這個零這個聲音的情形	2-1
基本上是有時間順序的	2-1
從一二三這樣一路下去的話	2-1
我描述那一個聲音的情形	2-1
那除了這樣子之外呢	2-1
那麼我們也可以通常把語音的訊號看成一系列的 vector	2-1
那這一點呢我們在上週可能有畫過	2-1
我有一點忘了	2-1
不過 anyway 我們重來一次	2-1
假設說我的聲音是假設說我的零是這樣子的	2-1
那麼如果是這樣的話呢	2-1
我的取第一個 frame 譬如說是裡面的兩百五十六點	2-1
或者是五百一十二點	2-1
那你知道我裡面每一個都是一堆 sample嘛齁	2-1
都是一堆 sample嘛	2-1
那等等假設說我的第一個 frame 是兩百五十六點	2-1
我把他呢經過某一個演算法之後	2-1
把他變成一系列的參數	2-1
這堆參數就是我們所謂的這個 feature  parameters	2-1
那麼他就構成一個 vector 我們叫做 o  one	2-1
那就是這邊的這個 o  one	2-1
就是我的第一個 feature  vector	2-1
那麼這種東西呢我們稱之為 feature  vector	2-1
因為這裡面的每一個呢就是我們所稱的 feature  parameter	2-1
也就是說他是描述這堆聲音裡面的一些特徵	2-1
那麼待會我把這個 window 向右 shift 過來之後	2-1
我可以產生我可以算出第二個來	2-1
他也是這堆那麼這個呢我們叫做 o  two	2-1
待會兒我再 shift 一個過來我就算出一個來	2-1
那這個呢就是 o 三等等等等	2-1
那麼換句話說我們上週提過	2-1
這個訊號這裡面千變萬化	2-1
你每一個聲音永遠不一樣	2-1
但是呢這個通常你在訊號裡看不出什麼東西出來	2-1
但是你如果把他求想辦法從裡面求出一些特別的特徵參數來描述這個聲音就會比較清楚	2-1
這就是我們所謂 feature  parameter 跟 feature  vector	2-1
那我們把這些 parameter 兜成一個 vector 就是所謂的 feature  vector	2-1
那麼於是呢現在我可以把一系列的訊號變更成一系列的 vector	2-1
那這些 factor 我稱為 o  one  o  two 等等等等	2-1
我叫做 observation  sequence	2-1
因為這是真正我可以 observe 的我可以看得到的我收的到的我量的到的這是我的 observation  sequence	2-1
那麼每一個呢我叫的 o  one  o  two 呢我就通稱為 o  t	2-1
這個小 t 呢就等於我的 time  index	2-1
不過這個 t 是整數	2-1
這個 t 是整數就是一二三四這樣就是第 t 個 factor 的意思	2-1
那麼那麼每一個 t 的 ot 裡面呢	2-1
就是這些東西呢我總共有幾個參數呢	2-1
X 一 X 二到 X 大 D	2-1
這個大 D 是我們這個 feature 的 dimension	2-1
或者說就是我的參數的總數	2-1
那我們後面就會說我們這個大 D 其實滿大的	2-1
我們常用的大 D 大約是三十九的 order喔	2-1
不是一定要三十九啦大概是這樣的數字	2-1
三十多個到五十多個差不多這樣的數字才夠	2-1
你大概會有那麼多個	2-1
那麼所以這個大 D 是一個滿大的數字	2-1
這樣這些 x 就是我們所謂的 feature  parameter	2-1
那這樣的 o  t 這些 o 呢就是我的 feature  vector	2-1
就構成我們所謂的 observation  sequence	2-1
那如果你是這樣看的話呢	2-1
那麼現在我假設這個是零好了	2-1
這是零的 model	2-1
這是零的聲音	2-1
那如果是這樣的話呢	2-1
那我事實上是會有	2-1
舉例來講也許前面這前面這三個是屬於第一個 state	2-1
我們剛剛說的第一個 state 可能是在描述那個了那個還沒有出來的聲音	2-1
如果是那樣的話也許是前面三個 state 前面三個的 vector	2-1
到第四五六七的時候很可能是代表第二個 state	2-1
這個時候是了一出來了	2-1
那麼如果是這樣來看的話你可以把這個 observation  sequence拆成 一段一段一段	2-1
那這一段我們可以想像他好比是描述這個 state 這一段好比是這個 state 這一段好比是這個 state 等等	2-1
那為了要區別這件事我們就 define 第二個 sequence 叫做 state  sequence	2-1
這些 q  one  q  two 就是分別對應到 o  one  o  two	2-1
那他是什麼呢他就是一到 n 的 state  number	2-1
那換句話說如果我們說一二三這三個是屬於 state  one 的話	2-1
表示說我就讓 q  one  q  two  q 三都等於一	2-1
q  one  q  two  q 三都等於一的話表示說他是第一個 state	2-1
如果 q 四這個四到七是在第二個 state 的話呢	2-1
我就是 q 四 q 五 q 六 q 七等於二就表示他們是在第二個 state 等等	2-1
因此呢我這個 q 的這個 sequence 其實只是一一一二二二二三三等等等等	2-1
因為他們都是一到 n 的整數	2-1
分別 indicate 我的第幾個 observation 是掉在第幾個 state 裡面的意思	2-1
所以呢這是我的 q 的這個 state  sequence	2-1
那這個時候有一個重要的東西在這裡	2-1
就是我們有這個 state 會跳的這個 a 一二 a 二二這些東西	2-1
那這些東西呢我們稱之為 state  transition  PROBABILITY	2-1
那這裡的 a  i  j 的意思是說我在 t 減一的時候是在 state  i 那裏	2-1
t 的時候是在 j 那裡的跳的機率	2-1
ok 換句話說呢就是我在前一個瞬間	2-1
這個 t 就是這個 t 嘛就是這個 t 嘛就是說我的前一瞬間	2-1
前一個 t 還在 state  i 但是下一個 t 會到 state  j 的機率叫做 a  i  j	2-1
所以呢 a 一一就是跳回原來的 a 一二就是從一跳到二的 a 一三就是從一跳到三的等等	2-1
這是 a 二二 a 二三 a 二四等等	2-1
喀喀那麼這個究竟是什麼意思我們舉個簡單的例子來看你就可以想像他是在說什麼	2-1
譬如說呢現在如果這個是這個 state 這是下一個 state	2-1
我們假設說這裡只有兩個	2-1
如果這個是零點一而這個是零點九的話	2-1
那表示什麼意思呢絕大多數的狀況他仍然會下一個仍然是原來的	2-1
只有零點百分之十的機會他下一個是跳到下一個去了	2-1
對不對所以呢你可以想像假設這是一這是二	2-1
q  one 呢可能是等於一 q  two 可能還是等於一 q 三可能還是一	2-1
因為我一開始是這個的話呢有百分之九十的機會下一個仍然會是他	2-1
對不對然後呢還是有百分之九十的機會下來還是他	2-1
還是有百分之九十的機會下來還是他所以呢他可能一直會這樣在這邊好多個	2-1
你可以想像可能到了八九個以後他可能會跳到下一個來	2-1
因為我十分之一的機會會到下一個去嘛	2-1
所以大概八九個才會到這邊來對不對	2-1
因此呢這個時候你可以想像我的 state  one 可能有好多個	2-1
要夠多個之後他才會跳過來	2-1
那反過來呢我現在如果這個是零點一這個是零點九的話	2-1
表示什麼意思呢	2-1
表示 q  one 是一的時候 q  q  two 很可能就跳到二了對不對因為我有百分之九十的機會會過來嗎	2-1
所以 q  two 很可能就匯二了是不是	2-1
那麼我會繼續留在這裡的機會很小呀只有十分之一的機會呀	2-1
所以他就不斷的往這邊一進來就會跳出去嘛對不對	2-1
那麼因此呢這些個機率就告訴我說	2-1
這個訊號會停留在這個 state 裡面長短	2-1
以我們剛才的例子如果是這個是零點一這個是零點九的話	2-1
在這邊就會比較長要夠長之後他才會有機率跳過來嘛對不對	2-1
那麼如果現在現在是零點九這個是零點一的話就表示這裡面很短一下子就可以跳出來嘛 ok	2-1
那麼因此呢我們事實上是在描述一個現象	2-1
因為在在我們的聲音裡面我們永遠不知道我們隨便講一個講一個字的時候我們永遠不知道我們到底會在哪一個 state 到底會多長	2-1
這是一個 random 的情形	2-1
我們舉個簡單的例子譬如說零有的人可以說是零有的人說是這個零	2-1
那麼你的每一個這裡面每一個 state 到底有多長是不一定的是可長可短的	2-1
我們舉更複雜的例子譬如說一個英文字 San  FRANCISCO	2-1
San  FRANCISCO 這裡有好多好多的音	2-1
你是 San  FRANCISCO 還是 San  FRANCISCO 還是怎樣你的每一個音是可長可短的	2-1
換句話說在我們講話的時候的任何一個你講的任何一個	2-1
如果用一個 model 來描述他的話到底哪一段到底哪一個要多長齁	2-1
我們剛才講譬如說這是屬於第一個 state 這屬於第二個 state 這屬於第三個第四個第五個第六個 state 的時候	2-1
到底每一個 state 的時間是多長多短實在是很難說的他是非常 random 的	2-1
同一個人講兩次的話他的長短都不一樣的	2-1
所以我們只能用一個這樣的 model 來描述他的一個時間上的長短伸縮的 random 的特性	2-1
那麼我們剛才講過這個這個意思就是	2-1
這個大小的話就可以看得出來他在這裡會長還是會短	2-1
那同理呢我有的時候可以跳像這個地方我可以從 a  one 會跳到 s	2-1
從 state  one 會直接跳到三的意思是有些音我其實可能會根本就沒有念就滑過去了	2-1
那麼一個簡單例子譬如說零	2-1
你念零的時後裡面到底有沒有那個一的音不一定有的時候那個一根本沒有發他就是跳掉了啊	2-1
那麼那麼因此呢你有的時候是可以跳的	2-1
所以呢那這些呢就是我們這一堆 a  i  j 這些個 state  transition  PROBABILITY 的角色的功能	2-1
那 in  general 這應該是 define  for 所有的 i 跟 j	2-1
因為任何一個 state 可以跳到另一個 state 去	2-1
所以呢他是一個 a  i  j 的 matrix	2-1
那這個 matrix  in  general 應該是 n  by  n 的	2-1
我的總共有 n  by  n 個 element	2-1
但是在大多數狀況我們都把他簡化到只有這樣子就是他跳回自己跟下一個跟下兩個	2-1
再多我們就不弄了因為太複雜了	2-1
那你可以想像如果是只有這三個的話我的這個 matrix 會簡化到只有每一行只有三個嘛	2-1
a 一一 a 一二 a 一三只有這三個別的都是零	2-1
然後呢這是 a 二一 a 二二	2-1
呃沒有二一應該是呃 a 二二 a 二三 a 二四然後呢是零等等	2-1
所以呢我基本上只有這一排讓他不是零其他的呢都是零	2-1
那如果是這樣的話我們的 model 稍微簡化一點	2-1
我們通常會作這樣的假設以免太過複雜	2-1
那事實上你也可以更 general 的情形應該是讓他每一個 a  i  j 都可以存在	2-1
如果每一個 a  i  j 都可以存在的話他甚至可以從後面跳到前面來	2-1
對不對你四可以跳到二來那這樣的話太複雜了我們通常簡化到只讓他只讓他那樣就好了	2-1
好那這是講這些 state 的情形	2-1
那再來呢你如果是知道這些聲音 o 一 o 二 o 三是在這個 state 一裡面呢	2-1
他會長怎樣還是不一定的	2-1
即使這個是發了的那個音的話	2-1
我都知道是那個音了並不表示他的 o 會長的怎樣他會長任何一個樣子	2-1
這個是語音特別難的地方就是即使是同樣的聲音同樣的人發兩次訊號都是絕對不一樣的	2-1
我們舉例來講同樣的一個人一個人發啊你把你發啊發三次	2-1
你用你用這個這個電腦收進去把 wave  form 畫出來看看那三個啊鐵定不一樣	2-1
絕對不會三個一樣的阿	2-1
你就是同樣的人發同樣的音三次鐵定是不一樣的阿	2-1
那麼同音詞你算出來的這個東西鐵定是不一樣的	2-1
所以即使你知道他們都是這個 state 並不表示他們會長怎樣他會長任何一個樣子	2-1
所以怎麼辦我們只能用一個機率的 distribution 來描述他這就是 b  one 的 o 的意思	2-1
就是說你如果是在 state  one 的話這個 o 會長怎樣呢我們不知道	2-1
我們只知道他會有一個某一種 distribution 這是 b  one	2-1
你如果說這些東西是在 state  two 裡面的話呢他會長怎樣我們不知道	2-1
我們只知道他會有一種 distribution 那這個音跟這個音有何不同呢	2-1
這兩個 distribution 不一樣就是了	2-1
但是我並不能夠知道說這個 o 一定是怎樣跟這個 o 一定怎樣這個我們很難講	2-1
只是我只能說他們是這個 distribution 他們是這個 distribution 而這兩個不一樣	2-1
ok 那這些 b  one  b  two  b 三就是當我這些 o 在 state  one  two  three 的時候他們的 distribution	2-1
那我們叫做 b  j 的 o 那我們叫做 observation 的 probability	2-1
那也就是這個 b  j 的 o 你看這就知道了	2-1
這個 j 就是這個 state 一到 n 嘛這個 j 就是這個 state 的 index	2-1
然後他有一個 distribution	2-1
那麼這個會是怎樣呢	2-1
你可以想像他的複雜性就是他	2-1
配這任何一個 distribution 去找任何一個樣子	2-1
那麼我們簡直無法去描述了	2-1
那我們的辦法就是說我們無法描述我只知道他是這個 distribution 我就說他是 gaussian	2-1
我就說他是 gaussian	2-1
那你說怎麼可能是 gaussian 呢因為我可以是任何長相呀	2-1
沒有錯你可以是任何長相因為他可以是任何長相	2-1
我們不見得能夠說他是一個 gaussian	2-1
這是一個 gaussian 對不對	2-1
如果是一個 gaussian 他的長相就是這個樣子啦那不見得是這樣啦	2-1
那怎麼辦呢我們就說他是一把 gaussian 齁	2-1
所以呢我們這邊是是一堆 gaussian 的組合	2-1
換句話說呢我如果是亂七八糟一堆東西的話呢我永遠可以說他是一堆 gaussian	2-1
譬如說這是一個 gaussian 這是一個 gaussian 這是一個 gaussian 這是一個 gaussian	2-1
我永遠可以說他是一堆 gaussian 的組合	2-1
當我的 gaussian 夠多的時候他大概可以描述他的 distribution	2-1
ok 因此呢我們從這個觀念來講不論他長得多麼複雜	2-1
如果給我一把夠多數目的 Gaussian 的話那麼他大概可以描述他這個樣子	2-1
那就是我們這邊的意思	2-1
所以呢是譬如說我不是一個 Gaussian	2-1
而是大 M 個大 M 這個大 M 是這個小 k 從一到 M 小 k 就是 Gaussian 的 index	2-1
所以呢總共有 k 個 Gaussian 總共有 k 個 Gaussian	2-1
然後呢把他們分別 waited  by 這個 c 的喔然後呢加起來才是我這個 distribution	2-1
這樣的話呢我就能描述一個任何一個 distribution 的長相我都可以用這個方式來描述了	2-1
那麼因此呢我的 b  j  k 的第一個 index  j 是這個 j 就是這個 j 就是我的 state  index 是指我第 j 個 state 的 distribution	2-1
那第二個 index  k 呢就代表他是第 k 個 Gaussian 我總共有大 M 個	2-1
那麼通常呢這裡的每一個 Gaussian 我們通常叫另一個名字叫做 mixture	2-1
所以第 k 個的 mixture 其實就是第 k 個 gaussian 的意思	2-1
我就是把 k 個 Gaussian 混在一起啦所以變成 k 個所以呢總共有這是第 k 個 Gaussian  mixture 第 k 個 mixture	2-1
然後第 j 個 state	2-1
然後呢如果是這樣的話我必須這個這個 weight 這個 c 的 j  k 是 b 的 j  k 的 weight	2-1
這些 weight 加起來要等於一	2-1
換句話說這個 distribution 我積分還是要等於一嘛	2-1
我本來一個 Gaussian 積分是一	2-1
我這邊如果是十個 Gaussian 積分不是等於是十了嗎	2-1
所以顯然不行我一定要我一定要全部積分仍然是一呀	2-1
所以顯然譬如說這個 Gaussian 的積分是零點一這個 Gaussian 我乘上 weight 是零點一	2-1
這個乘上 weight 是零點零五那個是零點二等等這樣加起來我積分才會是一嘛	2-1
所以呢我就必須要有所有的 weight 要 summation 等於一的這個條件	2-1
那這樣的話呢我的這個積分起來才會仍然是一嘛	2-1
好那麼我們這樣畫好像很簡單其實不對因為我這個只是一個 one  D 的 DISTRIBUTION	2-1
我現在不是我現在這個 o 是什麼 o 是一個很多 D 我們說三十九維的 vector	2-1
那這個的 Gaussian 是怎樣呢	2-1
這個有點複雜那麼我們用一個呃簡單的式子來寫也不簡單	2-1
那這個式子其實是你從前學機率的時候都學過的 Gaussian  distribution	2-2
那麼最上面的式子是一維的 Gaussian 那是你所熟悉的	2-2
底下呢變成 n 維 n 維的 Gaussian	2-2
你唸過的不過也許你忘了我們很快的複習一下	2-2
一維的 Gaussian 是你所熟悉的假設我這個是 x 他的 distribution 是一個這樣子的話	2-2
那麼這個是他的 mean 叫做小 m	2-2
那麼這邊他有一個他的 variance  sigma 是他的肥度看他有多肥	2-2
因此呢我的一維的 GAUSSIAN 寫起來就是這個樣子	2-2
那這個寫法呢這個大 X 表示說是我那個 random  VARIABLE  X 的符號	2-2
大 X 那個 random  variable 的符號就是這個 X	2-2
小 x 是他的這個真正的值	2-2
而這個值呢可以從負無限大到正無限大	2-2
不過他有一個他的他的 distribution 的 mean 是小 m	2-2
所以我得到一個這樣的關係我想這個式子是你所熟悉的	2-2
這是一個人的 variable 的時候	2-2
當我有一把 random  variable 的時候像那樣我們那邊有三十九個嘛喔我有這麼多	2-2
當我有一打一把 random  variable 的時候他們之間的關係我是可以寫成這樣一個這樣子的 vector	2-2
就變成一個 random  vector  random  variable 所構成的 vector 大 X	2-2
這個大 X	2-2
那麼我們真的寫起來應該是這個樣子 X  one  X  two 到 X 的 n	2-2
我這邊如果是有大我如果有小 n 個 random  variable 的話呢我總共有	2-2
我總共有小 n 個 random  variable 把他寫起來寫成一個 random  vector 所構成 random  variable 所構成的 vector	2-2
那麼這就是我們這邊所寫的 x	2-2
我這邊的小 t 是代表他的 transpose	2-2
那麼我的符號這裡有一點 confuse	2-2
因為剛才前一頁的 transpose 是大 T	2-2
抱歉有一點不清楚	2-2
我這邊是用大 T 代表 transpose	2-2
下一頁變成小 t 了你你你清楚就好啊這個就這個 transpose	2-2
當我寫成這樣之後我也可以寫成這整個 vector 的 distribution	2-2
那麼就像我寫成上面那一個一樣那我寫成底下這一個	2-2
因此呢我這個變成是我把這個一個 random  variable 的 x 寫成是整個 random  vector 大 X 就是這個東西的 distribution 就變成這樣	2-2
這個式子看起來有點複雜不過其實他長得跟上面那個是完全一樣只是這是 SCALAR  form 這是 vector  form	2-2
所以呢這裡每一個每一個 x 就變成他的值	2-2
那這裡面的每一個都可以有一個值	2-2
所以這個值呢變成這個 random  variable 值是這個 x  one	2-2
相當於這個 random  variable 的值這個一樣的意思嘛齁	2-2
那麼因此呢這個這個 x  one 的值是 x  one	2-2
這個 x  two 的值是 x  two	2-2
這個的值是 x  n 這些小的 x 是代表他的值	2-2
那麼因此呢我這個呢叫做小 x 他也是一個 vector	2-2
然後呢那就是我這邊寫的小 x 的 vector 的意思	2-2
然後呢那他的每一個 random  variable 他也有他的 mean	2-2
那這個 mean 呢就是 mean 的 x  one  mean 的 x  two 這一堆就是他的 mean	2-2
那寫起來呢就是我的這個 mean 的 x  one 這是這是指這個的 mean	2-2
對這個而言他有一個這個 mean 嘛對不對	2-2
對 x  two 也有一個這個東西他就是就是也是他的 mean 就是 mean 的 x  two	2-2
等等等等一直到 mean 的 x  n	2-2
那他構成了一個就是我們這邊所寫的這個 mean 的 vector	2-2
那這個也就是我這邊寫的這個式子我們真正講起來應該是這樣	2-2
那麼因此你會看到這上面的這個 x 減 m 的平方的這個地方本來是 x 減 m 的平方在這裡嘛	2-2
我現在就變成 x 的 vector 減這兩個 vector 相減就是就是他減他嘛他減他嘛	2-2
那也就是他減他他減他那就是每一個對每一個 random  variable 而言就是在算這個 x 減 m 是一樣的嘛	2-2
只不過我現在是怎麼樣呢是	2-2
你可以先如果我們先不看中間這個的話那這個只是他他跟他他的 transpose 是不是再乘上他因此呢他減他跟他相減的 transpose 變成一個橫的 vector	2-2
然後呢另外一個呢是縱的 vector	2-2
那這兩個相乘是什麼呢這兩個 vector 相乘就是 square 相加嘛	2-2
他跟他平方就是平方相加嘛對不對	2-2
所以呢這個就是所有的 mean 所有的這一項	2-2
對每一個 variable 而言相乘相加就是這個東西	2-2
那然後這裡要除以 sigma 平方怎麼辦	2-2
我現在不是 sigma 平方了我的 sigma 會變成一大堆 sigma  i  j 就是所謂的 covariance  matrix	2-2
換句話說呢我還有一個 matrix	2-2
這個 matrix 裡面的任何一個 element 叫做 sigma  i  j	2-2
是第 i 個跟第 j 個	2-2
那他是什麼呢他是這個	2-2
第 i 個第 i 個 random  variable 減掉他的 mean	2-2
以及第 j 個 random  variable 減掉他的 mean 的平均 expected  value	2-2
那你可以看到當 i 等於 j 的時候就是在對角線上的這些的話	2-2
就是我們原來的每一個那裡的 variance 對不對	2-2
我本來如果是一個的話如果是一個 random  variable 的話	2-2
他的這個 variance 是什麼就是這個 sigma 平方這個 variance 就是他跟他自己嘛	2-2
就是就是這兩個就是就是 x 減掉他的 mean 的平方的 expectation 嘛	2-2
對不對所以呢當 i 等於 j 的時候也就是這個 matrix 的對角線上其實就是分別每一個他自己自己的的那個 variance	2-2
或是那個 gaussian 的肥度	2-2
但是我現在除了在對角線以外我的任何一個位置都有	2-2
那那個是什麼呢是第 i 個第 j 個之間的關係	2-2
所以呢我就會有的 i 個跟第 j 個之間的關係那構成一個大的 matrix	2-2
那這個 matrix 呢我們叫做這個 sigma 的話	2-2
那麼於是呢我在原來在 skeleton 這裡是除以就除這個 square 除以這個 variance 平方就好了	2-2
我現在沒這麼複沒這麼簡單我現在是變成怎樣要乘上他的 inverse	2-2
所以這個除以就變成他的 inverse 就變成一個這樣的式子	2-2
所以你如果仔細看的話哦還有那這個是甚麼呢我們這邊是除以他的 square  root 嘛	2-2
這個 variance 平方再開 square  root 這個東西在這裡呢變成這個這是什麼這是那個 covariance 的 determinant	2-2
ok 就是這個 matrix 的 determinant 就是這個東西在這裡	2-2
那除了這個之外呢其實他是長的很像的你如果仔細看的話這裡的每一項分別對應到這邊的每一個 vector	2-2
只不過這邊是 scalar 這邊變成 vector 了	2-2
然後呢這邊變成 matrix 了	2-2
那原來這邊的 scalar 呢變成 determinant	2-2
然後這邊本來是 two  pi 的 square  root 現在變成他有 n 個了就這樣不同而已	2-2
那麼這些你如果覺得不熟悉的話那麼會建議你回去翻翻開你讀機率的時候的課本他一定會仔細的講這一段的	2-2
那麼我們也許舉一個例子來說會比較容易說	2-2
當我只有一個 DIMENSION 的時候這個是 x 我們剛才說這個是一個 gaussian	2-2
這是我的 mean 這是我的肥度就是我的 variance	2-2
這是你所熟悉的	2-2
如果說這樣一個 n  dimension 一看起來有一點太複雜你不太容易想像他是什麼的話	2-2
那麼最簡單的辦法通常如果太多了不容易想像我們最簡單的方法就是把他簡化變成只有兩個	2-2
如果 n 等於二的話只有兩個會怎樣那你可以想像他是這樣這是 x  one 這是 x  two	2-2
那麼在 x  one 的時候他有這樣的一個 distribution 假設他的 mean 在這裡	2-2
x  one 的話我們把他的 mean 畫在原點比較容易畫當然 mean 不一定要是原點	2-2
那麼這是 x  one 那麼然後呢這裡是 x  two 是另外一個 gaussian	2-2
ok 那麼於是呢這個是 x  two	2-2
那麼這個是 x  one	2-2
這樣你大概可以想樣他是這是這是是 two  D 的對不對	2-2
那麼在這個時候呢我我如果這樣的話看起來他們兩個肥度是一樣的	2-2
也就是說在 x  one 而言我在這上面看到的是他的 x  one 的肥度 sigma  one  one	2-2
那我也在這裡可以看到另外一個	2-2
是他跟他的呃是他跟他的	2-2
這個是 sigma  two  two 是指白色的那個的那個的肥度是 sigma  two  two  ok	2-2
那麼我們姑且現在看起來他們兩個是一樣的	2-2
你可以想像我的那個 sigma 是什麼呢	2-2
是 sigma  sigma  one  one 跟 sigma  two  two 是一樣的這兩個值相同我們不讓他不同我們讓他一樣	2-2
那麼如果是這樣的話	2-2
喔這個應該是有平方的	2-2
這樣子的話那然後呢我讓他的這個 ma 那這個時候的這個 matrix 很簡單這個 matrix 只有二乘二	2-2
這只有兩個嘛只有二乘二然後呢我讓他對角線以外這個都是零	2-2
這表示什麼意思呢表示他們這個相鄰的這個 i 跟 j 一跟二呢是 independent 他們 independent 所以呢這兩個是零這個時候就是長得像這樣	2-2
那就變成一個這樣的東西那這個我想這你比較容易想像這個就變成這樣子	2-2
等於說是兩個 gaussian 在 x 跟 x 各是高各是同樣長得完全一樣連肥度都完全一樣的兩個 gaussian	2-2
那麼就變成一個這樣的 distribution	2-2
那這樣的 distribution 也許我們簡單畫起來把它畫成這樣可不可以也可以	2-2
這是 x  one 這是 x  two 的時候呢他就是一個這個我畫等高線又是球形的對不對	2-2
呃你可以想像是一個這樣子的	2-2
那麼中間比較高外面比較	2-2
ok 那如果我畫這樣的話等於是說從上面來看那他就是一個這樣的東西	2-2
這是這個情形	2-2
好那我現在如果改變一下我讓他的這個變成是這兩個值不一樣了	2-2
譬如是 one  one 是比較小的 two  two 是比較大的會怎樣	2-2
那你知道呀我的意思就是說我的 one  one 的這個呢這個會變得很瘦	2-2
但是呢那個 two 的那個呢會變得比較肥對不對如果這個小這個大的話	2-2
那那個時候會是怎樣的意思呢	2-2
那個時候在這裡比較難畫在這裡就很清楚了那這裡的時候就變成這個是 x  one 這個是 x  two	2-2
我 x  one 變得很瘦 x  two 變得很肥的話就變成一個橢圓形	2-2
變成這樣子了	2-2
或者說呢就是我的這個 distribution 是長的	2-2
對不對那麼這個時候呢還就是這個是這邊比較瘦這邊比較寬	2-2
但基本上他們仍然是 independent 變成這樣子	2-2
好如果這樣子講你可以瞭解的話我現在再下一步我讓這兩個不是零	2-2
我現在第在第三個狀況呢我現在讓這兩個除了一一跟二二	2-2
這個小這個大之外我現在讓這個一二跟這個二一不是零	2-2
那是什麼意思當一二跟二一不是零的時候表示說他們之間是有 CORRELATION 的	2-2
他們之間的關係是存在的他不是 independent 了	2-2
那那個時候是什麼意思呢	2-2
那個時候如果我們要再畫其實就把這個圖把他歪過來那麼你就會變成一個像這樣的圖	2-2
這是 x  one 這是 x  two 他變成一個變成一個這樣子的了	2-2
這是什麼意思呢這個就是說我 x  one 跟 x  two 是有 CORRELATION 嘛	2-2
那麼我變成一個這樣子的橢圓	2-2
這個時候的意思是說譬如說我如果 x  one 的值在這裡的話我的 distribution 是這樣子的	2-2
可是我如果 x  one 在這裡的話我的 distribution 是在這裡的是不一樣的	2-2
我的 x  two 的 distribution 是 depend  on  x  one 的值會不同的	2-2
呃這是什麼意思呢	2-2
在這裡的話不會在這裡的話我在這個地方得到的也是這個 distribution 在這個地方得到的也是這個 distribution	2-2
是一樣的那麼你這邊看起來不太一樣只是因為我的要乘上這邊的關係	2-2
那這個是一樣的那我們打個比喻來講你就容易瞭解是什麼意思了	2-2
假設說這是這些 x 都是這個學生考試的成績列在教育教務處的成績股裡面的話	2-2
如果 x  one 是體育的成績 x  two 是微積分的成績這兩個基本上是沒有關係的	2-2
如果他體育成績很高很會打球不表示他的微積分會特別好或者特別不好	2-2
所以呢結果不管體育成績是八十分還是體育成績是七十七十分還是還是體育成績是九十分	2-2
他的微積分的成績永遠是一個這樣 distribution	2-2
但是呢如果說我的 x  one 是物理的成績 x  two 是微積分的成績的話	2-2
這兩個就有 CORRELATION 了	2-2
你如果把物理都考九十分的人都拿出來作個 distribution 的話	2-2
他的微積分的成績大概也是比較高的就是這個 distribution	2-2
反過來呢如果物理只有考二十分的人你都拿出來作個 distribution 的話呢	2-2
他的微積分的成績也是個 distribution 不過呢他在這裡	2-2
ok 那這兩個就是有 CORRELATION  ok	2-2
那麼因此呢你如果看這個物理的成績跟微積分的成績他們是有 CORRELATION 的所以呢他們的 gaussian 呢就是一個 y 的 gaussian	2-2
那因為這個高低跟這個高低是有關係的	2-2
那麼你如果是體育成績跟微積分成績呢就是沒有關係的所以他們就是就是一個正的	2-2
那麼這個關係他們之間的關係呢就表現在這裡那麼這個	2-2
就是他們的所謂的 CORRELATION	2-2
就這兩個之間的東西就是那個東西他們各自的關係啊	2-2
你如果喔沒有這麼清楚的話回去再去翻你的機率課本這就是他的 CORRELATION  coefficient	2-2
那他是在算他的這些東西	2-2
那當這兩個東西等於零的時候就表示他們沒有關係就好像體育跟微積分沒有關係一樣	2-2
於是呢這個時候他就變成長的像這樣子那這變成一個正的	2-2
好那如果這兩個 dimension 的狀況這樣子我們舉這些例子你大概瞭解這兩個 dimension 狀況是這個意思的話	2-2
我們再延伸為 n 個 dimension 是一樣的意思	2-2
那我們剛才說我們現在不是只有兩個 dimension	2-2
我們有幾個 dimension 我們剛才講我在這裡其實我的 o 是有很多 dimension	2-2
我們剛才講這個大 D 其實是三十九我有三十九維	2-2
三十九維不是那麼容易可以畫的那麼我們無法畫	2-2
不過呢你可以想像我們講的是一個三十九維的狀況所以呢我就變成是	2-2
三十九維我沒辦法畫我們只能畫出一個三維的來	2-2
但是但是呢假設這是三十九維的話呢我就是等於我有一堆 gaussian	2-2
這一堆 gaussian 的 distribution 加起來描述某一個 state 裡面的長相	2-2
那你可以想像譬如說這好比這是一個 gaussian 這是一個 gaussian 這是一個 gaussian	2-2
這一個個的 GAUSSIAN 就像我剛才畫在這邊的	2-2
如果一維的時候	2-2
我如果一維的話我說任何一個長相我沒關係我都可以把它畫成一堆 Gaussian 對不對	2-2
這是一個一個一個我把他看成是一堆就是了	2-2
一維我可以看成這樣	2-2
那 n 維怎麼辦呢也是一樣哪 n 維就看成一堆 gaussian 相加	2-2
不過每一個 gaussian 我我們剛才說呢你可以想像他是一個橢圓形的東西一個 n  dimension 裡面的一個橢圓	2-2
那麼他可以是歪來歪去的嘛因為他們之間可能互相有關所以他可能是歪來歪去一個橢圓	2-2
那如果這樣的話我的每一個個在 n 維空間裡面每一個 gaussian 是一個橢圓	2-2
那我現在有一把我這邊有一把 gaussian 大 M 個那這一把 gaussian 呢就東一個西一個那麼散成一團	2-1
那麼他們加在一起的話那這個呢那這就是我的譬如說 b  one 的 o	2-1
就是一個長成這樣的東西 ok	2-1
所以呢假設我的 b  one 的 o	2-1
我的在 state  one 的話是一個這樣子的東西	2-1
那在 state  two 的時候他就是不一樣就對了	2-1
state  two 的時候我可能是另外一堆譬如說這個 gaussian 是長這樣的這個 gaussian 長這樣的	2-1
那你可以想像這是一堆 gaussian 這是一個 gaussian 這是一個 gaussian	2-1
那麼可能還有這也是一個這也是一個啊等等	2-1
那這個呢就是 b  two  o	2-1
那換句話說呢	2-1
我們剛才講當你是在 state  one 跟 state  two 的時候我 really 並不知道這堆 o 跟這堆 o 會長怎樣	2-1
在這個 state 裡面這個 o 是任何一個可以長任何樣子	2-1
我只知道他有一個 distribution	2-1
那那個 distribution 是這樣的	2-1
同樣呢我如果知道這個	2-1
這堆東西在第二個 state 裡面的話呢那他是怎樣我仍然不知道他是可以任何一個長相	2-1
不過呢我知道他的 distribution 是這樣的	2-1
那現在第一個 state 跟第二個 state 有何不同呢	2-1
只是說這兩個 distribution 不一樣而已	2-1
ok 因此呢是有一個問題譬如說某一個 vector  o 四	2-1
到底是在一裡面還是二裡面我們其實不知道	2-1
因為我只有 observe 到這裡而已這是我的 observation	2-1
我只有 observe 到這個他並沒有告訴我誰在哪一個 state 裡面	2-1
那譬如說 o 四真的是在第二個裡面不在第一個裡面嗎我們不知道	2-1
我們只知道他的機率不一樣	2-1
o 四是某一個 vector	2-1
譬如說這裡的時候是掉在這個位置	2-1
在這裡的話是掉在這個位置	2-1
嗯你會發現如果是掉在這個位置的話呢他接近這個 gaussian 的 mean	2-1
所以他機率是滿大的	2-1
這個的話呢是在他的邊緣他機率是很小的	2-1
我只知道這個區別而已 ok	2-1
所以呢對這裡面任何一個 observation  o 是一個 vector 而言	2-1
他掉到任何一個地方去	2-1
他在哪一個 state 裡面其實是我們是不知道的	2-1
那麼我只知道我可以算機率那麼他在這裡的位置在這裡	2-1
他比較接近這個橢圓的 mean 所以他是機率是比較大的	2-1
我如果掉在這裡的話他只在某一個橢圓的邊緣它機率是比較小的我只知道這樣子而已	2-1
ok 那我就是用這堆機率來描述每個 state 不一樣	2-1
所以呢那你就可以想像我的真實狀況變成一群這樣子的 state	2-1
譬如說我是這是第一個 state 這是第二個 state 這是第三個 state 等等	2-1
這是什麼音這是什麼音這樣一路滑過來就是某一個聲音	2-1
那所不同的就是說他們的第一個 state 呢有他的一個 distribution	2-1
那麼第二個 state 呢有他的 distribution	2-1
是有不同的	2-1
第三個 state 也有他的 distribution	2-1
他們就是不一樣就是了	2-1
那這個就是所謂 b  one 的 o 這是所謂 b  two 的 o 這是 b 三的 o	2-1
ok 那這堆不同的 o 呢 b  one  b  b  j 的 o 呢就描述了這個 state 長他們會怎樣	2-1
ok 那就是我們底下這段所說的事情	2-1
那麼於是呢我現在就把所有這些 b  j 的 o 我用一個大 B 來代表	2-1
也就是說呢這些東西呢	2-1
我叫做大 B 叫做大 B	2-1
那我用大 B 也就是這一堆 distribution 來描述每一個 state 的聲音會長的怎樣	2-1
那然後呢大 A 是什麼呢大 A 是我們的剛才的 state  transition  PROBABILITY	2-1
這些 a  i  j 我們說他是一個 matrix 嘛	2-1
那這些 A  I  J 構成一個大 A 呢就代表我所有的 state  transition  PROBABILITY 所構成的集合就大 A	2-1
那除了這個大 A 大 B 之外我還有第三個參數叫 pi	2-1
pi 是什麼呢 pi 是 initial  PROBABILITY 就是 q  one 等於 i 的機率	2-1
q  one 是什麼 q  one 是第一個 observation 我的第一個 vector 會掉在哪一個 state 裡面	2-1
ok 這個 q  one 等於一就表示我的第一個 state	2-1
第一個 observation  o  one 掉在 state  one 裡面	2-1
q  one 等於二呢表示我一開始是從二開始的	2-1
q  one 等於三呢表示是從三開始的	2-1
所以呢那麼這個 q  one 等於 i 呢叫做 pi  i	2-1
換句話說就是我的整個的 observation 會從哪一個 state 開始跳	2-1
理論上我不見得需要從第一個 state 開始跳我可以從第三個開始跳我可以從第四個開始跳我可以從任何地方開始跳	2-1
看你從哪一個開始跳的機率是多少	2-1
所以呢現在 pi  one 就是我從第一個開始跳的機率 pi  two 就是我從第二個開始跳的機率等等	2-1
那這些個 pi  i 構成的集合呢叫做一個 pi	2-1
ok 於是呢我這三組參數加起來就構成我所說的一個 hidden  markov  model	2-1
那麼這個 A 跟這個 B 跟這個 pi	2-1
就構成我的一個我的 hidden  markov  model 我們叫做 lambda	2-1
這都是我們的簡寫因為這東西太多了	2-1
一個 hidden  markov  model 這樣的一個 model 有這麼一大把的參數	2-1
喔有一大把參數那麼因此呢我們就簡寫一個是大 A 一個是大 B 一個是 pi	2-1
那這個 pi 裡面呢	2-1
我們可以簡單說一下這個 pi 基本上應該	2-1
任何一個 pi  one  pi  two 都可以是不同的機率	2-1
對不對他們加起來是一嘛	2-1
你可以想像我可以是譬如說	2-1
我有零點五的機率在第一個 state 開始從第一個 state 開始跳	2-1
零點一的機率在第二個 state 開始跳	2-1
零點一零點一譬如說這樣子後面是零	2-1
對不對我有一半的機率從第一個開始跳另外有零點一的機率從第二個或第三個等等等等	2-1
這可以這樣子的這是我的 pi	2-1
所以這是 pi  one 這是 pi  two 等等	2-1
不過這樣有點太複雜了	2-1
就如我們剛才講我的 transition  PROBABILITY 這個 a  i  j 我後來簡化了	2-1
這裡我們通常也簡化	2-1
我們絕大多數可以就讓他從第一個開始跳不要那麼複雜	2-1
因此很多時候我們都簡化成為就是 pi  one 等於一後面全部都是零	2-1
我就一律讓他從第一個開始跳	2-1
這樣比較簡單	2-1
所以我們後面做的時候常常是把他作這個簡化	2-1
雖然 in  general 他們可以是任何一個樣子	2-1
好當我有了這些之後這是一個簡單的描述講 H  M  M 是什麼	2-3
那麼這裡面最大的特徵應該有兩個	2-3
第一個就是他的 state 是跳來跳去的	2-3
這個這些都是 random 的	2-3
我們並沒有規定他幾個 observation 之後會跳到哪一個 state 去沒有規定	2-3
他只是有一個機率會跳而已這是一個機率	2-3
他只是有一個機率會跳而已他沒有一定要怎麼跳法	2-3
所以這個是 random 的	2-3
那麼因此呢你假設這個這是一個零的 model 的話	2-3
零裡面到底那一個 s  tate 會怎麼跳我們其實是不知道的因為每一個人講的零都不一樣	2-3
同一個人講兩次你可以講零可以講零也可以講零都不一樣	2-3
所以呢我們說這個是一個 random 的	2-3
第二個 random 是說呢	2-3
我 given 在哪一個 state 裡面在這一個 state 裡面他的 observation 仍然不是固定的仍然是一個 distribution	2-3
這就是我們底下所說的雙重的	2-3
雙重的 double  layer  stochastic  process 他有雙重的這個這個隨機的特性	2-3
第一層就是我的 state 本身是 random  transition	2-3
為什麼 for  time  wrapping	2-3
這個 time  wrapping 我們剛才講過就是	2-3
我的發一個聲音的時候我每一個每一個音段的長短是很 random 的	2-3
就是說我們剛才講過你假設這個是 san  FRANCISCO	2-3
裡面的 s 到底有多長然後 fran 每一個音到底多長是不一定的	2-3
你如果把每一個對應到某一個 state 去的話	2-3
他的這個每一個每一個 event 每一個 acoustic  event 長短都是可伸可縮可長可短的	2-3
那因此我在時間軸上都每一個 event 都可以伸縮的	2-3
那這個特性就是所謂的這個這兩個字就是 time 的 wrapping 這兩個字的意思	2-3
那我就用那個 state  transition  PROBABILITY 來描述這現象	2-3
因此我的 state 本身是所謂的 hidden  state	2-3
什麼叫 hidden  state 你在想意思就是說我其實只 observe 到這個	2-3
我並不真的知道他在誰在那個 state 裡面我只是假設他在他這裡他在他這裡	2-3
但是其實譬如說 o 四 o 五一定要在這裡嗎	2-3
他不能在這裡嗎當然也可以我們剛才說只是機率不一樣而已	2-3
你可能把他放在這邊來的時候發現這個機率比較小	2-3
把他放到這邊機率比較大就這樣子而已	2-3
那麼我其實永遠不知道到底誰在哪個 state 裡面	2-3
所以這個 state 本身是隱藏起來的我其實沒有看到	2-3
那就是我們所謂 hidden  state 的意思這是第一層的 random	2-3
第二層的 random 是說呢即使你知道他是哪一個 state 的話	2-3
他真的會長怎麼樣還是不知道那就是我們剛才講的這件事	2-3
就是我們剛才講的	2-3
即使你知道他是在這裡面這個 o 一 o 二 o 三會長怎麼樣仍然不知道	2-3
我只知道他的一個 distribution	2-3
所以呢我我即使知道這個 state 我也只知道他長那樣	2-3
至於長那樣到底是怎樣會是哪一個我是不知道的	2-3
所以呢就是這邊講的即使是	2-3
知道哪一個 state 我其實仍然有一個 random 的 output 我是不知道	2-3
所以是雙重的 stochastic	2-3
那麼第一層是說我在這裡是 random 的第二層是說即使在那裡我這邊仍然是 random 的	2-3
好那麼在這個情形之下	2-3
那麼喔我們真正會怎麼作我們舉例來講	2-3
如果我要辨識零到九的十個聲音	2-3
我就是為零建一個 model 一建一個 model 每一個都建一個	2-3
怎麼建我要有夠多的 training  data 說零這個是零這個是零	2-3
我有夠多譬如說有十個人各唸十個零就有一百個零	2-3
把那一百個零拿來計算	2-3
然後可以求出這裡面所有的參數那那就是零的 model 的參數	2-3
有很多參數包括你的每一個 state 裡面的這個 A  I  J 的機率是多少	2-3
那這裡面 b 最多了因為我的每一個 gaussian 有他的 mean 有他的 covariance 對不對	2-3
我們舉例來講這個我每一個 gaussian	2-3
每一個 gaussian 我要算他的 mean	2-3
假設這一個 gaussian 的話他的 mean 在這裡	2-3
還有他的 covariance 對不對	2-3
covariance 可以看成是他的在每一個 dimension 他的肥度就相當於這個這種東西但是我現在有很多很多個	2-3
假設我現在是三十九維的話這個 n 是三十九	2-3
我的 mean 就有三十九個 mean	2-3
covariance 有多少個有三十九乘以三十九個	2-3
喔所以這裡非常多個參數這是一個 gaussian 然後我可以有一把 gaussian	2-3
所以呢這個參數非常多	2-3
我可以把一大堆的零拿來來 train 出零的零的所有的參數那就是這個零的 model 等等	2-3
那麼於是呢我怎麼作辨識	2-3
我如果有一大堆我可以為零建一個 model	2-3
我們現在一個一個 ma 一個 model 我們就用一個 LAMBDA 代表	2-3
零的 model 一的 model 二的 model  k 的 model	2-3
總共到九的 model	2-3
那這時候進來一個新的聲音某一個未知的聲音進來譬如說我們用就是像這種東西我們用一個大 O 來代表	2-3
或者是甚麼呃我這邊是用 ok 用大 O 來代表	2-3
假設我進了一個新的聲音大 O 的話他就是一堆 observation  vector	2-3
那我就算這個東西	2-3
算我這個大 O	2-3
如果 given 他是零的話機率是多少	2-3
如果他是 k 的話機率是多少	2-3
然後看誰的機率最大他就是誰	2-3
舉例來講我零有零的 model 一有一的 model 二有二的 model 八有八的 model 九有九的 model	2-3
今天我如果進來這個聲音是八的話	2-3
八的聲音放到零的 model 的話這個機率會很低我一樣可以放進去的	2-3
一樣可以放進去因為你永遠可以把前面的若干個 vector 放在第一個 state 裡面	2-3
這個仍然放在第二個 state 裡面這當然可以	2-3
問題是如果這個聲音是八那個 model 是零的話你放進去機率都不太對嘛	2-3
你就會都會放在一些一些 gaussian 的邊緣他的機率都很小	2-3
反過來如果那個 model 那個聲音是八	2-3
你放在八的 model 裡面的話呢那他們就會都對上於是我的機率就會比較大	2-3
於是你的就會分別就會掉在 gaussian 比較靠近中間的地方機率就會大	2-3
因此呢我現在一個未知聲音進來我就把他放在每一個 model 裡面	2-3
去算這個機率機率最大的那個就是我的答案齁	2-3
這是用 hidden  markov  model 來作辨識一個最簡單的解釋	2-3
因為這樣子的關係所以我們這邊就會有	2-3
一系列的三個 basic  problem 要解的	2-3
那麼這個詳細的解法我們事實上就會在下週以及下下週	2-3
的兩次上課裡面我們會講這些 problem 怎麼解	2-3
那這些 problem 不容易解因為你要把之前之前所有的數學通通用進來	2-3
齁那麼然後呢不過我們到時候就會發現其實也還好因為其實我們並不真的用數學解他	2-3
而是用 computer 去解他	2-3
我們到時候這三個 problem 都是變成一個 iteration 的程式	2-3
經過好幾個 iteration 之後答案就出來了啊	2-3
不過這個 iteration 的過程是用這些機率來算的就是了	2-3
那第一個 problem 就是我們剛才講的所謂的 evaluation  problem 就是在算這個機率	2-3
給我一個八的聲音那放在八的 model 裡面應該是機率最高的	2-3
放在五還是三的 model 裡面應該是機率是很低的	2-3
我用這個來作 recognition 齁所以這是第一個 problem	2-3
那這個怎麼算就是我們下週會講的怎麼算這個東西	2-3
第二個 decoding  problem 是說假設你這個聲音是八而且我這個 model 也是八的話	2-3
那到底我的 state  sequence 是什麼	2-3
到底哪幾個 vector 放在第一個 state 裡面哪幾個放在第二個裡面才是最合理的一個 state  sequence	2-3
ok 就是我們剛才講的這個問題那麼其實你永遠不知道他在哪裡他在哪裡	2-3
那麼因此呢你就只能夠這個這個找出一個比較好的說這些是他這些是他那就是我的 sequence	2-3
所以呢第二個 problem 就是說	2-3
你如果知道他是八而且這個是八的話那麼到底哪些放在哪個 state 裡面這是 decode  problem	2-3
第三個是 learning  problem 就是我怎麼 train 這個 model	2-3
假設說這個 model 是八	2-3
我也知道這個聲音是八了	2-3
這是一個新的聲音我知道他是八那我想把這個新的聲音的八 train 到這個裡面去讓這個學到他的聲音	2-3
所以呢我要想辦法作這件事情就是讓調這個調這個 lambda 讓我的這個新的八放進去之後機率能夠調到最大	2-3
也就是說讓我這個這個 lambda 裡面的所有參數學了這個新的聲音之後	2-3
他的參數會被調一調之後使的我這個新的聲音放進去之後會機率會變大	2-3
那這個呢就是所謂的 learning  problem 就我這個 model 要不斷的學習	2-3
根據新的聲音進來我要不斷的學習齁	2-3
喔那這是我們講的三個 basic  problem	2-3
ok 好我們先停在這邊休息十分鐘	2-3
OK 呃有一件事情要說一下我們需要討論補課的時間	2-3
不過我想我們等一下在十二點下課的時候我們再來討論呃	2-3
呃也就是說我們上週放掉一週我們整個進度 delay	2-3
那麼我發現下四月初還有一次所謂的溫書假還要放一次那呃	2-3
我其實如果我禮拜一知道我我如果第一週知道上週會停課的話我就會希望上週最好是可以是原時間原時原地補課啊	2-3
因為否則我們我們現在是是進度 delay 的很厲害然後呃本學期我還會出國兩次	2-3
所以會有兩週要停課所以我們的進度是有嚴重的問題	2-3
那呃所以我們需要找這個補課的時間不過我想我們在呃下十二點下課之後再來討論	2-3
我們先來講這邊的就是說我們說剛才有三個 basic  problem	2-3
這個詳細的我們從下週會講這兩個再下週會講這一個	2-3
那基本上就是要用剛才我們的這一堆數學的 formulation 來解這些 problem	2-3
所以呢你這些數學符號他的意思要弄清楚否則下週下下週你就會聽不進去了	2-3
那麼那我們到時候就會知道其實這些這些 problem 的 solution 都是 ITERATION 的程式就寫程式就可以了倒不需要寫的這麼詳細嗯	2-3
那麼我們再補充一件事情就是有有很多課本用這個比喻我覺得也不錯他就是說你可以想像一個狀況假設有三個桶子	2-3
三個或者五個桶都可以啦假設我有三個桶一桶二桶三	2-3
裡邊有一堆不同顏色的球	2-3
譬如說有這個紅的綠的黃的	2-3
譬如說這些有一些紅球有一些黃球有一些綠球阿白球好了等等	2-3
那麼你這三個桶子裡面的各自的紅綠白的數數目比例不一樣	2-3
因此你 random 裡面拿一個球的話拿紅色或者綠色或者黃色的白的球的話這三個桶的機率是不同的	2-3
那你今天如果有一個有一個幕把這全部擋起來	2-3
一個人躲在幕後他可以 random 選任何一個桶抽任何一個球然後告訴你說我的是紅的	2-3
然後待會兒呢之後他又可以再 random 再選另一個桶再抽另外一個球他說我這個是黃的	2-3
那麼於是呢你就會得到他聽到他說紅的黃的黃的綠的綠的紅的紅的黃的	2-3
但是呢你不知道他到底是從哪一個桶抽哪一個球出來	2-3
那這個比喻就是我們這邊講的這這個呃這 state 是完全一樣的那每一個桶就是一個 state	2-3
那所謂 hidden  state 就是說你其實是抽球的那個人躲在幕後你並不知道他是從哪一個桶裡面抽的球	2-3
然後呢他這次是抽這個桶下次是可以抽另外一個桶的你也不知道所以這就 state  one 相當於這邊的這三個 state	2-3
然後在每一個 state 裡面到底紅的綠的機率是多少你是不知道的那麼他們就是不一樣就對了	2-3
那就相當於我們這邊所說的他這每一個 state	2-3
這好像是三個桶他的每一個桶裡面的每一個就是不一樣就是了	2-3
那麼你並不知道他是從哪一個出來的今天你得到某一個得到某一個 O  T 的時候	2-3
他可以是在這裡也可以是在這裡也可以是在這裡只是機率不一樣而已就像你得到一個紅球他可以是從這裡出來從這裡出來從這裡出來是一樣的只是機率不同而已	2-3
那麼如果這個人可以 random 的隨便抓任何個桶來抽的話就相當於我們這邊講的這個有這個 random  state 的 transaction	2-3
ok 那然後呢即使他知道哪一個桶之後他仍然不知只知道是哪一個桶的話你仍然不知道會抽出哪一個桶來哪哪一種顏色球來就相當於我們說的這個 B  B 這個東西呢也是一個機率	2-3
那如果這樣子講的話你比較容易想像	2-3
這所謂的 Hidden  Markov  Models 的意思	2-3
什麼叫 hidden 就是他躲在幕後	2-3
他在幕後去做這些事情	2-3
你只知道紅	2-3
紅紅黃白白這樣你只你只知道這些事情	2-3
這是我的 observation  sequence  O  t	2-3
那我我 observe 到的是這個但是我並不知道這個幕後是怎樣的	2-3
在我們的整個 model 裡面你其實只只知道這個	2-3
你並不知道他到底是從哪個 state 在哪兒跳的你是看不到的對所以這些都是 hidden 的那這就是這個 H  M  M 的意思	2-3
好這一段講完我們再要講第二件事情就是怎麼樣找這些個 feature	2-4
那麼怎麼樣算這個 feature 我們之前大概說了一下就是我可以把	2-4
我用一個在移動的 window 那麼不斷取一段	2-4
譬如說兩百五十六兩百五十六點來算一個 feature	2-4
譬如說這個是 signal	2-4
我拿這一段譬如說兩百五十六點	2-4
我算出一個 feature 來	2-4
就是一系列的 feature  vector	2-4
我們說三十九個我叫做 O  one	2-4
那麼待會兒我再 shift 多少得到第二個我得到一個叫做 O  two 等等	2-4
那麼我們現在來講的是這怎麼做	2-4
怎麼樣 given 這兩百五十六點或五百一十二點我怎麼算這個東西	2-4
那這這就是我們所謂的 Front  end  Signal  Processing 就是最前端的做這件事情	2-4
然後呢就是我們所謂的 Feature  Extraction 抽這個 feature	2-4
這裡面要做的第一件事情就是所謂的 Pre  emphasis	2-4
喔要講這個東西呢我們就要說一下 time 跟 frequency 之間的關係	2-4
那我假設多數人都知道都了解 time 跟 frequency 之間的關係	2-4
但是也許有少數人並不見得 exactly 清楚所以我做一個非常簡單的解釋	2-4
我們知道在時間軸上	2-4
我們很習慣的在時間軸上來看一個 signal	2-4
譬如說這個是這樣子的	2-4
如果這個是 X  T 也好然後或者我在上面取 sample 就得到 X  N 也好	2-4
這個是 X  N	2-4
這是我們所謂的在時間軸上來呈現一個 signal 就是 time  domain 的	2-4
representation	2-4
那事實上還有另外一種 representation 方法呢是在 frequency 的軸上	2-4
譬如說這個是 omega 這是 frequency	2-4
所以我就有 frequency  domain 的 representation	2-4
那在這上面的話呢我任何的一個點	2-4
就像這上面任何的一個點代表某一個時間一樣	2-4
這裡面任何一個點所代表的是一個 frequency 譬如說 omega  one 這是一個 frequency	2-4
omega  two 代表另外一個 frequency	2-4
那他是什麼意思呢 omega  one 所代表的是某一個	2-4
譬如說 e 的 J  omega one T 的一個東西	2-4
omega  two 的話所代表的是另外一個 e 的 J  omega  two  T 的這個東西	2-4
那麼這些東西是什麼東西呢	2-4
我們簡單的解釋就是在這上面也會有一個	2-4
就像這上面會有一個一樣這上面也會有一個	2-4
那麼我如果把這個叫做 X 的 omega  one	2-4
就像這邊是譬如說這個是 T 的話這邊我有 X 的 T 是完全一樣的	2-4
這個是 T  one 的話這邊有 X  T  one 嘛	2-4
這邊如果 omega  one 我也有 X 的 omega  one	2-4
那這個東西呢我可以寫成譬如說 E 的呃我可以寫成譬如說他通常是一個複數是一個 complex  number A  one  E 的 j  phy  one	2-4
就是這個東西	2-4
那麼這是什麼玩意兒	2-4
那麼他有一個複數他是一個 complex  number 所以有一個 amplitude 有一個 phase	2-4
你如果不容易想像他是什麼東西的話呢就把他跟他兜在一起他其實就是這個東西的 coefficient	2-4
你就會得到譬如說 A  one  E 的 J  phy  one	2-4
就是這個東西乘上 E 的 J  omega  one  T	2-4
那麼這個東西	2-4
就是這個 frequency	2-4
就是這個這個 omega  one 代表的是這個 frequency E 的 J  omega  one  T 的這個東西	2-4
那他的大小呢是他的 coefficient 呢是這個 A  one  E 的 J  phy  one	2-4
你因此得到這個東西	2-4
這是什麼東西呢一個最簡單的想法就是我取他的 real  part	2-4
我如果取他的 real  part 的話就得到 A  one  cosine 的 omega  one  T 加上 phy  one	2-4
噢你這樣這樣你就了解了他只不過是一個 cosine	2-4
這個 cosine 的大小 A  one 就是剛才這邊的這個 A  one	2-4
他的 phace  phy  one 就是這邊的 phy  one	2-4
而他的 frequency  omega  one 就是這邊的 omega  one	2-4
ok 那同理我這邊也會有另外一個譬如說是	2-4
x 的 omega  two 在 omega  two 上面有 x 的 omega  two	2-4
他呢是 A  two 他也是一個 complex  number 是這個東西	2-4
那他其實跟這個兜在一起的話呢跟剛才一樣他所代表的呢是另外一個就是 A  two  cosine 的 omega  two  T 加上 phy  two 的一個這樣的 cosine	2-4
那這個 cosine 的大小是在這裡他的 phase 是在這裡他的 frequency 是在這裡等等	2-4
那麼於是呢我們真正可以做的事情是把它變成一個	2-4
complex 的 representation	2-4
我這邊的任何一點	2-4
任何一個 omega 三上面也都有一個	2-4
omega 四上面也有一個	2-4
他分別其實是都像這個一樣都是代表某一個 cosine	2-4
那麼於是呢我在這上面你可以想像成是一大把 cosine	2-4
這是一個這是一大把 cosine	2-4
呃這是一個 cosine	2-4
這是一個 cosine	2-4
這是一個 cosine 等等	2-4
那每一個 cosine 的大小	2-4
到底他的振幅有多大這個大小呢就由這個 A  one  A  two 決定了	2-4
那麼這個 phy  one  phy  two 是什麼呢是他這個 cosine 的相對於時間 T 零的位置	2-4
對不對你這個這個 cosine 後面這個的東西代表零在哪裡嘛	2-4
那如果你這個值不同的話這是這個 cosine 是前後移動	2-4
這個前後移動的位置是這個 phase	2-4
然後它到底震動的有多快就是這個 frequency	2-4
所以這三樣東西就決定了這這 cosine 裡面的三個參數這個大小它相對於零的位置以及它震動的多快	2-4
那麼我可以把這個 signal 拆解成為這一大堆的 cosine	2-4
那麼我這裡的每一點都代表一個 cosine 你把它加起來就是這一個	2-4
ok 因此我有兩種 representation	2-4
第一種是 time  domain  representation 我直接在 time 上來看每一個時間上它是有多少	2-4
我也可以是一個 frequency  domain  representation	2-4
那麼它是等於是說我在每一個 frequency 代表某一個 cosine 它的它的這個大小它的這個位置在哪裡	2-4
那麼這些這些 cosine 加起來應該會等於那一個	2-4
那這兩種 representation 之間的關係呢那就是我們所知道的所謂的 fourier  transform	2-4
那麼也包括你如果修別的課你學到所謂的 fast  fourier  transform  F  F  T	2-4
那我如果用程式來算通常是算這個東西那我有個快速演算法可以把這個算成這個這個算成這個呃那之間的關係就是這個	2-4
好我們簡單解釋這個 time 跟 frequency 之間的關係是這樣那我這邊馬上就要用到這裡	2-4
我們說我做第一件事情是所謂的 pre  emphasis	2-4
這事情是幹麻的其實只是一個這樣的過程而已	2-4
就是假設我原來的聲音這個叫做 X  pron 的 N 的話	2-4
我做的第一件事情就是把它做一個這樣子的運算	2-4
把 X  pron 的 N 變成 X 的 N	2-4
什麼運算就是這邊的這一個式子	2-4
很簡單的只是他減掉前一個然後乘上一個 A	2-4
通常這一個 A 是一個非常接近於一比一小一點點的一個值譬如說零點九六或者零點九七這樣子的一個數字	2-4
我這樣做之後得到了這個 A 那這個過程叫做 pre  emphasis	2-4
那麼熟悉 Z 的人各位之中如果有人對於 Z  transform 很熟的話就知道這個關係就可以寫成這個 Z 的關係	2-4
如果你不熟的話不知道也沒關係	2-4
那也就是說這個呢我把它寫成 h  of  z	2-4
這個 h  of  z 呢就是	2-4
我這邊寫的一減掉 A 這個負一	2-4
那其實這個 h  of  z 的這樣的意思其實就是底下這個式子的意思就是我做了這麼一個這個運算	2-4
那如果你並不了解這個為什麼是這樣的話沒什麼關係我們這只是一個符號而已	2-4
那麼這樣是幹什麼的	2-4
那你如果熟悉 Z  TRANSFORM 的人就了解你可以分析出來說它其實就是把高頻拉高的意思	2-4
那什麼意思呢就是我們剛才說我的我的 signal	2-4
我 always 可以作經過這個 fourier  TRANSFORM 把它轉到 frequency  domain 來看	2-4
看這個 frequency  domain 的 representation 來看每一個 frequency 的那個 cosine 長怎樣的話	2-4
那麼我來做的時候	2-4
我拿這個做剛才的那個 fourier  transform 我基本上會得到	2-4
像這樣子的	2-4
我的聲音基本上是越高頻的它就越低	2-4
越到了高頻它就會一直掉下來它會一直掉下來	2-4
因此呢越高頻的聲音基本上是越微弱我比較不容易分析出來那些東西	2-4
因為這樣子的關係所以呢這個 pre  emphasis 一個很簡單的目的是把它重新拉高	2-4
把這個地方呢把它拉上去	2-4
讓他們這些高頻的部分都往上拉	2-4
越是高頻掉的越多我就越拉的越多讓它基本上比較平一點	2-4
這樣高頻的聲音也夠	2-4
夠強到我可以分析的出來 ok 這個是 pre  emphasis 最基本的意思是這樣子解釋	2-4
ya 不是	2-4
很多很多很多聲音是很高頻但是很重要的譬如說嘶	2-4
這是非常高頻的像雜訊一樣你講的也沒錯但是我們聲音裡面沙的 sh 撒的 s 都是非常高頻的	2-4
ok 是很像你講的雜訊但是都是我們聲音很重要的部分 ok	2-4
好那麼所以這個是講這個 pre  emphasis 的部分	2-4
之後呢再下一件事要說的就是 End  point  Detection	2-4
也就是說我們其實我們的聲音裡面	2-4
充滿了沒有說話的部分	2-4
當我講一句話的時候從某一個某一段時間開始講	2-4
前面沒有講	2-4
那麼你不要以為一定是這樣的乾乾淨淨的其實沒有講的部分呢它也一樣有東西這就是什麼呢就是剛才你講的雜訊	2-4
那麼通常	2-4
雜訊是 everywhere 都存在的它永遠有的當你有有聲音的時候其實雜訊是加在這上面	2-4
但是當你沒有聲音的時候它也是有的	2-4
因此就 computer 而言就你的那個 speech  processor 部分而言它怎麼知道哪裡是聲音哪裡是雜訊呢	2-4
我們需要有辦法知道 ok 從這裡以後是有聲音之前都是雜訊那我盡可能不要把這些東西拿來算	2-4
你可以想像如果我不知道這一步我沒有做這一步的話	2-4
我會把一堆雜訊都拿來當成是某一個聲音我去辨識它是什麼聲音我可以辨識出來一堆東西不過那一堆東西是錯的那這樣會非常複雜所以我需要做這件事情把它切開	2-4
切出來哪裡是真正聲音的 ENDPOINT	2-4
或者說做這個這個我們叫做 SPEECH 這個我們叫做 silence  silence 不是真的 silence 是 noisy 的 silence	2-4
所以呢我要做這個 speech 跟 silence 的 discrimination 所以我需要做這件事情	2-4
那這個問題其實是很難的	2-4
那麼但是用處是非常多的就以你今天	2-4
打電話而言那你知道你跟你的跟你的朋友用手機通電話的時候	2-4
你們兩個在講話其實是等於是	2-4
用了兩個線等於是用了兩條線	2-4
當你在說話的時候你的朋友在聽	2-4
當你說完的時候他開始講話	2-4
他講的時候你你是聽的	2-4
除非你們兩個在吵架	2-4
那麼否則的話呢	2-4
他講完你再講	2-4
所以其實你說話的時間只有這些	2-4
他說話的時間只有這些	2-4
基本上來講一個人說話的時間不到一半不到百分之五十	2-4
那這些部分是什麼這些部分都是 silence 或者說都是 noisy	2-4
那你如果有本領把它切出來的話那這些我都不要送	2-4
我只傳這個我只傳這個這樣我才節省我的 bits  per  second	2-4
我如果這些東西都當成是語音在那邊傳送的話我用了很多 bits 傳的都是 noise	2-4
那這些這個所以這個是用途在很多地方都有用的東西	2-4
問題是怎麼做這件事	2-4
因為他根本跟我們的聲音是看起來是很像的這就是所謂的 Endpoint  Detection	2-4
那我這邊講的是一個最基本簡單的方法就是算這個 short  time  energy	2-4
那麼所謂 short  short  time  energy 你看這個式子就了解了	2-4
我只是把每一個 sample 平方	2-4
然後讓他加到某一個 window 裡面	2-4
這個 window 最常用的 window 就是	2-4
不是最常用最簡單的一個 window 就是長方形的	2-4
也就是說	2-4
也就是說我現在是一個	2-4
零到 L 減一是一別的地方是零	2-4
對不對這樣子這個是 window 的 N	2-4
阿我這邊寫 M 也可以這個是 M window 的 M	2-4
這個是一個最容易想像的最簡單的長方形的 window  window 是 M	2-4
如果是這樣的話呢你看這意思不外乎只是說	2-4
我把我的我只算一個 window 裡面的 square 加起來嘛	2-4
那那在這個式子而言你可以想像我是把那 window 放在 N 的位置上	2-4
就得到時間 N 的這個 short  time  energy	2-4
譬如說在這裡而言我的這個橫軸是 M	2-4
然後呢假設假設這一點是 N	2-4
我就在這邊放了一個 window	2-4
我就把這裡面的每一個 sample 的平方加起來	2-4
只加這個 window 而已嘛這外面都是零嘛對不對於是我就得到一個點得到某一個點叫做 E  N	2-4
這個就是我的 E  N 對不對	2-4
我我是把我這個 window 放在這個這個零的位置放在 N 的位置	2-4
我就得到一個這個 window 然後我把這裡面的每一個點分別把它平方	2-4
平方加起來我就得到這個 E  N	2-4
然後我現在把這個 window 不斷的移動我這個 E  N 就變成一個一個數值	2-4
那你可以想像我這個 E  N 大約是這樣子	2-4
當我 window 移到這兒來的時候開始有這個訊號比較大的話他就會比較上來	2-4
E  N 就會慢慢的上來	2-4
那這就是我的 E  N	2-4
這 short  time  energy  as  a  function  of  time	2-4
那我這樣的 window 不斷移動我的 E  N 不斷大起來我可以定某一個 threshold	2-4
譬如說呢當我超過這裡的時候這是我的 threshold	2-4
我就假設從這裡開始是語音	2-4
這是一個非常簡單的方法	2-4
那麼他的基本的精神是假設說	2-4
畢竟你講話的聲音應該比 noise 大一些	2-4
所以呢你你如果講的聲音也跟 noise 一樣大的話那當然就 noise 跟聲音混在一起你就不知道了嘛	2-4
所以你講講的聲音 suppose 你的語音要比 noise 要大一些所以我就不斷的求這個 short  time  energy	2-4
然後我這個 E  N 不斷的 shift	2-4
當我超過某個 threshold 的時候我就認為是開始講話了	2-4
這是一個最簡單的方法即使是這個方法呢我仍然需要 depends  on 這個 noise 有多大	2-4
你可以想像我在教室裡面我如果開了冷氣或者什麼或者是	2-4
背景雜訊比較厲害的話這個比較大的話	2-4
我的 threshold 要不一樣	2-4
我的 threshold 可能是隨時要調的	2-4
那這個時候的情形很可能是	2-4
你在還沒開始講話一開機還沒開始說話的時候先收一次	2-4
當時的 noise 狀況算一個當時的 E  N 然後用那個來 define 一個 threshold	2-4
於是呢我的 threshold 超過那個我就算是我有在說話等等	2-4
所以這就是我這邊所謂的 adaptive  threshold 你就是要隨時	2-4
在一開機還沒說話之前隨時收當時的雜訊算當時的 background  noise 的 E  N 是多少然後定那個東西等等	2-4
那即使是這樣做那個方法顯然不夠好	2-4
那麼那麼你可以想像出來我在很吵雜的環境	2-4
我在街上走的時候我打的手機	2-4
這個街上的雜訊跟我講的話幾乎是一樣大聲的話我怎麼辦等等顯然不是這麼容易	2-4
所以這只是一個非常簡單的 example 說明這一類的方法那真的做法顯然要比這個複雜的多	2-4
那這個我們在有個專有名詞叫做 V  A  D 就是 VOICE  activity  detection	2-4
這是所謂的 V  A  D 那麼也就是說	2-4
我其實隨時要去自動 detect 什麼時候是 VOICE 的	2-4
active 譬如說只有這一段這一段是 active 那這一中間這一段呢是沒有 active 我要能夠抓的出來	2-4
那一直到今天這仍然是一個非常重要的研究的課題	2-4
那麼一大堆 paper 在在做這個因為你到底怎麼做的好	2-4
尤其在一個非常吵的環境在路上在街上在地鐵裡面在這個	2-4
雞尾酒會裡面在餐廳裡面那麼各有不同的狀況你如何做這件事情這是所謂的 Endpoint  detection	2-4
那麼當我做到這點之後 ok 那再來就比較好了	2-4
我再來我就可以在這裡面從這裡面來做我取一個 window 我再來算我的那些參數	2-5
那這裡我們順便說一下利用這個機會我們就講一下這個要	2-5
剛才這個的 short  time  energy 其實是一個例子在說明做 window 的過程	2-5
那我的這個這個式子是一個 general  form 其實就是底下這個式子的一個 general  form 底下這個它的 special  case	2-5
你看我我我如果把這裡的話呢我把這個 T 的是某一種 operator	2-5
等於說我對這個 signal 操作某一個 operator 之後乘上一個 window 然後加起來得到我的某一個參數	2-5
這是一個 general  form 這是一個 special  case	2-5
當我這個 operator 只是一個平方的時候我所得到 Q  N 就是 short  time  energy	2-5
但是呢我這個可以是更複雜的 operation 我就得到更多的東西	2-5
就像我們後面所講的我現在要	2-5
取一個 window 之後經過一堆演演算得到它的 feature  vector 的話也是一樣的意思	2-5
我也是這個求一個 window 之後在 window 上面做一堆 operation 之後得到我我所要的參數是一樣的	2-5
那這裡面我們最簡單容易了解的 window 是長方形就是我們剛講的這一個	2-5
只不過長方形的 window 其實不好	2-5
我們最常用的不是長方形而是這一個所謂的 hamming	2-5
這個 hamming 是怎樣的呢 hamming 不是這樣而是一個 cosine 的形狀	2-5
它是在這個裡面是是這樣	2-5
那麼從零到 L 減一其他都是零	2-5
或者說跟長方形比起來他就是	2-5
他的兩邊很小中間很大是一個 cosine	2-5
你如果把它這兩邊都擦掉的話	2-5
就變成 hamming hamming 是一個這樣子的東西這個數學式子寫成這樣	2-5
那麼為什麼要變成這樣這這個關係比較複雜要解釋 hamming 的什麼要用 hamming 的原因我們會在七點零的時候會再仔仔細說 hamming	2-5
跟 Rectangular 之間的關係	2-5
我我們這邊可以做一個非常簡單的解釋	2-5
這個解釋比較容易想像	2-5
沒有太多學理	2-5
那麼假設說我的因為我的語音很可能是這樣子我們舉一個例子哼	2-5
我們後面會看到語音常常長這樣那麼	2-5
如果是這樣的話	2-5
我用 Rectangular 來求的時候呢我這個 window 可能算到這裡	2-5
在這裡面算的某某一個譬如說 short  time  energy	2-5
待會兒當我的 window 搬到這來的時候	2-5
會怎樣	2-5
我一下少掉一個非常高的 peak 的很多東西但是多的是一些很小的值	2-5
因此從這裡到這裡會變化非常大	2-5
然後我如果再過來一點的話呢	2-5
我又丟掉了一堆這邊很大的東西多了一堆是很小的東西	2-5
因此我這樣子的話我得到的那個值會會 fluctuate 非常厲害	2-5
不像我這邊畫的這麼 smooth	2-5
而很可能是非常不 smooth 的	2-5
會變成這樣子非常不 smooth	2-5
為什麼非常不 smooth 就是我們剛才講的原因因為我其實	2-5
我這裡邊變化非常大很可能我向前移動一下的時候丟掉的是的值很大增加的值很小	2-5
或者你在移動過來的話我增加的值很大丟掉的值很小所以我這個變化非常多	2-5
那如何如何那如果它這個 fluctuate 非常厲害的話	2-5
我其實不容易判斷到底那個是什麼就很難講了我其實比較希望他 smooth 嘛	2-5
那它如何那它 smooth 呢一個簡單的辦法就是我不要讓它從頭到尾一樣 weight	2-5
而我把它兩邊 weight 降低我主要算中間的	2-5
那這樣的話是不是就會比較好呢當然會好很多對不對因為我這兩邊我都我都都拿掉了嘛	2-5
所以呢當我搬過來的時候我我丟掉的東西本來就很少	2-5
我增加的東西也就很少嘛我主要是以中間這堆東西為主	2-5
那中間這堆變化比較少嘛就比較 smooth 嘛	2-5
我們可以用這樣子簡單的理由來說	2-5
那詳細其實它是有學理的關係的不過那個我們留到七點零再講	2-5
我們現在先姑且用這個方法來解釋那大致的意思就是我們比較喜歡用的其實是 win 其實是像 hamming 這樣子的 window 而不是一個長方形的 window	2-5
好那有了這個之後再來是怎麼做	2-5
再來就變成說	2-5
我們剛才講我第一步先做了這個 pre  emphasis	2-5
把這個高頻的部分拉高了以後	2-5
那第二步我現在把這個呃呃哪裡是 noise 切掉了我知道這堆是聲音了	2-5
第三步呢我現在就可以取取一個 window 做一個 hamming  window 我就一段就出來了之後我這一段怎麼算東西呢	2-5
我我這一段這個我這個 hamming 加在這裡嘛對不對	2-5
我的這個我這個 hamming 加在加在這裡譬如說我從這裡加一個 hamming 過來	2-5
我得到這一些點假設這個 hamming 是 L 是兩百五十六或者五百一十二我就得到五百一十二點這個時候我可以拿來怎麼做呢	2-5
那麼第一件事情所謂的 discrete  fourier  transform 這就是我們剛才說的 fourier  transform	2-5
也就是我把 time  domain 轉到 frequency 上的 representation 去	2-5
於是我得到一堆 frequency 上面的 representation 像這樣像這樣嗯是在 frequency 上面的	2-5
那為什麼要這樣子做	2-5
那也是一個很簡單的原因是因為我們人的耳朵是聽 frequency 的	2-5
我們剛才說 ok 我們得到一個像這樣的東西	2-5
這是 frequency	2-5
那麼根據嗯心理學聽研研究人的認知醫學研究人的聽覺的人的研究那麼人的耳朵聽的是 frequency	2-5
我們本來就是聽 frequency 的所以呢嗯我們就要學人怎麼做的人是這樣做的	2-5
所以呢舉例來講	2-5
人的聽覺這個據他們的研究結果是	2-5
一組一組的聽覺神經分別在管一堆一堆的 frequency	2-5
譬如說這堆 frequency 是某一組聽覺神經在管的	2-5
下一堆是另外一組聽覺神經在管的	2-5
再過來是另外一組聽覺神經在管的	2-5
他們每一組在管裡面一堆	2-5
而這一堆呢彼此是 over  lap 的	2-5
那麼換句話說今天如果有兩個 frequency 都在這個裡面的話我們聽是聽不出來他們有區別	2-5
可是如果一個在這裡一個在這裡的話呢我們會聽出來兩個不同的聲音	2-5
ok 那麼一個在這裡一個在這裡我聽出兩個不同的聲音如果兩個都在這裡我是聽不出來他們有什麼區別的嗯有這樣的關係	2-5
然後呢在這些 over  lap 的地方呢是這一組也聽到這一組也聽到的	2-5
那麼如何去去做一個這樣子的事情呢	2-5
那麼早年的 engineer 想了很多辦法之後他們想的辦法就是所謂的這邊講的這個 filter  bank	2-5
它的意思就等於是說 ok 既然是這樣的話	2-5
你可以想像其實	2-5
我們是 ok 這一堆聽覺神經這一組它聽的這一堆 frequency 之後呢它有一個訊息	2-5
送到大腦去	2-5
那這一堆的呢它也聽到了之後有一個訊息	2-5
送到大腦裡面的聽覺的部分等等這有另外一個	2-5
既然如此我們也來做這一件事	2-5
那譬如說呢	2-5
這一堆呢你可以想像也許就是從這裡到這裡吧	2-5
我想辦法把這一些個 frequency	2-5
這一些個 frequency 的 signal 我把它加乘起來變成某一個訊號	2-5
拿來用	2-5
那這一堆呢我也加乘起來	2-5
譬如說這個我我加乘起來當成當成某一個訊號來用等等等等	2-5
那這個怎麼做呢就是你就取這當你做了做了這個	2-5
當你做了這個 discrete  fourier  transform 的時候你就會得到像這樣子的東西嘛	2-5
你就可以把這堆 frequency 加起來把這堆 frequency 加起來等等	2-5
那這個的這個的功能就是所謂的 filter  bank	2-5
就是一個一個的 filter	2-5
那它的做法呢就變成像這樣	2-5
就是我我想辦法取這一堆 frequency	2-5
那麼分別乘上這些值對不對	2-5
我取這分別乘上這些值把它加起來	2-5
可是如果是這樣的話呢這個 engineer 覺得很困擾	2-5
因為這樣子的話那這個呢要加這個對不對	2-5
那難道說這些 frequency 聽兩次呀	2-5
那我們真的聽兩次呀在這邊聽到在這邊聽到呀	2-5
難道他們聽兩次嗎這好像有點怪怪的	2-5
那麼後來他們就想了一個辦法	2-5
這個辦法其實沒有什麼特別道理這只是一個一個一個非常 ENGINEERING 的想法	2-5
它就說那這樣子吧我們把它變成三角形	2-5
這個呢把它變成另外一個三角形	2-5
那麼也就是說呢我現在的這個這個呢	2-5
這是所謂的 filter 就是我只取這一段的 frequency	2-5
但是呢我這個是乘它這個是乘它這個是乘它因此呢在三角形的中間的地方我 weight 最大	2-5
兩邊我就 weight 比較小為什麼兩邊 weight 比較小呢因為這邊會有另外一組會聽到它嘛	2-5
那麼因此在中間這些 frequency 呢兩邊都聽到	2-5
它應該是兩邊聽到的比較弱好像比較合理	2-5
其實這一點並沒有真正的科學根據只是他們是很 ARBITRARY 這樣畫一個三角形	2-5
那麼因此呢我等於說是在這個 frequency 這一堆 frequency 裡面我讓中間 weight 最大	2-5
距離中間越來越遠的話呢 weight 越來越小所於他們這邊分別乘上這邊 weight 加起來才是那個 signal	2-5
那這樣之後我這個才是一個 signal 送出去拿去計算	2-5
同樣這是另外一個 signal 拿去計算	2-5
那用這個方式的話呢表示說我在兩邊的 frequency 被 weight 比較小	2-5
我離越靠近邊緣 weight 越小	2-5
那這樣的話呢我這兩個三角形這樣加起來好像比較合理嗯	2-5
表示說這樣中間雖然被聽被聽兩次了	2-5
可是呢左邊也聽的比較小聲一點右邊也聽的比較小聲一點用用這樣來看	2-5
那如果是這樣的話呢我們就得到一系列的	2-5
這每一個就是所謂的 filter	2-5
就是我只取一個	2-5
我只取一個這個	2-5
這一這一堆 frequency 裡面的訊號一個 filter 的意思	2-5
因次我就得到這樣的一個一個的三角形的 filter	2-5
那這樣子呢就是所謂的三角形的 filter  bank	2-5
那這個就是我們這邊講的 triangular  shape  in  frequency 然後互相 overlapped 意思嗯	2-5
ok 我的每一個 filter 是三角形的然後讓它互相 overlap	2-5
其實這個三角形是沒有什麼太大道理喔我們後來知道其實它不見得應該是這樣	2-5
不過呢這是一個簡單的做法那就是非常 engineering 的想法就把它變成這樣子	2-5
然後呢它是 uniformly  spaced  below 一個 kilo  H  Z	2-5
在這個一個 kilo  H  Z 之下	2-5
這個也是根據人的聽覺來的	2-5
一個一個 kilo  H  Z 這些低頻是我們最多的聲音的特徵在這裡	2-5
所以呢我們要把它做的最細密嗯所以呢是 uniformly  DISTRIBUTION 在這個地方	2-5
可是呢到了一個 kilo  H  Z 以上的話我就不再是 uniform 而是以 log 的 scale	2-5
所謂的 log  scale 是說	2-5
畫大一點就會變成這樣	2-5
我在一個 kilo  H  Z 以下是一個 uniform 的	2-5
1 kilo H Z	2-5
在這上面的話我會變成 log 的越來越大越來越大嗯	2-5
我會我會以 log 的 scale 它越變越大越變越大到了高頻的時候我的	2-5
那事實上這個也是跟這個聽覺神經有關的	2-5
也就是說你越到高頻的時候你那一組聽覺神經是聽的越多 frequency 一起聽的	2-5
嗯那麼那麼因此呢我們就會變成一個 log  scale 的關係在高頻就變成 log  scale	2-5
那這一點其實你也可以解讀成為另外一個現象就是我們人的聽覺	2-5
本來聽 frequency 的高低就是聽它的 log 的	2-5
那一個最直接的	2-5
事實就是你如果學音樂你就知道	2-5
do  re  mi  FA  so  la  si	2-5
這是 do 這是 re	2-5
mi 這中間距離是完全一樣的	2-5
你可以知道這個是這是 do 這是 re 這是換一個調而已這是一樣的	2-5
那 mi 跟 FAR 是半音中間是這距離是它的一半	2-5
然後呢 so  la  si 跟下一個 do	2-5
這兩個是半音就是 mi 跟 F  A 之間跟 si 跟 do 之間是半音它的距離是其他的一半其他距離是相同的	2-5
那麼到這裡的時候	2-5
那這個是什麼 scale 這個就是 log 的 frequency	2-5
你如果是以你如果是以 它的 frequency 的 log 來看的話他們剛好就是這個等距的關係	2-5
那麼我們耳朵聽起來它就是一個這樣的關係	2-5
那其實這就是在 log  scale 上面嗯	2-5
所以其實我們的聽覺對於 frequency 感覺本來就是在我們所講的那個	2-5
喔 cosine 的那個 frequency 取了 log 之後就是我們聽到的	2-5
那你知道從這裡到這裡是變成多提高一倍嘛	2-5
這個到這邊提高一倍	2-5
然後再高八度就是提高一倍嘛差八度就是差一半的意思嘛	2-5
嗯那這個本來就是一個這樣子的這個	2-5
所以呢我們人的聽覺本來就是一個 log	2-5
那麼因此呢在高頻的時候你用 log 的 frequency 來做這樣的三角形是非常合理的	2-5
那在低頻的時候是因為我們的聲音非常大部分的區別在這裡	2-5
喔最主要的這個特徵都在這裡所以這地方我們做比較細膩一點是做 uniform 的	2-5
這個是這樣我們就得到一組這個所謂的 filter  bank	2-5
filter  bank 就是一系列的 filter	2-5
所謂的 Mel  scale 的意思 Mel  scale 的意思就是	2-5
這個詳細有定義不過我們簡單的講就是在高頻都是以 log 方式增加上去	2-5
低頻是 uniform 的	2-5
好這個就是我們做的這個 log  scale 的呃 mel  SCALE 的這個 filter  bank	2-5
再下來呢我們就每一個裡面得到一個就好像	2-5
這個聽覺神經得到一組訊號送到大腦去一樣我們就得到一組訊號	2-5
這個呢我們把它取絕對值取 energy 取 log 平方嗯	2-5
這個沒什麼特別	2-5
都有原因的我們後面會再講我們大概是這樣子你大概了解就是等於是我取的一個個的訊號然後拿來去做後面的分析	2-5
至於後面為什麼做這個 inverse  discrete  fourier  transform	2-5
這比較複雜我們留到後面再說在七點零我們會詳細的說	2-5
那這個東西我們也詳細的後面後面再說	2-5
那這樣所得到的東西呢呃我們取一個名字叫做 mel  Frequency  Cepstral  Coefficient	2-5
那這個名字怎麼來的我們後面也在後面再說好了嗯	2-5
那基本上我們取它叫做 M  F  C  C 這是我們後面所習慣的名字	2-5
那麼呃詳細的有一堆理由有一堆我們在七點零會詳細的說這個 Front  end  Signal  Front  end 我們會講這些東西	2-5
那基本上這樣做之後我們得到的是十三個參數叫做 M  F  C  C	2-5
這樣是十三個	2-5
欸那我們剛才說有三十九維呀怎麼只有有三十九怎麼只有十三個還有二十六個就是所謂的 first 跟 second  order 的 difference	2-5
也就是他們的微分	2-5
那麼換句話說呢	2-5
我如果照剛才那樣子做的話	2-5
我這樣子得到一個	2-5
我現在是十三維	2-5
十三個這樣子得到一個也是十三個	2-5
這樣子得到一個也是十三個	2-5
之後呢我再來呢就是做他們的 first  difference	2-5
這個 first  difference 最簡單的做法是兩兩相減	2-5
雖然你可以做更複雜更複雜我們也是後面會講	2-5
它減它得到它它減它得到它對不對它減它得到它它減它得到它等等我就可以得到底下的十三維	2-5
就是所謂的 first  difference  ok	2-5
它減它得到它它減它得到它它減它得到它它減它得到它等等的話呢其實這十三個	2-5
那底下的十三個等於是它的微分的意思	2-5
那同理我還可以它減它得到它我還可以再底下第兩次微分	2-5
它減它得到它它減它得到它那麼它減它得到它它減它得到它等等我又可以底下	2-5
這樣子因此我就有原始的我用剛才的方法得到的是原始的 M  F  C  C	2-5
就是 Mel  Frequency  Cepstral  Coefficients 這個名字我們後面都會解釋我們現在姑且就偷懶一點就大概了解是這麼回事兒就好我們不要花太多時間來說它我們在七點零的時候會仔細的講嗯	2-5
那麼這邊所得到 M  F  C  C 是十三個在這裡	2-5
然後我就有十三個是它的一次微分 first  difference	2-5
然後再有十三個它的兩次微分就是 first  difference 的 first  difference 就是 second  difference	2-5
那就是這邊講的 first 跟 second  order 的 difference	2-5
你可以想像他們就像是微分一樣的意思	2-5
那事實上呢還可以做的更精細我們在七點零會說呢	2-5
你不一定是要它減它你還可以做更精細的做法不過我想這些都留到後面嗯我們後面再講	2-5
同樣呢在這邊還有講到譬如說 Pitch	2-5
這些音高呀什麼這些東西我們留到後面再說	2-5
那總之我們到這裡的我們大概可以看到	2-5
基本上這些是這樣出來的	2-5
那詳細情形後面再說那這段就是我們講的 Feature  Extraction	2-5
那有了 Feature  Extraction 之後呢我們再來就可以進入後面的這個 Language  Model	2-5
那麼我們在這裡休息十分鐘	2-5
第三部份就是 language  model	2-6
我們在第一週的時候曾經說過一下	2-6
也就是說並不是我們所想的那麼容易	2-6
而是很多聲音都很像	2-6
然後很多聲音都會搞不清楚	2-6
我們舉例來講	2-6
這個假設這是時間你這個一個位置的聲音進來的時候	2-6
我們可以把裡面的很多的基本的單位音都變成一個 Hidden  Markov  Model	2-6
記得我們那時候說譬如說	2-6
this  is 你可以想成是這個這是一個基本單位音	2-6
這是一個這三個拼成這個	2-6
然後呢再來恩這個可能是這個	2-6
this  is 等等	2-6
那也就是把每一個基本的單位音都做成我們剛才講的 Hidden  Markov  Model	2-6
於是呢你可以判斷說那裡有一個這個音那裡有一個這個音這裡有一個這個音	2-6
然後呢這些音呢那那於是呢這些個東西呢拼起來呢是這個字這些個音拼起來呢是這個字等等	2-6
但事實上不是這麼簡單因為這個音也跟這個音很像這個音也跟這個音很像	2-6
於是呢你這三個可能都很像而這個呢這個跟這個音也很像這個跟這個音也很像	2-6
譬如說那麼你都可以拼來拼去變成很多不同的字出來	2-6
那我們舉例來講譬如說這個我們說一句話譬如說 the  computer  is  listening	2-6
基本上你講的是這句話	2-6
但是呢你很可能因為變成這裡面是很多小的單位音在裡面拼嘛	2-6
你可能會拼成譬如說這個是有一個 they 這裡有一個是 come 這邊有一個音是 tutor 這邊有一個這個這個 is 這裡有一個 list 這裡有一個字是 sunny	2-6
有很多這種字啊	2-6
當你有辨識出一堆這種基本單位音的時候因為都有很多 confuse 的音嘛	2-6
所以可以拼出很多奇奇怪怪的字出來	2-6
變成說they  come  tutor  is  list  sunny 這也是一句話	2-6
那憑什麼不是這句話而是這句話呢	2-6
那我們就希望知道到底哪些個字連起來哪些個 word 連起來比較像一句話	2-6
那這個方法就是我們算他的機率	2-6
我希望我算出來機率是這個 the  computer  is  listening 的這個機率跟另外一個 probability  of  they  come  tutor  is  list  sunny	2-6
那我要這個機率要比他大很多才行	2-6
因此我到時候就知道呢這個應該不對吧應該是這個吧	2-6
那這個就是 language  model 的功能	2-6
換句話說光靠前面的 hidden  markov  model 我知道它是這些基本的音	2-6
我把它拼出字來可能拼出來完全不對的字	2-6
所以呢我要靠這些這些這些字串起來到底通不通來來來算他到底合理不合理然後得到一個比較合理的答案那這就是 language  model 在做的事	2-6
所以我們就是要算假設我的一個 word  sequence 一個 sequence  of  words  w  one  w  two 都是一個 word 總共有 r 個 word 構成一個 r 的 sequence 叫做大 W 的話	2-6
我就是要算這個大 W 的機率	2-6
這個機率就好比是這個這個句子的機率或者這個 word  sequence 的機率	2-6
我要他算機率比它大	2-6
那怎麼算呢	2-6
那基本上你很容易想像這個算法	2-6
其實我後面上面的那個數學式子其實就是我們最基本的機率的算法	2-6
這個是 word  one  word  two  word 三 word 四 word 五等等等到 word  r	2-6
我先算他的機率	2-6
有了他的機率之後呢	2-6
在 w  one 之後會接 w  two 的機率	2-6
然後 w  one  w  two 之後會接 w 三的機率	2-6
然後呢一二三後面會接四的機率	2-6
一二三四後面會接五的機率以此類推一直到最後會接最後一個機率	2-6
這把他這樣一路乘起來那就是上面的這個式子的意思 ok	2-6
所以你是先把第一個 w  one 的機率算出來	2-6
先把它它的機率算出來	2-6
然後呢 given 它之後有下面的一個機率	2-6
那 given 這兩個之後有下面第三個機率	2-6
given 這三個機率有第四個等等	2-6
因此呢就是 given 一到 i 減一之後到第 i 個的機率	2-6
然後呢我 i 從二開始一直算到 r	2-6
所以後面這一堆乘起來就是我剛才講的這一些乘起來	2-6
基本上要算這個東西那這是一個標準的算法問題	2-6
只是這個無法算	2-6
為什麼無法算	2-6
因為我們的 word 很多我們以英文為例	2-6
英文日常用語我們每天日常用語的英文大約三萬是免不了的	2-6
少一點我們說兩萬五多的話要六萬才夠	2-6
depend  on 你要 cover 多少東西	2-6
我們以三萬為例	2-6
假設你有三萬個 word 的話第一個 word 有三萬種	2-6
第二個 word 也有三萬種	2-6
這都有三萬種	2-6
所以我 r 個 word  sequence 有三萬的 r 次方種	2-6
那我隨便舉裡面的一個	2-6
總共有 i 個嘛所以就是有三萬的 i 次方種對不對	2-6
因為這個有三萬種	2-6
有這麼多你要把每一個機率都求的出來會求死人的	2-6
而且也沒辦法做	2-6
所以這個是一個這是一個很直接的答案	2-6
但是不好做那我們怎麼辦	2-6
我們就做一個簡單的假設假設說你出現一個 word	2-6
只跟前面的 n 減一個 word 有關這是一啊只跟前面的 n 減一個 word 有關	2-6
也就是說我不要算那麼多啦	2-6
我不要每一次都是從一到 i 減一之後	2-6
given 從一開始到 i 減一再去看下一個 i 的機率	2-6
這個太多了	2-6
我每一次只看前面的 n 減一個因此呢就會變成怎樣呢	2-6
我們舉例來講呢我這個時候呢我要看的這個五的機率不是一二三四 given 一二三四之後五的機率	2-6
而是 given 譬如說前面的 n 減一個之後五的機率	2-6
那六的時候怎麼辦呢我要看六的機率的話呢也是前面的 n 減一個然後看這個的機率	2-6
也就是我每次都只看 n 減一個在 n 減一	2-6
再前面的我就不看了	2-6
好這是基本假設是這樣子的	2-6
這個是一個 assumption 沒有理由說他一定是這樣	2-6
那我們也知道從我們日常對語言的了解我們也知道不是這樣	2-6
那這只是為了所以這個 assumption 其實是不通的	2-6
但是為了讓它可以數學可以做	2-6
那這是一個這個 approximation 不是真的	2-6
那麼呢因此呢我每一次只用前面的 n 減一個來看下一個	2-6
因此你看我這邊的不同就是我把這個機率從 i 一二到 i 減一之後看到下一個 i 的機率	2-6
我簡化成為不是全部這麼多而是只有從 i 減 n 加一 i 減 n 加二到 i 減一	2-6
也就是我只算這是 i 的話這個是 i 減一 i 減二到一直到 i 減 n 加一	2-6
到 i 減一為止的這 n 減一個我只看前面的 n 減一個就好了	2-6
所以這個我這個機率就用這個機率來替代	2-6
那這個等號其實是不成立的	2-6
我們只是一個假設或是 approximation	2-6
如果這樣假設的話我只算前面的 n 減一個 word	2-6
那於是呢其實我每一次其實就是多少個	2-6
這邊的 n 減一個再加下一個就是 n 個嘛	2-6
所以我總共就是算這樣子嘛	2-6
對不對總共就是 n 個這 n 個 word 之間這 n 個 word 會連在一起的	2-6
given 前面這 n 減一個下面會有 n 的機率	2-6
那其實總共有多少種呢	2-6
就是三萬的 n 次方種	2-6
那這個數字至少在我的 control 之內我 n 可以小一點嘛	2-6
n 可以是比較小的數	2-6
那這種東西呢我們就叫做 n  gram  language  model	2-6
這個 gram 這個字當初他們用的時候是	2-6
grammer 的簡寫grammer 就是文法	2-6
所謂的文法的意思解讀成為就是前面什麼字後面要接什麼字	2-6
這樣叫做文法所以這個叫做 grammer	2-6
所以它叫做 n  grammer 就是我這個 n 個 word 之間的關係叫做 n  gram	2-6
那麼如果是這樣的話呢n 等於二的時候呢叫做 bi  gram 就是 given 前面一個 word 會出現下面一個 word 的機率這叫做 bi  gram	2-6
n 等於三的叫做 tri  gram 就是 given 前面兩個 n 等於四叫做 four  gram  given 前面三個等等	2-6
那那當然也可以有 uniform 當 n 等於一的時候就只算一個 word 的機率	2-6
所以我們就有這些這所謂的 n  gram	2-6
那通常我們在做的時候絕大多數簡單一點的情形我們做到 tri  gram	2-6
就是這三個的那麼比較複雜一點的就做到 four  gram 就是做四個的	2-6
你可以想像 bi  gram 是兩兩相連的機率有了這個會出現這個的機率	2-6
tri  gram 是三三相連有了前面這兩個會出現下一個的機率	2-6
那 four  gram 呢就是有前面四個等等我們大多數做到 tri  gram 複雜一點做到 four  gram 再多大概不做了因為再多太多了	2-6
那麼於是這些機率怎麼來我們用一個 training  text  code  database	2-6
也就是說你去上你最簡單的去上網嘛他們今天最常用的辦法是上網	2-6
你上網可以抓上千千萬萬個網頁每個網頁上都有一大堆文字就用那些文字去算	2-6
就可以算前面有哪幾個 word 會出現下一個的機率就可以算的出來	2-6
那麼我們以這個 tri  gram 為例那麼這樣的一個機這樣的一個 word  sequence 的機率怎麼算就是底下這個算法	2-6
那這個算法其實很容易看就是這樣	2-6
比如說這是 w  one  w  two  word 三 word 四 word 五 word 六一直到 word  r	2-6
你第一個機率是他單獨出現的機率第二個呢是有了他之後出現他的機率	2-6
從第三個開始呢就是三三相連了就是這個有了他之後出現	2-6
有了一二之後出現三然後呢有了二三之後出現四	2-6
有了三四之後出現五有了四五之後出現六等等	2-6
我每次都只看兩個不是這個都全部從頭看這裡全部從頭看我現在不是了我只看前面的兩個的下一個	2-6
這就是用 tri  gram 來算的方法就變成這樣所以呢這個是第一個這個 w  one  word  one 的機率	2-6
然後我這個呢是 word  one 之後會出現 word  two 的機率	2-6
之後呢我就是有出現 i 減一 i 減二之後的 i 的機率	2-6
所以就是兩兩兩個出現第三個等等這樣出來的	2-6
那這樣子我就用這個方法算就可以算到一個 word  FREQUENCY 的機率	2-6
也就是我這邊講的是 they  come  tutor  is  list  sunny 的機率呢還是 the  computer  is  listening 的機率等等	2-6
好那這個是這個我們用 n  gram 來做 language  model 最簡單的解釋是這樣子	2-6
那怎麼來算這些東西呢我們在下一頁有說那基本上這個算法很簡單就是這幾個式子	2-6
那這幾個式子其實是很容易解釋的很容易想像的	2-7
我們剛才說你如果上網去抓一大堆網頁出來有幾億個文字	2-7
幾億個字的構成的大的這個 database 你很容易算這些東西舉例來講	2-7
this 這個字出現了這麼多次 this  is 連在一起的時候	2-7
這是 this 出現這是 this  is 出現的機率呢剩下這樣多次	2-7
如果這樣的話那就告訴我說這個我會看到 is 前面會出現前面有 this	2-7
前面有 this 後面會看到 is 的機率是什麼就是它分之它嘛	2-7
就是有這麼多分之對不對這是一個很直覺的答案	2-7
就是說我我 this 裡面有多少五百萬個但是呢五百萬個 this 裡面只有五千個後面接了 is	2-7
對不對所以五百萬分之五千呢就是 this 後面會出現 is 的機率	2-7
那我這邊講這個簡單的例子就是這個式子這個 bi  gram 怎麼算的就是這麼算的	2-7
那我這裡跟剛才不同的我剛才的上一頁的下標是表示第幾個 word 在 word  sequence 裡面第幾個 word	2-7
我現在是上標只是說不同的 word 所以呢是某一個 word 會在某一個 word 後面的機率	2-7
就是這個 word 全部出現就像就像這個 this 有多少次有五百萬次	2-7
但是 this  is 連在一起有幾次有五千次那麼這麼一除所以這個就是這個 bi  gram 的算法	2-7
那這裡的這個 n 括號就是 number  of  counts	2-7
在你的 database 裡面有多少個 counts 就是這個東西	2-7
然後所以你這個除這個就是我們這邊講這個意思	2-7
那如果說是這一點 bi  gram 沒有問題的話 tri  gram 一樣的意思	2-7
就如果 this  is 有五千次可是 this  is  a 這個時候剩下五十次了的話	2-7
那你是不是就可以知道我的 probability  of 這個看到一個 a  given 前面有 this  is 的機率	2-7
就是這個五千分之五十對不對就是這樣的意思嘛	2-7
就是說你當我有 this  is 總共有五千個	2-7
但是裡面只有五十個後面還接了 a	2-7
所以你看到 this  is 後面會有 a 的機率就是五十個分之五五十除以五千對不對那這一個式子就是我們這邊的 tri  gram 的計算方式 ok	2-7
那這個就是這個的意思那這個是 bi  gram  tri  gram 等等 FOUR  GRAM 用類似的方法你就可以知道是怎麼做的了	2-7
那麼 uni  gram  uni  gram 最簡單了沒有什麼特別就是你單獨那一個 word 出現的機率	2-7
那譬如說我現在如果 this 出現五百萬次	2-7
可是我總共的總共的 database 是有十的二十次方個的話	2-7
那個 database 是有十的二十次方個的話那我就是十的二十次方分之五百萬	2-7
那這個就是 this 這個字的 uni  gram 它就是上面這個的意思	2-7
那就是這個你那個字出現五百萬次做為分子	2-7
那這裡面我現在是這樣寫的話其實是意思就是我的 data 總共有多少 word 的意思	2-7
那你看我這個寫法其實一樣就是把所有的 word 的次數全部加起來	2-7
你這是 w  j 是某一個 word 然後他有多少次那我把所有的 word 全部加起來	2-7
那就是其實我的 database 總共有多少 word 那麼這麼一除呢就是我的 uni  gram 就是我那個 word 的機率因此以此類推我的所有的 gram  n  gram 都可以這樣算出來	2-7
不過呢事實上是沒那麼容易因為其實會有很多這個 real  events	2-7
你是不能這樣算的就是說你我們這樣講起來都很合理但是呢我們必須了解這個盡信統計不如無統計	2-7
那麼這些統計常常是不對的那麼我們舉一個很簡單的例子來說為什麼這樣譬如說這個	2-7
Mary  immediately  cry 這是一個很普通的句子啊 Mary  immediately  cry	2-7
但是如果你照 database 去找的話呢 database 裡面有非常多個 Mary 它後面接很多很多東西它就是沒有接這個字	2-7
你有很多個 immediately 它裡面就是沒有接 Mary	2-7
因此呢你這個後面接這個的 bi  gram 就是零你算出來的這個字這個句子的機率就是零	2-7
那你就出不來你這句話就永遠不會對因為你的機率就是零但是其實不是零啊	2-7
那只是說這兩個字剛好在你的 database 他們沒有連起來過啊	2-7
所以呢你不能完全相信這個 n  gram 這樣講就會對它是有問題的	2-7
那麼這種 Mary 後面會接 immediately 這種情形呢就是我們所謂的 real  events	2-7
它就是沒有出現	2-7
那麼你怎麼辦我們要有解決的辦法	2-7
這個這個詳細我們在這裡我們在後面會講在六點零我們會仔細說這一段就會說怎麼做這些事情還有很多的所以呢我們這邊的	2-7
之前講的 language  model 之前講的那些 Hidden  Markov  Model	2-7
在四點零五點零我們還會仔細說怎麼做然後 language  model 我們會在六點零說怎麼做	2-7
然後我們上一堂課講的那些這個 front  end  signal  processing 在七點零會說怎麼做	2-7
所以我們這個大概先有一個初步的了解有這麼回事兒就好了	2-7
我這只是讓你有初步的了解	2-7
好底下我們來說 given 這些東西之後我們怎麼把它兜起來	2-8
我們先有一個初步的了解我們要做的事情是什麼假設有人說了一句話	2-8
這句話是這個 word  sequence  w  one  w  two 到 w  r	2-8
有人說這句話是這個 sequence 而我聽到的聲音是一個這個 feature  vector  sequence 就是他說的話呢我們就這個我抱歉我符號有一點前後不 consist 這就是我們前面講的那個 o 啦這就是我們之前講的 o	2-8
o  one  o  two 到 o 的大 T	2-8
我構成一個叫做大 O 這在我今天的第一堂課講是用這個符號講就是他就是每一個每一個 feature  vector	2-8
這是一個一個的 vector 那得到一系列的 vector 我叫做這個大 O	2-8
那我現在在這一頁裡面叫做 x 它是一樣的意思這個是我的聲音	2-8
所以當我聽到這堆聲音的時候我希望得到這個 word  sequence 的答案	2-8
那這個究竟是什麼呢你可以想像成就是這樣的一個 problem	2-8
就是我想要估計這個機率就 given 的這些聲音對不對這個 x 就是我的聲音嘛	2-8
given 這堆聲音之後我要找所有可能的 word  sequence  w	2-8
看看哪一個 word  sequence 給我的這個機率是最大的機率最大的那一個就是我的答案 ok	2-8
這是一個非常直覺的一個式子雖然它有非常嚴謹的理論基礎叫做 MAP  principle 這個我待會兒再解釋	2-8
我們先看我們不要看這個 MAP  principle 我們光看這個其實是一個非常直覺的答案	2-8
就是 given 我 observe 到這一堆聲音 x 我希望找所有可能的 w  word  sequence	2-8
我都可以算這個機率 given 這個聲音之後它是這個 word  sequence 的機率是多少	2-8
given 這個聲音的時候它是它是那個 word  sequence 的機率是多少我都可以算	2-8
然後我在所有可能的 word  sequence 找一個機率最大的那一個	2-8
機率最大這個最大那一個就是我要的答案 ok	2-8
所以這樣看起來這是一個非常直覺的一個式子	2-8
其實不需要什麼 MAP 來解釋也是可以的但是問題是這個機率怎麼求這個機率很難求	2-8
不過沒有關係我們要求這個機率常用的辦法就是這個式子	2-8
那這個式子其實就是我們你在機率學過的 Bayes  theorem 就是你要求 A  given  B 的機率的話如果這個不會求你可以倒過來	2-8
先求 B  given  A 然後乘上 A 除以 B 這樣子所得到的就是這個	2-8
那這個沒什麼特別應該很容易想像	2-8
那麼因此呢當我這個機率不會求的時候我可以倒過來先求我就把它倒過來就變成了是這個如果 given 這個聲這個 word  sequence 的話這個聲音的機率是多少	2-8
乘上這個 word  sequence 的機率除以這個聲音的機率變成這樣那這個只是這樣的倒過來而已	2-8
於是我現在要 maximize 這個東西就會變成 maximize 這個東西	2-8
那你再仔細看看這個式子裡面呢其實這個機率可以不要看	2-8
為什麼不要看因為我是 given 這個聲音我要找所有可能的 w 看哪一個 w 給我機率最大	2-8
對不對所以這個聲音是 given 的這已經 given 了	2-8
問題是不同的 w 給我的機率不一樣大嘛	2-8
所以我是要在不同的 w 裡面去找一個最大的	2-8
不是要看這一個這個是 given 這就是只有那一個	2-8
所以我這個可以不看當我這個可以不看的時候我就剩下這兩項	2-8
於是我就變成要求這個機率跟這個機率相乘最大	2-8
那一個 w 就是我們要的 w  ok	2-8
那麼這個你再仔細一看這個這個是什麼這個就是我們剛才所講的 language  model	2-8
我們剛才的 language  model 不是就是在算這件事嗎	2-8
就是一個 word  sequence 我就去算它的機率	2-8
所以呢我們剛才的 language  model 就給我這個機率了	2-8
那這個機率是什麼這個機率其實就是 Hidden  Markov  Model 啊	2-8
為什麼呢 Hidden  Markov  Model 有一個好處我們剛才沒有提到的	2-8
就是他很容易把小的 model 串起來變成大的 model	2-8
假設說假設說假設說這個是 s 然後呢這個是 ah 我就把它串起來這就是撒	2-8
我如果這邊後面再來一個我後面再有一個是 n 再把它串起來這就是三	2-8
ok 所以 Hidden  Markov  Model 很好的好處就是你雖然原來這是一個小 model 這是一個小 model 這是一個小 model 你只要把它裝上連結讓它可以跳過來那麼它就可以連起來	2-8
所以我可以把很多小的基本音串成 word 再把 word 串成句子都可以	2-8
因此呢我的這一個 word  sequence 也是一個可以變成一個這個 word  sequence 的 Hidden  Markov  Model	2-8
於是 given 那個 model 來算這個機率的話這就是我們剛才所講的 basic  problem  one 我現在符號有點不一致	2-8
你如果回過頭去看我們剛才的 Hidden  Markov  Model 的最後一張的的這個三個 basic  problem 的這個東西	2-8
不就是剛才那個嗎 given 某一個 model 我去算看到那個聲音的機率對不對	2-8
given 某一個 model 算看這個我們下週就會說怎麼算這個東西	2-8
那這個不就是我這邊講的這個是一樣的是一樣的 ok	2-8
所以這個是可以用 Hidden  Markov  Model 算的這是可以用 language  model 算的所以這根本就是可以算出來的	2-8
因此我是可以求這個 maximum 的 ok	2-8
那我這裡是完全用是個數學式子來看就是這個意思應該不難了解	2-8
就是我在 given 這個聲音的時候我要知道到底是哪一個 word  sequence 的話那我就看哪一個 word  sequence 給我最大的機率	2-8
那這個機率我把它倒過來那這個東西我不要看然後就看這兩個就是這兩個相乘	2-8
那在這個情形之下呢那就回到這張圖	2-8
這張圖其實就是在我們的第一週一點零裡面的那張圖的核心的那塊就是這樣子的	2-8
聲音進來我有 acoustic  model 就是在幫我算 Hidden  Markov  Model 分數	2-8
我有 language  model 就是在幫我算這個的分數	2-8
然後呢辭典告訴我說哪一些音兜起來變成哪一個詞就這些東西加起來我就在這邊找一個最大就是這件事	2-8
那麼因此呢這裡就是 acoustic  model 就是所有剛才講的譬如說s 有 s 的 model  ah 有 ah 的 model  n 有 n 的 model	2-8
這些東西呢就是所謂的他們都是以 Hidden  Markov  Model 方式存在這裡面就是所謂的 acoustic  model	2-8
那麼這些東西就是所謂的 basic  voice  units	2-8
每一個 unit 都有它的 Hidden  Markov  Model 這個字是什麼意思我們後面再講	2-8
以後後面會說到現在先不說了	2-8
那麼呢辭典告訴我說哪一些可以拼起來	2-8
告訴我說 ok 這個三其實是這個三它就是用這三個音拼起來的	2-8
那這就是一個辭典辭典裡面的事情就是一個 database 裡面所有可能的 words	2-8
然後呢每一個 word 告訴我是哪些個 basic  voice  unit 拼起來的這就是我的辭典	2-8
然後 language  model 就剛才講的我看看哪些 word 跟 word 會連起來等等	2-8
它就變成這個 ok	2-8
那這張圖的這個圖其實就是對應到我們之前講的這個在一點零裡面的那張圖	2-8
這個滑鼠非常不好用我不會用這個滑鼠	2-8
ok 好不在這裡是在哪裡	2-8
ok 應該是在這裡喔	2-8
yeah 這張圖喔	2-8
我們在一點零的這張圖裡面講的這塊嘛就是我剛才的剛才最後那張的下半就是這張圖嘛	2-8
那其實我們說了半天就是在說這張圖	2-8
那我的那些 H M M 那些 Hidden  Markov  Model 呢就是這些所有基本的音這些所有基本音存在這個裡面那我來判斷裡面到底是哪些基本的音	2-8
然後辭典告訴我說哪些個音湊起來會是什麼字等等	2-8
然後 language  model 告訴我說他們應該怎麼連起來我有 bi  gram  tri  gram 然後他們可以算它的機率	2-8
那這樣子的話呢那還有 front  end 呢就是我們剛才講怎麼樣算那個 M F C C 的參數	2-8
我進來怎麼算等等	2-8
然後剩下就在這一塊要怎麼做所以呢我們之前講的是這些事情	2-8
那麼我剛才還有一件事情沒有說的就是我想剩下的時間說一下	2-8
剛才講的那個 MAP 的 principle 稍微複雜一點	2-8
其實我們用很簡單的例子來看你就知道它是什麼意思	2-8
我們說如果有一個 random  variable 叫做 w 它是一個 random  variable	2-8
它可能有三個值只有三個值可能是 w  one  w  two 或者是 w 三	2-8
他描述今天的天氣 w  one 呢說今天是 sunny  w  two 說今天是 RAINY w 三說今天呢既不是 sunny 也不是 RAINY 那就是 cloudy	2-8
那我有一個 observation  x 它是譬如說有一堆參數是我可以觀測的天氣的參數譬如說氣溫的變化或者是說早上比起晚上來這個濕度的變化等等等等	2-8
這些東西是我的參我的 observation	2-8
那我的 problem 就是 given  observation  today 然後我去 estimate 天氣 tomorrow	2-8
我現在 given 今天所 observe 到的東西我要估計明天到底是晴天陰天還是下雨	2-8
那怎麼做這件事情呢我第一個我可以根據譬如說我現在是三月	2-8
我現在是三月我可以算出三月裡面晴天的機率是多少零點四五	2-8
雨天的機率是多少零點二零陰天的機率是多少就是零點三零	2-8
譬如說這樣子這個是我預先就知道的	2-8
可是你說我今天問我明天的機率是說少我也可以從這裡來算明天到底是什麼呢	2-8
看起來是這個機率最大我就說明天是晴天	2-8
我會對但是我也有可能錯對不對	2-8
我如果純粹從這個來看的話這個是我只是根據過去三月份晴陰雨的機率是這樣算的	2-8
那比較好的辦法呢其實我應該是算 observation	2-8
那怎麼來呢我就應該算這個東西	2-8
就是我如果 observe 到今天 observe 到這個之後明天會晴天的機率是多少	2-8
那麼我今天 observe  observe 到這個之後我明天是雨天的機率是多少	2-8
我今天 observe  observe 到這個之後明天是陰天的機率是多少	2-8
我其實算的如果是這個的話然後看誰最大	2-8
看誰最大那如果這樣看的話這個比較像嘛	2-8
對不對那這個時候我就把今天的 observation 算進去了	2-8
given 這個之後看明天的機率	2-8
可是這個怎麼算呢這有點不好算於是呢我怎麼辦呢	2-8
我可以把它倒過來這就是剛才講的是完全一樣的就這邊把它倒過來	2-8
於是呢我就可以變成我要算的是這個嘛	2-8
w  i  given  x  i 等於一二三嘛我就那邊的就是這個東西	2-8
那這個怎麼算呢我可以把它倒過來就是然後乘上 w  i 再除以 x	2-8
然後這個就這個一樣就把我剛才的那個那個那些機率其實就是這個機率	2-8
然後把它變成這樣倒過來這樣倒過來之後因為我是要在這三個裡面看哪個 w  i 最大	2-8
所以呢我就這個 x 是沒什麼關係的我不要看就看這個就好了	2-8
於是我就是算這個機率跟這個機率兩個相乘那這個機率呢不難算	2-8
為什麼呢	2-8
我只要算譬如說三月裡面是晴天的時候它前一天的 x  one  x  two  x 三等等	2-8
他們有一個 distribution	2-8
三月裡面如果是雨天的話它前一天的 x  one  x  n 也有一個 DISTRIBUTION	2-8
三月裡面如果是陰天的話它前它的前一天的也有一個 distribution	2-8
那我這個不難求啊這個不難求啊我只要去統計就好了	2-8
如果晴天的話前一天是怎樣雨天的話前一天是怎樣陰天的話這都可以求	2-8
那麼因此呢我現在現在 observe 到這個 x 的時候我可以把這個 x 放在這裡得到一個機率	2-8
放在這裡得到一個機率放在這裡得到一個機率	2-8
那就是這一項	2-8
然後呢這個是什麼呢這就是我們剛才講的我三月我也統計過晴天是零點四五雨天是零點二一這我也有啊	2-8
我把這個也弄進去當我這兩個都有的時候這兩個乘出來看誰大就表示誰這個大	2-8
那就表示誰這個大那就表示我 given 這個 observation 到底明天是什麼	2-8
那我這邊講的問題就是剛才的最後那一張抱歉現在要再轉回去很累	2-8
不過你去看我剛才的那一張的話的上面的所有的符號跟這裡是完全一樣的	2-8
我不跳過去了	2-8
就是我剛才的那裡的 word  sequence 就是這個 w	2-8
就是這個 w 那麼我這邊是只有三種可能但實際上我們 recognition 的時候這個有千萬種	2-8
我有千萬種 word  sequence 我要找一個所以這個對應到的就是這個	2-8
同樣呢我的聲音呢	2-8
我我我現在是只有是是這個 observation 相當我的聲音就是我的那個 x	2-8
等於 x  one  x  two 的那個就是這個 x	2-8
那於是呢我最後算出來的那個我們剛才說的是我算的這個嘛	2-8
就是這個 given  w 乘上這個用 language  model 算其實就是這個東西	2-8
那 language  model 算的這些東西呢就是所其實就是我們這邊的晴天陰天雨天各是機率是多少的意思	2-8
而這個用 Hidden  Markov  Model 算的呢就是這個東西	2-8
所以呢我這邊等於是用一個簡單的例子來來對應到我們剛才的那個 complicated  problem	2-8
那這裡面我們可以講幾個名字這種機率	2-8
這些個機率叫做 prior  probability	2-8
而這樣的機率呢叫做 A  posterior  probability	2-8
那麼這些字的意思 prior 的意思是事前的	2-8
這個呢是事後的	2-8
那麼所謂事前跟事後是你有所謂事後就是你 observe 到 x 之後你 observe 到 x 之後我的機率	2-8
事前就是我還沒有 observe 到 x 我完全根據過去的統計得到的機率	2-8
那麼因為這樣的關係呢我們剛才你看到我們上面有我現在不跳過去了	2-8
我那邊有一個所謂的 MAP  principle	2-8
什麼是 MAP 呢 MAP 就是 maximum  a  posterior 的 principle	2-8
就是我要 maximize 你看那個 MAP 的他所要 maximize 的就是這個東西	2-8
就是 given  x 要找出那個 w 的機率最大的那一個	2-8
那就是在 maximum  maximize 這個 A  posterior  possibility 所以呢叫做 MAP 就是 maximum  a  posterior  possibility	2-8
而在這裡面我們所用的這一個那也有一個名字	2-8
叫做這個呢或者是這個呢這是所謂的 likely  function	2-8
所謂的 likelihood  function 是指這個	2-8
就是在這個這個天氣之下或在這個情形之下我會 observe 到這個的機率的observe 到這個 likelihood 它的可能性這是 likely  function	2-8
那當然這個就是我們剛才講的 prior  probability	2-8
因此呢所謂的 language  model 所得到的分數是 prior  probability	2-8
也就是說你根本還沒有聽到聲音你就可以算的	2-8
這個是我根本沒有聽到聲音就可以算的是 prior  probability	2-8
而這個用 H M M 算的呢這個是 likely  function	2-8
是我如果聽到這個聲音的話呢它跟這個 word 之間的關係呢	2-8
跟這個 word  sequence 的關係呢是這個機率等等	2-8
那麼那麼呢這個就就你現在如果對應到我剛才的那張圖的話	2-8
我現在就跳不回去我就不跳回去了不過你你對到那個圖就知道我們這邊講的所有的東西那有的時候呢	2-8
我有一個簡單的辦法就是假設這個都是三分之一	2-8
晴陰雨各是假設我現在不知道這個我沒有 prior  probability 的話	2-8
假設我沒有 prior  probability 的話我可以假設他們都是三分之一	2-8
當他們都是三分之一個時候呢我要 maximize 這個東西的時候呢我可以不看它因為它都一樣	2-8
就剩下這個了我只要看這個就好了	2-8
那這就是所謂的 maximum  likelihood	2-8
你你如果不看這個假設我這個都是一樣的我不看了	2-8
我只看這個的話這個是 likely  function	2-8
所以這個時候我就叫做 maximum  likelihood 的 principle	2-8
因此呢在我們這裡我們剛才那一張講的我們這裡講的其實不是 make  a  likelihood	2-8
因為我們把 language  model 這個 prior  probability 算進去了	2-8
我即使沒聽到聲音我也知道這些 word  sequence 該有多少機率	2-8
在我是事前就可以算的這個機率我把這個算進去了	2-8
所以我得到的是 maximum 的 A  posterior 的 principle 就是所謂的 MAP	2-8
ok 那這些例子是比較簡單的你比較容易想到那對應到我們上面那張圖的東西	2-8
好我想我們上到這裡我倒是需要討論一下補課問題	2-8
歐我們的我想在我們這樣的一個 class 裡面要補課唯一的可能是 weekday 的晚上
or 是週末因此呢我們可能要算一下可能的補課時間
在 weekday 的晚上而言我可以補課的時間是星期二三跟五
如果要到 weekday 的話呢就是禮拜六了 ok 
所以呢就是二的晚上三的晚上五的晚上
或者是六的六我們也許可以不要晚上了白天我想可能是這個時間
我們現在也許先不說是哪個禮拜
我們先看看有沒有哪一個時間是現在大家都 ok 的
我們很快看一下好不好禮拜二晚上有人不行的請舉手
有很多人不行禮拜三晚上有人不行的請舉手
禮拜四晚上阿禮拜五晚上有人不行的請舉手
三個晚上似乎都有人不行禮拜六下午不行的請舉手
禮拜六早上不行的請舉手我們似乎沒有 solution 
如果沒有 solution 就比較麻煩一點
今天是三月七號下週十四下週二十一下週二十八然後是四月三號
四月四號十一號十八號按照校曆規定四月四號是溫書假是放假的
那當然有一個辦法我們就是原時原地補課如果我們沒有別的時間的話
那另外呢是這一週我出國三月二十一我出國
我比較希望在我出國前先補一次的話這樣我們進度比較不會落太遠
如果這個時候都不能補的話這又 delay 一次然後再下來問題很多
所以我會比較 prefer 這裡面我們先找一個可以的時間的話我們在這裡面補一次
這樣會比較好一點 
otherwise 我們這個進度是非常糟糕
我們要不要再來一次這個如果你 really 不行才舉手好不好
因為這樣我們剛才已經顯然沒有 solution 了嘛
really 不行才舉手如果你是其實我可以調的
那盡量不要舉手這樣我們可以盡量看看可不可以找一個時間出來好不好
星期二晚上 really 不行的請舉手
我們現在先不說哪一個禮拜
我們先找出時間來好不好
星期二晚上是比較多人不行星期三晚上 really 不行的
比較少人一點我們看看幾個一二三四五六六位
星期五晚上 really 不行的一二三四五六也是六位
星期六下午 really 不行的只有一位
所以好像比較可行一點是不是有沒有辦法
星期六下午好像比較可以啦是不是現在只有一位星期六早上呢
星期六早上不行的一二兩位三位現在這裡可行性比較大嘛是不是
我們在星期六星期六下午這樣看起來是比較有沒有辦法可不可以
ok 如果我們這樣好了我們如果是星期六下午的話我們再來看
今天是三月七號所以是本週六是十一號下週六是十八號
不過下週六十八號我已經要上飛機了
所以如果要在我走以前補的禮拜六的話變成要在本週六囉
本週六行不行不行
那就變成必須要在 ok 
那這樣好不好我們現在第一個第一個我們現在先 reserve 星期六的下午的時間
那麼現在是哪一個星期六呢我們再來 check 基本上本週六大家是不行的對不對
我們就就不排除本週六
那我要來 check 這個跟這個
好像我已經出發了
不知道這天我回來了沒有就是了
好像我還沒回來就到這兒來了
這個的話就變成可能要到這一天了這樣進度會會慢很多就是了
那當然另外一個狀況就是我們能不能在這一天原時原地補課
我想這個也許大家回去想一想好了
我想也許今天這樣看起來的話我們基本上是一個可能是禮拜六下午啦
可能會在這個時候或者這個時候看看啦
這個不行就到這兒來了啦就這樣子嘛
那一個就是這一天原時原地補課啦這是另外一種可能啦好不好
我們回去回去看一看時間但我想補課是少不了非補不可就是了
因為我除了這個出國我還要再出國一次
所以我們一定要補課就是了 
ok 好今天上到這
我們上週二點零還剩最後一張沒有講	2-9
這一張其實就是在	2-9
我們在說二點零的時候呢	2-9
我們下課的時候是講到這個地方	2-9
我們就是說m a p的principle	2-9
也就是說given 一個word sequence given一段聲音進來 given一段聲音進來	2-9
那麼我們要想辦法找一個word sequence可以對應到他的也就是這個機率最大這就是m a p的principle	2-9
那麼我們的辦法是把這個這個這個sylable matrix 變成兩個機率在乘	2-9
那麼一個是independent 這就很簡單	2-9
我們上週下課的時候在說這件事情	2-9
那麼我們可以把它看成是一個很簡單的example 就是今天天氣	2-9
那麼我們說這樣一來的話我現在就有這個用hidden markov model來算acoustic model	2-9
然後用language model來算你可以把他們所有的mu 都	2-9
這件事情這件事情我們如果用一個比較簡單的例子來講就是我剛才的那張	2-9
那麼在那張圖裡面的我們是以國語的聲音為例	2-9
假設說我現在輸入一段聲音	2-9
問題是這段聲音我們不知道你有幾個字也不知道哪裡是你到底是哪一個syllable boundery 一個語音的boundery哪裡我也不知道	2-9
所以我只能用一堆acoustic model一些基本的音譬如說ㄅㄆㄇㄈ或者是ㄚㄨ一ㄟㄨ去兜它 given你會得到這樣的情形	2-9
舉例來講這個第第一個字第一個音我不曉得是從這裡到這裡呢還是到這裡還是到這裡	2-9
一個很簡單的例子譬如說如果我的第一個字是金	2-9
但是也很可能是雞	2-9
我第一個字是向不曉得是西跟樣呢還是向這個我都不知道嘛	2-9
那麼因此呢你就會發生說像這樣的情形我的第一個音可能是從這裡到這裡也可能是到這裡也可能是到這裡	2-9
如果是到這裡的話那第二個音從這裡開始如果是但是也可能到這裡也可能到這裡也可能到這裡	2-9
如果是第一個音到這裡的話那第二個音也可能從這兒開始等等	2-9
對每一個音而言又不一定你譬如說你說他是ㄐ好了他跟ㄑ很像他跟ㄒ很像他跟ㄓ很像那麼你你不能確定他到底是哪一個	2-9
所以呢即使是這一段而言我也不知道他究竟是哪一個我有好幾個 candidate	2-9
如果是這一段呢我也有好幾個 candidate 等等那這樣子呢我雖然有了 HMM 這基本的音的 model 我都有	2-9
可是我得到的是這樣的一個很複雜的一個我們稱之為 Syllable  Lattice	2-9
也就是說我的 syllable 可以是我這些都是 candidate 他可能是這個可能是這個可能是這個可能是這個可能是這個可能是這個等等每一個裡面有很多種可能的 candidate	2-9
這樣的一個 Syllable  Lattice 其實不太容易往下怎麼走因此我們第二步就是要一個詞典	2-9
這個辭典告訴我說雖然有這麼多音都可以但是呢他們可以湊成一個詞的沒那麼多	2-9
舉例來講呢也許如果這是一個音這個就這個音而言的話他裡面的某一個音的 candidate 跟下一個音的某一個 candidate 確實可以兜成兩個譬如說這是今這是天今天是一個詞	2-9
那同理呢如果這個詞真的是到這裡的話呢那這底下我又有兩個音可以兜成另外一個雙字詞等等	2-9
可是當然這不是唯一的可能我也許呢這裡也可以兜我如果這裡有一個音的話呢這個跟另外這個呢也可以兜成另外一個雙字詞	2-9
如果這是第一個雙字詞的話呢那從這裡開始也有另一個音可以兜下個雙字詞等等	2-9
那麼於是呢我這邊就得到一堆可能的詞由詞典兜起來這個呢我們稱之為 Word  Graph	2-9
那有了 Word  Graph 之後顯然我們還沒有答案我的答案要從哪兒來要從 language  model 來	2-9
於是我就可以算如果是這個詞接這個詞等等呢我可以算他的 n  gram 如果是這個詞接這個詞我也可以算他的 n  gram 等等	2-9
那到時候看他的 n  gram 到底誰大等等	2-9
那這樣的話我們就會得到這個 n  gram 的 language  model 分數跟 acoustic 分數然後通通兜在一起就是我們剛剛那個式子從那裡面來判斷哪個才是答案	2-9
那這裡我們講的這個這個整個的 search 的過程等於是這樣子的	2-9
那這裡所謂的這個 one  path 的意思是說你這裡我們後面在八點零還會詳細說這個	2-9
不過基本上在這個 case 的話可以假設最理想的狀況就是從頭一路走下來	2-9
當我這個聲音一路進來的時候我一路同時用 acoustic  model 去比對於是得到這些 Syllable  La 這個 Syllable  Lattice 一路建起來	2-9
建到這個地方的時候呢我這個 Word  Graph 就開始一路建起來	2-9
那麼等到這一路建下去呢我這邊就會 language  model 就會出來然後就會選這些詞等等	2-9
那麼等到這一句講完的時候我答案就出來	2-9
如果這樣我只要一步就可以走完這是最理想的 one  path  search	2-9
那當然如果這樣的話那需要的計算量是非常大而你中間這個程式寫起來是非常 challenging 的	2-9
那麼但是這個是做的到的你一路走下來一路就最後全部出來那這是所謂的 one  path  search	2-9
那這個大概解釋我們上週最後在講的事情	2-9
那麼在底下的這一張是在不是在上面的這一張是我們歐不對那我還有一張是在這個後的最後一張是在講這個這二點零的 reference	2-9
沒有啊那是在三點零那裡 ok 好好那我們就進入三點零	3-0
不對我上面有一個二點零 reference 這是在我的那個 reference 課本的最最後一本	3-0
這一本也是說這幾本裡面最容易讀的一本是這一本所以這本其實不難唸	3-0
那我剛才二點零裡面所講的東西大概涉及這些個章節	3-0
但是我當然不是講那麼多我中間是隨便抓我只是抓裡面的要點來說而已	3-0
但是如果你要多唸一下的話呢我想這都是很好的 reference 是可以唸而且你可以學到比我講的多很多的東西	3-0
那有些地方如果你唸不下去也沒關係就可以先跳掉或先不唸	3-0
那麼在我們在未來的若干週裡面還再還會陸續再說到這些東西	3-0
所以呢我們在這個譬如說四點零五點零會說這邊六點零七點零會說這邊我們還會有很多東西會說到的	3-0
所以呢你如果現在覺得不太不太容易看你可以先跳過去晚一點再看也行	3-0
不過這些章節應該是比較核心的章節那麼我都已經把重要東西我有提過一點了就是了	3-0
好那麼再來呢我們來說一下的就是底下這張圖	3-1
我們到目前為止都只在做 overview 我們並沒有講細節	3-1
我們在一點零是在講整個領域的 review 的 overview	3-1
我們二點零是在講核心的 recognition 的簡單的解釋我們都還沒有進入詳細的	3-1
那我們這張圖是等於是說把整個的語音的領域的所有的重要的東西我們把它畫成一個 structure 來看	3-1
大概這個整個的研究領域大概有這些東西其中我們已經提到過你比較了解的是這邊的一二三	3-1
一就是我們稱之為 acoustic  processing 就包括了怎麼求那些個 feature  M  F  C  C 這些東西包括怎麼做 modeling 像 H  M  M 然後包括怎麼樣把它們做的更好做更好的 feature 做更好的 model 等等這個就是這一塊	3-1
二呢就是 language  model 做 n  gram 等等這些那進一步來講應該還包括很多怎麼把它做的更好包括譬如說詞字詞的處理句型的處理等等	3-1
那這些東西都屬於 ling 我們稱之為 linguistic  processing	3-1
那第三塊呢就是 search 也就是我們剛才就在剛才講的那張圖就是一個 search	3-1
你怎麼樣做那個程式讓他能夠有效的整合左邊跟右邊整合左邊跟右邊之後你得到一個比較好的答案而能夠有效的達成那這是所謂的 decoding 或者是 search	3-1
那這三個如果兜的好兜的成功的話就可以得到一個成功的大字彙的連續語音的辨識系統	3-1
那它的 application 可能是我們這邊講的四就是 dictation  and  transcription	3-1
那我們之前大概提過譬如說 remote  dictation 你可以假設說我坐在地鐵裡面我坐在捷運上面我就可以打個電話回我的電腦然後告訴它說我今天做了什麼事我今天怎麼樣怎麼樣	3-1
那麼於是呢你回到辦公室的時候你的報告已經寫好了你只要再改幾個字就行了這就是所謂的 dictation	3-1
那 transcription 呢是說一般的正常的譬如說會議記錄或是什麼的你都不用再用人去做	3-1
那麼你可以就是現場錄音喔包括課程包括演講什麼都可以現場錄音之後讓它直接 recognition 之後變成文字你再修改一下就可以了這就是所謂 transcription	3-1
這是一二三跟四這是最傳統的語音辨識的核心部份	3-1
那麼在一九九五年之前大概多數做語音的人就集中在這一塊就是一二三四	3-1
當然還有一塊也是很傳統的就是 text  to  speech 也就是這個語音合成	3-1
那這個等於是這個的 inverse 那你知道這個這個語音辨識等於是 speech  to  text 那語音合成是 text to speech	3-1
給我一段文字把它合成聲音	3-1
那這個情形的話呢我通常是那麼你可以想像它是它的 inverse	3-1
所以它我們今天的大部分的技術其實是用這個技術來做把它倒過來而已	3-1
所以呢這個我把它的畫的位置跟這個是完全平行的中間有一條橫的線表示它們是完全平行的互相對應的	3-1
那麼它是它的 inverse 所以它也是用這些 model  feature 這個 language  model 它都用然後呢反過來做那就是 text  to  speech	3-1
這幾塊算是比較傳統的	3-1
那麼從九五年以後到現在這十多年那麼主要的領域是發展到外面這一大片	3-1
那原因是我們從傳統的這個這個語音辨識跟語音合成比較是 p  c  based 的想法在我個人電腦上 notebook 上面你可以做這些事	3-1
可是後來慢慢我們就走向網路世界的話呢想的是網路環境之下的東西於是多了很多東西	3-1
那多了哪些呢我們現在來看底下這個譬如說 speaker  adaptation 就是不同的 speaker 聲音是不一樣的	3-1
尤其網路上網路的 user 千變萬化每個人的聲音都不一樣你的 server 如何能 handle 不同的聲音	3-1
那麼那麼我們顯然需要什麼人的聲音進來要配合他的聲音做出他的 model 調成他的 model	3-1
所以這個呢就變成你要如何能夠那還有呢就是你如何辨識那個人聲音是誰的聲音等等	3-1
所以呢你要考慮網路上的大量的不同的 user 聲音的不同不同的特性	3-1
那麼這是所謂的 speaker 的 recognition  speaker 的 adaptation 調 model 調到他的聲音去等等	3-1
那 hands  free 這塊呢是說我們總不喜歡永遠拿一隻麥克風在手裡最好是不要這隻麥克風嘛	3-1
那我不要這隻麥克風的辦法很可能是說那你今天開車可能有的人會把手機裝在我的前面或者把麥克風貼在車窗上等等	3-1
這種就是所謂的 hands  free 就是我不要用一隻手來拿一隻麥克風	3-1
那這個裡面我們今天看到最成功的方法可能是所謂的 microphone  array	3-1
這個時候如果只有一隻麥克風的話效果可能不夠好只有一隻麥克風的時候那支大概要很靠近你的嘴才會比較好	3-1
你如果要給它遠一點的話怎麼辦遠的話就是多幾隻麥克風大概弄個若干隻至少是兩隻啦	3-1
讓兩隻的話你可以讓兩個不同的聲音進去之後組合成為一個聲音等等那就會好	3-1
所以呢這裡面現在最好的方法就是 microphone  array	3-1
那有相當高的這個成功度就是你如果距離夠遠但是我麥克風夠多的話可以得到不錯的效果就是這一塊	3-1
再來呢就是 user 在在網路上的關係所以他可能在路上他可能在餐廳他可能在一個地鐵裡面或什麼飛機場在街上的話有各種的雜訊	3-1
你如何處理這些雜訊背景雜訊	3-1
然後他可能是打電話進來這個電話一路走的時候呢有很多的 distortion  channel 會破壞你的聲音	3-1
就好像你打電話你就知道你聽到的聲音不是在對面的聲音是有相當的失真的	3-1
這些東西如何處理這是另外一個很重大的問題	3-1
那我們通常處理的辦法呢有調它的 model 有調它的 feature 或者這一類的那這是所謂的 robustness	3-1
那這三個呢基本上都是在這個裡面就是我如何讓我的 feature 跟 model 做的更好如何可以 handle 這些問題所以我把它都接在這個底下	3-1
它們都是觀念都是通的所以我們講這樣連起來這是五六七	3-1
那八的話呢就是做 keyword  spotting	3-1
那你很多時候 user 從網路上跟 server 打電話要幹麻幹麻的話呢其實你要抓的可能是它的 keyword	3-1
那這個時候 keyword  spotting 它用的是包括這裡的 acoustic  model 這些東西也包括這個 search 的方法主要是這兩個所以呢這是 keyword  spotting	3-1
那再來一個很重要的問題呢這個我們稱之為 pronunciation  modeling	3-1
就是其實我們在說話的時候並不是真的照字典上的音在講而是我們的 pronunciation 我們發的音是會動的	3-1
這個最常見最簡單的例子就是譬如說我說今天早上天氣很冷我們大概不會真的說今天早上天氣很冷	3-1
那我會怎麼說今天早上很冷今天早上你仔細聽的話會是我沒有仔細說今天早上四個字我是說今天早上	3-1
今天早上我講的是這個跟這個	3-1
有一點像是這樣子但是呢這個我們人的耳朵是聰明到不需沒有這個問題我一聽就知道這個是今天	3-1
可是你對機器而言今天跟間是兩個不同聲音	3-1
你必須要告訴它說你聽到一個間的時候有可能是中間也可能是今天	3-1
然後呢你如果聽到一個盎的話可能是早上裡面的上	3-1
等等那這一些就是我們在真正在說話的時候的我們所謂的 spontaneous  speech	3-1
就是你自發性的在那兒說不是朗讀性的 read  speech 的時候你會出這種東西出來	3-1
那這是一大堆需要解決的問題所以這個我們稱之為 pronunciation 的 model	3-1
你就是要做我們真的說的時候我的 pron 會怎樣	3-1
那這個東西的技術大概來自也是來自這三塊你就是用這三樣東西去分析用這個東西來分析得到我一個新的 pronunciation  model	3-1
這個是今天一個非常重要的研究題材	3-1
那然後呢再來呢就是 speech  understand  understanding	3-1
那這個我們在有講過一點就是你不只是要把它這個聲音辨識出來是哪一堆字而是要知道它在說什麼意思你要 understand 它的意思	3-1
那這個時候我需要的呢是所謂的 speech  understanding	3-1
那這個有兩種做法一種是直接從 speech 去做 understanding 一種呢是把它辨識成為一個句子之後根據句子的字詞的關係去了解去 understand 那是我們稱之為 language  understanding	3-1
那這些東西這些 understanding 基本上是跟這個最有關係	3-1
它也會用到裡面的 n  gram 啦也會用到字跟詞的結構啦等等所以跟這個最有關係我把它畫在旁邊它跟它等於是平行的兩塊	3-1
這邊只是在做 n  gram 的或是 language  model 那這邊呢就要進一步做 understanding 就是這一塊	3-1
那麼這些東西基本上我們可以想像成為是底下這一塊我都把它說成是比較 basic  technology 就是比較基礎的	3-1
這個比較基礎的這邊而言的話其實最底層的是我們感覺最基礎的東西就是這些東西都是在做這上面的東西然後想辦法把它做的做的更把一些基本的東西都解決這是所謂的 Basic  Technology	3-1
所以愈底下的愈基礎	3-1
那這些東西之後呢我們加以整合就變成上面的東西	3-1
這邊所以這一行呢我稱之為 integrated technology 就是比較整合性的	3-1
那我們剛才講一個例子 dictation 跟 transcription 也可以算是把一二三都兜了起來整合起來就變這個	3-1
那除此之外還有很多	3-1
譬如說 spoken  document  understanding  and  organization 這就是我們第一堂課 demo 的那個新聞系統	3-1
你怎麼去 understand 去 organize 這些新聞這就是一個例子那一種就是所謂的 spoken document	3-1
那你所需要的除了這一大堆的 recognition 之外呢還要有 understanding 然後要想辦法把這些東西兜起來可以多這一塊	3-1
然後呢如果是 dialogue 的話我們在在這個第一堂課裡面 demo 的那個查號台系統就是它系統跟你對話	3-1
你跟它查號你跟它幹麻跟它問它問題	3-1
那這個時候的話你是需要有 understanding 有 recognition 還有呢語音合成	3-1
你要能夠辨識能夠了解然後呢跟它對話你要合成跟他對話所以要這三樣東西兜起來這就是 dialogue	3-1
那你如果是要搜尋的話要上網搜尋你用聲音去搜尋或是搜尋一個帶有聲音的東西的話呢那就是我們所謂的 information  retrieval	3-1
這時候所需要的呢除了這些語音的辨識的技術之外你還需要 information 的 indexing 跟 retrieval 的技術	3-1
那這個東西其實就是喔譬如說你今天上 google 那個 google 就是一堆這種東西不過它是在做文字就是了	3-1
它想辦法把所有的網頁做 indexing 然後讓你只要輸入一堆 keyword 就可以找到你相關的網頁這一類的事情	3-1
這是所謂的 information  indexing  retrieval	3-1
不過它是在做文字	3-1
那我們現在把這一堆技術跟語音的技術來整合的話呢我可以做聲音的 retrieval	3-1
那同樣的呢這邊有一堆是什麼呢這個是這個 Wireless 的 Transmission 跟 Network	3-1
這一些相關的技術呢我們跟它整合了就可以做這個分散式的語音辨識跟在 wireless 環境之下的語音的問題	3-1
所以呢這個是要跟跟這個 wireless 的合作然後呢技術來整合可以做這一塊等等	3-1
那所以上面這些可以算是我們稱之為是整合性的技術 integrated	3-1
那這裡面其實還有兩塊我們沒有講的	3-1
最左邊最右邊的這個是 Multilingual 也就是說我們也提過我們很可能是一個多語言的世界	3-1
所以我們要解決的問題應該是譬如說國台語雙聲帶的或者這個中英文中英台或者是什麼這個台客閩喔	3-1
你有你有很多種語言混在一起的	3-1
或者是我們今天有另外一個很很有名的很重要的題目就是 speech  to  speech  translation	3-1
也就是說你如果去你到國外去旅行也許我就直接跟對方講我的語言	3-1
它自動幫我翻成對方的語言讓他聽	3-1
這是 speech  to  speech  translation 這些都是屬於 Multilingual 的問題	3-1
那左邊還有這一塊是 Multimedia 的問題就是你跟別的 media 來結合	3-1
包括裡面可能有音樂聲音的背景有音樂或者是有這個有有 video 等等	3-1
那有各種相關的這種問題	3-1
那這個的另外一個最最常用最常見的例子就是這個跟這個口形的 video 來整合	3-1
那麼也就是說當你在非常吵雜的環境的時候 recognition 真的很難做的話嘴唇的唇形是不會受到 noise 的影響	3-1
唇形是非常清楚的	3-1
那你今天是用手機的話今天手機都有攝影機嘛	3-1
所以你其實手機上面有一個攝影機可以照著你的唇形的話 more  or  less 可以知道你在說的話是什麼話	3-1
那麼因此呢你可以把這個唇形的形狀跟這個聲音來整合等等這都是屬於我們講的 Multimedia	3-1
那所以呢這些呢等等那上面這這邊我稱之為是 Integrated  Technologies	3-1
那真正的 Application 可能是應該講在最上面這些還不見得是 Application	3-1
那真正要用可能是要把這些東西再整合	3-1
我們舉例來講你很可能在網路端的那些不管是新聞還是電影還是演講還是什麼它呢需要做 understanding 跟 organization	3-1
在 user 端呢我們需要透過網路來跟 server 連絡所以要做 wireless 的或者是這個 Distributed 分散式的	3-1
那同樣呢我真的怎麼做呢我需要借助 DIALOGUE 跟系統去對話才能夠做的好	3-1
而這中間我可能需要搜尋等等	3-1
那我可能要把這些東西都兜起來才能做我要的 Application	3-1
那麼因此真的 Application 可能還包括不只一樣東西的整合所以這是我們的 Application	3-1
那這整一張圖大致說明我們這個領域現在所看到的一些最主要的的問題	3-1
那我們到目前為止稍微說了一點點的是一二三	3-1
然後每一塊大概都有提到一點但是我們底下都會陸陸續續講	3-1
那麼我們其實在這個八點零之前幾乎集中在這裡	3-1
所以底下的四點零五點零還在講一的這一塊	3-1
六點零是講二的這一塊	3-1
七點零是在講歐 front  end 還不在這裡來回到這裡是 feature 這一塊	3-1
八點零講這個 search	3-1
到八點零我們才把這一塊講完	3-1
從九點零開始我們 cover 外面的東西	3-1
所以九點零到十六點零是 cover 外面的東西	3-1
大概的我們這門課的結構大概是這樣這樣讓你有一個初步了解我們所有的東西	3-1
好我們到這裡為止是 overview	3-1
就是我們都在想辦法讓你在最短的時間對這個領域相關問題有足夠多的了解	3-1
